[{"ID":"abadeer-2020-assessment","methods":["bert"],"center_method":["bert"],"tasks":["detection of protected health information","named entity recognition"],"center_task":[null,"named entity recognition"],"Goal":["Good Health and Well-Being"],"title_clean":"Assessment of DistilBERT performance on Named Entity Recognition task for the detection of Protected Health Information and medical concepts","abstract_clean":"Bidirectional Encoder Representations from Transformers (BERT) models achieve state ofthe art performance on a number of Natural Language Processing tasks. However, their model size on disk often exceeds 1 GB and the process of fine tuning them and using them to run inference consumes significant hardware resources and runtime. This makes them hard to deploy to production environments. This paper fine tunes DistilBERT, a lightweight deep learning model, on medical text for the named entity recognition task of Protected Health Information (PHI) and medical concepts. This work provides a full assessment of the performance of DistilBERT in comparison with BERT models that were pre trained on medical text. For Named Entity Recognition task of PHI, DistilBERT achieved almost the same results as medical versions of BERT in terms of F 1 score at almost half the runtime and consuming approximately half the disk space. On the other hand, for the detection of medical concepts, DistilBERT's F 1 score was lower by 4 points on average than medical BERT variants."},{"ID":"adams-etal-2016-distributed","methods":["distributed vector representations","unsupervised algorithm"],"center_method":[null,"unsupervised algorithm"],"tasks":["short answer grading"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Distributed Vector Representations for Unsupervised Automatic Short Answer Grading","abstract_clean":"We address the problem of automatic short answer grading, evaluating a collection of approaches inspired by recent advances in distributional text representations. In addition, we propose an unsupervised approach for determining text similarity using one to many alignment of word vectors. We evaluate the proposed technique across two datasets from different domains, namely, computer science and English reading comprehension, that additionally vary between highschool level and undergraduate students. Experiments demonstrate that the proposed technique often outperforms other compositional distributional semantics approaches as well as vector space methods such as latent semantic analysis. When combined with a scoring scheme, the proposed technique provides a powerful tool for tackling the complex problem of short answer grading. We also discuss a number of other key points worthy of consideration in preparing viable, easy to deploy automatic short answer grading systems for the real world."},{"ID":"afzal-etal-2020-cora","methods":["machine learning methods","benchmark set"],"center_method":["machine learning methods",null],"tasks":["identify core scientific articles"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"CORA: A Deep Active Learning Covid 19 Relevancy Algorithm to Identify Core Scientific Articles","abstract_clean":"Ever since the COVID 19 pandemic broke out, the academic and scientific research community, as well as industry and governments around the world have joined forces in an unprecedented manner to fight the threat. Clinicians, biologists, chemists, bioinformaticians, nurses, data scientists, and all of the affiliated relevant disciplines have been mobilized to help discover efficient treatments for the infected population, as well as a vaccine solution to prevent further the virus' spread. In this combat against the virus responsible for the pandemic, key for any advancements is the timely, accurate, peer reviewed, and efficient communication of any novel research findings. In this paper we present a novel framework to address the information need of filtering efficiently the scientific bibliography for relevant literature around COVID 19. The contributions of the paper are summarized in the following: we define and describe the information need that encompasses the major requirements for COVID 19 articles' relevancy, we present and release an expert curated benchmark set for the task, and we analyze the performance of several state of the art machine learning classifiers that may distinguish the relevant from the non relevant COVID 19 literature. 1 https:\/\/covid19.who.int\/"},{"ID":"aggarwal-etal-2019-ltl","methods":["bert","multi - layer perceptron"],"center_method":["bert",null],"tasks":["categorizing offensiveness"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"LTL UDE at SemEval 2019 Task 6: BERT and Two Vote Classification for Categorizing Offensiveness","abstract_clean":"This paper describes LTL UDE's systems for the SemEval 2019 Shared Task 6. We present results for Subtask A and C. In Subtask A, we experiment with an embedding representation of postings and use a Multi Layer Perceptron and BERT to categorize postings. Our best result reaches the 10th place (out of 103) using BERT. In Subtask C, we applied a two vote classification approach with minority fallback, which is placed on the 19th rank (out of 65)."},{"ID":"akhlaghi-etal-2020-constructing","methods":["open source platform"],"center_method":[null],"tasks":["constructing multimodal language learner texts"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Constructing Multimodal Language Learner Texts Using LARA: Experiences with Nine Languages","abstract_clean":"LARA (Learning and Reading Assistant) is an open source platform whose purpose is to support easy conversion of plain texts into multimodal online versions suitable for use by language learners. This involves semi automatically tagging the text, adding other annotations and recording audio. The platform is suitable for creating texts in multiple languages via crowdsourcing techniques that can be used for teaching a language via reading and listening. We present results of initial experiments by various collaborators where we measure the time required to produce substantial LARA resources, up to the length of short novels, in Dutch, English, Farsi, French, German, Icelandic, Irish, Swedish and Turkish. The first results are encouraging. Although there are some startup problems, the conversion task seems manageable for the languages tested so far. The resulting enriched texts are posted online and are freely available in both source and compiled form."},{"ID":"alexeeva-etal-2020-mathalign","methods":["rule - based approach"],"center_method":[null],"tasks":["linking formula identifiers to their contextual natural language descriptions"],"center_task":[null],"Goal":["Industry, Innovation and Infrastructure"],"title_clean":"MathAlign: Linking Formula Identifiers to their Contextual Natural Language Descriptions","abstract_clean":"Extending machine reading approaches to extract mathematical concepts and their descriptions is useful for a variety of tasks, ranging from mathematical information retrieval to increasing accessibility of scientific documents for the visually impaired. This entails segmenting mathematical formulae into identifiers and linking them to their natural language descriptions. We propose a rule based approach for this task, which extracts L A T E X representations of formula identifiers and links them to their in text descriptions, given only the original PDF and the location of the formula of interest. We also present a novel evaluation dataset for this task, as well as the tool used to create it."},{"ID":"amason-etal-2019-harvey","methods":["bag - of - words","naive bayes"],"center_method":[null,"naive bayes"],"tasks":["hyperpartisan news detection"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Harvey Mudd College at SemEval 2019 Task 4: The D.X. Beaumont Hyperpartisan News Detector","abstract_clean":"We use the 600 hand labelled articles from Se mEval Task 4 (Kiesel et al., 2019) to handtune a classifier with 3000 features for the Hyperpartisan News Detection task. Our final system uses features based on bag of words (BoW), analysis of the article title, language complexity, and simple sentiment analysis in a naive Bayes classifier. We trained our final system on the 600,000 articles labelled by publisher. Our final system has an accuracy of 0.653 on the hand labeled test set. The most effective features are the Automated Readability Index and the presence of certain words in the title. This suggests that hyperpartisan writing uses a distinct writing style, especially in the title."},{"ID":"ananiadou-etal-2010-evaluating","methods":["metrics"],"center_method":[null],"tasks":["evaluation of the search engine"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Evaluating a Text Mining Based Educational Search Portal","abstract_clean":"In this paper, we present the main features of a text mining based search engine for the UK Educational Evidence Portal available at the UK National Centre for Text Mining (NaCTeM), together with a user centred framework for the evaluation of the search engine. The framework is adapted from an existing proposal by the ISLE (EAGLES) Evaluation Working group. We introduce the metrics employed for the evaluation, and explain how these relate to the text mining based search engine. Following this, we describe how we applied the framework to the evaluation of a number of key text mining features of the search engine, namely the automatic clustering of search results, classification of search results according to a taxonomy, and identification of topics and other documents that are related to a chosen document. Finally, we present the results of the evaluation in terms of the strengths, weaknesses and improvements identified for each of these features."},{"ID":"arslan-etal-2020-modeling","methods":["semantic frames","annotation"],"center_method":[null,"annotation"],"tasks":["modeling factual claims"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Modeling Factual Claims with Semantic Frames","abstract_clean":"In this paper, we introduce an extension of the Berkeley FrameNet for the structured and semantic modeling of factual claims. Modeling is a robust tool that can be leveraged in many different tasks such as matching claims to existing fact checks and translating claims to structured queries. Our work introduces 11 new manually crafted frames along with 9 existing FrameNet frames, all of which have been selected with fact checking in mind. Along with these frames, we are also providing 2, 540 fully annotated sentences, which can be used to understand how these frames are intended to work and to train machine learning models. Finally, we are also releasing our annotation tool to facilitate other researchers to make their own local extensions to FrameNet."},{"ID":"asgari-etal-2020-topic","methods":["topic - based measures"],"center_method":[null],"tasks":["detecting mild cognitiveimpairment"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Topic Based Measures of Conversation for Detecting Mild CognitiveImpairment","abstract_clean":"Conversation is a complex cognitive task that engages multiple aspects of cognitive functions to remember the discussed topics, monitor the semantic and linguistic elements, and recognize others' emotions. In this paper, we propose a computational method based on the lexical coherence of consecutive utterances to quantify topical variations in semistructured conversations of older adults with cognitive impairments. Extracting the lexical knowledge of conversational utterances, our method generates a set of novel conversational measures that indicate underlying cognitive deficits among subjects with mild cognitive impairment (MCI). Our preliminary results verify the utility of the proposed conversation based measures in distinguishing MCI from healthy controls."},{"ID":"azab-etal-2013-nlp","methods":["reading tool"],"center_method":[null],"tasks":["aiding non - native english readers"],"center_task":[null],"Goal":["Reduced Inequalities"],"title_clean":"An NLP based Reading Tool for Aiding Non native English Readers","abstract_clean":"This paper describes a text reading tool that makes extensive use of widelyavailable NLP tools and resources to aid non native English speakers overcome language related hindrances while reading a text. It is a web based tool, that can be accessed from browsers running on PCs or tablets, and provides the reader with an intelligent e book functionality."},{"ID":"bajaj-etal-2022-evaluating","methods":["word embeddings","bert - based models","siamese networks"],"center_method":["word embeddings",null,null],"tasks":["vocabulary alignment"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Evaluating Biomedical Word Embeddings for Vocabulary Alignment at Scale in the UMLS Metathesaurus Using Siamese Networks","abstract_clean":"Recent work uses a Siamese Network, initialized with BioWordVec embeddings (distributed word embeddings), for predicting synonymy among biomedical terms to automate a part of the UMLS (Unified Medical Language System) Metathesaurus construction process. We evaluate the use of contextualized word embeddings extracted from nine different biomedical BERT based models for synonymy prediction in the UMLS by replacing BioWordVec embeddings with embeddings extracted from each biomedical BERT model using different feature extraction methods. Surprisingly, we find that Siamese Networks initialized with BioWordVec embeddings still outperform the Siamese Networks initialized with embedding extracted from biomedical BERT model."},{"ID":"basta-etal-2019-evaluating","methods":["word embeddings"],"center_method":["word embeddings"],"tasks":["evaluating the underlying gender bias"],"center_task":[null],"Goal":["Gender Equality"],"title_clean":"Evaluating the Underlying Gender Bias in Contextualized Word Embeddings","abstract_clean":"Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased."},{"ID":"berzak-etal-2015-contrastive","methods":["contrastive analysis"],"center_method":[null],"tasks":["typology driven estimation of grammatical error distributions in esl"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Contrastive Analysis with Predictive Power: Typology Driven Estimation of Grammatical Error Distributions in ESL","abstract_clean":"This work examines the impact of crosslinguistic transfer on grammatical errors in English as Second Language (ESL) texts. Using a computational framework that formalizes the theory of Contrastive Analysis (CA), we demonstrate that language specific error distributions in ESL writing can be predicted from the typological properties of the native language and their relation to the typology of English. Our typology driven model enables to obtain accurate estimates of such distributions without access to any ESL data for the target languages. Furthermore, we present a strategy for adjusting our method to low resource languages that lack typological documentation using a bootstrapping approach which approximates native language typology from ESL texts. Finally, we show that our framework is instrumental for linguistic inquiry seeking to identify first language factors that contribute to a wide range of difficulties in second language acquisition."},{"ID":"bestgen-2019-tintin","methods":["supervised learning"],"center_method":["supervised learning"],"tasks":["hyperpartisan news detection"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Tintin at SemEval 2019 Task 4: Detecting Hyperpartisan News Article with only Simple Tokens","abstract_clean":"Tintin, the system proposed by the CECL for the Hyperpartisan News Detection task of Se mEval 2019, is exclusively based on the tokens that make up the documents and a standard supervised learning procedure. It obtained very contrasting results: poor on the main task, but much more effective at distinguishing documents published by hyperpartisan media outlets from unbiased ones, as it ranked first. An analysis of the most important features highlighted the positive aspects, but also some potential limitations of the approach."},{"ID":"bilbao-jayo-almeida-2018-political","methods":["convolutional neural network"],"center_method":["convolutional neural network"],"tasks":["political discourse classification"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Political discourse classification in social networks using context sensitive convolutional neural networks","abstract_clean":"In this study we propose a new approach to analyse the political discourse in online social networks such as Twitter. To do so, we have built a discourse classifier using Convolutional Neural Networks. Our model has been trained using election manifestos annotated manually by political scientists following the Regional Manifestos Project (RMP) methodology. In total, it has been trained with more than 88,000 sentences extracted from more that 100 annotated manifestos. Our approach takes into account the context of the phrase in order to classify it, like what was previously said and the political affiliation of the transmitter. To improve the classification results we have used a simplified political message taxonomy developed within the Electronic Regional Manifestos Project (E RMP). Using this taxonomy, we have validated our approach analysing the Twitter activity of the main Spanish political parties during 2015 and 2016 Spanish general election and providing a study of their discourse."},{"ID":"blokker-etal-2020-swimming","methods":["manual annotation and analysis","classification"],"center_method":[null,"classification"],"tasks":["positional claim detection"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Swimming with the Tide? Positional Claim Detection across Political Text Types","abstract_clean":"Manifestos are official documents of political parties, providing a comprehensive topical overview of the electoral programs. Voters, however, seldom read them and often prefer other channels, such as newspaper articles, to understand the party positions on various policy issues. The natural question to ask is how compatible these two formats (manifesto and newspaper reports) are in their representation of party positioning. We address this question with an approach that combines political science (manual annotation and analysis) and natural language processing (supervised claim identification) in a cross text type setting: we train a classifier on annotated newspaper data and test its performance on manifestos. Our findings show a) strong performance for supervised classification even across text types and b) a substantive overlap between the two formats in terms of party positioning, with differences regarding the salience of specific issues."},{"ID":"bobic-etal-2013-scai","methods":["feature vector","lexical , syntactical and semantic based feature sets"],"center_method":[null,null],"tasks":["extracting drug - drug interactions","relation extraction"],"center_task":[null,"relation extraction"],"Goal":["Good Health and Well-Being"],"title_clean":"SCAI: Extracting drug drug interactions using a rich feature vector","abstract_clean":"Automatic relation extraction provides great support for scientists and database curators in dealing with the extensive amount of biomedical textual data. The DDIExtraction 2013 challenge poses the task of detecting drugdrug interactions and further categorizing them into one of the four relation classes. We present our machine learning system which utilizes lexical, syntactical and semantic based feature sets. Resampling, balancing and ensemble learning experiments are performed to infer the best configuration. For general drugdrug relation extraction, the system achieves 70.4% in F 1 score."},{"ID":"bokaie-hosseini-etal-2020-identifying","methods":[null],"center_method":[null],"tasks":["identifying and classifying third - party entities"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Identifying and Classifying Third party Entities in Natural Language Privacy Policies","abstract_clean":"App developers often raise revenue by contracting with third party ad networks, which serve targeted ads to end users. To this end, a free app may collect data about its users and share it with advertising companies for targeting purposes. Regulations such as General Data Protection Regulation (GDPR) require transparency with respect to the recipients (or categories of recipients) of user data. These regulations call for app developers to have privacy policies that disclose those third party recipients of user data. Privacy policies provide users transparency into what data an app will access, collect, shared, and retain. Given the size of app marketplaces, verifying compliance with such regulations is a tedious task. This paper aims to develop an automated approach to extract and categorize third party data recipients (i.e., entities) declared in privacy policies. We analyze 100 privacy policies associated with most downloaded apps in the Google Play Store. We crowdsource the collection and annotation of app privacy policies to establish the ground truth with respect to third party entities. From this, we train various models to extract third party entities automatically. Our best model achieves average F1 score of 66% when compared to crowdsourced annotations."},{"ID":"boriola-paetzold-2020-utfpr","methods":["ensemble methods","cbow","convolutional neural network"],"center_method":["ensemble methods","cbow","convolutional neural network"],"tasks":["toxicity detection"],"center_task":["toxicity detection"],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"UTFPR at SemEval 2020 Task 12: Identifying Offensive Tweets with Lightweight Ensembles","abstract_clean":"Offensive language is a common issue on social media platforms nowadays. In an effort to address this issue, the SemEval 2020 event held the OffensEval 2020 shared task where the participants were challenged to develop systems that identify and classify offensive language in tweets. In this paper, we present a system that uses an Ensemble model stacking a BOW model and a CNN model that led us to place 29th in the ranking for English sub task A."},{"ID":"boytcheva-etal-2009-extraction","methods":["correlations"],"center_method":[null],"tasks":["information extraction"],"center_task":["information extraction"],"Goal":["Good Health and Well-Being"],"title_clean":"Extraction and Exploration of Correlations in Patient Status Data","abstract_clean":"The paper discusses an Information Extraction approach, which is applied for the automatic processing of hospital Patient Records (PRs) in Bulgarian language. The main task reported here is retrieval of status descriptions related to anatomical organs. Due to the specific telegraphic PR style, the approach is focused on shallow analysis. Missing text descriptions and default values are another obstacle. To overcome it, we propose an algorithm for exploring the correlations between patient status data and the corresponding diagnosis. Rules for interdependencies of the patient status data are generated by clustering according to chosen metrics. In this way it becomes possible to fill in status templates for each patient when explicit descriptions are unavailable in the text. The article summarises evaluation results which concern the performance of the current IE prototype."},{"ID":"boytcheva-etal-2012-automatic","methods":["automatic analysis"],"center_method":[null],"tasks":["information extraction"],"center_task":["information extraction"],"Goal":["Good Health and Well-Being"],"title_clean":"Automatic Analysis of Patient History Episodes in Bulgarian Hospital Discharge Letters","abstract_clean":"This demo presents Information Extraction from discharge letters in Bulgarian language. The Patient history section is automatically split into episodes (clauses between two temporal markers); then drugs, diagnoses and conditions are recognised within the episodes with accuracy higher than 90%. The temporal markers, which refer to absolute or relative moments of time, are identified with precision 87% and recall 68%. The direction of time for the episode starting point: backwards or forward (with respect to certain moment orienting the episode) is recognised with precision 74.4%."},{"ID":"brekke-etal-2006-automatic","methods":["statistical filtering"],"center_method":[null],"tasks":["automatic term extraction"],"center_task":[null],"Goal":["Decent Work and Economic Growth"],"title_clean":"Automatic Term Extraction from Knowledge Bank of Economics","abstract_clean":"KB N is a web accessible searchable Knowledge Bank comprising A) a parallel corpus of quality assured and calibrated English and Norwegian text drawn from economic administrative knowledge domains, and B) a domain focused database representing that knowledge universe in terms of defined concepts and their respective bilingual terminological entries. A central mechanism in connecting A and B is an algorithm for the automatic extraction of term candidates from aligned translation pairs on the basis of linguistic, lexical and statistical filtering (first ever for Norwegian). The system is designed and programmed by Paul Meurer at Aksis (UiB). An important pilot application of the term base is subdomain and collocations based word sense disambiguation for LOGON, a system for Norwegian to English MT currently being developed."},{"ID":"buyukoz-etal-2020-analyzing","methods":["elmo","bert"],"center_method":["elmo","bert"],"tasks":["news classification"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Analyzing ELMo and DistilBERT on Socio political News Classification","abstract_clean":"This study evaluates the robustness of two state of the art deep contextual language representations, ELMo and DistilBERT, on supervised learning of binary protest news classification (PC) and sentiment analysis (SA) of product reviews. A \"cross context\" setting is enabled using test sets that are distinct from the training data. The models are fine tuned and fed into a Feed Forward Neural Network (FFNN) and a Bidirectional Long Short Term Memory network (BiLSTM). Multinomial Naive Bayes (MNB) and Linear Support Vector Machine (LSVM) are used as traditional baselines. The results suggest that DistilBERT can transfer generic semantic knowledge to other domains better than ELMo. DistilBERT is also 30% smaller and 83% faster than ELMo, which suggests superiority for smaller computational training budgets. When generalization is not the utmost preference and test domain is similar to the training domain, the traditional machine learning (ML) algorithms can still be considered as more economic alternatives to deep language representations."},{"ID":"cadel-ledouble-2000-extraction","methods":["information schemes","linguistic triggers","linguistic analysis"],"center_method":[null,null,null],"tasks":["extraction of concepts"],"center_task":[null],"Goal":["Decent Work and Economic Growth"],"title_clean":"Extraction of Concepts and Multilingual Information Schemes from French and English Economics Documents","abstract_clean":"This paper focuses on the linguistic analysis of economic information in French and English documents. Our objective is to establish domain specific information schemes based on structural and conceptual information. At the structural level, we define linguistic triggers that take into account each language's specificity. At the conceptual level, analysis of concepts and relations between concepts result in a classification, prior to the representation of schemes. The final outcome of this study is a mapping between linguistic and conceptual structures in the field of economics."},{"ID":"caines-etal-2018-aggressive","methods":["binary classifier"],"center_method":[null],"tasks":["abuse detection"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Aggressive language in an online hacking forum","abstract_clean":"We probe the heterogeneity in levels of abusive language in different sections of the Internet, using an annotated corpus of Wikipedia page edit comments to train a binary classifier for abuse detection. Our test data come from the CrimeBB Corpus of hacking related forum posts and we find that (a) forum interactions are rarely abusive, (b) the abusive language which does exist tends to be relatively mild compared to that found in the Wikipedia comments domain, and tends to involve aggressive posturing rather than hate speech or threats of violence. We observe that the purpose of conversations in online forums tend to be more constructive and informative than those in Wikipedia page edit comments which are geared more towards adversarial interactions, and that this may explain the lower levels of abuse found in our forum data than in Wikipedia comments. Further work remains to be done to compare these results with other inter domain classification experiments, and to understand the impact of aggressive language in forum conversations."},{"ID":"cardellino-etal-2017-legal","methods":["curriculum learning","named entity recognition","classification"],"center_method":[null,"named entity recognition","classification"],"tasks":["develop resources for the legal domain"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Legal NERC with ontologies, Wikipedia and curriculum learning","abstract_clean":"In this paper, we present a Wikipediabased approach to develop resources for the legal domain. We establish a mapping between a legal domain ontology, LKIF (Hoekstra et al., 2007), and a Wikipediabased ontology, YAGO (Suchanek et al., 2007), and through that we populate LKIF. Moreover, we use the mentions of those entities in Wikipedia text to train a specific Named Entity Recognizer and Classifier. We find that this classifier works well in the Wikipedia, but, as could be expected, performance decreases in a corpus of judgments of the European Court of Human Rights. However, this tool will be used as a preprocess for human annotation. We resort to a technique called curriculum learning aimed to overcome problems of overfitting by learning increasingly more complex concepts. However, we find that in this particular setting, the method works best by learning from most specific to most general concepts, not the other way round."},{"ID":"caselli-etal-2021-dalc","methods":["annotation"],"center_method":["annotation"],"tasks":["data collection"],"center_task":["data collection"],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"DALC: the Dutch Abusive Language Corpus","abstract_clean":"As socially unacceptable language become pervasive in social media platforms, the need for automatic content moderation become more pressing. This contribution introduces the Dutch Abusive Language Corpus (DALC v1.0), a new dataset with tweets manually annotated for abusive language. The resource address a gap in language resources for Dutch and adopts a multi layer annotation scheme modeling the explicitness and the target of the abusive messages. Baselines experiments on all annotation layers have been conducted, achieving a macro F1 score of 0.748 for binary classification of the explicitness layer and .489 for target classification."},{"ID":"cavicchio-2009-modulation","methods":["data collection"],"center_method":["data collection"],"tasks":["modulation of cooperation and emotion"],"center_task":[null],"Goal":["Partnership for the Goals"],"title_clean":"The Modulation of Cooperation and Emotion in Dialogue: The REC Corpus","abstract_clean":"In this paper we describe the Rovereto Emotive Corpus (REC) which we collected to investigate the relationship between emotion and cooperation in dialogue tasks. It is an area where still many unsolved questions are present. One of the main open issues is the annotation of the socalled \"blended\" emotions and their recognition. Usually, there is a low agreement among raters in annotating emotions and, surprisingly, emotion recognition is higher in a condition of modality deprivation (i. e. only acoustic or only visual modality vs. bimodal display of emotion). Because of these previous results, we collected a corpus in which \"emotive\" tokens are pointed out during the recordings by psychophysiological indexes (ElectroCardioGram, and Galvanic Skin Conductance). From the output values of these indexes a general recognition of each emotion arousal is allowed. After this selection we will annotate emotive interactions with our multimodal annotation scheme, performing a kappa statistic on annotation results to validate our coding scheme. In the near future, a logistic regression on annotated data will be performed to find out correlations between cooperation and negative emotions. A final step will be an fMRI experiment on emotion recognition of blended emotions from face displays."},{"ID":"ch-wang-jurgens-2021-using","methods":["sociolinguistic variables","quasi - causal analysis"],"center_method":[null,null],"tasks":["gender bias mitigation"],"center_task":["gender bias mitigation"],"Goal":["Gender Equality"],"title_clean":"Using Sociolinguistic Variables to Reveal Changing Attitudes Towards Sexuality and Gender","abstract_clean":"Individuals signal aspects of their identity and beliefs through linguistic choices. Studying these choices in aggregate allows us to examine large scale attitude shifts within a population. Here, we develop computational methods to study word choice within a sociolinguistic lexical variable alternate words used to express the same concept in order to test for change in the United States towards sexuality and gender. We examine two variables: i) referents to significant others, such as the word \"partner\" and ii) referents to an indefinite person, both of which could optionally be marked with gender. The linguistic choices in each variable allow us to study increased rates of acceptances of gay marriage and gender equality, respectively. In longitudinal analyses across Twitter and Reddit over 87M messages, we demonstrate that attitudes are changing but that these changes are driven by specific demographics within the United States. Further, in a quasi causal analysis, we show that passages of Marriage Equality Acts in different states are drivers of linguistic change."},{"ID":"chakraborti-tendulkar-2013-parallels","methods":["analysis"],"center_method":["analysis"],"tasks":["parallels between linguistics and biology"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Parallels between Linguistics and Biology","abstract_clean":"In this paper we take a fresh look at parallels between linguistics and biology. We expect that this new line of thinking will propel cross fertilization of two disciplines and open up new research avenues."},{"ID":"chandu-etal-2017-tackling","methods":["agglomerative clustering","word embedding based tf - idf similarity metric"],"center_method":[null,null],"tasks":["biomedical text summarization"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Tackling Biomedical Text Summarization: OAQA at BioASQ 5B","abstract_clean":"In this paper, we describe our participation in phase B of task 5b of the fifth edition of the annual BioASQ challenge, which includes answering factoid, list, yes no and summary questions from biomedical data. We describe our techniques with an emphasis on ideal answer generation, where the goal is to produce a relevant, precise, non redundant, query oriented summary from multiple relevant documents. We make use of extractive summarization techniques to address this task and experiment with different biomedical ontologies and various algorithms including agglomerative clustering, Maximum Marginal Relevance (MMR) and sentence compression. We propose a novel word embedding based tf idf similarity metric and a soft positional constraint which improve our system performance. We evaluate our techniques on test batch 4 from the fourth edition of the challenge. Our best system achieves a ROUGE 2 score of 0.6534 and ROUGE SU4 score of 0.6536."},{"ID":"chathuranga-etal-2017-opinion","methods":["conditional random field"],"center_method":["conditional random field"],"tasks":["opinion target extraction","student feedback summarization"],"center_task":[null,null],"Goal":["Quality Education"],"title_clean":"Opinion Target Extraction for Student Course Feedback","abstract_clean":"Student feedback is an essential part of the instructor student relationship. Traditionally student feedback is manually summarized by instructors, which is time consuming. Automatic student feedback summarization provides a potential solution to this. For summarizing student feedback, first, the opinion targets should be identified and extracted. In this context, opinion targets such as \"lecture slides\", \"teaching style\" are the important key points in the feedback that the students have shown their sentiment towards. In this paper, we focus on the opinion target extraction task of general student feedback. We model this problem as an information extraction task and extract opinion targets using a Conditional Random Fields (CRF) classifier. Our results show that this classifier outperforms the state of the art techniques for student feedback summarization."},{"ID":"chen-etal-2020-analyzing","methods":["neural models"],"center_method":["neural models"],"tasks":["analyzing political bias and unfairness"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Analyzing Political Bias and Unfairness in News Articles at Different Levels of Granularity","abstract_clean":"Media organizations bear great reponsibility because of their considerable influence on shaping beliefs and positions of our society. Any form of media can contain overly biased content, e.g., by reporting on political events in a selective or incomplete manner. A relevant question hence is whether and how such form of imbalanced news coverage can be exposed. The research presented in this paper addresses not only the automatic detection of bias but goes one step further in that it explores how political bias and unfairness are manifested linguistically. In this regard we utilize a new corpus of 6964 news articles with labels derived from adfontesmedia.com and develop a neural model for bias assessment. By analyzing this model on article excerpts, we find insightful bias patterns at different levels of text granularity, from single words to the whole article discourse."},{"ID":"chikka-2016-cde","methods":["conditional random field","support vector machine","deep neural network"],"center_method":["conditional random field","support vector machine","deep neural network"],"tasks":["extraction of temporal information"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"CDE IIITH at SemEval 2016 Task 12: Extraction of Temporal Information from Clinical documents using Machine Learning techniques","abstract_clean":"In this paper, we demonstrate our approach for identification of events, time expressions and temporal relations among them. This work was carried out as part of SemEval 2016 Challenge Task 12: Clinical TempEval. The task comprises six sub tasks: identification of event spans, time spans and their attributes, document time relation and the narrative container relations among events and time expressions. We have participated in all six subtasks. We have provided with a manually annotated dataset which comprises of training dataset (293 documents), development dataset (147 documents) and 151 documents as test dataset. We have submitted our work as two systems for the challenge. One system is developed using machine learning techniques, Conditional Random Fields (CRF) and Support Vector machines (SVM) and the other system is developed using deep neural network (DNN) techniques. The results show that both systems have given relatively same performance on these tasks."},{"ID":"chiril-etal-2020-annotated","methods":["data collection","deep neural network"],"center_method":["data collection","deep neural network"],"tasks":["sexism detection"],"center_task":[null],"Goal":["Gender Equality"],"title_clean":"An Annotated Corpus for Sexism Detection in French Tweets","abstract_clean":"Social media networks have become a space where users are free to relate their opinions and sentiments which may lead to a large spreading of hatred or abusive messages which have to be moderated. This paper presents the first French corpus annotated for sexism detection composed of about 12,000 tweets. In a context of offensive content mediation on social media now regulated by European laws, we think that it is important to be able to detect automatically not only sexist content but also to identify if a message with a sexist content is really sexist (i.e. addressed to a woman or describing a woman or women in general) or is a story of sexism experienced by a woman. This point is the novelty of our annotation scheme. We also propose some preliminary results for sexism detection obtained with a deep learning approach. Our experiments show encouraging results."},{"ID":"chiril-etal-2020-said","methods":["dataset","deep neural network","speech acts theory"],"center_method":[null,"deep neural network",null],"tasks":["sexism detection","offensive content mediation"],"center_task":[null,null],"Goal":["Gender Equality"],"title_clean":"He said ``who's gonna take care of your children when you are at ACL?'': Reported Sexist Acts are Not Sexist","abstract_clean":"In a context of offensive content mediation on social media now regulated by European laws, it is important not only to be able to automatically detect sexist content but also to identify if a message with a sexist content is really sexist or is a story of sexism experienced by a woman. We propose: (1) a new characterization of sexist content inspired by speech acts theory and discourse analysis studies, (2) the first French dataset annotated for sexism detection, and (3) a set of deep learning experiments trained on top of a combination of several tweet's vectorial representations (word embeddings, linguistic features, and various generalization strategies). Our results are encouraging and constitute a first step towards offensive content moderation."},{"ID":"cieri-dipersio-2014-intellectual","methods":["web services"],"center_method":[null],"tasks":["intellectual property rights management"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Intellectual Property Rights Management with Web Service Grids","abstract_clean":"This paper enumerates the ways in which configurations of web services may complicate issues of licensing language resources, whether data or tools. It details specific licensing challenges within the context of the US Language Application (LAPPS) Grid, sketches a solution under development and highlights ways in which that approach may be extended for other web service configurations."},{"ID":"clippinger-jr-1980-meaning","methods":[null],"center_method":[null],"tasks":["psychoanalytic speech and cognition"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Meaning and Discourse  A Computer Model of Psychoanalytic Speech and Cognition","abstract_clean":"Colby, and Schank; he offers homage to HACKER and kudos to CONNIVER; he ignores both linguistics and AI work in natural language generation; he invents a grammar of English; he performs validation tests on a hand simulated program; and he closes by warning us about ignoring the social impact of computers in the future. All of this is background to a program that models one, halting paragraph of speech by a depressed patient whose request to change the form in which she pays her therapist is, we are told in great detail, a desire for intercourse."},{"ID":"cohan-goharian-2016-revisiting","methods":["extensive analysis","alternative metric"],"center_method":[null,null],"tasks":["summarization evaluation for scientific articles"],"center_task":[null],"Goal":["Industry, Innovation and Infrastructure"],"title_clean":"Revisiting Summarization Evaluation for Scientific Articles","abstract_clean":"Evaluation of text summarization approaches have been mostly based on metrics that measure similarities of system generated summaries with a set of human written gold standard summaries. The most widely used metric in summarization evaluation has been the ROUGE family. ROUGE solely relies on lexical overlaps between the terms and phrases in the sentences; therefore, in cases of terminology variations and paraphrasing, ROUGE is not as effective. Scientific article summarization is one such case that is different from general domain summarization (e.g. newswire data). We provide an extensive analysis of ROUGE's effectiveness as an evaluation metric for scientific summarization; we show that, contrary to the common belief, ROUGE is not much reliable in evaluating scientific summaries. We furthermore show how different variants of ROUGE result in very different correlations with the manual Pyramid scores. Finally, we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries. We call our metric SERA (Summarization Evaluation by Relevance Analysis). Unlike ROUGE, SERA consistently achieves high correlations with manual scores which shows its effectiveness in evaluation of scientific article summarization."},{"ID":"collier-etal-2014-impact","methods":["domain adaption","feature augmentation","pooling","stacking"],"center_method":["domain adaption",null,null,null],"tasks":["biomedical named entity recognition"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"The impact of near domain transfer on biomedical named entity recognition","abstract_clean":"Current research in fully supervised biomedical named entity recognition (bioNER) is often conducted in a setting of low sample sizes. Whilst experimental results show strong performance in domain it has been recognised that quality suffers when models are applied to heterogeneous text collections. However the causal factors have until now been uncertain. In this paper we describe a controlled experiment into near domain bias for two Medline corpora on hereditary diseases. Five strategies are employed for mitigating the impact of near domain transference including simple transference, pooling, stacking, class re labeling and feature augmentation. We measure their effect on f score performance against an in domain baseline. Stacking and feature augmentation mitigate f score loss but do not necessarily result in superior performance except for selected classes. Simple pooling of data across domains failed to exploit size effects for most classes. We conclude that we can expect lower performance and higher annotation costs if we do not adequately compensate for the distributional dissimilarities of domains during learning."},{"ID":"concordia-etal-2020-store","methods":["software framework"],"center_method":[null],"tasks":["store scientific workflows"],"center_task":[null],"Goal":["Industry, Innovation and Infrastructure"],"title_clean":"Store Scientific Workflows Data in SSHOC Repository","abstract_clean":"Today scientific workflows are used by scientists as a way to define automated, scalable, and portable in silico experiments. Having a formal description of an experiment can improve replicability and reproducibility of the experiment. However, simply publishing the workflow may be not enough to achieve reproducibility and re usability, in particular workflow description should be enriched with provenance data generated during the workflow life cycle. This paper presents a software framework being designed and developed in the context of the Social Sciences and Humanities Open Cloud (SSHOC) project, whose overall objective is to realise the social sciences and humanities' part of European Open Science Cloud initiative. The framework will implement functionalities to use the SSHOC Repository service as a cloud repository for scientific workflows."},{"ID":"coppersmith-etal-2015-adhd","methods":["self - reported diagnoses"],"center_method":[null],"tasks":["analyzing the language of mental health"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"From ADHD to SAD: Analyzing the Language of Mental Health on Twitter through Self Reported Diagnoses","abstract_clean":"Many significant challenges exist for the mental health field, but one in particular is a lack of data available to guide research. Language provides a natural lens for studying mental health much existing work and therapy have strong linguistic components, so the creation of a large, varied, language centric dataset could provide significant grist for the field of mental health research. We examine a broad range of mental health conditions in Twitter data by identifying self reported statements of diagnosis. We systematically explore language differences between ten conditions with respect to the general population, and to each other. Our aim is to provide guidance and a roadmap for where deeper exploration is likely to be fruitful."},{"ID":"da-san-martino-etal-2020-prta","methods":[null],"center_method":[null],"tasks":["analysis of propaganda"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Prta: A System to Support the Analysis of Propaganda Techniques in the News","abstract_clean":"Recent events, such as the 2016 US Presidential Campaign, Brexit and the COVID 19 \"infodemic\", have brought into the spotlight the dangers of online disinformation. There has been a lot of research focusing on factchecking and disinformation detection. However, little attention has been paid to the specific rhetorical and psychological techniques used to convey propaganda messages. Revealing the use of such techniques can help promote media literacy and critical thinking, and eventually contribute to limiting the impact of \"fake news\" and disinformation campaigns. Prta (Propaganda Persuasion Techniques Analyzer) allows users to explore the articles crawled on a regular basis by highlighting the spans in which propaganda techniques occur and to compare them on the basis of their use of propaganda techniques. The system further reports statistics about the use of such techniques, overall and over time, or according to filtering criteria specified by the user based on time interval, keywords, and\/or political orientation of the media. Moreover, it allows users to analyze any text or URL through a dedicated interface or via an API. The system is available online: https:\/\/www.tanbih.org\/prta."},{"ID":"dalan-sharoff-2016-genre","methods":["analysis","annotation"],"center_method":["analysis","annotation"],"tasks":["genre classification"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Genre classification for a corpus of academic webpages","abstract_clean":"In this paper we report our analysis of the similarities between webpages that are crawled from European academic websites, and comparison of their distribution in terms of the English language variety (native English vs English as a lingua franca) and their language family (based on the country's official language). After building a corpus of university webpages, we selected a set of relevant descriptors that can represent their text types using the framework of the Functional Text Dimensions. Manual annotation of a random sample of academic pages provides the basis for classifying the remaining texts on each dimension. Reliable thresholds are then determined in order to evaluate precision and assess the distribution of text types by each dimension, with the ultimate goal of analysing language features over English varieties and language families."},{"ID":"dale-etal-2002-evangelising","methods":["undergraduate program"],"center_method":[null],"tasks":["evangelising language technology"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Evangelising Language Technology: A Practically Focussed Undergraduate Program","abstract_clean":"This paper describes an undergraduate program in Language Technology that we have developed at Macquarie University. We question the industrial relevance of much that is taught in NLP courses, and emphasize the need for a practical orientation as a means to growing the size of the field. We argue that a more evangelical approach, both with regard to students and industry, is required. The paper provides an overview of the material we cover, and makes some observations for the future on the basis of our experiences so far."},{"ID":"decamp-2008-working","methods":["information"],"center_method":[null],"tasks":["working with the us government"],"center_task":[null],"Goal":["Industry, Innovation and Infrastructure"],"title_clean":"Working with the US Government: Information Resources","abstract_clean":"This document provides information on how companies and researchers in machine translation can work with the U.S. Government. Specifically, it addresses information on (1) groups in the U.S. Government working with translation and potentially having a need for machine translation; (2) means for companies and researchers to provide information to the United States Government about their work; and (3) U.S. Government organizations providing grants of possible interest to this community."},{"ID":"del-tredici-fernandez-2020-words","methods":["language - based user representations"],"center_method":[null],"tasks":["fake news detection"],"center_task":["fake news detection"],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Words are the Window to the Soul: Language based User Representations for Fake News Detection","abstract_clean":"Cognitive and social traits of individuals are reflected in language use. Moreover, individuals who are prone to spread fake news online often share common traits. Building on these ideas, we introduce a model that creates representations of individuals on social media based only on the language they produce, and use them to detect fake news. We show that language based user representations are beneficial for this task. We also present an extended analysis of the language of fake news spreaders, showing that its main features are mostly domain independent and consistent across two English datasets. Finally, we exploit the relation between language use and connections in the social graph to assess the presence of the Echo Chamber effect in our data."},{"ID":"dennis-henderson-etal-2020-life","methods":["distant reading"],"center_method":[null],"tasks":["analysing australian ww1 diaries"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Life still goes on: Analysing Australian WW1 Diaries through Distant Reading","abstract_clean":"An increasing amount of historic data is now available in digital (text) formats. This gives quantitative researchers an opportunity to use distant reading techniques, as opposed to traditional close reading, in order to analyse larger quantities of historic data. Distant reading allows researchers to view overall patterns within the data and reduce researcher bias. One such data set that has recently been transcribed is a collection of over 500 Australian World War I (WW1) diaries held by the State Library of New South Wales. Here we apply distant reading techniques to this corpus to understand what soldiers wrote about and how they felt over the course of the war. Extracting dates accurately is important as it allows us to perform our analysis over time, however, it is very challenging due to the variety of date formats and abbreviations diarists use. But with that data, topic modelling and sentiment analysis can then be applied to show trends, for instance, that despite the horrors of war, Australians in WW1 primarily wrote about their everyday routines and experiences. Our results detail some of the challenges likely to be encountered by quantitative researchers intending to analyse historical texts, and provide some approaches to these issues."},{"ID":"dernoncourt-etal-2017-neural","methods":["neural network"],"center_method":["neural network"],"tasks":["joint sentence classification"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Neural Networks for Joint Sentence Classification in Medical Paper Abstracts","abstract_clean":"Existing models based on artificial neural networks (ANNs) for sentence classification often do not incorporate the context in which sentences appear, and classify sentences individually. However, traditional sentence classification approaches have been shown to greatly benefit from jointly classifying subsequent sentences, such as with conditional random fields. In this work, we present an ANN architecture that combines the effectiveness of typical ANN models to classify sentences in isolation, with the strength of structured prediction. Our model outperforms the state ofthe art results on two different datasets for sequential sentence classification in medical abstracts."},{"ID":"diaz-torres-etal-2020-automatic","methods":["mexican spanish dataset","linguistic - based criteria"],"center_method":[null,null],"tasks":["automatic detection of offensive language"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Automatic Detection of Offensive Language in Social Media: Defining Linguistic Criteria to build a Mexican Spanish Dataset","abstract_clean":"Phenomena such as bullying, homophobia, sexism and racism have transcended to social networks, motivating the development of tools for their automatic detection. The challenge becomes greater when speakers make use of popular sayings, colloquial expressions and idioms which may contain vulgar, profane or rude words, but not always have the intention to offend; a situation often found in the Mexican Spanish variant. Under these circumstances, the identification of the offense goes beyond the lexical and syntactic elements of the message. This first work aims to define the main linguistic features of aggressive, offensive and vulgar language in social networks in order to establish linguistic based criteria to facilitate the identification of abusive language. For this purpose, a Mexican Spanish Twitter corpus was compiled and analyzed. The dataset included words that, despite being rude, need to be considered in context to determine they are part of an offense. Based on the analysis of this corpus, linguistic criteria were defined to determine whether a message is offensive. To simplify the application of these criteria, an easy to follow diagram was designed. The paper presents an example of the use of the diagram, as well as the basic statistics of the corpus."},{"ID":"dimov-etal-2020-nopropaganda","methods":["lstm","autoregressive transformer"],"center_method":["lstm",null],"tasks":["detection of propaganda"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"NoPropaganda at SemEval 2020 Task 11: A Borrowed Approach to Sequence Tagging and Text Classification","abstract_clean":"This paper describes our contribution to SemEval 2020 Task 11: Detection Of Propaganda Techniques In News Articles. We start with simple LSTM baselines and move to an autoregressive transformer decoder to predict long continuous propaganda spans for the first subtask. We also adopt an approach from relation extraction by enveloping spans mentioned above with special tokens for the second subtask of propaganda technique classification. Our models report an F score of 44.6% and a micro averaged F score of 58.2% for those tasks accordingly."},{"ID":"ding-etal-2013-detecting","methods":["graph regularization"],"center_method":[null],"tasks":["detecting spammers"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Detecting Spammers in Community Question Answering","abstract_clean":"As the popularity of Community Question Answering(CQA) increases, spamming activities also picked up in numbers and variety. On CQA sites, spammers often pretend to ask questions, and select answers which were published by their partners or themselves as the best answers. These fake best answers cannot be easily detected by neither existing methods nor common users. In this paper, we address the issue of detecting spammers on CQA sites. We formulate the task as an optimization problem. Social information is incorporated by adding graph regularization constraints to the text based predictor. To evaluate the proposed approach, we crawled a data set from a CQA portal. Experimental results demonstrate that the proposed method can achieve better performance than some state of the art methods."},{"ID":"ding-feng-2020-learning","methods":["zero - shot learning"],"center_method":[null],"tasks":["classify events"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Learning to Classify Events from Human Needs Category Descriptions","abstract_clean":"We study the problem of learning an event classifier from human needs category descriptions, which is challenging due to: (1) the use of highly abstract concepts in natural language descriptions, (2) the difficulty of choosing key concepts. To tackle these two challenges, we propose LEAPI, a zero shot learning method that first automatically generate weak labels by instantiating high level concepts with prototypical instances and then trains a human needs classifier with the weakly labeled data. To filter noisy concepts, we design a reinforced selection algorithm to choose high quality concepts for instantiation. Experimental results on the human needs categorization task show that our method outperforms baseline methods, producing substantially better precision. Physiological Needs Description: the need for a person to obtain food, to have meals \u2026 food\u00e0fruit, vegetable, meat, egg, fish,\u2026 \"I bought fruits\", \"I had eggs this morning\" Concept\u00e0Instances Labeled Events Leisure Needs Description: the need for a person to have leisure activities, to enjoy art \u2026 leisure activities\u00e0fishing, shopping, golf \"I went to fishing\", \"Dad went to play golf\" Concept\u00e0Instances Labeled Events"},{"ID":"doddington-1989-initial","methods":["guidelines"],"center_method":[null],"tasks":["development of the next - generation spoken language systems speech research database"],"center_task":[null],"Goal":["Industry, Innovation and Infrastructure"],"title_clean":"Initial Draft Guidelines for the Development of the Next Generation Spoken Language Systems Speech Research Database","abstract_clean":"To best serve the strategic needs of the DARPA SLS research program by creating the next generation speech database(s)."},{"ID":"dong-etal-2021-discourse","methods":["unsupervised graph - based ranking model"],"center_method":[null],"tasks":["summarization for long scientific documents"],"center_task":[null],"Goal":["Industry, Innovation and Infrastructure"],"title_clean":"Discourse Aware Unsupervised Summarization for Long Scientific Documents","abstract_clean":"We propose an unsupervised graph based ranking model for extractive summarization of long scientific documents. Our method assumes a two level hierarchical graph representation of the source document, and exploits asymmetrical positional cues to determine sentence importance. Results on the PubMed and arXiv datasets show that our approach 1 outperforms strong unsupervised baselines by wide margins in automatic metrics and human evaluation. In addition, it achieves performance comparable to many state of the art supervised approaches which are trained on hundreds of thousands of examples. These results suggest that patterns in the discourse structure are a strong signal for determining importance in scientific articles. * Equal contribution. 1 Link to our code: https:\/\/github.com\/ mirandrom\/HipoRank. Introduction anxiety affects quality of life in those living with parkinson's disease (pd) more so than overall cognitive status, motor deficits, apathy, and depression."},{"ID":"du-etal-2019-extracting","methods":["data collection","deep neural network","hierarchical span - attribute tagging","sa - t","curriculum learning","sequence - to - sequence model"],"center_method":["data collection","deep neural network",null,null,null,null],"tasks":["extracting symptoms"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Extracting Symptoms and their Status from Clinical Conversations","abstract_clean":"This paper describes novel models tailored for a new application, that of extracting the symptoms mentioned in clinical conversations along with their status. Lack of any publicly available corpus in this privacy sensitive domain led us to develop our own corpus, consisting of about 3K conversations annotated by professional medical scribes. We propose two novel deep learning approaches to infer the symptom names and their status: (1) a new hierarchical span attribute tagging (SA T) model, trained using curriculum learning, and (2) a variant of sequence to sequence model which decodes the symptoms and their status from a few speaker turns within a sliding window over the conversation. This task stems from a realistic application of assisting medical providers in capturing symptoms mentioned by patients from their clinical conversations. To reflect this application, we define multiple metrics. From inter rater agreement, we find that the task is inherently difficult. We conduct comprehensive evaluations on several contrasting conditions and observe that the performance of the models range from an F score of 0.5 to 0.8 depending on the condition. Our analysis not only reveals the inherent challenges of the task, but also provides useful directions to improve the models."},{"ID":"economou-etal-2000-lexiploigissi","methods":["educational platform"],"center_method":[null],"tasks":["teaching"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"LEXIPLOIGISSI: An Educational Platform for the Teaching of Terminology in Greece","abstract_clean":"This paper introduces a project, LEXIPLOIGISSI * , which involves use of language resources for educational purposes. More particularly, the aim of the project is to develop written corpora, electronic dictionaries and exercises to enhance students' reading and writing abilities in six different school subjects. It is the product of a small scale pilot program that will be part of the school curriculum in the three grades of Upper Secondary Education in Greece. The application seeks to create exploratory learning environments in which digital sound, image, text and video are fully integrated through the educational platform and placed under the direct control of users who are able to follow individual pathways through data stores. * The Institute for Language and Speech Processing has undertaken this project as the leading contractor and Kastaniotis Publications as a subcontractor. The first partner was responsible for the design, development and implementation of the educational platform, as well as for the provision of pedagogic scenarios of use; the second partner provided the resources (texts and multimedia material). The starting date of the project was June 1999, the development of the software and the collection of material lasted nine months."},{"ID":"ehara-2021-extent-lexical","methods":["lexical normalization"],"center_method":[null],"tasks":["evaluate the readability of texts"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"To What Extent Does Lexical Normalization Help English as a Second Language Learners to Read Noisy English Texts?","abstract_clean":"How difficult is it for English as a second language (ESL) learners to read noisy English texts? Do ESL learners need lexical normalization to read noisy English texts? These questions may also affect community formation on social networking sites where differences can be attributed to ESL learners and native English speakers. However, few studies have addressed these questions. To this end, we built highly accurate readability assessors to evaluate the readability of texts for ESL learners. We then applied these assessors to noisy English texts to further assess the readability of the texts. The experimental results showed that although intermediate level ESL learners can read most noisy English texts in the first place, lexical normalization significantly improves the readability of noisy English texts for ESL learners."},{"ID":"falenska-cetinoglu-2021-assessing","methods":["insights"],"center_method":[null],"tasks":["assessing gender bias"],"center_task":[null],"Goal":["Gender Equality"],"title_clean":"Assessing Gender Bias in Wikipedia: Inequalities in Article Titles","abstract_clean":"Potential gender biases existing in Wikipedia's content can contribute to biased behaviors in a variety of downstream NLP systems. Yet, efforts in understanding what inequalities in portraying women and men occur in Wikipedia focused so far only on biographies, leaving open the question of how often such harmful patterns occur in other topics. In this paper, we investigate gender related asymmetries in Wikipedia titles from all domains. We assess that for only half of gender related articles, i.e., articles with words such as women or male in their titles, symmetrical counterparts describing the same concept for the other gender (and clearly stating it in their titles) exist. Among the remaining imbalanced cases, the vast majority of articles concern sports and social related issues. We provide insights on how such asymmetries can influence other Wikipedia components and propose steps towards reducing the frequency of observed patterns."},{"ID":"fierro-etal-2017-200k","methods":["manual tagging"],"center_method":[null],"tasks":["data collection"],"center_task":["data collection"],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"200K+ Crowdsourced Political Arguments for a New Chilean Constitution","abstract_clean":"In this paper we present the dataset of 200,000+ political arguments produced in the local phase of the 2016 Chilean constitutional process. We describe the human processing of this data by the government officials, and the manual tagging of arguments performed by members of our research group. Afterwards we focus on classification tasks that mimic the human processes, comparing linear methods with neural network architectures. The experiments show that some of the manual tasks are suitable for automatization. In particular, the best methods achieve a 90% top 5 accuracy in a multiclass classification of arguments, and 65% macro averaged F1 score for tagging arguments according to a three part argumentation model."},{"ID":"finlayson-etal-2014-n2","methods":["data collection"],"center_method":["data collection"],"tasks":["data collection"],"center_task":["data collection"],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"The N2 corpus: A semantically annotated collection of Islamist extremist stories","abstract_clean":"We describe the N2 (Narrative Networks) Corpus, a new language resource. The corpus is unique in three important ways. First, every text in the corpus is a story, which is in contrast to other language resources that may contain stories or story like texts, but are not specifically curated to contain only stories. Second, the unifying theme of the corpus is material relevant to Islamist Extremists, having been produced by or often referenced by them. Third, every text in the corpus has been annotated for 14 layers of syntax and semantics, including: referring expressions and co reference; events, time expressions, and temporal relationships; semantic roles; and word senses. In cases where analyzers were not available to do high quality automatic annotations, layers were manually doubleannotated and adjudicated by trained annotators. The corpus comprises 100 texts and 42,480 words. Most of the texts were originally in Arabic but all are provided in English translation. We explain the motivation for constructing the corpus, the process for selecting the texts, the detailed contents of the corpus itself, the rationale behind the choice of annotation layers, and the annotation procedure."},{"ID":"finzel-etal-2021-conversational","methods":["conversational agent","bidirectional long short - term memory","gpt - 2"],"center_method":[null,null,null],"tasks":["daily living assessment coaching"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Conversational Agent for Daily Living Assessment Coaching Demo","abstract_clean":"Conversational Agent for Daily Living Assessment Coaching (CADLAC) is a multi modal conversational agent system designed to impersonate \"individuals\" with various levels of ability in activities of daily living (ADLs: e.g., dressing, bathing, mobility, etc.) for use in training professional assessors how to conduct interviews to determine one's level of functioning. The system is implemented on the Mind Meld platform for conversational AI and features a Bidirectional Long Short Term Memory topic tracker that allows the agent to navigate conversations spanning 18 different ADL domains, a dialogue manager that interfaces with a database of over 10,000 historical ADL assessments, a rule based Natural Language Generation (NLG) module, and a pre trained open domain conversational sub agent (based on GPT 2) for handling conversation turns outside of the 18 ADL domains. CADLAC is delivered via state of the art web frameworks to handle multiple conversations and users simultaneously and is enabled with voice interface. The paper includes a description of the system design and evaluation of individual components followed by a brief discussion of current limitations and next steps."},{"ID":"fischer-1979-powerful","methods":[null],"center_method":[null],"tasks":["computational linguistics"],"center_task":["computational linguistics"],"Goal":["Quality Education"],"title_clean":"Powerful ideas in computational linquistics  Implications for problem solving, and education","abstract_clean":"It is our firm belief that solving problems in the domain of computational linguistics (CL) can provide a set of metaphors or powerful ideas which are of great importance to many fields. We have taught several experimental classes to students from high schools and universities and s major part of our work was centered around problems dealing with language. We have set up an experimental Language Laboratory in which the students can explore existing computer programs, modify them, design new ones and implement them. The goal was that the student should gain a deeper understanding of language itself and that he\/she should learn general and transferable problem solving skills. exercise in pattern matching and symbol manipulation,\nwhere certain keywords trigger a few prestored answers. It may also serve as an example for how little machinery is necessary to create the illusion of understanding."},{"ID":"fortuna-etal-2020-toxic","methods":["similarity analysis"],"center_method":[null],"tasks":["analysis of hate speech datasets"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Toxic, Hateful, Offensive or Abusive? What Are We Really Classifying? An Empirical Analysis of Hate Speech Datasets","abstract_clean":"The field of the automatic detection of hate speech and related concepts has raised a lot of interest in the last years. Different datasets were annotated and classified by means of applying different machine learning algorithms. However, few efforts were done in order to clarify the applied categories and homogenize different datasets. Our study takes up this demand. We analyze six different publicly available datasets in this field with respect to their similarity and compatibility. We conduct two different experiments. First, we try to make the datasets compatible and represent the dataset classes as Fast Text word vectors analyzing the similarity between different classes in a intra and inter dataset manner. Second, we submit the chosen datasets to the Perspective API Toxicity classifier, achieving different performances depending on the categories and datasets. One of the main conclusions of these experiments is that many different definitions are being used for equivalent concepts, which makes most of the publicly available datasets incompatible. Grounded in our analysis, we provide guidelines for future dataset collection and annotation."},{"ID":"fourtassi-etal-2019-development","methods":["evolving network"],"center_method":[null],"tasks":["children learning"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"The Development of Abstract Concepts in Children's Early Lexical Networks","abstract_clean":"How do children learn abstract concepts such as animal vs. artifact? Previous research has suggested that such concepts can partly be derived using cues from the language children hear around them. Following this suggestion, we propose a model where we represent the children's developing lexicon as an evolving network. The nodes of this network are based on vocabulary knowledge as reported by parents, and the edges between pairs of nodes are based on the probability of their co occurrence in a corpus of child directed speech. We found that several abstract categories can be identified as the dense regions in such networks. In addition, our simulations suggest that these categories develop simultaneously, rather than sequentially, thanks to the children's word learning trajectory which favors the exploration of the global conceptual space."},{"ID":"friedberg-2011-turn","methods":["analysis"],"center_method":["analysis"],"tasks":["identify prosodic turn - taking cues"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Turn Taking Cues in a Human Tutoring Corpus","abstract_clean":"Most spoken dialogue systems are still lacking in their ability to accurately model the complex process that is human turntaking. This research analyzes a humanhuman tutoring corpus in order to identify prosodic turn taking cues, with the hopes that they can be used by intelligent tutoring systems to predict student turn boundaries. Results show that while there was variation between subjects, three features were significant turn yielding cues overall. In addition, a positive relationship between the number of cues present and the probability of a turn yield was demonstrated."},{"ID":"fudholi-suominen-2018-importance","methods":["recommender and feedback features"],"center_method":[null],"tasks":["pronunciation learning aid"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"The Importance of Recommender and Feedback Features in a Pronunciation Learning Aid","abstract_clean":"Verbal communication and pronunciation as its part is a core skill that can be developed through guided learning. An artificial intelligence system can take a role in these guided learning approaches as an enabler of an application for pronunciation learning with a recommender system to guide language learners through exercises and feedback system to correct their pronunciation. In this paper, we report on a user study on language learners' perceived usefulness of the application. 16 international students who spoke non native English and lived in Australia participated. 13 of them said they need to improve their pronunciation skills in English because of their foreign accent. The feedback system with features for pronunciation scoring, speech replay, and giving a pronunciation example was deemed essential by most of the respondents. In contrast, a clear dichotomy between the recommender system perceived as useful or useless existed; the system had features to prompt new common words or old poorly scored words. These results can be used to target research and development from information retrieval and reinforcement learning for better and better recommendations to speech recognition and speech analytics for accent acquisition."},{"ID":"gamoran-etal-2021-using","methods":["bayesian modeling"],"center_method":[null],"tasks":["suicide prediction"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Using Psychologically Informed Priors for Suicide Prediction in the CLPsych 2021 Shared Task","abstract_clean":"This paper describes our approach to the CLPsych 2021 Shared Task, in which we aimed to predict suicide attempts based on Twitter feed data. We addressed this challenge by emphasizing reliance on prior domain knowledge. We engineered novel theorydriven features, and integrated prior knowledge with empirical evidence in a principled manner using Bayesian modeling. While this theory guided approach increases bias and lowers accuracy on the training set, it was successful in preventing over fitting. The models provided reasonable classification accuracy on unseen test data (0.68 \u2264 AU C \u2264 0.84). Our approach may be particularly useful in prediction tasks trained on a relatively small data set."},{"ID":"garimella-etal-2021-intelligent","methods":["lexical co - occurrence - based bias penalization"],"center_method":[null],"tasks":["mitigating social biases in language modelling and generation"],"center_task":[null],"Goal":["Reduced Inequalities"],"title_clean":"He is very intelligent, she is very beautiful? On Mitigating Social Biases in Language Modelling and Generation","abstract_clean":"Social biases with respect to demographics (e.g., gender, age, race) in datasets are often encoded in the large pre trained language models trained on them. Prior works have largely focused on mitigating biases in context free representations, with recent shift to contextual ones. While this is useful for several word and sentence level classification tasks, mitigating biases in only the representations may not suffice to use these models for language generation tasks, such as auto completion, summarization, or dialogue generation. In this paper, we propose an approach to mitigate social biases in BERT, a large pre trained contextual language model, and show its effectiveness in fill in the blank sentence completion and summarization tasks. In addition to mitigating biases in BERT, which in general acts as an encoder, we propose lexical co occurrence based bias penalization in the decoder units in generation frameworks, and show bias mitigation in summarization. Finally, our approach results in better debiasing of BERT based representations compared to post training bias mitigation, thus illustrating the efficacy of our approach to not just mitigate biases in representations, but also generate text with reduced biases."},{"ID":"gavankar-etal-2012-enriching","methods":["linked open data"],"center_method":[null],"tasks":["enriching an academic knowledge base"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Enriching An Academic knowledge base using Linked Open Data","abstract_clean":"In this paper we present work done towards populating a domain ontology using a public knowledge base like DBpedia. Using an academic ontology as our target we identify mappings between a subset of its predicates and those in DBpedia and other linked datasets. In the semantic web context, ontology mapping allows linking of independently developed ontologies and inter operation of heterogeneous resources. Linked open data is an initiative in this direction. We populate our ontology by querying the linked open datasets for extracting instances from these resources. We show how these along with semantic web standards and tools enable us to populate the academic ontology. Resulting instances could then be used as seeds in spirit of the typical bootstrapping paradigm."},{"ID":"gi-etal-2021-verdict","methods":["roberta"],"center_method":["roberta"],"tasks":["verdict inference","fact checking"],"center_task":[null,"fact checking"],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Verdict Inference with Claim and Retrieved Elements Using RoBERTa","abstract_clean":"Automatic fact verification has attracted recent research attention as the increasing dissemination of disinformation on social media platforms. The FEVEROUS shared task introduces a benchmark for fact verification, in which a system is challenged to verify the given claim using the extracted evidential elements from Wikipedia documents. In this paper, we propose our 3 rd place three stage system consisting of document retrieval, element retrieval, and verdict inference for the FEVER OUS shared task. By considering the context relevance in the fact extraction and verification task, our system achieves 0.29 FEVER OUS score on the development set and 0.25 FEVEROUS score on the blind test set, both outperforming the FEVEROUS baseline."},{"ID":"goel-sharma-2019-usf","methods":["lstm","word embeddings"],"center_method":["lstm","word embeddings"],"tasks":["toxicity detection"],"center_task":["toxicity detection"],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"USF at SemEval 2019 Task 6: Offensive Language Detection Using LSTM With Word Embeddings","abstract_clean":"In this paper, we present a system description for the SemEval 2019 Task 6 submitted by our team. For the task, our system takes tweet as an input and determine if the tweet is offensive or non offensive (Sub task A). In case a tweet is offensive, our system identifies if a tweet is targeted (insult or threat) or nontargeted like swearing (Sub task B). In targeted tweets, our system identifies the target as an individual or group (Sub task C). We used data pre processing techniques like splitting hashtags into words, removing special characters, stop word removal, stemming, lemmatization, capitalization, and offensive word dictionary. Later, we used keras tokenizer and word embeddings for feature extraction. For classification, we used the LSTM (Long shortterm memory) model of keras framework. Our accuracy scores for Sub task A, B and C are 0.8128, 0.8167 and 0.3662 respectively. Our results indicate that fine grained classification to identify offense target was difficult for the system. Lastly, in the future scope section, we will discuss the ways to improve system performance."},{"ID":"gorrell-etal-2016-identifying","methods":["machine learning methods"],"center_method":["machine learning methods"],"tasks":["identifying first episodes of psychosis"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Identifying First Episodes of Psychosis in Psychiatric Patient Records using Machine Learning","abstract_clean":"Natural language processing is being pressed into use to facilitate the selection of cases for medical research in electronic health record databases, though study inclusion criteria may be complex, and the linguistic cues indicating eligibility may be subtle. Finding cases of first episode psychosis raised a number of problems for automated approaches, providing an opportunity to explore how machine learning technologies might be used to overcome them. A system was delivered that achieved an AUC of 0.85, enabling 95% of relevant cases to be identified whilst halving the work required in manually reviewing cases. The techniques that made this possible are presented."},{"ID":"grandeit-etal-2020-using","methods":["bert","qualitative content analysis"],"center_method":["bert",null],"tasks":["psychosocial online counseling"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Using BERT for Qualitative Content Analysis in Psychosocial Online Counseling","abstract_clean":"Qualitative content analysis is a systematic method commonly used in the social sciences to analyze textual data from interviews or online discussions. However, this method usually requires high expertise and manual effort because human coders need to read, interpret, and manually annotate text passages. This is especially true if the system of categories used for annotation is complex and semantically rich. Therefore, qualitative content analysis could benefit greatly from automated coding. In this work, we investigate the usage of machine learning based text classification models for automatic coding in the area of psychosocial online counseling. We developed a system of over 50 categories to analyze counseling conversations, labeled over 10.000 text passages manually, and evaluated the performance of different machine learning based classifiers against human coders."},{"ID":"grimes-2016-sentiment","methods":["surveying","industry overview"],"center_method":[null,null],"tasks":["social analysis"],"center_task":[null],"Goal":["Decent Work and Economic Growth"],"title_clean":"Sentiment, Subjectivity, and Social Analysis Go ToWork: An Industry View  Invited Talk","abstract_clean":"Seth Grimes Alta Plana Corporation grimes@altaplana.com\nAffective computing has a commercial side. Numerous products and projects provide sentiment, emotion, and intent extraction capabilities, applied in consumer and financial markets, for healthcare and customer care, and for media, policy, and politics. Academic and industry researchers are naturally interested how sentiment and social technologies are being applied and in commercial market opportunities and trends, in what's being funded, what's falling flat, and what's on business's roadmap. Analyst Seth Grimes will provide an industry overview, surveying companies and applications in the sentiment and social analytics spaces as well as work at the tech giants. He will discuss commercialization strategy and the affective market outlook."},{"ID":"grover-etal-2012-aspects","methods":["legal framework"],"center_method":[null],"tasks":["language resource management"],"center_task":[null],"Goal":["Industry, Innovation and Infrastructure"],"title_clean":"Aspects of a Legal Framework for Language Resource Management","abstract_clean":"The management of language resources requires several legal aspects to be taken into consideration. In this paper we discuss a number of these aspects which lead towards the formation of a legal framework for a language resources management agency. The legal framework entails examination of the agency's stakeholders and the relationships that exist amongst them, the privacy and intellectual property rights that exist around the language resources offered by the agency, and the external (e.g. laws, acts, policies) and internal legal instruments (e.g. end user licence agreements) required for the agency's operation."},{"ID":"guo-etal-2021-pre","methods":["pre - trained transformer - based models","ensembling"],"center_method":[null,null],"tasks":["classification","span detection","healthcare"],"center_task":["classification",null,"healthcare"],"Goal":["Good Health and Well-Being"],"title_clean":"Pre trained Transformer based Classification and Span Detection Models for Social Media Health Applications","abstract_clean":"This paper describes our approach for six classification tasks (Tasks 1a, 3a, 3b, 4 and 5) and one span detection task (Task 1b) from the Social Media Mining for Health (SMM4H) 2021 shared tasks. We developed two separate systems for classification and span detection, both based on pre trained Transformer based models. In addition, we applied oversampling and classifier ensembling in the classification tasks. The results of our submissions are over the median scores in all tasks except for Task 1a. Furthermore, our model achieved first place in Task 4 and obtained a 7% higher F 1 score than the median in Task 1b."},{"ID":"ha-yaneva-2018-automatic","methods":["concept embeddings","information retrieval"],"center_method":[null,"information retrieval"],"tasks":["distractor suggestion"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Automatic Distractor Suggestion for Multiple Choice Tests Using Concept Embeddings and Information Retrieval","abstract_clean":"Developing plausible distractors (wrong answer options) when writing multiple choice questions has been described as one of the most challenging and time consuming parts of the item writing process. In this paper we propose a fully automatic method for generating distractor suggestions for multiple choice questions used in high stakes medical exams. The system uses a question stem and the correct answer as an input and produces a list of suggested distractors ranked based on their similarity to the stem and the correct answer. To do this we use a novel approach of combining concept embeddings with information retrieval methods. We frame the evaluation as a prediction task where we aim to \"predict\" the human produced distractors used in large sets of medical questions, i.e. if a distractor generated by our system is good enough it is likely to feature among the list of distractors produced by the human item writers. The results reveal that combining concept embeddings with information retrieval approaches significantly improves the generation of plausible distractors and enables us to match around 1 in 5 of the human produced distractors. The approach proposed in this paper is generalisable to all scenarios where the distractors refer to concepts."},{"ID":"hahn-schulz-2002-towards","methods":["ontology engineering methodology"],"center_method":[null],"tasks":["medical language processing"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Towards Very Large Ontologies for Medical Language Processing","abstract_clean":"We describe an ontology engineering methodology by which conceptual knowledge is extracted from an informal medical thesaurus (UMLS) and automatically converted into a formal description logics system. Our approach consists of four steps: concept definitions are automatically generated from the UMLS source, integrity checking of taxonomic and partonomic hierarchies is performed by the terminological classifier, cycles and inconsistencies are eliminated, and incremental refinement of the evolving knowledge base is performed by a domain expert. We report on experiments with a knowledge base composed of 164,000 concepts and 76,000 relations."},{"ID":"hamalainen-etal-2021-detecting","methods":["bert","lstm"],"center_method":["bert","lstm"],"tasks":["detecting depression"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Detecting Depression in Thai Blog Posts: a Dataset and a Baseline","abstract_clean":"We present the first openly available corpus for detecting depression in Thai. Our corpus is compiled by expert verified cases of depression in several online blogs. We experiment with two different LSTM based models and two different BERT based models. We achieve a 77.53% accuracy with a Thai BERT model in detecting depression. This establishes a good baseline for future researcher on the same corpus. Furthermore, we identify a need for Thai embeddings that have been trained on a more varied corpus than Wikipedia. Our corpus, code and trained models have been released openly on Zenodo."},{"ID":"hartvigsen-etal-2022-toxigen","methods":["large - scale machine - generated dataset","adversarial classifier"],"center_method":[null,null],"tasks":["adversarial and implicit hate speech detection"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"ToxiGen: A Large Scale Machine Generated Dataset for Adversarial and Implicit Hate Speech Detection","abstract_clean":"Toxic language detection systems often falsely flag text that contains minority group mentions as toxic, as those groups are often the targets of online hate. Such over reliance on spurious correlations also causes systems to struggle with detecting implicitly toxic language. To help mitigate these issues, we create TOXIGEN, a new large scale and machinegenerated dataset of 274k toxic and benign statements about 13 minority groups. We develop a demonstration based prompting framework and an adversarial classifier in the loop decoding method to generate subtly toxic and benign text with a massive pretrained language model (Brown et al., 2020). Controlling machine generation in this way allows TOXIGEN to cover implicitly toxic text at a larger scale, and about more demographic groups, than previous resources of human written text. We conduct a human evaluation on a challenging subset of TOXIGEN and find that annotators struggle to distinguish machine generated text from human written language. We also find that 94.5% of toxic examples are labeled as hate speech by human annotators. Using three publicly available datasets, we show that finetuning a toxicity classifier on our data improves its performance on human written data substantially. We also demonstrate that TOXI GEN can be used to fight machine generated toxicity as finetuning improves the classifier significantly on our evaluation subset."},{"ID":"hede-etal-2021-toxicity","methods":["data collection","error analysis"],"center_method":["data collection","error analysis"],"tasks":["detecting online incivility for english"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"From Toxicity in Online Comments to Incivility in American News: Proceed with Caution","abstract_clean":"The ability to quantify incivility online, in news and in congressional debates, is of great interest to political scientists. Computational tools for detecting online incivility for English are now fairly accessible and potentially could be applied more broadly. We test the Jigsaw Perspective API for its ability to detect the degree of incivility on a corpus that we developed, consisting of manual annotations of civility in American news. We demonstrate that toxicity models, as exemplified by Perspective, are inadequate for the analysis of incivility in news. We carry out error analysis that points to the need to develop methods to remove spurious correlations between words often mentioned in the news, especially identity descriptors and incivility. Without such improvements, applying Perspective or similar models on news is likely to lead to wrong conclusions, that are not aligned with the human perception of incivility."},{"ID":"henriksson-etal-2013-corpus","methods":["distributional methods"],"center_method":[null],"tasks":["corpus - driven terminology development"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Corpus Driven Terminology Development: Populating Swedish SNOMED CT with Synonyms Extracted from Electronic Health Records","abstract_clean":"The various ways in which one can refer to the same clinical concept needs to be accounted for in a semantic resource such as SNOMED CT. Developing terminological resources manually is, however, prohibitively expensive and likely to result in low coverage, especially given the high variability of language use in clinical text. To support this process, distributional methods can be employed in conjunction with a large corpus of electronic health records to extract synonym candidates for clinical terms. In this paper, we exemplify the potential of our proposed method using the Swedish version of SNOMED CT, which currently lacks synonyms. A medical expert inspects two thousand term pairs generated by two semantic spacesone of which models multiword terms in addition to single words for one hundred preferred terms of the semantic types disorder and finding."},{"ID":"hieu-etal-2020-reintel","methods":["phobert embeddings","ensemble methods"],"center_method":[null,"ensemble methods"],"tasks":["fake news detection"],"center_task":["fake news detection"],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"ReINTEL Challenge 2020: Vietnamese Fake News Detection usingEnsemble Model with PhoBERT embeddings","abstract_clean":"Along with the increasing traffic of social networks in Vietnam in recent years, the number of unreliable news has also grown rapidly. As we make decisions based on the information we come across daily, fake news, depending on the severity of the matter, can lead to disastrous consequences. This paper presents our approach for the Fake News Detection on Social Network Sites (SNSs), using an ensemble method with linguistic features extracted using PhoBERT (Nguyen and Nguyen, 2020). Our method achieves AUC score of 0.9521 and got 1 st place on the private test at the 7 th International Workshop on Vietnamese Language and Speech Processing (VLSP). For reproducing the result, the code can be found at https:\/\/gitlab.com\/thuan."},{"ID":"hoang-kan-2010-towards","methods":["related work summarization system"],"center_method":[null],"tasks":["related work summarization"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Towards Automated Related Work Summarization","abstract_clean":"We introduce the novel problem of automatic related work summarization. Given multiple articles (e.g., conference\/journal papers) as input, a related work summarization system creates a topic biased summary of related work specific to the target paper. Our prototype Related Work Summarization system, ReWoS, takes in set of keywords arranged in a hierarchical fashion that describes a target paper's topics, to drive the creation of an extractive summary using two different strategies for locating appropriate sentences for general topics as well as detailed ones. Our initial results show an improvement over generic multi document summarization baselines in a human evaluation."},{"ID":"hopkins-etal-2017-beyond","methods":["tree transducers"],"center_method":[null],"tasks":["sentential semantic parsing"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Beyond Sentential Semantic Parsing: Tackling the Math SAT with a Cascade of Tree Transducers","abstract_clean":"We present an approach for answering questions that span multiple sentences and exhibit sophisticated cross sentence anaphoric phenomena, evaluating on a rich source of such questions the math portion of the Scholastic Aptitude Test (SAT). By using a tree transducer cascade as its basic architecture, our system (called EU CLID) propagates uncertainty from multiple sources (e.g. coreference resolution or verb interpretation) until it can be confidently resolved. Experiments show the first ever results (43% recall and 91% precision) on SAT algebra word problems. We also apply EUCLID to the public Dolphin algebra question set, and improve the state of the art F 1 score from 73.9% to 77.0%."},{"ID":"hsueh-etal-2006-automatic","methods":["automatic speech recognition"],"center_method":["automatic speech recognition"],"tasks":["automatic segmentation of multiparty dialogue"],"center_task":[null],"Goal":["Partnership for the Goals"],"title_clean":"Automatic Segmentation of Multiparty Dialogue","abstract_clean":"In this paper, we investigate the problem of automatically predicting segment boundaries in spoken multiparty dialogue. We extend prior work in two ways. We first apply approaches that have been proposed for predicting top level topic shifts to the problem of identifying subtopic boundaries. We then explore the impact on performance of using ASR output as opposed to human transcription. Examination of the effect of features shows that predicting top level and predicting subtopic boundaries are two distinct tasks: (1) for predicting subtopic boundaries, the lexical cohesion based approach alone can achieve competitive results, (2) for predicting top level boundaries, the machine learning approach that combines lexical cohesion and conversational features performs best, and (3) conversational cues, such as cue phrases and overlapping speech, are better indicators for the toplevel prediction task. We also find that the transcription errors inevitable in ASR output have a negative impact on models that combine lexical cohesion and conversational features, but do not change the general preference of approach for the two tasks."},{"ID":"hu-etal-2021-collaborative","methods":["collaborative data relabeling"],"center_method":[null],"tasks":["voice apps recommendation"],"center_task":[null],"Goal":["Industry, Innovation and Infrastructure"],"title_clean":"Collaborative Data Relabeling for Robust and Diverse Voice Apps Recommendation in Intelligent Personal Assistants","abstract_clean":"Intelligent personal assistants (IPAs) such as Amazon Alexa, Google Assistant and Apple Siri extend their built in capabilities by supporting voice apps developed by third party developers. Sometimes the smart assistant is not able to successfully respond to user voice commands (aka utterances). There are many reasons including automatic speech recognition (ASR) error, natural language understanding (NLU) error, routing utterances to an irrelevant voice app or simply that the user is asking for a capability that is not supported yet. The failure to handle a voice command leads to customer frustration. In this paper, we introduce a fallback skill recommendation system to suggest a voice app to a customer for an unhandled voice command. One of the prominent challenges of developing a skill recommender system for IPAs is partial observation. To solve the partial observation problem, we propose collaborative data relabeling (CDR) method. In addition, CDR also improves the diversity of the recommended skills. We evaluate the proposed method both offline and online. The offline evaluation results show that the proposed system outperforms the baselines. The online A\/B testing results show significant gain of customer experience metrics."},{"ID":"hu-yang-2020-privnet","methods":["adversarial game"],"center_method":[null],"tasks":["safeguarding private attributes"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"PrivNet: Safeguarding Private Attributes in Transfer Learning for Recommendation","abstract_clean":"Transfer learning is an effective technique to improve a target recommender system with the knowledge from a source domain. Existing research focuses on the recommendation performance of the target domain while ignores the privacy leakage of the source domain. The transferred knowledge, however, may unintendedly leak private information of the source domain. For example, an attacker can accurately infer user demographics from their historical purchase provided by a source domain data owner. This paper addresses the above privacy preserving issue by learning a privacyaware neural representation by improving target performance while protecting source privacy. The key idea is to simulate the attacks during the training for protecting unseen users' privacy in the future, modeled by an adversarial game, so that the transfer learning model becomes robust to attacks. Experiments show that the proposed PrivNet model can successfully disentangle the knowledge benefitting the transfer from leaking the privacy."},{"ID":"huang-bai-2021-hub","methods":["bert"],"center_method":["bert"],"tasks":["identify and classify offensive text in multilingual code mixing"],"center_task":[null],"Goal":["Reduced Inequalities"],"title_clean":"HUB@DravidianLangTech EACL2021: Identify and Classify Offensive Text in Multilingual Code Mixing in Social Media","abstract_clean":"This paper introduces the system description of the HUB team participating in Dravidian LangTech EACL2021: Offensive Language Identification in Dravidian Languages. The theme of this shared task is the detection of offensive content in social media. Among the known tasks related to offensive speech detection, this is the first task to detect offensive comments posted in social media comments in the Dravidian language. The task organizer team provided us with the code mixing task data set mainly composed of three different languages: Malayalam, Kannada, and Tamil. The tasks on the code mixed data in these three different languages can be seen as three different comment\/post level classification tasks. The task on the Malayalam data set is a five category classification task, and the Kannada and Tamil language data sets are two six category classification tasks. Based on our analysis of the task description and task data set, we chose to use the multilingual BERT model to complete this task. In this paper, we will discuss our fine tuning methods, models, experiments, and results."},{"ID":"huang-bai-2021-team","methods":["pre - trained language model","xlm - roberta","tf - idf"],"center_method":[null,null,null],"tasks":["hope speech detection"],"center_task":["hope speech detection"],"Goal":["Good Health and Well-Being"],"title_clean":"TEAM HUB@LT EDI EACL2021: Hope Speech Detection Based On Pre trained Language Model","abstract_clean":"This article introduces the system description of TEAM HUB team participating in LT EDI 2021: Hope Speech Detection. This shared task is the first task related to the desired voice detection. The data set in the shared task consists of three different languages (English, Tamil, and Malayalam). The task type is text classification. Based on the analysis and understanding of the task description and data set, we designed a system based on a pre trained language model to complete this shared task. In this system, we use methods and models that combine the XLM RoBERTa pre trained language model and the Tf Idf algorithm. In the final result ranking announced by the task organizer, our system obtained F1 scores of 0.93, 0.84, 0.59 on the English dataset, Malayalam dataset, and Tamil dataset. Our submission results are ranked 1, 2, and 3 respectively."},{"ID":"huang-etal-2020-coda","methods":["annotation"],"center_method":["annotation"],"tasks":["data collection"],"center_task":["data collection"],"Goal":["Good Health and Well-Being"],"title_clean":"CODA 19: Using a Non Expert Crowd to Annotate Research Aspects on 10,000+ Abstracts in the COVID 19 Open Research Dataset","abstract_clean":"This paper introduces CODA 19 1 , a humanannotated dataset that codes the Background, Purpose, Method, Finding\/Contribution, and Other sections of 10,966 English abstracts in the COVID 19 Open Research Dataset. CODA 19 was created by 248 crowd workers from Amazon Mechanical Turk within 10 days, and achieved labeling quality comparable to that of experts. Each abstract was annotated by nine different workers, and the final labels were acquired by majority vote. The inter annotator agreement (Cohen's kappa) between the crowd and the biomedical expert (0.741) is comparable to inter expert agreement (0.788). CODA 19's labels have an accuracy of 82.2% when compared to the biomedical expert's labels, while the accuracy between experts was 85.0%. Reliable human annotations help scientists access and integrate the rapidly accelerating coronavirus literature, and also serve as the battery of AI\/NLP research, but obtaining expert annotations can be slow. We demonstrated that a non expert crowd can be rapidly employed at scale to join the fight against COVID 19."},{"ID":"hurriyetoglu-etal-2021-challenges","methods":["transformers","word embeddings"],"center_method":["transformers","word embeddings"],"tasks":["automated extraction of socio - political events"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Challenges and Applications of Automated Extraction of Socio political Events from Text (CASE 2021): Workshop and Shared Task Report","abstract_clean":"This workshop is the fourth issue of a series of workshops on automatic extraction of sociopolitical events from news, organized by the Emerging Market Welfare Project, with the support of the Joint Research Centre of the European Commission and with contributions from many other prominent scholars in this field. The purpose of this series of workshops is to foster research and development of reliable, valid, robust, and practical solutions for automatically detecting descriptions of sociopolitical events, such as protests, riots, wars and armed conflicts, in text streams. This year workshop contributors make use of the stateof the art NLP technologies, such as Deep Learning, Word Embeddings and Transformers and cover a wide range of topics from text classification to news bias detection. Around 40 teams have registered and 15 teams contributed to three tasks that are i) multilingual protest news detection, ii) fine grained classification of socio political events, and iii) discovering Black Lives Matter protest events. The workshop also highlights two keynote and four invited talks about various aspects of creating event data sets and multi and cross lingual machine learning in few and zero shot settings."},{"ID":"inan-etal-2022-modeling","methods":["supervised intensity tagger","transformers"],"center_method":[null,"transformers"],"tasks":["sign language generation"],"center_task":[null],"Goal":["Reduced Inequalities"],"title_clean":"Modeling Intensification for Sign Language Generation: A Computational Approach","abstract_clean":"End to end sign language generation models do not accurately represent the prosody in sign language. A lack of temporal and spatial variations leads to poor quality generated presentations that confuse human interpreters. In this paper, we aim to improve the prosody in generated sign languages by modeling intensification in a data driven manner. We present different strategies grounded in linguistics of sign language that inform how intensity modifiers can be represented in gloss annotations. To employ our strategies, we first annotate a subset of the benchmark PHOENIX 14T, a German Sign Language dataset, with different levels of intensification. We then use a supervised intensity tagger to extend the annotated dataset and obtain labels for the remaining portion of it. This enhanced dataset is then used to train state of the art transformer models for sign language generation. We find that our efforts in intensification modeling yield better results when evaluated with automatic metrics. Human evaluation also indicates a higher preference of the videos generated using our model."},{"ID":"iyyer-etal-2014-political","methods":["recursive neural networks"],"center_method":[null],"tasks":["political ideology detection"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Political Ideology Detection Using Recursive Neural Networks","abstract_clean":"An individual's words often reveal their political ideology. Existing automated techniques to identify ideology from text focus on bags of words or wordlists, ignoring syntax. Taking inspiration from recent work in sentiment analysis that successfully models the compositional aspect of language, we apply a recursive neural network (RNN) framework to the task of identifying the political position evinced by a sentence. To show the importance of modeling subsentential elements, we crowdsource political annotations at a phrase and sentence level. Our model outperforms existing models on our newly annotated dataset and an existing dataset."},{"ID":"jacobson-dalianis-2016-applying","methods":["stacked sparse auto encoders","boltzmann machines"],"center_method":[null,null],"tasks":["predict healthcare - associated infections"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Applying deep learning on electronic health records in Swedish to predict healthcare associated infections","abstract_clean":"Detecting healthcare associated infections pose a major challenge in healthcare. Using natural language processing and machine learning applied on electronic patient records is one approach that has been shown to work. However the results indicate that there was room for improvement and therefore we have applied deep learning methods. Specifically we implemented a network of stacked sparse auto encoders and a network of stacked restricted Boltzmann machines. Our best results were obtained using the stacked restricted Boltzmann machines with a precision of 0.79 and a recall of 0.88."},{"ID":"jagannatha-yu-2016-structured","methods":["conditional random field","recurrent neural networks"],"center_method":["conditional random field","recurrent neural networks"],"tasks":["sequence labeling"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Structured prediction models for RNN based sequence labeling in clinical text","abstract_clean":"Sequence labeling is a widely used method for named entity recognition and information extraction from unstructured natural language data. In the clinical domain one major application of sequence labeling involves extraction of relevant entities such as medication, indication, and side effects from Electronic Health Record Narratives. Sequence labeling in this domain presents its own set of challenges and objectives. In this work we experiment with Conditional Random Field based structured learning models with Recurrent Neural Networks. We extend the previously studied CRF LSTM model with explicit modeling of pairwise potentials. We also propose an approximate version of skip chain CRF inference with RNN potentials. We use these methods 1 for structured prediction in order to improve the exact phrase detection of clinical entities."},{"ID":"jang-etal-2020-exploratory","methods":["exploratory analysis","topic models"],"center_method":[null,"topic models"],"tasks":["inform public health institutes"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Exploratory Analysis of COVID 19 Related Tweets in North America to Inform Public Health Institutes","abstract_clean":"Social media is a rich source where we can learn about people's reactions to social issues. As COVID 19 has significantly impacted on people's lives, it is essential to capture how people react to public health interventions and understand their concerns. In this paper, we aim to investigate people's reactions and concerns about COVID 19 in North America, especially focusing on Canada. We analyze COVID 19 related tweets using topic modeling and aspect based sentiment analysis, and interpret the results with public health experts. We compare timeline of topics discussed with timing of implementation of public health interventions for COVID 19. We also examine people's sentiment about COVID 19 related issues. We discuss how the results can be helpful for public health agencies when designing a policy for new interventions. Our work shows how Natural Language Processing (NLP) techniques could be applied to public health questions with domain expert involvement."},{"ID":"jehl-riezler-2018-document","methods":["document - level information","sentence - attached features"],"center_method":[null,null],"tasks":["neural patent translation"],"center_task":[null],"Goal":["Industry, Innovation and Infrastructure"],"title_clean":"Document Level Information as Side Constraints for Improved Neural Patent Translation","abstract_clean":"We investigate the usefulness of document information as side constraints for machine translation. We adapt two approaches to encoding this information as features for neural patent translation: As special tokens which are attached to the source sentence, and as tags which are attached to the source words. We found that sentence attached features produced the same or better results as word attached features. Both approaches produced significant gains of over 1% BLEU over the baseline on a German English translation task, while sentence attached features also produced significant gains of 0.7% BLEU on a Japanese English task. We also describe a method to encode document information as additional phrase features for phrasebased translation, but did not find any improvements."},{"ID":"joshi-etal-2015-computational","methods":["n - gram","distant supervision"],"center_method":[null,"distant supervision"],"tasks":["drunk - texting prediction"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"A Computational Approach to Automatic Prediction of Drunk Texting","abstract_clean":"Alcohol abuse may lead to unsociable behavior such as crime, drunk driving, or privacy leaks. We introduce automatic drunk texting prediction as the task of identifying whether a text was written when under the influence of alcohol. We experiment with tweets labeled using hashtags as distant supervision. Our classifiers use a set of N gram and stylistic features to detect drunk tweets. Our observations present the first quantitative evidence that text contains signals that can be exploited to detect drunk texting."},{"ID":"kardas-etal-2020-axcell","methods":["machine learning pipeline"],"center_method":[null],"tasks":["automatic extraction of results"],"center_task":[null],"Goal":["Industry, Innovation and Infrastructure"],"title_clean":"AxCell: Automatic Extraction of Results from Machine Learning Papers","abstract_clean":"Tracking progress in machine learning has become increasingly difficult with the recent explosion in the number of papers. In this paper, we present AXCELL, an automatic machine learning pipeline for extracting results from papers. AXCELL uses several novel components, including a table segmentation subtask, to learn relevant structural knowledge that aids extraction. When compared with existing methods, our approach significantly improves the state of the art for results extraction. We also release a structured, annotated dataset for training models for results extraction, and a dataset for evaluating the performance of models on this task. Lastly, we show the viability of our approach enables it to be used for semi automated results extraction in production, suggesting our improvements make this task practically viable for the first time. Code is available on GitHub. 1 Back translation. . ."},{"ID":"kennedy-etal-2017-technology","methods":["classification","harassment dataset"],"center_method":["classification",null],"tasks":["identify online harassment"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Technology Solutions to Combat Online Harassment","abstract_clean":"This work is part of a new initiative to use machine learning to identify online harassment in social media and comment streams. Online harassment goes underreported due to the reliance on humans to identify and report harassment, reporting that is further slowed by requirements to fill out forms providing context. In addition, the time for moderators to respond and apply human judgment can take days, but response times in terms of minutes are needed in the online context. Though some of the major social media companies have been doing proprietary work in automating the detection of harassment, there are few tools available for use by the public. In addition, the amount of labeled online harassment data and availability of cross platform online harassment datasets is limited. We present the methodology used to create a harassment dataset and classifier and the dataset used to help the system learn what harassment looks like."},{"ID":"khalifa-etal-2016-utahbmi","methods":["conditional random field","support vector machine"],"center_method":["conditional random field","support vector machine"],"tasks":["extracting temporal information from clinical text"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"UtahBMI at SemEval 2016 Task 12: Extracting Temporal Information from Clinical Text","abstract_clean":"The 2016 Clinical TempEval continued the 2015 shared task on temporal information extraction with a new evaluation test set. Our team, UtahBMI, participated in all subtasks using machine learning approaches with ClearTK (LIBLINEAR), CRF++ and CRFsuite packages. Our experiments show that CRF based classifiers yield, in general, higher recall for multi word spans, while SVM based classifiers are better at predicting correct attributes of TIMEX3. In addition, we show that an ensemble based approach for TIMEX3 could yield improved results. Our team achieved competitive results in each subtask with an F1 75.4% for TIMEX3, F1 89.2% for EVENT, F1 84.4% for event relations with document time (DocTimeRel), and F1 51.1% for narrative container (CONTAINS) relations."},{"ID":"khanna-etal-2022-idiap","methods":["bert"],"center_method":["bert"],"tasks":["hope speech detection"],"center_task":["hope speech detection"],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"IDIAP\\_TIET@LT EDI ACL2022 : Hope Speech Detection in Social Media using Contextualized BERT with Attention Mechanism","abstract_clean":"With the increase of users on social media platforms, manipulating or provoking masses of people has become a piece of cake. This spread of hatred among people, which has become a loophole for freedom of speech, must be minimized. Hence, it is essential to have a system that automatically classifies the hatred content, especially on social media, to take it down. This paper presents a simple modular pipeline classifier with BERT embeddings and attention mechanism to classify hope speech content in the Hope Speech Detection shared task for Equality, Diversity, and Inclusion ACL 2022. Our system submission ranks fourth with an F1 score of 0.84. We release our code base here https: \/\/github.com\/Deepanshu beep\/ hope speech attention."},{"ID":"kim-park-2015-statistical","methods":["statistical modeling"],"center_method":[null],"tasks":["premise testing"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"A Statistical Modeling of the Correlation between Island Effects and Working memory Capacity for L2 Learners","abstract_clean":"The cause of island effects has evoked considerable debate within syntax and other fields of linguistics. The two competing approaches stand out: the grammatical analysis; and the working memory (WM) based processing analysis. In this paper we report three experiments designed to test one of the premises of the WM based processing analysis: that the strength of island effects should vary as a function of individual differences in WM capacity. The results show that island effects present even for L2 learners are more likely attributed to grammatical constraints than to limited processing resources."},{"ID":"kinnunen-etal-2012-swan","methods":["scientific writing assistant","rule - based system"],"center_method":[null,null],"tasks":["identify and correct potential writing problems"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"SWAN  Scientific Writing AssistaNt. A Tool for Helping Scholars to Write Reader Friendly Manuscripts","abstract_clean":"Difficulty of reading scholarly papers is significantly reduced by reader friendly writing principles. Writing reader friendly text, however, is challenging due to difficulty in recognizing problems in one's own writing. To help scholars identify and correct potential writing problems, we introduce SWAN (Scientific Writing AssistaNt) tool. SWAN is a rule based system that gives feedback based on various quality metrics based on years of experience from scientific writing classes including 960 scientists of various backgrounds: life sciences, engineering sciences and economics. According to our first experiences, users have perceived SWAN as helpful in identifying problematic sections in text and increasing overall clarity of manuscripts."},{"ID":"kiritchenko-cherry-2011-lexically","methods":["hidden markov models"],"center_method":["hidden markov models"],"tasks":["document coding"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Lexically Triggered Hidden Markov Models for Clinical Document Coding","abstract_clean":"The automatic coding of clinical documents is an important task for today's healthcare providers. Though it can be viewed as multi label document classification, the coding problem has the interesting property that most code assignments can be supported by a single phrase found in the input document. We propose a Lexically Triggered Hidden Markov Model (LT HMM) that leverages these phrases to improve coding accuracy. The LT HMM works in two stages: first, a lexical match is performed against a term dictionary to collect a set of candidate codes for a document. Next, a discriminative HMM selects the best subset of codes to assign to the document by tagging candidates as present or absent. By confirming codes proposed by a dictionary, the LT HMM can share features across codes, enabling strong performance even on rare codes. In fact, we are able to recover codes that do not occur in the training set at all. Our approach achieves the best ever performance on the 2007 Medical NLP Challenge test set, with an F measure of 89.84."},{"ID":"kirk-etal-2021-memes","methods":["evaluation methods"],"center_method":["evaluation methods"],"tasks":["assessing the generalizability","hateful meme detection"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Memes in the Wild: Assessing the Generalizability of the Hateful Memes Challenge Dataset","abstract_clean":"Hateful memes pose a unique challenge for current machine learning systems because their message is derived from both text and visual modalities. To this effect, Facebook released the Hateful Memes Challenge, a dataset of memes with pre extracted text captions, but it is unclear whether these synthetic examples generalize to 'memes in the wild'. In this paper, we collect hateful and non hateful memes from Pinterest to evaluate out of sample performance on models pre trained on the Facebook dataset. We find that memes in the wild differ in two key aspects: 1) Captions must be extracted via OCR, injecting noise and diminishing performance of multimodal models, and 2) Memes are more diverse than 'traditional memes', including screenshots of conversations or text on a plain background. This paper thus serves as a reality check for the current benchmark of hateful meme detection and its applicability for detecting real world hate."},{"ID":"koeva-etal-2020-natural","methods":["web crawling"],"center_method":[null],"tasks":["annotate bulgarian legislative documents"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Natural Language Processing Pipeline to Annotate Bulgarian Legislative Documents","abstract_clean":"The paper presents the Bulgarian MARCELL corpus, part of a recently developed multilingual corpus representing the national legislation in seven European countries and the NLP pipeline that turns the web crawled data into structured, linguistically annotated dataset. The Bulgarian data is web crawled, extracted from the original HTML format, filtered by document type, tokenised, sentence split, tagged and lemmatised with a fine grained version of the Bulgarian Language Processing Chain, dependency parsed with NLP Cube, annotated with named entities (persons, locations, organisations and others), noun phrases, IATE terms and EuroVoc descriptors. An orchestrator process has been developed to control the NLP pipeline performing an end to end data processing and annotation starting from the documents identification and ending in the generation of statistical reports. The Bulgarian MARCELL corpus consists of 25,283 documents (at the beginning of November 2019), which are classified into eleven types."},{"ID":"kontos-etal-2000-arista","methods":["generative lexicon"],"center_method":[null],"tasks":["compound greek medical terms"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"ARISTA Generative Lexicon for Compound Greek Medical Terms","abstract_clean":"A Generative Lexicon for Compound Greek Medical Terms based on the ARISTA method is proposed in this paper. The concept of a representation independent definition generating lexicon for compound words is introduced in this paper following the ARISTA method. This concept is used as a basis for developing a generative lexicon of Greek compound medical terminology using the senses of their component words expressed in natural language and not in a formal language. A Prolog program that was implemented for this task is presented that is capable of computing implicit relations between the components words in a sublanguage using linguistic and extra linguistic knowledge. An extra linguistic knowledge base containing knowledge derived from the domain or microcosm of the sublanguage is used for supporting the computation of the implicit relations. The performance of the system was evaluated by generating possible senses of the compound words automatically and judging the correctness of the results by comparing them with definitions given in a medical lexicon expressed in the language of the lexicographer."},{"ID":"koulierakis-etal-2020-recognition","methods":["sequential models"],"center_method":[null],"tasks":["recognition of static features in sign language"],"center_task":[null],"Goal":["Reduced Inequalities"],"title_clean":"Recognition of Static Features in Sign Language Using Key Points","abstract_clean":"In this paper we report on a research effort focusing on recognition of static features of sign formation in single sign videos. Three sequential models have been developed for handshape, palm orientation and location of sign formation respectively, which make use of key points extracted via OpenPose software. The models have been applied to a Danish and a Greek Sign Language dataset, providing results around 96%. Moreover, during the reported research, a method has been developed for identifying the time frame of real signing in the video, which allows to ignore transition frames during sign recognition processing."},{"ID":"kulkarni-etal-2018-annotated","methods":["data collection"],"center_method":["data collection"],"tasks":["machine reading of instructions in wet lab protocols"],"center_task":[null],"Goal":["Industry, Innovation and Infrastructure"],"title_clean":"An Annotated Corpus for Machine Reading of Instructions in Wet Lab Protocols","abstract_clean":"We describe an effort to annotate a corpus of natural language instructions consisting of 622 wet lab protocols to facilitate automatic or semi automatic conversion of protocols into a machine readable format and benefit biological research. Experimental results demonstrate the utility of our corpus for developing machine learning approaches to shallow semantic parsing of instructional texts. We make our annotated Wet Lab Protocol Corpus available to the research community. 1 1 The dataset is available on the authors' websites."},{"ID":"kurniawan-etal-2020-ir3218","methods":["lstm","word embeddings"],"center_method":["lstm","word embeddings"],"tasks":["toxicity detection","automatic categorization of offense types","offense target identification"],"center_task":["toxicity detection",null,null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"IR3218 UI at SemEval 2020 Task 12: Emoji Effects on Offensive Language IdentifiCation","abstract_clean":"In this paper, we present our approach and the results of our participation in OffensEval 2020. There are three sub tasks in OffensEval 2020, namely offensive language identification (sub task A), automatic categorization of offense types (sub task B), and offense target identification (subtask C). We participated in sub task A of English OffensEval 2020. Our approach emphasizes on how the emoji affects offensive language identification. Our model used LSTM combined with GloVe pre trained word vectors to identify offensive language on social media. The best model obtained macro F1 score of 0.88428."},{"ID":"kurtic-etal-2012-corpus","methods":["data collection","audio and video recordings"],"center_method":["data collection",null],"tasks":["multi - party conversation"],"center_task":[null],"Goal":["Partnership for the Goals"],"title_clean":"A Corpus of Spontaneous Multi party Conversation in Bosnian Serbo Croatian and British English","abstract_clean":"In this paper we present a corpus of audio and video recordings of spontaneous, face to face multi party conversation in two languages. Freely available high quality recordings of mundane, non institutional, multi party talk are still sparse, and this corpus aims to contribute valuable data suitable for study of multiple aspects of spoken interaction. In particular, it constitutes a unique resource for spoken Bosnian Serbo Croatian (BSC), an under resourced language with no spoken resources available at present. The corpus consists of just over 3 hours of free conversation in each of the target languages, BSC and British English (BE). The audio recordings have been made on separate channels using head set microphones, as well as using a microphone array, containing 8 omni directional microphones. The data has been segmented and transcribed using segmentation notions and transcription conventions developed from those of the conversation analysis research tradition. Furthermore, the transcriptions have been automatically aligned with the audio at the word and phone level, using the method of forced alignment. In this paper we describe the procedures behind the corpus creation and present the main features of the corpus for the study of conversation."},{"ID":"lai-etal-2016-better","methods":["embedding"],"center_method":[null],"tasks":["combining language and social interactions"],"center_task":[null],"Goal":["Partnership for the Goals"],"title_clean":"Better Together: Combining Language and Social Interactions into a Shared Representation","abstract_clean":"Despite the clear inter dependency between analyzing the interactions in social networks, and analyzing the natural language content of these interactions, these aspects are typically studied independently. In this paper we present a first step towards finding a joint representation, by embedding the two aspects into a single vector space. We show that the new representation can help improve performance in two social relations prediction tasks."},{"ID":"lai-etal-2021-joint","methods":["attention","graph neural networks"],"center_method":["attention","graph neural networks"],"tasks":["biomedical entity and relation extraction"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Joint Biomedical Entity and Relation Extraction with Knowledge Enhanced Collective Inference","abstract_clean":"Compared to the general news domain, information extraction (IE) from biomedical text requires much broader domain knowledge. However, many previous IE methods do not utilize any external knowledge during inference. Due to the exponential growth of biomedical publications, models that do not go beyond their fixed set of parameters will likely fall behind. Inspired by how humans look up relevant information to comprehend a scientific text, we present a novel framework that utilizes external knowledge for joint entity and relation extraction named KECI (Knowledge Enhanced Collective Inference). Given an input text, KECI first constructs an initial span graph representing its initial understanding of the text. It then uses an entity linker to form a knowledge graph containing relevant background knowledge for the the entity mentions in the text. To make the final predictions, KECI fuses the initial span graph and the knowledge graph into a more refined graph using an attention mechanism. KECI takes a collective approach to link mention spans to entities by integrating global relational information into local representations using graph convolutional networks. Our experimental results show that the framework is highly effective, achieving new state of theart results in two different benchmark datasets: BioRelEx (binding interaction detection) and ADE (adverse drug event extraction). For example, KECI achieves absolute improvements of 4.59% and 4.91% in F1 scores over the stateof the art on the BioRelEx entity and relation extraction tasks 1 ."},{"ID":"langer-schulder-2020-collocations","methods":["collocation analysis"],"center_method":[null],"tasks":["word sense discrimination"],"center_task":[null],"Goal":["Reduced Inequalities"],"title_clean":"Collocations in Sign Language Lexicography: Towards Semantic Abstractions for Word Sense Discrimination","abstract_clean":"In general monolingual lexicography a corpus based approach to word sense discrimination (WSD) is the current standard. Automatically generated lexical profiles such as Word Sketches provide an overview on typical uses in the form of collocate lists grouped by their part of speech categories and their syntactic dependency relations to the base item. Collocates are sorted by their typicality according to frequency based rankings. With the advancement of sign language (SL) corpora, SL lexicography can finally be based on actual language use as reflected in corpus data. In order to use such data effectively and gain new insights on sign usage, automatically generated collocation profiles need to be developed under the special conditions and circumstances of the SL data available. One of these conditions is that many of the prerequesites for the automatic syntactic parsing of corpora are not yet available for SL. In this article we describe a collocation summary generated from DGS Corpus data which is used for WSD as well as in entry writing. The summary works based on the glosses used for lemmatisation. In addition, we explore how other resources can be utilised to add an additional layer of semantic grouping to the collocation analysis. For this experimental approach we use glosses, concepts, and wordnet supersenses."},{"ID":"lee-etal-2004-analysis","methods":["automatic speech recognition"],"center_method":["automatic speech recognition"],"tasks":["detection of reading miscues"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Analysis and Detection of Reading Miscues for Interactive Literacy Tutors","abstract_clean":"The Colorado Literacy Tutor (CLT) is a technology based literacy program, designed on the basis of cognitive theory and scientifically motivated reading research, which aims to improve literacy and student achievement in public schools. One of the critical components of the CLT is a speech recognition system which is used to track the child's progress during oral reading and to provide sufficient information to detect reading miscues. In this paper, we extend on prior work by examining a novel labeling of children's oral reading audio data in order to better understand the factors that contribute most significantly to speech recognition errors. While these events make up nearly 8% of the data, they are shown to account for approximately 30% of the word errors in a state of the art speech recognizer. Next, we consider the problem of detecting miscues during oral reading. Using features derived from the speech recognizer, we demonstrate that 67% of reading miscues can be detected at a false alarm rate of 3%."},{"ID":"lee-etal-2021-unifying","methods":[null],"center_method":[null],"tasks":["misinformation detection"],"center_task":["misinformation detection"],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"On Unifying Misinformation Detection","abstract_clean":"In this paper, we introduce UNIFIEDM2, a general purpose misinformation model that jointly models multiple domains of misinformation with a single, unified setup. The model is trained to handle four tasks: detecting news bias, clickbait, fake news and verifying rumors. By grouping these tasks together, UNIFIEDM2 learns a richer representation of misinformation, which leads to stateof the art or comparable performance across all tasks. Furthermore, we demonstrate that UNIFIEDM2's learned representation is helpful for few shot learning of unseen misinformation tasks\/datasets and model's generalizability to unseen events. * Work partially done while interning at Facebook AI. \u2020 Work partially done while working at Facebook AI."},{"ID":"lemon-liu-2006-dude","methods":["dialogue and understanding development environment"],"center_method":[null],"tasks":["mapping business process models"],"center_task":[null],"Goal":["Industry, Innovation and Infrastructure"],"title_clean":"DUDE: A Dialogue and Understanding Development Environment, Mapping Business Process Models to Information State Update Dialogue Systems","abstract_clean":"We demonstrate a new development environment 1 \"Information State Update\" dialogue systems which allows non expert developers to produce complete spoken dialogue systems based only on a Business Process Model (BPM) describing their application (e.g. banking, cinema booking, shopping, restaurant information). The environment includes automatic generation of Grammatical Framework (GF) grammars for robust interpretation of spontaneous speech, and uses application databases to generate lexical entries and grammar rules. The GF grammar is compiled to an ATK or Nuance language model for speech recognition. The demonstration system allows users to create and modify spoken dialogue systems, starting with a definition of a Business Process Model and ending with a working system. This paper describes the environment, its main components, and some of the research issues involved in its development."},{"ID":"levi-etal-2019-identifying","methods":["contextual language model"],"center_method":[null],"tasks":["identifying nuances in fake news vs . satire"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Identifying Nuances in Fake News vs. Satire: Using Semantic and Linguistic Cues","abstract_clean":"The blurry line between nefarious fake news and protected speech satire has been a notorious struggle for social media platforms. Further to the efforts of reducing exposure to misinformation on social media, purveyors of fake news have begun to masquerade as satire sites to avoid being demoted. In this work, we address the challenge of automatically classifying fake news versus satire. Previous work have studied whether fake news and satire can be distinguished based on language differences. Contrary to fake news, satire stories are usually humorous and carry some political or social message. We hypothesize that these nuances could be identified using semantic and linguistic cues. Consequently, we train a machine learning method using semantic representation, with a state of the art contextual language model, and with linguistic features based on textual coherence metrics. Empirical evaluation attests to the merits of our approach compared to the language based baseline and sheds light on the nuances between fake news and satire. As avenues for future work, we consider studying additional linguistic features related to the humor aspect, and enriching the data with current news events, to help identify a political or social message."},{"ID":"li-caragea-2021-target","methods":["data augmentation"],"center_method":["data augmentation"],"tasks":["stance detection"],"center_task":["stance detection"],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Target Aware Data Augmentation for Stance Detection","abstract_clean":"The goal of stance detection is to identify whether the author of a text is in favor of, neutral or against a specific target. Despite substantial progress on this task, one of the remaining challenges is the scarcity of annotations. Data augmentation is commonly used to address annotation scarcity by generating more training samples. However, the augmented sentences that are generated by existing methods are either less diversified or inconsistent with the given target and stance label. In this paper, we formulate the data augmentation of stance detection as a conditional masked language modeling task and augment the dataset by predicting the masked word conditioned on both its context and the auxiliary sentence that contains target and label information. Moreover, we propose another simple yet effective method that generates target aware sentence by replacing a target mention with the other. Experimental results show that our proposed methods significantly outperforms previous augmentation methods on 11 targets."},{"ID":"li-etal-2019-detecting","methods":["transfer learning"],"center_method":["transfer learning"],"tasks":["detecting dementia"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Detecting dementia in Mandarin Chinese using transfer learning from a parallel corpus","abstract_clean":"Machine learning has shown promise for automatic detection of Alzheimer's disease (AD) through speech; however, efforts are hampered by a scarcity of data, especially in languages other than English. We propose a method to learn a correspondence between independently engineered lexicosyntactic features in two languages, using a large parallel corpus of outof domain movie dialogue data. We apply it to dementia detection in Mandarin Chinese, and demonstrate that our method outperforms both unilingual and machine translation based baselines. This appears to be the first study that transfers feature domains in detecting cognitive decline."},{"ID":"li-etal-2020-adviser","methods":["toolkit"],"center_method":[null],"tasks":["developing multi - modal , multi - domain and socially - engaged conversational agents"],"center_task":[null],"Goal":["Industry, Innovation and Infrastructure"],"title_clean":"ADVISER: A Toolkit for Developing Multi modal, Multi domain and Socially engaged Conversational Agents","abstract_clean":"We present ADVISER 1 an open source, multi domain dialog system toolkit that enables the development of multi modal (incorporating speech, text and vision), sociallyengaged (e.g. emotion recognition, engagement level prediction and backchanneling) conversational agents. The final Python based implementation of our toolkit is flexible, easy to use, and easy to extend not only for technically experienced users, such as machine learning researchers, but also for less technically experienced users, such as linguists or cognitive scientists, thereby providing a flexible platform for collaborative research."},{"ID":"li-etal-2020-multi-task","methods":["multi - task shared structure encoding approach"],"center_method":[null],"tasks":["peer - review score prediction"],"center_task":[null],"Goal":["Industry, Innovation and Infrastructure"],"title_clean":"Multi task Peer Review Score Prediction","abstract_clean":"Automatic prediction of the peer review aspect scores of academic papers can be a useful assistant tool for both reviewers and authors. To handle the small size of published datasets on the target aspect of scores, we propose a multi task approach to leverage additional information from other aspects of scores for improving the performance of the target aspect. Because one of the problems of building multi task models is how to select the proper resources of auxiliary tasks and how to select the proper shared structures, we thus propose a multi task shared structure encoding approach that automatically selects good shared network structures as well as good auxiliary resources. The experiments based on peer review datasets show that our approach is effective and has better performance on the target scores than the single task method and na\u00efve multi task methods."},{"ID":"li-etal-2022-gpt","methods":["gpt - 2","language models"],"center_method":[null,"language models"],"tasks":["inducing dementia - related linguistic anomalies"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"GPT D: Inducing Dementia related Linguistic Anomalies by Deliberate Degradation of Artificial Neural Language Models","abstract_clean":"Deep learning (DL) techniques involving finetuning large numbers of model parameters have delivered impressive performance on the task of discriminating between language produced by cognitively healthy individuals, and those with Alzheimer's disease (AD). However, questions remain about their ability to generalize beyond the small reference sets that are publicly available for research. As an alternative to fitting model parameters directly, we propose a novel method by which a Transformer DL model (GPT 2) pre trained on general English text is paired with an artificially degraded version of itself (GPT D), to compute the ratio between these two models' perplexities on language from cognitively healthy and impaired individuals. This technique approaches state ofthe art performance on text data from a widely used \"Cookie Theft\" picture description task, and unlike established alternatives also generalizes well to spontaneous conversations. Furthermore, GPT D generates text with characteristics known to be associated with AD, demonstrating the induction of dementia related linguistic anomalies. Our study is a step toward better understanding of the relationships between the inner workings of generative neural language models, the language that they produce, and the deleterious effects of dementia on human speech and language characteristics."},{"ID":"li-hovy-2014-sentiment","methods":["semi - supervised bootstrapping algorithm","hierarchical bayesian model"],"center_method":[null,null],"tasks":["sentiment analysis"],"center_task":["sentiment analysis"],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Sentiment Analysis on the People's Daily","abstract_clean":"We propose a semi supervised bootstrapping algorithm for analyzing China's foreign relations from the People's Daily. Our approach addresses sentiment target clustering, subjective lexicons extraction and sentiment prediction in a unified framework. Different from existing algorithms in the literature, time information is considered in our algorithm through a hierarchical bayesian model to guide the bootstrapping approach. We are hopeful that our approach can facilitate quantitative political analysis conducted by social scientists and politicians."},{"ID":"liang-etal-2012-expert","methods":["machine intelligence"],"center_method":[null],"tasks":["misinformation detection"],"center_task":["misinformation detection"],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Expert Finding for Microblog Misinformation Identification","abstract_clean":"The growth of social media provides a convenient communication scheme for people, but at the same time it becomes a hotbed of misinformation. The wide spread of misinformation over social media is injurious to public interest. We design a framework, which integrates collective intelligence and machine intelligence, to help identify misinformation. The basic idea is: (1) automatically index the expertise of users according to their microblog contents; and (2) match the experts with given suspected misinformation. By sending the suspected misinformation to appropriate experts, we can collect the assessments of experts to judge the credibility of information, and help refute misinformation. In this paper, we focus on expert finding for misinformation identification. We propose a tag based method to index the expertise of microblog users with social tags. Experiments on a real world dataset demonstrate the effectiveness of our method for expert finding with respect to misinformation identification in microblogs."},{"ID":"liang-etal-2021-evaluation","methods":["annotation","strategy classifier"],"center_method":["annotation",null],"tasks":["evaluation of in - person counseling strategies"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Evaluation of In Person Counseling Strategies To Develop Physical Activity Chatbot for Women","abstract_clean":"Artificial intelligence chatbots are the vanguard in technology based intervention to change people's behavior. To develop intervention chatbots, the first step is to understand natural language conversation strategies in human conversation. This work introduces an intervention conversation dataset collected from a real world physical activity intervention program for women. We designed comprehensive annotation schemes in four dimensions (domain, strategy, social exchange, and taskfocused exchange) and annotated a subset of dialogs. We built a strategy classifier with context information to detect strategies from both trainers and participants based on the annotation. To understand how human intervention induces effective behavior changes, we analyzed the relationships between the intervention strategies and the participants' changes in the barrier and social support for physical activity. We also analyzed how participant's baseline weight correlates to the amount of occurrence of the corresponding strategy. This work lays the foundation for developing a personalized physical activity intervention bot. 1"},{"ID":"lin-etal-2012-online","methods":["ensemble methods","lexical , syntactic and semantic features"],"center_method":["ensemble methods",null],"tasks":["online plagiarized detection"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Online Plagiarized Detection Through Exploiting Lexical, Syntax, and Semantic Information","abstract_clean":"In this paper, we introduce a framework that identifies online plagiarism by exploiting lexical, syntactic and semantic features that includes duplication gram, reordering and alignment of words, POS and phrase tags, and semantic similarity of sentences. We establish an ensemble framework to combine the predictions of each model. Results demonstrate that our system can not only find considerable amount of real world online plagiarism cases but also outperforms several state of the art algorithms and commercial software."},{"ID":"lin-etal-2019-enhancing","methods":["dataset","attention"],"center_method":[null,"attention"],"tasks":["symptom diagnosis"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Enhancing Dialogue Symptom Diagnosis with Global Attention and Symptom Graph","abstract_clean":"Symptom diagnosis is a challenging yet profound problem in natural language processing. Most previous research focus on investigating the standard electronic medical records for symptom diagnosis, while the dialogues between doctors and patients that contain more rich information are not well studied. In this paper, we first construct a dialogue symptom diagnosis dataset based on an online medical forum with a large amount of dialogues between patients and doctors. Then, we provide some benchmark models on this dataset to boost the research of dialogue symptom diagnosis. In order to further enhance the performance of symptom diagnosis over dialogues, we propose a global attention mechanism to capture more symptom related information, and build a symptom graph to model the associations between symptoms rather than treating each symptom independently. Experimental results show that both the global attention and symptom graph are effective to boost dialogue symptom diagnosis. In particular, our proposed model achieves the state of the art performance on the constructed dataset."},{"ID":"litman-2012-cohesion","methods":["corpus - based measures"],"center_method":[null],"tasks":["educational dialog"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Cohesion, Entrainment and Task Success in Educational Dialog","abstract_clean":"Researchers often study dialog corpora to better understand what makes some dialogs more successful than others. In this talk I will examine the relationship between coherence\/entrainment and task success, in several types of educational dialog corpora: 1) one on one tutoring, where students use dialog to interact with a human tutor in the physics domain, 2) one on one tutoring, where students instead interact with a spoken dialog system, and 3) engineering design, where student teams engage in multi party dialog to complete a group project. I will first introduce several corpus based measures of both lexical and acousticprosodic dialog cohesion and entrainment, and extend them to handle multi party conversations. I will then show that the amount of cohesion and\/or entrainment positively correlates with measures of educational task success in all of our corpora. Finally, I will discuss how we are using our findings to build better tutorial dialog systems."},{"ID":"loukina-etal-2015-feature","methods":["feature engineering","shrinkage methods","lasso regression"],"center_method":["feature engineering",null,null],"tasks":["automated speech scoring"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Feature selection for automated speech scoring","abstract_clean":"Automated scoring systems used for the evaluation of spoken or written responses in language assessments need to balance good empirical performance with the interpretability of the scoring models. We compare several methods of feature selection for such scoring systems and show that the use of shrinkage methods such as Lasso regression makes it possible to rapidly build models that both satisfy the requirements of validity and intepretability, crucial in assessment contexts as well as achieve good empirical performance."},{"ID":"lu-chu-2013-evaluation","methods":["questionnaire"],"center_method":[null],"tasks":["evaluation of corpus"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Evaluation of Corpus Assisted Spanish Learning","abstract_clean":"In the development of corpus linguistics, the creation of corpora has had a critical role in corpus based studies. The majority of created corpora have been associated with English and native languages, while other languages and types of corpora have received relatively less attention. Because an increasing number of corpora have been constructed, and each corpus is constructed for a definite purpose, this study identifies the functions of corpora and combines the values of various types of corpora for auto learning based on the existing corpora. Specifically, the following three corpora are adopted: (a) the Corpus of Spanish; (b) the Corpus of Taiwanese Learners of Spanish; and (c) the Parallel Corpus of Spanish, English, and Chinese. These corpora represent a type of native, learner, and parallel language, respectively. We apply these corpora as auxiliary resources to identify the advantages of applying various types of corpora in language learning from a learner's perspective. In the environment of auto learning, 28 participants completed frequency questions related to semantic and lexical aspects. After analyzing the questionnaire data, we obtained the following findings: (a) the native corpus requires a more advanced level of Spanish proficiency to manage ampler and deeper context; (b) the learners' corpus facilitates the distinction between error and correction during the learning process; (c) the parallel corpus assists learners in connecting form and meaning; (d) learning is more efficient if the learner can capitalizes on specific functions provided by various corpora in the application order of parallel, learner and native corpora."},{"ID":"lu-etal-2021-parameter-efficient","methods":["adapters","language models"],"center_method":[null,"language models"],"tasks":["domain knowledge integration"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Parameter Efficient Domain Knowledge Integration from Multiple Sources for Biomedical Pre trained Language Models","abstract_clean":"Domain specific pre trained language models (PLMs) have achieved great success over various downstream tasks in different domains. However, existing domain specific PLMs mostly rely on self supervised learning over large amounts of domain text, without explicitly integrating domain specific knowledge, which can be essential in many domains. Moreover, in knowledge sensitive areas such as the biomedical domain, knowledge is stored in multiple sources and formats, and existing biomedical PLMs either neglect them or utilize them in a limited manner. In this work, we introduce an architecture to integrate domain knowledge from diverse sources into PLMs in a parameter efficient way. More specifically, we propose to encode domain knowledge via adapters, which are small bottleneck feed forward networks inserted between intermediate transformer layers in PLMs. These knowledge adapters are pre trained for individual domain knowledge sources and integrated via an attention based knowledge controller to enrich PLMs. Taking the biomedical domain as a case study, we explore three knowledge specific adapters for PLMs based on the UMLS Metathesaurus graph, the Wikipedia articles for diseases, and the semantic grouping information for biomedical concepts. Extensive experiments on different biomedical NLP tasks and datasets demonstrate the benefits of the proposed architecture and the knowledge specific adapters across multiple PLMs."},{"ID":"ma-etal-2017-detect","methods":["kernel learning","propagation trees"],"center_method":[null,null],"tasks":["identifying rumors"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Detect Rumors in Microblog Posts Using Propagation Structure via Kernel Learning","abstract_clean":"How fake news goes viral via social media? How does its propagation pattern differ from real stories? In this paper, we attempt to address the problem of identifying rumors, i.e., fake information, out of microblog posts based on their propagation structure. We firstly model microblog posts diffusion with propagation trees, which provide valuable clues on how an original message is transmitted and developed over time. We then propose a kernel based method called Propagation Tree Kernel, which captures high order patterns differentiating different types of rumors by evaluating the similarities between their propagation tree structures. Experimental results on two real world datasets demonstrate that the proposed kernel based approach can detect rumors more quickly and accurately than state ofthe art rumor detection models."},{"ID":"maegaard-etal-2008-medar","methods":[null],"center_method":[null],"tasks":["collaboration"],"center_task":[null],"Goal":["Partnership for the Goals"],"title_clean":"MEDAR: Collaboration between European and Mediterranean Arabic Partners to Support the Development of Language Technology for Arabic","abstract_clean":"After the successful completion of the NEMLAR project 2003 2005, a new opportunity for a project was opened by the European Commission, and a group of largely the same partners is now executing the MEDAR project. MEDAR will be updating the surveys and BLARK for Arabic already made, and will then focus on machine translation (and other tools for translation) and information retrieval with a focus on language resources, tools and evaluation for these applications. A very important part of the MEDAR project is to reinforce and extend the NEMLAR network and to create a cooperation roadmap for Human Language Technologies for Arabic. It is expected that the cooperation roadmap will attract wide attention from other parties and that it can help create a larger platform for collaborative projects. Finally, the project will focus on dissemination of knowledge about existing resources and tools, as well as actors and activities; this will happen through newsletter, website and an international conference which will follow up on the Cairo conference of 2004. Dissemination to user communities will also be important, e.g. through participation in translators' conferences. The goal of these activities is to create a stronger and lasting collaboration between EU countries and Arabic speaking countries."},{"ID":"maldonado-harabagiu-2020-language","methods":["annotation"],"center_method":["annotation"],"tasks":["electroencephalography reports"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"The Language of Brain Signals: Natural Language Processing of Electroencephalography Reports","abstract_clean":"Brain signals are captured by clinical electroencephalography (EEG) which is an excellent tool for probing neural function. When EEG tests are performed, a textual EEG report is generated by the neurologist to document the findings, thus using language that describes the brain signals and their clinical correlations. Even with the impetus provided by the BRAIN initiative (brainitititive.nih.gov), there are no annotations available in texts that use natural language describing the brain activities and their correlations with various pathologies. In this paper we describe an annotation effort carried out on a large corpus of EEG reports, providing examples of EEG specific and clinically relevant concepts. In addition, we detail our annotation schema for brain signal attributes. We also discuss the resulting annotation of long distance relations between concepts in EEG reports. By exemplifying a self attention joint learning method used to predict concept, attribute and relation annotations in the EEG report corpus, we discuss the promising results of automatic annotations, hoping that our effort will inform the design of novel knowledge capture techniques that will include the language of brain signals."},{"ID":"marinelli-etal-2008-encoding","methods":["methodology report"],"center_method":[null],"tasks":["enhancing a maritime terminological database"],"center_task":[null],"Goal":["Industry, Innovation and Infrastructure"],"title_clean":"Encoding Terms from a Scientific Domain in a Terminological Database: Methodology and Criteria","abstract_clean":"This paper reports on the main phases of a research which aims at enhancing a maritime terminological database by means of a set of terms belonging to meteorology. The structure of the terminological database, according to EuroWordNet\/ItalWordNet model is described; the criteria used to build corpora of specialized texts are explained as well as the use of the corpora as source for term selection and extraction. The contribution of the semantic databases is taken into account: on the one hand, the most recent version of the Princeton WordNet has been exploited as reference for comparing and evaluating synsets; on the other hand, the Italian WordNet has been employed as source for exporting synsets to be coded in the terminological resource. The set of semantic relations useful to codify new terms belonging to the discipline of meteorology is examined, revising the semantic relations provided by the IWN model, introducing new relations which are more suitably tailored to specific requirements either scientific or pragmatic. The need for a particular relation is highlighted to represent the mental association which is made when a term intuitively recalls another term, but they are neither synonyms nor connected by means of a hyperonymy\/hyponymy relation."},{"ID":"marx-schuth-2010-dutchparl","methods":[null],"center_method":[null],"tasks":["data collection"],"center_task":["data collection"],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"DutchParl. The Parliamentary Documents in Dutch","abstract_clean":"A corpus called DutchParl is created which aims to contain all digitally available parliamentary documents written in the Dutch language. The first version of DutchParl contains documents from the parliaments of The Netherlands, Flanders and Belgium. The corpus is divided along three dimensions: per parliament, scanned or digital documents, written recordings of spoken text and others. The digital collection contains more than 800 million tokens, the scanned collection more than 1 billion. All documents are available as UTF 8 encoded XML files with extensive metadata in Dublin Core standard. The text itself is divided into pages which are divided into paragraphs. Every document, page and paragraph has a unique URN which resolves to a web page. Every page element in the XML files is connected to a facsimile image of that page in PDF or JPEG format. We created a viewer in which both versions can be inspected simultaneously. The corpus is available for download in several formats. The corpus can be used for corpus linguistic and political science research, and is suitable for performing scalability tests for XML information systems."},{"ID":"maxwelll-smith-etal-2020-applications","methods":["automated speech recognition","data collection"],"center_method":[null,"data collection"],"tasks":["bilingual language teaching"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Applications of Natural Language Processing in Bilingual Language Teaching: An Indonesian English Case Study","abstract_clean":"Multilingual corpora are difficult to compile and a classroom setting adds pedagogy to the mix of factors which make this data so rich and problematic to classify. In this paper, we set out methodological considerations of using automated speech recognition to build a corpus of teacher speech in an Indonesian language classroom. Our preliminary results (64% word error rate) suggest these tools have the potential to speed data collection in this context. We provide practical examples of our data structure, details of our piloted computer assisted processes, and fine grained error analysis. Our study is informed and directed by genuine research questions and discussion in both the education and computational linguistics fields. We highlight some of the benefits and risks of using these emerging technologies to analyze the complex work of language teachers and in education more generally."},{"ID":"mayfield-black-2019-stance","methods":["bert","word embeddings"],"center_method":["bert","word embeddings"],"tasks":["stance detection","studying group decision - making"],"center_task":["stance detection",null],"Goal":["Partnership for the Goals"],"title_clean":"Stance Classification, Outcome Prediction, and Impact Assessment: NLP Tasks for Studying Group Decision Making","abstract_clean":"In group decision making, the nuanced process of conflict and resolution that leads to consensus formation is closely tied to the quality of decisions made. Behavioral scientists rarely have rich access to process variables, though, as unstructured discussion transcripts are difficult to analyze. Here, we define ways for NLP researchers to contribute to the study of groups and teams. We introduce three tasks alongside a large new corpus of over 400,000 group debates on Wikipedia. We describe the tasks and their importance, then provide baselines showing that BERT contextualized word embeddings consistently outperform other language representations."},{"ID":"mcinnes-2008-unsupervised","methods":["unsupervised vector approach"],"center_method":[null],"tasks":["biomedical term disambiguation"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"An Unsupervised Vector Approach to Biomedical Term Disambiguation: Integrating UMLS and Medline","abstract_clean":"This paper introduces an unsupervised vector approach to disambiguate words in biomedical text that can be applied to all word disambiguation. We explore using contextual information from the Unified Medical Language System (UMLS) to describe the possible senses of a word. We experiment with automatically creating individualized stoplists to help reduce the noise in our dataset. We compare our results to SenseClusters and Humphrey et al. (2006) using the NLM WSD dataset and with SenseClusters using conflated data from the 2005 Medline Baseline."},{"ID":"medina-maza-etal-2020-event","methods":["adversarial neural model"],"center_method":[null],"tasks":["event - related bias removal"],"center_task":[null],"Goal":["Sustainable Cities and Communities"],"title_clean":"Event Related Bias Removal for Real time Disaster Events","abstract_clean":"Social media has become an important tool to share information about crisis events such as natural disasters and mass attacks. Detecting actionable posts that contain useful information requires rapid analysis of huge volume of data in real time. This poses a complex problem due to the large amount of posts that do not contain any actionable information. Furthermore, the classification of information in real time systems requires training on outof domain data, as we do not have any data from a new emerging crisis. Prior work focuses on models pre trained on similar event types. However, those models capture unnecessary event specific biases, like the location of the event, which affect the generalizability and performance of the classifiers on new unseen data from an emerging new event. In our work, we train an adversarial neural model to remove latent event specific biases and improve the performance on tweet importance classification."},{"ID":"meng-etal-2018-automatic","methods":["recurrent neural networks","sentence embeddings","lstm","conditional random field"],"center_method":["recurrent neural networks",null,"lstm","conditional random field"],"tasks":["automatic labeling"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Automatic Labeling of Problem Solving Dialogues for Computational Microgenetic Learning Analytics","abstract_clean":"This paper presents a recurrent neural network model to automate the analysis of students' computational thinking in problem solving dialogue. We have collected and annotated dialogue transcripts from middle school students solving a robotics challenge, and each dialogue turn is assigned a code. We use sentence embeddings and speaker identities as features, and experiment with linear chain CRFs and RNNs with a CRF layer (LSTM CRF). Both the linear chain CRF model and the LSTM CRF model outperform the na\u00efve baselines by a large margin, and LSTM CRF has an edge between the two. To our knowledge, this is the first study on dialogue segment annotation using neural network models. This study is also a stepping stone to automating the microgenetic analysis of cognitive interactions between students."},{"ID":"meyer-gamback-2019-platform","methods":["convolutional neural network","long short - term memory - networks","ngrams","word embeddings"],"center_method":["convolutional neural network",null,"ngrams","word embeddings"],"tasks":["toxicity detection"],"center_task":["toxicity detection"],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"A Platform Agnostic Dual Strand Hate Speech Detector","abstract_clean":"Hate speech detectors must be applicable across a multitude of services and platforms, and there is hence a need for detection approaches that do not depend on any information specific to a given platform. For instance, the information stored about the text's author may differ between services, and so using such data would reduce a system's general applicability. The paper thus focuses on using exclusively text based input in the detection, in an optimised architecture combining Convolutional Neural Networks and Long Short Term Memory networks. The hate speech detector merges two strands with character ngrams and word embeddings to produce the final classification, and is shown to outperform comparable previous approaches."},{"ID":"micklesen-smith-1963-algorithm","methods":[null],"center_method":[null],"tasks":[null],"center_task":[null],"Goal":["Industry, Innovation and Infrastructure"],"title_clean":"An algorithm for the translation of Russian inorganic chemistry terms","abstract_clean":null},{"ID":"mim-etal-2019-unsupervised","methods":["unsupervised algorithm"],"center_method":["unsupervised algorithm"],"tasks":["essay scoring"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Unsupervised Learning of Discourse Aware Text Representation for Essay Scoring","abstract_clean":"Existing document embedding approaches mainly focus on capturing sequences of words in documents. However, some document classification and regression tasks such as essay scoring need to consider discourse structure of documents. Although some prior approaches consider this issue and utilize discourse structure of text for document classification, these approaches are dependent on computationally expensive parsers. In this paper, we propose an unsupervised approach to capture discourse structure in terms of coherence and cohesion for document embedding that does not require any expensive parser or annotation. Extrinsic evaluation results show that the document representation obtained from our approach improves the performance of essay Organization scoring and Argument Strength scoring."},{"ID":"minematsu-etal-2002-english","methods":["english speech database"],"center_method":[null],"tasks":["speech applications"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"English Speech Database Read by Japanese Learners for CALL System Development","abstract_clean":"With the help of recent advances in speech processing techniques, we can see various kinds of practical speech applications in both laboratories and the real world. One of the major applications in Japan is CALL (Computer Assisted Language Learning) systems. It is well known that most of the recent speech technologies are based upon statistical methods, which require a large amount of speech data. Although we can find many speech corpora available from distribution sites such as Linguistic Data Consortium, European Language Resources Association, and so on, the number of speech corpora built especially for CALL system development is very small. In this paper, we firstly introduce a Japanese national project of \"Advanced Utilization of Multimedia to Promote Higher Educational Reform,\" under which some research groups are currently developing CALL systems. One of the main objectives of the project is to construct an English speech database read by Japanese students for CALL system development. This paper describes specification of the database and strategies adopted to select speakers and record their sentence\/word utterances in addition to preliminary discussions and investigations done before the database development. Further, by using the new database and WSJ database, corpus based analysis and comparison between Japanese English and American English is done in view of the entire phonemic system of English. Here, tree diagrams of the two kinds of English are drawn through their HMM sets. Results show many interesting characteristics of Japanese English."},{"ID":"mitrovic-etal-2019-nlpup","methods":["convolutional neural network","recurrent neural networks","word2vec"],"center_method":["convolutional neural network","recurrent neural networks","word2vec"],"tasks":["toxicity detection"],"center_task":["toxicity detection"],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"nlpUP at SemEval 2019 Task 6: A Deep Neural Language Model for Offensive Language Detection","abstract_clean":"This paper presents our submission for the SemEval shared task 6, sub task A on the identification of offensive language. Our proposed model, C BiGRU, combines a Convolutional Neural Network (CNN) with a bidirectional Recurrent Neural Network (RNN). We utilize word2vec to capture the semantic similarities between words. This composition allows us to extract long term dependencies in tweets and distinguish between offensive and non offensive tweets. In addition, we evaluate our approach on a different dataset and show that our model is capable of detecting online aggressiveness in both English and German tweets. Our model achieved a macro F1 score of 79.40% on the SemEval dataset."},{"ID":"moradi-etal-2014-graph","methods":["semantic - based method","graph - based analysis"],"center_method":[null,null],"tasks":["health information seeking"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"A Graph Based Analysis of Medical Queries of a Swedish Health Care Portal","abstract_clean":"Today web portals play an increasingly important role in health care allowing information seekers to learn about diseases and treatments, and to administrate their care. Therefore, it is important that the portals are able to support this process as well as possible. In this paper, we study the search logs of a public Swedish health portal to address the questions if health information seeking differs from other types of Internet search and if there is a potential for utilizing network analysis methods in combination with semantic annotation to gain insights into search behaviors. Using a semantic based method and a graph based analysis of word cooccurrences in queries, we show there is an overlap among the results indicating a potential role of these types of methods to gain insights and facilitate improved information search. In addition we show that samples, windows of a month, of search logs may be sufficient to obtain similar results as using larger windows. We also show that medical queries share the same structural properties found for other types of information searches, thereby indicating an ability to re use existing analysis methods for this type of search data."},{"ID":"morante-etal-2020-annotating","methods":["data collection"],"center_method":["data collection"],"tasks":["annotating perspectives on vaccination"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Annotating Perspectives on Vaccination","abstract_clean":"In this paper we present the Vaccination Corpus, a corpus of texts related to the online vaccination debate that has been annotated with three layers of information about perspectives: attribution, claims and opinions. Additionally, events related to the vaccination debate are also annotated. The corpus contains 294 documents from the Internet which reflect different views on vaccinations. It has been compiled to study the language of online debates, with the final goal of experimenting with methodologies to extract and contrast perspectives within the vaccination debate."},{"ID":"morgado-da-costa-etal-2016-syntactic","methods":["deep syntactic parsers","semantic based machine translation"],"center_method":[null,null],"tasks":["computer assisted language learning"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Syntactic Well Formedness Diagnosis and Error Based Coaching in Computer Assisted Language Learning using Machine Translation","abstract_clean":"We present a novel approach to Computer Assisted Language Learning (CALL), using deep syntactic parsers and semantic based machine translation (MT) in diagnosing and providing explicit feedback on language learners' errors. We are currently developing a proof of concept system showing how semantic based machine translation can, in conjunction with robust computational grammars, be used to interact with students, better understand their language errors, and help students correct their grammar through a series of useful feedback messages and guided language drills. Ultimately, we aim to prove the viability of a new integrated rule based MT approach to disambiguate students intended meaning in a CALL system. This is a necessary step to provide accurate coaching on how to correct ungrammatical input, and it will allow us to overcome a current bottleneck in the field an exponential burst of ambiguity caused by ambiguous lexical items (Flickinger, 2010). From the users interaction with the system, we will also produce a richly annotated Learner Corpus, annotated automatically with both syntactic and semantic information."},{"ID":"mosallanezhad-etal-2019-deep","methods":["deep reinforcement learning"],"center_method":[null],"tasks":["text anonymization"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Deep Reinforcement Learning based Text Anonymization against Private Attribute Inference","abstract_clean":"User generated textual data is rich in content and has been used in many user behavioral modeling tasks. However, it could also leak user private attribute information that they may not want to disclose such as age and location. User's privacy concerns mandate data publishers to protect privacy. One effective way is to anonymize the textual data. In this paper, we study the problem of textual data anonymization and propose a novel Reinforcement Learning based Text Anonymizor, RLTA, which addresses the problem of private attribute leakage while preserving the utility of textual data. Our approach first extracts a latent representation of the original text w.r.t. a given task, then leverages deep reinforcement learning to automatically learn an optimal strategy for manipulating text representations w.r.t. the received privacy and utility feedback. Experiments show the effectiveness of this approach in terms of preserving both privacy and utility."},{"ID":"moser-moore-1995-investigating","methods":["coding scheme"],"center_method":[null],"tasks":["investigating cue selection"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Investigating Cue Selection and Placement in Tutorial Discourse","abstract_clean":"Our goal is to identify the features that predict cue selection and placement in order to devise strategies for automatic text generation. Much previous work in this area has relied on ad hoc methods. Our coding scheme for the exhaustive analysis of discourse allows a systematic evaluation and refinement of hypotheses concerning cues. We report two results based on this analysis: a comparison of the distribution of Sn~CE and BECAUSE in our corpus, and the impact of embeddedness on cue selection."},{"ID":"mueller-etal-2008-knowledge","methods":["knowledge sources"],"center_method":[null],"tasks":["bridging resolution in multi - party dialog"],"center_task":[null],"Goal":["Partnership for the Goals"],"title_clean":"Knowledge Sources for Bridging Resolution in Multi Party Dialog","abstract_clean":"In this paper we investigate the coverage of the two knowledge sources WordNet and Wikipedia for the task of bridging resolution. We report on an annotation experiment which yielded pairs of bridging anaphors and their antecedents in spoken multi party dialog. Manual inspection of the two knowledge sources showed that, with some interesting exceptions, Wikipedia is superior to WordNet when it comes to the coverage of information necessary to resolve the bridging anaphors in our data set. We further describe a simple procedure for the automatic extraction of the required knowledge from Wikipedia by means of an API, and discuss some of the implications of the procedure's performance."},{"ID":"murakami-etal-2009-annotating","methods":["japanese corpus","annotation"],"center_method":[null,"annotation"],"tasks":["annotating semantic relations"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Annotating Semantic Relations Combining Facts and Opinions","abstract_clean":"As part of the STATEMENT MAP project, we are constructing a Japanese corpus annotated with the semantic relations bridging facts and opinions that are necessary for online information credibility evaluation. In this paper, we identify the semantic relations essential to this task and discuss how to efficiently collect valid examples from Web documents by splitting complex sentences into fundamental units of meaning called \"statements\" and annotating relations at the statement level. We present a statement annotation scheme and examine its reliability by annotating around 1,500 pairs of statements. We are preparing the corpus for release this winter."},{"ID":"muti-barron-cedeno-2022-checkpoint","methods":["transfer learning","monolingual transformers","transformers","bert"],"center_method":["transfer learning",null,"transformers","bert"],"tasks":["misogyny identification"],"center_task":[null],"Goal":["Gender Equality"],"title_clean":"A Checkpoint on Multilingual Misogyny Identification","abstract_clean":"We address the problem of identifying misogyny in tweets in mono and multilingual settings in three languages: English, Italian and Spanish. We explore model variations considering single and multiple languages both in the pre training of the transformer and in the training of the downstream task to explore the feasibility of detecting misogyny through a transfer learning approach across multiple languages. That is, we train monolingual transformers with monolingual data and multilingual transformers with both monolingual and multilingual data. Our models reach state of the art performance on all three languages. The single language BERT models perform the best, closely followed by different configurations of multilingual BERT models. The performance drops in zero shot classification across languages. Our error analysis shows that multilingual and monolingual models tend to make the same mistakes."},{"ID":"nagata-2019-toward","methods":["corpora","neural retrievalbased method"],"center_method":[null,null],"tasks":["feedback comment generation"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Toward a Task of Feedback Comment Generation for Writing Learning","abstract_clean":"In this paper, we introduce a novel task called feedback comment generation a task of automatically generating feedback comments such as a hint or an explanatory note for writing learning for non native learners of English. There has been almost no work on this task nor corpus annotated with feedback comments. We have taken the first step by creating learner corpora consisting of approximately 1,900 essays where all preposition errors are manually annotated with feedback comments. We have tested three baseline methods on the dataset, showing that a simple neural retrievalbased method sets a baseline performance with an F measure of 0.34 to 0.41. Finally, we have looked into the results to explore what modifications we need to make to achieve better performance. We also have explored problems unaddressed in this work."},{"ID":"nayak-etal-2017-v","methods":["flashcard application","grading algorithm"],"center_method":[null,null],"tasks":["learning new words"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"V for Vocab: An Intelligent Flashcard Application","abstract_clean":"Students choose to use flashcard applications available on the Internet to help memorize word meaning pairs. This is helpful for tests such as GRE, TOEFL or IELTS, which emphasize on verbal skills. However, monotonous nature of flashcard applications can be diminished with the help of Cognitive Science through Testing Effect. Experimental evidences have shown that memory tests are an important tool for long term retention (Roediger and Karpicke, 2006). Based on these evidences, we developed a novel flashcard application called \"V for Vocab\" that implements short answer based tests for learning new words. Furthermore, we aid this by implementing our short answer grading algorithm which automatically scores the user's answer. The algorithm makes use of an alternate thesaurus instead of traditional Wordnet and delivers state of theart performance on popular word similarity datasets. We also look to lay the foundation for analysis based on implicit data collected from our application."},{"ID":"neubig-etal-2011-safety","methods":["system to mine information"],"center_method":[null],"tasks":["safety information mining"],"center_task":[null],"Goal":["Sustainable Cities and Communities"],"title_clean":"Safety Information Mining   What can NLP do in a disaster  ","abstract_clean":"This paper describes efforts of NLP researchers to create a system to aid the relief efforts during the 2011 East Japan Earthquake. Specifically, we created a system to mine information regarding the safety of people in the disaster stricken area from Twitter, a massive yet highly unorganized information source. We describe the large scale collaborative effort to rapidly create robust and effective systems for word segmentation, named entity recognition, and tweet classification. As a result of our efforts, we were able to effectively deliver new information about the safety of over 100 people in the disasterstricken area to a central repository for safety information."},{"ID":"nguyen-2019-question","methods":["question answering"],"center_method":["question answering"],"tasks":["question answering"],"center_task":["question answering"],"Goal":["Good Health and Well-Being"],"title_clean":"Question Answering in the Biomedical Domain","abstract_clean":"Question answering techniques have mainly been investigated in open domains. However, there are particular challenges in extending these open domain techniques to extend into the biomedical domain. Question answering focusing on patients is less studied. We find that there are some challenges in patient question answering such as limited annotated data, lexical gap and quality of answer spans. We aim to address some of these gaps by extending and developing upon the literature to design a question answering system that can decide on the most appropriate answers for patients attempting to self diagnose while including the ability to abstain from answering when confidence is low."},{"ID":"nguyen-etal-2019-anu","methods":["deep contextual knowledge"],"center_method":[null],"tasks":["question answering"],"center_task":["question answering"],"Goal":["Good Health and Well-Being"],"title_clean":"ANU CSIRO at MEDIQA 2019: Question Answering Using Deep Contextual Knowledge","abstract_clean":"We report on our system for textual inference and question entailment in the medical domain for the ACL BioNLP 2019 Shared Task, MEDIQA. Textual inference is the task of finding the semantic relationships between pairs of text. Question entailment involves identifying pairs of questions which have similar semantic content. To improve upon medical natural language inference and question entailment approaches to further medical question answering, we propose a system that incorporates open domain and biomedical domain approaches to improve semantic understanding and ambiguity resolution. Our models achieve 80% accuracy on medical natural language inference (6.5% absolute improvement over the original baseline), 48.9% accuracy on recognising medical question entailment, 0.248 Spearman's rho for question answering ranking and 68.6% accuracy for question answering classification."},{"ID":"nina-alcocer-2019-haterecognizer","methods":["neural network","data augmentation"],"center_method":["neural network","data augmentation"],"tasks":["hate recognition"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"HATERecognizer at SemEval 2019 Task 5: Using Features and Neural Networks to Face Hate Recognition","abstract_clean":"This paper presents a detailed description of our participation in task 5 on SemEval 2019 1. This task consists of classifying English and Spanish tweets that contain hate towards women or immigrants. We carried out several experiments; for a finer grained study of the task, we analyzed different features and designing architectures of neural networks. Additionally, to face the lack of hate content in tweets, we include data augmentation as a technique to increase hate content in our datasets."},{"ID":"noh-etal-2011-pomy","methods":["conversational virtual environment"],"center_method":[null],"tasks":["language learning"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"POMY: A Conversational Virtual Environment for Language Learning in POSTECH","abstract_clean":"This demonstration will illustrate an interactive immersive computer game, POMY, designed to help Korean speakers learn English. This system allows learners to exercise their visual and aural senses, receiving a full immersion experience to increase their memory and concentration abilities to a greatest extent. In POMY, learners can have free conversations with game characters and receive corrective feedback to their errors. Game characters show various emotional expressions based on learners' input to keep learners motivated. Through this system, learners can repeatedly practice conversations in everyday life setting in a foreign language with no embarrassment."},{"ID":"nomoto-2016-neal","methods":["tfidf","neural network","embedding model"],"center_method":[null,"neural network",null],"tasks":["linking citation and reference"],"center_task":[null],"Goal":["Industry, Innovation and Infrastructure"],"title_clean":"NEAL: A Neurally Enhanced Approach to Linking Citation and Reference","abstract_clean":"As a way to tackle Task 1A in CL SciSumm 2016, we introduce a composite model consisting of TFIDF and Neural Network (NN), the latter being a adaptation of the embedding model originally proposed for the Q\/A domain [2, 7]. We discuss an experiment using a development data, results thereof, and some remaining issues."},{"ID":"nothdurft-etal-2014-probabilistic","methods":["webbased study"],"center_method":[null],"tasks":["probabilistic human - computer trust handling"],"center_task":[null],"Goal":["Decent Work and Economic Growth"],"title_clean":"Probabilistic Human Computer Trust Handling","abstract_clean":"Human computer trust has shown to be a critical factor in influencing the complexity and frequency of interaction in technical systems. Particularly incomprehensible situations in human computer interaction may lead to a reduced users trust in the system and by that influence the style of interaction. Analogous to human human interaction, explaining these situations can help to remedy negative effects. In this paper we present our approach of augmenting task oriented dialogs with selected explanation dialogs to foster the humancomputer trust relationship in those kinds of situations. We have conducted a webbased study testing the effects of different goals of explanations on the components of human computer trust. Subsequently, we show how these results can be used in our probabilistic trust handling architecture to augment pre defined task oriented dialogs."},{"ID":"oard-2007-invited","methods":["automatic speech recognition"],"center_method":["automatic speech recognition"],"tasks":["improve intellectual access"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Invited Talk: Lessons from the MALACH Project: Applying New Technologies to Improve Intellectual Access to Large Oral History Collections","abstract_clean":"In this talk I will describe the goals of the MALACH project (Multilingual Access to Large Spoken Archives) and our research results. I'll begin by describing the unique characteristics of the oral history collection that we used, in which Holocaust survivors, witnesses and rescuers were interviewed in several languages. Each interview has been digitized and extensively catalogued by subject matter experts, thus producing a remarkably rich collection for the application of machine learning techniques. Automatic speech recognition techniques originally developed for the domain of conversational telephone speech were adapted to process these materials with word error rates that are adequate to provide useful features to support interactive search and automated clustering, boundary detection, and topic classification tasks. As I describe our results, I will focus particularly on the evaluation methods that that we have used to assess the potential utility of this technology. I'll conclude with some remarks about possible future directions for research on applying new technologies to improve intellectual access to oral history and other spoken word collections."},{"ID":"oboronko-2000-wired","methods":["web - based translation"],"center_method":[null],"tasks":["continuous communication"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Wired for peace and multi language communication","abstract_clean":"Our project Wired for Peace: Virtual Diplomacy in Northeast Asia (Http:\/\/wwwneacd.ucsd.edu\/) has as its main aim to provide policymakers and researchers of the U.S., China, Russia, Japan, and Korea with Internet based tools to allow for continuous communication on issues of the regional security and cooperation. Since the very beginning of the project, we have understood that Web based translation between English and Asian languages would be one of the most necessary tools for successful development of the project. With this understanding, we have partnered with Systran (www.systransoft.com), one of the leaders in MT field, in order to develop Internet based tools for both synchronous and asynchronous translation of texts and discussions. This submission is a report on a work in progress."},{"ID":"osborne-etal-2014-real","methods":[null],"center_method":[null],"tasks":["event extraction"],"center_task":["event extraction"],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Real Time Detection, Tracking, and Monitoring of Automatically Discovered Events in Social Media","abstract_clean":"We introduce ReDites, a system for realtime event detection, tracking, monitoring and visualisation. It is designed to assist Information Analysts in understanding and exploring complex events as they unfold in the world. Events are automatically detected from the Twitter stream. Then those that are categorised as being security relevant are tracked, geolocated, summarised and visualised for the end user. Furthermore, the system tracks changes in emotions over events, signalling possible flashpoints or abatement. We demonstrate the capabilities of ReDites using an extended use case from the September 2013 Westgate shooting incident. Through an evaluation of system latencies, we also show that enriched events are made available for users to explore within seconds of that event occurring."},{"ID":"panicheva-etal-2010-personal","methods":["model combination"],"center_method":[null],"tasks":["authorship attribution","opinion analysis"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Personal Sense and Idiolect: Combining Authorship Attribution and Opinion Analysis","abstract_clean":"Subjectivity analysis and authorship attribution are very popular areas of research. However, work in these two areas has been done separately. Our conjecture is that by combining information about subjectivity in texts and authorship, the performance of both tasks can be improved. In the paper a personalized approach to opinion mining is presented, in which the notions of personal sense and idiolect are introduced; the approach is applied to the polarity classification task. It is assumed that different authors express their private states in text individually, and opinion mining results could be improved by analyzing texts by different authors separately. The hypothesis is tested on a corpus of movie reviews by ten authors. The results of applying the personalized approach to opinion mining are presented, confirming that the approach increases the performance of the opinion mining task. Automatic authorship attribution is further applied to model the personalized approach, classifying documents by their assumed authorship. Although the automatic authorship classification imposes a number of limitations on the dataset for further experiments, after overcoming these issues the authorship attribution technique modeling the personalized approach confirms the increase over the baseline with no authorship information used."},{"ID":"perez-miguel-etal-2018-biomedical","methods":["multilingual compendium","web - based interface"],"center_method":[null,null],"tasks":["biomedical term normalization"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Biomedical term normalization of EHRs with UMLS","abstract_clean":"This paper presents a novel prototype for biomedical term normalization of electronic health record excerpts with the Unified Medical Language System (UMLS) Metathesaurus, a large, multilingual compendium of biomedical and health related terminologies. Despite the prototype being multilingual and cross lingual by design, we first focus on processing clinical text in Spanish because there is no existing tool for this language and for this specific purpose. The tool is based on Apache Lucene TM to index the Metathesaurus and generate mapping candidates from input text. It uses the IXA pipeline for basic language processing and resolves lexical ambiguities with the UKB toolkit. It has been evaluated by measuring its agreement with MetaMap a mature software to discover UMLS concepts in English texts in two English Spanish parallel corpora. In addition, we present a web based interface for the tool."},{"ID":"perez-rosas-etal-2014-multimodal","methods":["multimodal dataset"],"center_method":[null],"tasks":["deception detection"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"A Multimodal Dataset for Deception Detection","abstract_clean":"This paper presents the construction of a multimodal dataset for deception detection, including physiological, thermal, and visual responses of human subjects under three deceptive scenarios. We present the experimental protocol, as well as the data acquisition process. To evaluate the usefulness of the dataset for the task of deception detection, we present a statistical analysis of the physiological and thermal modalities associated with the deceptive and truthful conditions. Initial results show that physiological and thermal responses can differentiate between deceptive and truthful states."},{"ID":"perez-rosas-mihalcea-2014-cross","methods":["classification","crowdsourcing"],"center_method":["classification","crowdsourcing"],"tasks":["deception detection"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Cross cultural Deception Detection","abstract_clean":"In this paper, we address the task of cross cultural deception detection. Using crowdsourcing, we collect three deception datasets, two in English (one originating from United States and one from India), and one in Spanish obtained from speakers from Mexico. We run comparative experiments to evaluate the accuracies of deception classifiers built for each culture, and also to analyze classification differences within and across cultures. Our results show that we can leverage cross cultural information, either through translation or equivalent semantic categories, and build deception classifiers with a performance ranging between 60 70%."},{"ID":"pergola-etal-2021-boosting","methods":["entity - aware masking","language models"],"center_method":[null,"language models"],"tasks":["biomedical question - answering"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Boosting Low Resource Biomedical QA via Entity Aware Masking Strategies","abstract_clean":"Biomedical question answering (QA) has gained increased attention for its capability to provide users with high quality information from a vast scientific literature. Although an increasing number of biomedical QA datasets has been recently made available, those resources are still rather limited and expensive to produce. Transfer learning via pre trained language models (LMs) has been shown as a promising approach to leverage existing general purpose knowledge. However, finetuning these large models can be costly and time consuming, often yielding limited benefits when adapting to specific themes of specialised domains, such as the COVID 19 literature. To bootstrap further their domain adaptation, we propose a simple yet unexplored approach, which we call biomedical entity aware masking (BEM). We encourage masked language models to learn entity centric knowledge based on the pivotal entities characterizing the domain at hand, and employ those entities to drive the LM fine tuning. The resulting strategy is a downstream process applicable to a wide variety of masked LMs, not requiring additional memory or components in the neural architectures. Experimental results show performance on par with state of the art models on several biomedical QA datasets."},{"ID":"pokkunuri-etal-2011-role","methods":["classification","lexico - syntactic patterns"],"center_method":["classification",null],"tasks":["design of a document triage application for biocuration"],"center_task":[null],"Goal":["Industry, Innovation and Infrastructure"],"title_clean":"The Role of Information Extraction in the Design of a Document Triage Application for Biocuration","abstract_clean":"Traditionally, automated triage of papers is performed using lexical (unigram, bigram, and sometimes trigram) features. This paper explores the use of information extraction (IE) techniques to create richer linguistic features than traditional bag of words models. Our classifier includes lexico syntactic patterns and more complex features that represent a pattern coupled with its extracted noun, represented both as a lexical term and as a semantic category. Our experimental results show that the IE based features can improve performance over unigram and bigram features alone. We present intrinsic evaluation results of full text document classification experiments to determine automatically whether a paper should be considered of interest to biologists at the Mouse Genome Informatics (MGI) system at the Jackson Laboratories. We also further discuss issues relating to design and deployment of our classifiers as an application to support scientific knowledge curation at MGI."},{"ID":"polat-saraclar-2020-unsupervised","methods":["unsupervised term discovery"],"center_method":[null],"tasks":["continuous sign language"],"center_task":[null],"Goal":["Reduced Inequalities"],"title_clean":"Unsupervised Term Discovery for Continuous Sign Language","abstract_clean":"Most of the sign language recognition (SLR) systems rely on supervision for training and available annotated sign language resources are scarce due to the difficulties of manual labeling. Unsupervised discovery of lexical units would facilitate the annotation process and thus lead to better SLR systems. Inspired by the unsupervised spoken term discovery in speech processing field, we investigate whether a similar approach can be applied in sign language to discover repeating lexical units. We adapt an algorithm that is designed for spoken term discovery by using hand shape and pose features instead of speech features. The experiments are run on a large scale continuous sign corpus and the performance is evaluated using gloss level annotations. This work introduces a new task for sign language processing that has not been addressed before."},{"ID":"pontiki-etal-2020-verbal","methods":[null],"center_method":[null],"tasks":["replication of a data - driven and linguistically inspired verbal aggression analysis"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Verbal Aggression as an Indicator of Xenophobic Attitudes in Greek Twitter during and after the Financial Crisis","abstract_clean":"We present a replication of a data driven and linguistically inspired Verbal Aggression analysis framework that was designed to examine Twitter verbal attacks against predefined target groups of interest as an indicator of xenophobic attitudes during the financial crisis in Greece, in particular during the period 2013 2016. The research goal in this paper is to reexamine Verbal Aggression as an indicator of xenophobic attitudes in Greek Twitter three years later, in order to trace possible changes regarding the main t argets, the types and the content of the verbal attacks against the same targets in the post crisis era, given also the ongoing refugee crisis and the political landscape in Greece as it was shaped after the elections in 2019. The results indicate an interesting rearrangement of the main targets of the verbal attacks, while the content and the types of the attacks provide valuable insights about the way these targets are being framed as compared to the respective dominant perceptions and stereotypes about them during the period 2013 2016."},{"ID":"prasad-etal-2020-opinion","methods":["opinion mining system"],"center_method":[null],"tasks":["processing hindi text for home remedies"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Opinion Mining System for Processing Hindi Text for Home Remedies Domain","abstract_clean":"Lexical and computational components developed for an Opinion Mining System that process Hindi text taken from weblogs are presented in the paper. Text chosen for processing are the ones demonstrating cause and effect relationship between related entities 'Food' and 'Health Issues'. The work is novel and lexical resources developed are useful in the current research and may be of importance for future research."},{"ID":"prost-etal-2019-debiasing","methods":["word embeddings"],"center_method":["word embeddings"],"tasks":["occupation classification","reduce bias"],"center_task":[null,null],"Goal":["Gender Equality"],"title_clean":"Debiasing Embeddings for Reduced Gender Bias in Text Classification","abstract_clean":"Bolukbasi et al., 2016) demonstrated that pretrained word embeddings can inherit gender bias from the data they were trained on. We investigate how this bias affects downstream classification tasks, using the case study of occupation classification (De Arteaga et al., 2019). We show that traditional techniques for debiasing embeddings can actually worsen the bias of the downstream classifier by providing a less noisy channel for communicating gender information. With a relatively minor adjustment, however, we show how these same techniques can be used to simultaneously reduce bias and maintain high classification accuracy."},{"ID":"pyysalo-etal-2007-unification","methods":["conversion from the link grammar to the stanford scheme"],"center_method":[null],"tasks":["unification of syntactic annotations"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"On the unification of syntactic annotations under the Stanford dependency scheme: A case study on BioInfer and GENIA","abstract_clean":"Several incompatible syntactic annotation schemes are currently used by parsers and corpora in biomedical information extraction. The recently introduced Stanford dependency scheme has been suggested to be a suitable unifying syntax formalism. In this paper, we present a step towards such unification by creating a conversion from the Link Grammar to the Stanford scheme. Further, we create a version of the BioInfer corpus with syntactic annotation in this scheme. We present an application oriented evaluation of the transformation and assess the suitability of the scheme and our conversion to the unification of the syntactic annotations of BioInfer and the GENIA Treebank. We find that a highly reliable conversion is both feasible to create and practical, increasing the applicability of both the parser and the corpus to information extraction."},{"ID":"pyysalo-etal-2011-overview","methods":["analysis"],"center_method":["analysis"],"tasks":["infectious diseases ( id ) information extraction"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Overview of the Infectious Diseases (ID) task of BioNLP Shared Task 2011","abstract_clean":"This paper presents the preparation, resources, results and analysis of the Infectious Diseases (ID) information extraction task, a main task of the BioNLP Shared Task 2011. The ID task represents an application and extension of the BioNLP'09 shared task event extraction approach to full papers on infectious diseases. Seven teams submitted final results to the task, with the highest performing system achieving 56% F score in the full task, comparable to state of the art performance in the established BioNLP'09 task. The results indicate that event extraction methods generalize well to new domains and full text publications and are applicable to the extraction of events relevant to the molecular mechanisms of infectious diseases."},{"ID":"rajalakshmi-etal-2022-dlrg-tamilnlp","methods":["usingbilstm - crf","bidirectional long short - term memory","glove embedding"],"center_method":[null,null,null],"tasks":["offensive span identification"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"DLRG@TamilNLP ACL2022: Offensive Span Identification in Tamil usingBiLSTM CRF approach","abstract_clean":"Identifying offensive speech is an exciting and essential area of research, with ample traction in recent times. This paper presents our system submission to the subtask 1, focusing on using supervised approaches for extracting Offensive spans from code mixed Tamil English comments. To identify offensive spans, we developed the Bidirectional Long Short Term Memory (BiLSTM) model with Glove Embedding. With this method, the developed system achieved an overall F1 of 0.1728. Additionally, for comments with less than 30 characters, the developed system shows an F1 of 0.3890, competitive with other submissions."},{"ID":"raleigh-2020-keynote","methods":["limitations"],"center_method":[null],"tasks":["data collection"],"center_task":["data collection"],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Keynote Abstract: Too soon? The limitations of AI for event data","abstract_clean":"Not all conflict datasets offer equal levels of coverage, depth, use ability, and content. A review of the inclusion criteria, methodology, and sourcing of leading publicly available conflict datasets demonstrates that there are significant discrepancies in the output produced by ostensibly similar projects. This keynote will question the presumption of substantial overlap between datasets, and identify a number of important gaps left by deficiencies across core criteria for effective conflict data collection and analysis, including: Data Collection and Oversight : A rigorous, human coder is the best way to ensure reliable, consistent, and accurate events that are not false positives. Automated event data projects are still being refined and are not yet at the point where they can be used as accurate representations of reality. It is not appropriate to use these event datasets to present trends, maps, or distributions of violence in a state."},{"ID":"ramesh-anand-2020-outcomes","methods":[null],"center_method":[null],"tasks":[null],"center_task":[null],"Goal":["Gender Equality"],"title_clean":"Outcomes of coming out: Analyzing stories of LGBTQ+","abstract_clean":null},{"ID":"regalado-etal-2015-salinlahi","methods":["tutoring system","interactive learning environment"],"center_method":[null,null],"tasks":["teaching","grammar and sentence construction"],"center_task":[null,null],"Goal":["Quality Education"],"title_clean":"Salinlahi III: An Intelligent Tutoring System for Filipino Heritage Language Learners","abstract_clean":"Heritage language learners are learners of the primary language of their parents which they might have been exposed to but have not learned it as a language they can fluently use to communicate with other people. Salinlahi, an Interactive Learning Environment, was developed to teach these young Filipino heritage learners about basic Filipino vocabulary while Salinlahi II included a support for collaborative learning. With the aim of teaching learners with basic knowledge in Filipino we developed Salinlahi III to teach higher level lessons focusing on Filipino grammar and sentence construction. An internal evaluation of the system has shown that the user interface and feedback of the tutor was appropriate. Moreover, in an external evaluation of the system, experimental and controlled field tests were done and results showed that there is a positive learning gain after using the system."},{"ID":"rehm-etal-2019-developing","methods":["processing services"],"center_method":[null],"tasks":["developing and orchestrating a portfolio of natural legal language processing"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Developing and Orchestrating a Portfolio of Natural Legal Language Processing and Document Curation Services","abstract_clean":"We present a portfolio of natural legal language processing and document curation services currently under development in a collaborative European project. First, we give an overview of the project and the different use cases, while, in the main part of the article, we focus upon the 13 different processing services that are being deployed in different prototype applications using a flexible and scalable microservices architecture. Their orchestration is operationalised using a content and document curation workflow manager."},{"ID":"resnik-etal-2015-university","methods":[null],"center_method":[null],"tasks":["mental health classification"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"The University of Maryland CLPsych 2015 Shared Task System","abstract_clean":"The 2015 ACL Workshop on Computational Linguistics and Clinical Psychology included a shared task focusing on classification of a sample of Twitter users according to three mental health categories: users who have self reported a diagnosis of depression, users who have self reported a diagnosis of post traumatic stress disorder (PTSD), and control users who have done neither Coppersmith et al., 2014) . Like other shared tasks, the goal here was to assess the state of the art with regard to a challenging problem, to advance that state of the art, and to bring together and hopefully expand the community of researchers interested in solving it."},{"ID":"rinaldi-etal-2008-dependency","methods":["dependency parsing"],"center_method":["dependency parsing"],"tasks":["relation mining"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Dependency Based Relation Mining for Biomedical Literature","abstract_clean":"We describe techniques for the automatic detection of relationships among domain entities (e.g. genes, proteins, diseases) mentioned in the biomedical literature. Our approach is based on the adaptive selection of candidate interactions sentences, which are then parsed using our own dependency parser. Specific syntax based filters are used to limit the number of possible candidate interacting pairs. The approach has been implemented as a demonstrator over a corpus of 2000 richly annotated MedLine abstracts, and later tested by participation to a text mining competition. In both cases, the results obtained have proved the adequacy of the proposed approach to the task of interaction detection."},{"ID":"rio-2002-compiling","methods":["web site"],"center_method":[null],"tasks":["literary translation"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Compiling an Interactive Literary Translation Web Site for Education Purposes","abstract_clean":"The project under discussion represents an attempt to exploit the potential of web resources for higher education and, more particularly, on a domain (that of literary translation) which is traditionally considered not very much in relation to technology and computer science. Translation and Interpreting students at the Universidad de M\u00e1laga are offered the possibility to take an English Spanish Literary Translation module, which epitomises the need for debate in the field of Humanities. Sadly enough, implementation of course methodology is rendered very difficult or impossible owing to time restrictions and overcrowded classrooms. It is our contention that the setting up of a web site may solve some of these issues. We intend to provide both students and the literary translation aware Internet audience with an integrated, scalable, multifunctional debate forum. Project contents will include a detailed course description, relevant reference materials and interaction services (mailing list, debate forum and chat rooms). This is obviously without limitation, as the Forum is open to any other contents that users may consider necessary or convenient, with a view to a more interdisciplinary approach, further research on the field of Literary Translation and future developments within the project framework."},{"ID":"risch-etal-2021-toxic","methods":["data integration"],"center_method":[null],"tasks":["toxicity detection"],"center_task":["toxicity detection"],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Data Integration for Toxic Comment Classification: Making More Than 40 Datasets Easily Accessible in One Unified Format","abstract_clean":"With the rise of research on toxic comment classification, more and more annotated datasets have been released. The wide variety of the task (different languages, different labeling processes and schemes) has led to a large amount of heterogeneous datasets that can be used for training and testing very specific settings. Despite recent efforts to create web pages that provide an overview, most publications still use only a single dataset. They are not stored in one central database, they come in many different data formats and it is difficult to interpret their class labels and how to reuse these labels in other projects."},{"ID":"roh-etal-2008-recognizing","methods":["similarity table"],"center_method":[null],"tasks":["machine translation"],"center_task":["machine translation"],"Goal":["Industry, Innovation and Infrastructure"],"title_clean":"Recognizing Coordinate Structures for Machine Translation of English Patent Documents","abstract_clean":"Patent machine translation is one of main target areas of current practical MT systems. Patent documents have their own peculiar description style. Especially, abstracts or claims in patent documents are characterized by their long and complex syntactic structures, which are often caused by coordination. So, syntactic analysis of patent documents requires special treatment for coordination. This paper describes a method to deal with long sentences in patent documents by recognizing coordinate structures. Coordinate structures are recognized using a similarity table which reflects parallelism between conjuncts. Our method is applied to a practical MT system and improves its quality and efficiency."},{"ID":"roller-stevenson-2014-applying","methods":["distantly supervised"],"center_method":[null],"tasks":["relation extraction"],"center_task":["relation extraction"],"Goal":["Good Health and Well-Being"],"title_clean":"Applying UMLS for Distantly Supervised Relation Detection","abstract_clean":"This paper describes first results using the Unified Medical Language System (UMLS) for distantly supervised relation extraction. UMLS is a large knowledge base which contains information about millions of medical concepts and relations between them. Our approach is evaluated using existing relation extraction data sets that contain relations that are similar to some of those in UMLS."},{"ID":"rudinger-etal-2017-social","methods":["statistical analysis"],"center_method":[null],"tasks":["investigation of bias and stereotyping in nlp data"],"center_task":[null],"Goal":["Reduced Inequalities"],"title_clean":"Social Bias in Elicited Natural Language Inferences","abstract_clean":"We analyze the Stanford Natural Language Inference (SNLI) corpus in an investigation of bias and stereotyping in NLP data. The human elicitation protocol employed in the construction of the SNLI makes it prone to amplifying bias and stereotypical associations, which we demonstrate statistically (using pointwise mutual information) and with qualitative examples."},{"ID":"rudzewitz-etal-2018-generating","methods":[null],"center_method":[null],"tasks":["generating feedback"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Generating Feedback for English Foreign Language Exercises","abstract_clean":"While immediate feedback on learner language is often discussed in the Second Language Acquisition literature (e.g., Mackey 2006), few systems used in real life educational settings provide helpful, metalinguistic feedback to learners. In this paper, we present a novel approach leveraging task information to generate the expected range of well formed and ill formed variability in learner answers along with the required diagnosis and feedback. We combine this offline generation approach with an online component that matches the actual student answers against the pre computed hypotheses. The results obtained for a set of 33 thousand answers of 7th grade German high school students learning English show that the approach successfully covers frequent answer patterns. At the same time, paraphrases and meaning errors require a more flexible alignment approach, for which we are planning to complement the method with the CoMiC approach successfully used for the analysis of reading comprehension answers (Meurers et al., 2011)."},{"ID":"saha-etal-2018-leveraging","methods":["web based evidence"],"center_method":[null],"tasks":["drug information identification"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Leveraging Web Based Evidence Gathering for Drug Information Identification from Tweets","abstract_clean":"In this paper, we have explored web based evidence gathering and different linguistic features to automatically extract drug names from tweets and further classify such tweets into Adverse Drug Events or not. We have evaluated our proposed models with the dataset as released by the SMM4H workshop shared Task 1 and Task 3 respectively. Our evaluation results shows that the proposed model achieved good results, with Precision, Recall and F scores of 78.5%, 88% and 82.9% respectively for Task1 and 33.2%, 54.7% and 41.3% for Task3."},{"ID":"salawu-etal-2021-large","methods":["multi - label twitter dataset","transformer - based deep learning"],"center_method":[null,null],"tasks":["cyberbullying and online abuse detection"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"A Large Scale English Multi Label Twitter Dataset for Cyberbullying and Online Abuse Detection","abstract_clean":"In this paper, we introduce a new English Twitter based dataset for online abuse and cyberbullying detection. Comprising 62,587 tweets, this dataset was sourced from Twitter using specific query terms designed to retrieve tweets with high probabilities of various forms of bullying and offensive content, including insult, profanity, sarcasm, threat, porn and exclusion. Analysis performed on the dataset confirmed common cyberbullying themes reported by other studies and revealed interesting relationships between the classes. The dataset was used to train a number of transformer based deep learning models returning impressive results."},{"ID":"saunders-etal-2020-neural","methods":["schemes","word - level gender inflection tags","tagged coreference adaptation data"],"center_method":[null,null,null],"tasks":["machine translation"],"center_task":["machine translation"],"Goal":["Gender Equality"],"title_clean":"Neural Machine Translation Doesn't Translate Gender Coreference Right Unless You Make It","abstract_clean":"Neural Machine Translation (NMT) has been shown to struggle with grammatical gender that is dependent on the gender of human referents, which can cause gender bias effects. Many existing approaches to this problem seek to control gender inflection in the target language by explicitly or implicitly adding a gender feature to the source sentence, usually at the sentence level. In this paper we propose schemes for incorporating explicit word level gender inflection tags into NMT. We explore the potential of this gender inflection controlled translation when the gender feature can be determined from a human reference, or when a test sentence can be automatically gender tagged, assessing on English to Spanish and English to German translation. We find that simple existing approaches can over generalize a gender feature to multiple entities in a sentence, and suggest effective alternatives in the form of tagged coreference adaptation data. We also propose an extension to assess translations of gender neutral entities from English given a corresponding linguistic convention, such as a non binary inflection, in the target language."},{"ID":"sawhney-etal-2020-time","methods":["transformers"],"center_method":["transformers"],"tasks":["suicide ideation detection"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"A Time Aware Transformer Based Model for Suicide Ideation Detection on Social Media","abstract_clean":"Social media's ubiquity fosters a space for users to exhibit suicidal thoughts outside of traditional clinical settings. Understanding the build up of such ideation is critical for the identification of at risk users and suicide prevention. Suicide ideation is often linked to a history of mental depression. The emotional spectrum of a user's historical activity on social media can be indicative of their mental state over time. In this work, we focus on identifying suicidal intent in English tweets by augmenting linguistic models with historical context. We propose STATENet, a timeaware transformer based model for preliminary screening of suicidal risk on social media. STATENet outperforms competitive methods, demonstrating the utility of emotional and temporal contextual cues for suicide risk assessment. We discuss the empirical, qualitative, practical, and ethical aspects of STATENet for suicide ideation detection. 1"},{"ID":"schoene-etal-2019-dilated","methods":["lstm","attention"],"center_method":["lstm","attention"],"tasks":["classification of suicide notes"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Dilated LSTM with attention for Classification of Suicide Notes","abstract_clean":"In this paper we present a dilated LSTM with attention mechanism for document level classification of suicide notes, last statements and depressed notes. We achieve an accuracy of 87.34% compared to competitive baselines of 80.35% (Logistic Model Tree) and 82.27% (Bi directional LSTM with Attention). Furthermore, we provide an analysis of both the grammatical and thematic content of suicide notes, last statements and depressed notes. We find that the use of personal pronouns, cognitive processes and references to loved ones are most important. Finally, we show through visualisations of attention weights that the Dilated LSTM with attention is able to identify the same distinguishing features across documents as the linguistic analysis."},{"ID":"sedoc-ungar-2019-role","methods":["word embeddings"],"center_method":["word embeddings"],"tasks":["bias identification of contextualized word representations"],"center_task":[null],"Goal":["Reduced Inequalities"],"title_clean":"The Role of Protected Class Word Lists in Bias Identification of Contextualized Word Representations","abstract_clean":"Systemic bias in word embeddings has been widely reported and studied, and efforts made to debias them; however, new contextualized embeddings such as ELMo and BERT are only now being similarly studied. Standard debiasing methods require large, heterogeneous lists of target words to identify the \"bias subspace\". We show that using new contextualized word embeddings in conceptor debiasing allows us to more accurately debias word embeddings by breaking target word lists into more homogeneous subsets and then combining (\"Or'ing\") the debiasing conceptors of the different subsets."},{"ID":"sellami-etal-2013-exploiting","methods":["data collection"],"center_method":["data collection"],"tasks":["patent translation"],"center_task":[null],"Goal":["Industry, Innovation and Infrastructure"],"title_clean":"Exploiting multiple resources for Japanese to English patent translation","abstract_clean":"This paper describes the development of a Japanese to English translation system using multiple resources and NTCIR 10 Patent translation collection. The MT system is based on different training data, the Wiktionary as a bilingual dictionary and Moses decoder. Due to the lack of parallel data on the patent domain, additional training data of the general domain was extracted from Wikipedia. Experiments using NTCIR 10 Patent translation data collection showed an improvement of the BLEU score when using a 5 grams language model and when adding the data extracted from Wikipedia but no improvement when adding the Wiktionary."},{"ID":"senda-etal-2004-support","methods":["support system"],"center_method":[null],"tasks":["revising titles"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"A Support System for Revising Titles to Stimulate the Lay Reader's Interest in Technical Achievements","abstract_clean":"When we write a report or an explanation on a newly developed technology for readers including laypersons, it is very important to compose a title that can stimulate their interest in the technology. However, it is difficult for inexperienced authors to come up with an appealing title. In this research, we developed a support system for revising titles. We call it \"title revision wizard\". The wizard provides a guidance on revising draft title to compose a title meeting three key points, and support tools for coming up with and elaborating on comprehensible or appealing phrases. In order to test the effect of our title revision wizard, we conducted a questionnaire survey on the effect of the titles with or without using the wizard on the interest of lay readers. The survey showed that the wizard is effective and helpful for the authors who cannot compose appealing titles for lay readers by themselves."},{"ID":"shaar-etal-2021-findings","methods":["analysis"],"center_method":["analysis"],"tasks":["infodemic and censorship detection"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Findings of the NLP4IF 2021 Shared Tasks on Fighting the COVID 19 Infodemic and Censorship Detection","abstract_clean":"We present the results and the main findings of the NLP4IF 2021 shared tasks. Task 1 focused on fighting the COVID 19 infodemic in social media, and it was offered in Arabic, Bulgarian, and English. Given a tweet, it asked to predict whether that tweet contains a verifiable claim, and if so, whether it is likely to be false, is of general interest, is likely to be harmful, and is worthy of manual fact checking; also, whether it is harmful to society, and whether it requires the attention of policy makers. Task 2 focused on censorship detection, and was offered in Chinese. A total of ten teams submitted systems for task 1, and one team participated in task 2; nine teams also submitted a system description paper. Here, we present the tasks, analyze the results, and discuss the system submissions and the methods they used. Most submissions achieved sizable improvements over several baselines, and the best systems used pre trained Transformers and ensembles. The data, the scorers and the leaderboards for the tasks are available at http:\/\/ gitlab.com\/NLP4IF\/nlp4if 2021."},{"ID":"shamanna-girishekar-etal-2021-training","methods":["bert","distant supervision","curriculum learning","multilingual training"],"center_method":["bert","distant supervision",null,null],"tasks":["adversarial advertisement detection"],"center_task":[null],"Goal":["Decent Work and Economic Growth"],"title_clean":"Training Language Models under Resource Constraints for Adversarial Advertisement Detection","abstract_clean":"Advertising on e commerce and social media sites deliver ad impressions at web scale on a daily basis driving value to both shoppers and advertisers. This scale necessitates programmatic ways of detecting unsuitable content in ads to safeguard customer experience and trust. This paper focusses on techniques for training text classification models under resource constraints, built as part of automated solutions for advertising content moderation. We show how weak supervision, curriculum learning and multilingual training can be applied effectively to fine tune BERT and its variants for text classification tasks in conjunction with different data augmentation strategies. Our extensive experiments on multiple languages show that these techniques detect adversarial ad categories with a substantial gain in precision at high recall threshold over the baseline."},{"ID":"shen-etal-2013-participant","methods":["participant - based approach"],"center_method":[null],"tasks":["event summarization"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"A Participant based Approach for Event Summarization Using Twitter Streams","abstract_clean":"Twitter offers an unprecedented advantage on live reporting of the events happening around the world. However, summarizing the Twitter event has been a challenging task that was not fully explored in the past. In this paper, we propose a participant based event summarization approach that \"zooms in\" the Twitter event streams to the participant level, detects the important sub events associated with each participant using a novel mixture model that combines the \"burstiness\" and \"cohesiveness\" properties of the event tweets, and generates the event summaries progressively. We evaluate the proposed approach on different event types. Results show that the participantbased approach can effectively capture the sub events that have otherwise been shadowed by the long tail of other dominant sub events, yielding summaries with considerably better coverage than the state of the art."},{"ID":"sheremetyeva-2002-mt","methods":["learning environment"],"center_method":[null],"tasks":["teaching of machine translation"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"An MT learning environment for computational linguistics students","abstract_clean":"This paper discusses the issue of suitability of software used for the teaching of Machine Translation. It considers requirements to such software, and describes a set of tools that have initially been created as developer environment of an APTrans MT system but can easily be included in the learning environment for MT training. The tools are user friendly and feature modularity and reusability."},{"ID":"shim-etal-2021-synthetic","methods":["synthetic data generation","multi - task learning"],"center_method":[null,null],"tasks":["extracting temporal information from health - related narrative text"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Synthetic Data Generation and Multi Task Learning for Extracting Temporal Information from Health Related Narrative Text","abstract_clean":"Extracting temporal information is critical to process health related text. Temporal information extraction is a challenging task for language models because it requires processing both texts and numbers. Moreover, the fundamental challenge is how to obtain a largescale training dataset. To address this, we propose a synthetic data generation algorithm. Also, we propose a novel multi task temporal information extraction model and investigate whether multi task learning can contribute to performance improvement by exploiting additional training signals with the existing training data. For experiments, we collected a custom dataset containing unstructured texts with temporal information of sleep related activities. Experimental results show that utilising synthetic data can improve the performance when the augmentation factor is 3. The results also show that when multi task learning is used with an appropriate amount of synthetic data, the performance can significantly improve from 82. to 88.6 and from 83.9 to 91.9 regarding micro and macro average exact match scores of normalised time prediction, respectively."},{"ID":"shreevastava-foltz-2021-detecting","methods":["sentence - bert embeddings","support vector machine"],"center_method":[null,"support vector machine"],"tasks":["detecting cognitive distortions"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Detecting Cognitive Distortions from Patient Therapist Interactions","abstract_clean":"An important part of Cognitive Behavioral Therapy (CBT) is to recognize and restructure certain negative thinking patterns that are also known as cognitive distortions. This project aims to detect these distortions using natural language processing. We compare and contrast different types of linguistic features as well as different classification algorithms and explore the limitations of applying these techniques on a small dataset. We find that pretrained Sentence BERT embeddings to train an SVM classifier yields the best results with an F1 score of 0.79. Lastly, we discuss how this work provides insights into the types of linguistic features that are inherent in cognitive distortions."},{"ID":"sigurbergsson-derczynski-2020-offensive","methods":["classification systems","danish dataset"],"center_method":[null,null],"tasks":["offensive language and hate speech detection"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Offensive Language and Hate Speech Detection for Danish","abstract_clean":"The presence of offensive language on social media platforms and the implications this poses is becoming a major concern in modern society. Given the enormous amount of content created every day, automatic methods are required to detect and deal with this type of content. Until now, most of the research has focused on solving the problem for the English language, while the problem is multilingual. We construct a Danish dataset DKHATE containing user generated comments from various social media platforms, and to our knowledge, the first of its kind, annotated for various types and target of offensive language. We develop four automatic classification systems, each designed to work for both the English and the Danish language. In the detection of offensive language in English, the best performing system achieves a macro averaged F1 score of 0.74, and the best performing system for Danish achieves a macro averaged F1 score of 0.70. In the detection of whether or not an offensive post is targeted, the best performing system for English achieves a macro averaged F1 score of 0.62, while the best performing system for Danish achieves a macro averaged F1 score of 0.73. Finally, in the detection of the target type in a targeted offensive post, the best performing system for English achieves a macro averaged F1 score of 0.56, and the best performing system for Danish achieves a macro averaged F1 score of 0.63. Our work for both the English and the Danish language captures the type and targets of offensive language, and present automatic methods for detecting different kinds of offensive language such as hate speech and cyberbullying."},{"ID":"sikdar-etal-2018-flytxt","methods":["conditional random field","na\\\"\\ive bayes classifiers"],"center_method":["conditional random field",null],"tasks":["identifying and classifying malware text"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Flytxt\\_NTNU at SemEval 2018 Task 8: Identifying and Classifying Malware Text Using Conditional Random Fields and Na\\\"\\ive Bayes Classifiers","abstract_clean":"Cybersecurity risks such as malware threaten the personal safety of users, but to identify malware text is a major challenge. The paper proposes a supervised learning approach to identifying malware sentences given a document (subTask1 of SemEval 2018, Task 8), as well as to classifying malware tokens in the sentences (subTask2). The approach achieved good results, ranking second of twelve participants for both subtasks, with F scores of 57% for subTask1 and 28% for subTask2."},{"ID":"singh-li-2021-exploiting","methods":["auxiliary data","bert"],"center_method":[null,"bert"],"tasks":["toxicity detection"],"center_task":["toxicity detection"],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Exploiting Auxiliary Data for Offensive Language Detection with Bidirectional Transformers","abstract_clean":"Offensive language detection (OLD) has received increasing attention due to its societal impact. Recent work shows that bidirectional transformer based methods obtain impressive performance on OLD. However, such methods usually rely on large scale well labeled OLD datasets for model training. To address the issue of data\/label scarcity in OLD, in this paper, we propose a simple yet effective domain adaptation approach to train bidirectional transformers. Our approach introduces domain adaptation (DA) training procedures to ALBERT, such that it can effectively exploit auxiliary data from source domains to improve the OLD performance in a target domain. Experimental results on benchmark datasets show that our approach, AL BERT (DA), obtains the state of the art performance in most cases. Particularly, our approach significantly benefits underrepresented and under performing classes, with a significant improvement over ALBERT."},{"ID":"sinha-etal-2014-capturing","methods":["graph based approach"],"center_method":[null],"tasks":["predicting student attrition"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Capturing ``attrition intensifying'' structural traits from didactic interaction sequences of MOOC learners","abstract_clean":"This work is an attempt to discover hidden structural configurations in learning activity sequences of students in Massive Open Online Courses (MOOCs). Leveraging combined representations of video clickstream interactions and forum activities, we seek to fundamentally understand traits that are predictive of decreasing engagement over time. Grounded in the interdisciplinary field of network science, we follow a graph based approach to successfully extract indicators of active and passive MOOC participation that reflect persistence and regularity in the overall interaction footprint. Using these rich educational semantics, we focus on the problem of predicting student attrition, one of the major highlights of MOOC literature in the recent years. Our results indicate an improvement over a baseline ngram based approach in capturing \"attrition intensifying\" features from the learning activities that MOOC learners engage in. Implications for some compelling future research are discussed."},{"ID":"soh-etal-2019-legal","methods":["comparative study"],"center_method":[null],"tasks":["legal area classification"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Legal Area Classification: A Comparative Study of Text Classifiers on Singapore Supreme Court Judgments","abstract_clean":"This paper conducts a comparative study on the performance of various machine learning (\"ML\") approaches for classifying judgments into legal areas. Using a novel dataset of 6,227 Singapore Supreme Court judgments, we investigate how state of the art NLP methods compare against traditional statistical models when applied to a legal corpus that comprised few but lengthy documents. All approaches tested, including topic model, word embedding, and language model based classifiers, performed well with as little as a few hundred judgments. However, more work needs to be done to optimize state of the art methods for the legal domain."},{"ID":"solorio-etal-2013-case","methods":["authorship attribution","voting scheme"],"center_method":[null,null],"tasks":["sockpuppet detection"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"A Case Study of Sockpuppet Detection in Wikipedia","abstract_clean":"This paper presents preliminary results of using authorship attribution methods for the detection of sockpuppeteering in Wikipedia. Sockpuppets are fake accounts created by malicious users to bypass Wikipedia's regulations. Our dataset is composed of the comments made by the editors on the talk pages. To overcome the limitations of the short lengths of these comments, we use an voting scheme to combine predictions made on individual user entries. We show that this approach is promising and that it can be a viable alternative to the current human process that Wikipedia uses to resolve suspected sockpuppet cases."},{"ID":"somasundaran-etal-2009-opinion","methods":["opinion graphs"],"center_method":[null],"tasks":["discourse classification"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Opinion Graphs for Polarity and Discourse Classification","abstract_clean":"This work shows how to construct discourse level opinion graphs to perform a joint interpretation of opinions and discourse relations. Specifically, our opinion graphs enable us to factor in discourse information for polarity classification, and polarity information for discourse link classification. This interdependent framework can be used to augment and improve the performance of local polarity and discourse link classifiers."},{"ID":"somasundaran-etal-2014-lexical","methods":["investigation","lexical chain features"],"center_method":[null,null],"tasks":["measuring discourse coherence quality"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Lexical Chaining for Measuring Discourse Coherence Quality in Test taker Essays","abstract_clean":"This paper presents an investigation of lexical chaining (Morris and Hirst, 1991) for measuring discourse coherence quality in test taker essays. We hypothesize that attributes of lexical chains, as well as interactions between lexical chains and explicit discourse elements, can be harnessed for representing coherence. Our experiments reveal that performance achieved by our new lexical chain features is better than that of previous discourse features used for this task, and that the best system performance is achieved when combining lexical chaining features with complementary discourse features, such as those provided by a discourse parser based on rhetorical structure theory, and features that reflect errors in grammar, word usage, and mechanics. This work is licensed under a Creative Commons Attribution 4.0 International Licence."},{"ID":"somers-etal-1997-multilingual","methods":["query engine","pattern matcher"],"center_method":[null,null],"tasks":["text summarization","multilingual generation"],"center_task":["text summarization",null],"Goal":["Decent Work and Economic Growth"],"title_clean":"Multilingual Generation and Summarization of Job Adverts: the TREE Project","abstract_clean":"A multilingual Internet based employment advertisement system is described. Job ads are submitted as e mail texts, analysed by an example based pattern matcher and stored in language independent schemas in an object oriented database. Users can search the database in their own language and get customized summaries of the job ads. The query engine uses symbolic case based reasoning techniques, while the generation module integrates canned text, templates, and grammar rules to produce texts and hypertexts in a simple way."},{"ID":"song-etal-2020-using","methods":["deep neural network"],"center_method":["deep neural network"],"tasks":["classify suicidal behaviour"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Using Deep Neural Networks with Intra and Inter Sentence Context to Classify Suicidal Behaviour","abstract_clean":"Identifying statements related to suicidal behaviour in psychiatric electronic health records (EHRs) is an important step when modeling that behaviour, and when assessing suicide risk. We apply a deep neural network based classification model with a lightweight context encoder, to classify sentence level suicidal behaviour in EHRs. We show that incorporating information from sentences to left and right of the target sentence significantly improves classification accuracy. Our approach achieved the best performance when classifying suicidal behaviour in Autism Spectrum Disorder patient records. The results could have implications for suicidality research and clinical surveillance."},{"ID":"sotnikova-etal-2021-analyzing","methods":["analyzing stereotypes"],"center_method":[null],"tasks":["generative text inference"],"center_task":[null],"Goal":["Reduced Inequalities"],"title_clean":"Analyzing Stereotypes in Generative Text Inference Tasks","abstract_clean":"Stereotypes are inferences drawn about people based on their demographic attributes, which may result in harms to users when a system is deployed. In generative language inference tasks, given a premise, a model produces plausible hypotheses that follow either logically (natural language inference) or commonsensically (commonsense inference). Such tasks are therefore a fruitful setting in which to explore the degree to which NLP systems encode stereotypes. In our work, we study how stereotypes manifest when the potential targets of stereotypes are situated in real life, neutral contexts. We collect human judgments on the presence of stereotypes in generated inferences, and compare how perceptions of stereotypes vary due to annotator positionality. Domain Target Categories Gender man, woman, non binary person, trans man, trans woman, cis man, cis woman"},{"ID":"stajner-etal-2017-effects","methods":["parallel gaze data","eye tracking studies"],"center_method":[null,null],"tasks":["investigating the reading difficulties"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Effects of Lexical Properties on Viewing Time per Word in Autistic and Neurotypical Readers","abstract_clean":"Eye tracking studies from the past few decades have shaped the way we think of word complexity and cognitive load: words that are long, rare and ambiguous are more difficult to read. However, online processing techniques have been scarcely applied to investigating the reading difficulties of people with autism and what vocabulary is challenging for them. We present parallel gaze data obtained from adult readers with autism and a control group of neurotypical readers and show that the former required higher cognitive effort to comprehend the texts as evidenced by three gaze based measures. We divide all words into four classes based on their viewing times for both groups and investigate the relationship between longer viewing times and word length, word frequency, and four cognitively based measures (word concreteness, familiarity, age of acquisition and imagability)."},{"ID":"sun-etal-2021-medai","methods":["negationaware pre - training"],"center_method":[null],"tasks":["negation detection domain adaptation"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"MedAI at SemEval 2021 Task 10: Negation aware Pre training for Source free Negation Detection Domain Adaptation","abstract_clean":"Due to the increasing concerns for data privacy, source free unsupervised domain adaptation attracts more and more research attention, where only a trained source model is assumed to be available, while the labeled source data remains private. To get promising adaptation results, we need to find effective ways to transfer knowledge learned in source domain and leverage useful domain specific information from target domain at the same time. This paper describes our winning contribution to SemEval 2021 Task 10: Source Free Domain Adaptation for Semantic Processing. Our key idea is to leverage the model trained on source domain data to generate pseudo labels for target domain samples. Besides, we propose Negationaware Pre training (NAP) to incorporate negation knowledge into model. Our method wins the 1st place with F1 score of 0.822 on the official blind test set of Negation Detection Track."},{"ID":"suster-etal-2017-short","methods":["privacy analysis"],"center_method":[null],"tasks":["review of ethical challenges"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"A Short Review of Ethical Challenges in Clinical Natural Language Processing","abstract_clean":"Clinical NLP has an immense potential in contributing to how clinical practice will be revolutionized by the advent of large scale processing of clinical records. However, this potential has remained largely untapped due to slow progress primarily caused by strict data access policies for researchers. In this paper, we discuss the concern for privacy and the measures it entails. We also suggest sources of less sensitive data. Finally, we draw attention to biases that can compromise the validity of empirical research and lead to socially harmful applications."},{"ID":"tabak-purver-2020-temporal","methods":["distantsupervision"],"center_method":[null],"tasks":["temporal mental health dynamics"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Temporal Mental Health Dynamics on Social Media","abstract_clean":"We describe a set of experiments for building a temporal mental health dynamics system. We utilise a pre existing methodology for distantsupervision of mental health data mining from social media platforms and deploy the system during the global COVID 19 pandemic as a case study. Despite the challenging nature of the task, we produce encouraging results, both explicit to the global pandemic and implicit to a global phenomenon, Christmas Depression, supported by the literature. We propose a methodology for providing insight into temporal mental health dynamics to be utilised for strategic decision making."},{"ID":"tanaka-etal-2014-linguistic","methods":["linguistic and acoustic features","exploratory study"],"center_method":[null,null],"tasks":["automatic identification of autism"],"center_task":[null],"Goal":["Reduced Inequalities"],"title_clean":"Linguistic and Acoustic Features for Automatic Identification of Autism Spectrum Disorders in Children's Narrative","abstract_clean":"Autism spectrum disorders are developmental disorders characterised as deficits in social and communication skills, and they affect both verbal and non verbal communication. Previous works measured differences in children with and without autism spectrum disorders in terms of linguistic and acoustic features, although they do not mention automatic identification using integration of these features. In this paper, we perform an exploratory study of several language and speech features of both single utterances and full narratives. We find that there are characteristic differences between children with autism spectrum disorders and typical development with respect to word categories, prosody, and voice quality, and that these differences can be used in automatic classifiers. We also examine the differences between American and Japanese children and find significant differences with regards to pauses before new turns and linguistic cues."},{"ID":"tang-shen-2020-categorizing","methods":["data collection","hierarchical attention capsule network","densely annotated data - set"],"center_method":["data collection",null,null],"tasks":["toxicity detection"],"center_task":["toxicity detection"],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Categorizing Offensive Language in Social Networks: A Chinese Corpus, Systems and an Explainable Tool","abstract_clean":"Recently, more and more data have been generated in the online world, filled with offensive language such as threats, swear words or straightforward insults. It is disgraceful for a progressive society, and then the question arises on how language resources and technologies can cope with this challenge. However, previous work only analyzes the problem as a whole but fails to detect particular types of offensive content in a more fine grained way, mainly because of the lack of annotated data. In this work, we present a densely annotated data set COLA (Categorizing Offensive LAnguage), consists of fine grained insulting language, antisocial language and illegal language. We study different strategies for automatically identifying offensive language on COLA data. Further, we design a capsule system with hierarchical attention to aggregate and fully utilize information, which obtains a state of the art result. Results from experiments prove that our hierarchical attention capsule network (HACN) performs significantly better than existing methods in offensive classification with the precision of 94.37% and recall of 95.28%. We also explain what our model has learned with an explanation tool called Integrated Gradients. Meanwhile, our system's processing speed can handle each sentence in 10msec, suggesting the potential for efficient deployment in real situations."},{"ID":"trajanovski-etal-2021-text","methods":["exploration of contextual signals"],"center_method":[null],"tasks":["text prediction"],"center_task":[null],"Goal":["Industry, Innovation and Infrastructure"],"title_clean":"When does text prediction benefit from additional context? An exploration of contextual signals for chat and email messages","abstract_clean":"Email and chat communication tools are increasingly important for completing daily tasks. Accurate real time phrase completion can save time and bolster productivity. Modern text prediction algorithms are based on large language models which typically rely on the prior words in a message to predict a completion. We examine how additional contextual signals (from previous messages, time, and subject) affect the performance of a commercial text prediction model. We compare contextual text prediction in chat and email messages from two of the largest commercial platforms Microsoft Teams and Outlook, finding that contextual signals contribute to performance differently between these scenarios. On emails, time context is most beneficial with small relative gains of 2% over baseline. Whereas, in chat scenarios, using a tailored set of previous messages as context yields relative improvements over the baseline between 9.3% and 18.6% across various critical serviceoriented text prediction metrics."},{"ID":"tymoshenko-moschitti-2021-strong","methods":["transformers"],"center_method":["transformers"],"tasks":["fact - checking","claim detection"],"center_task":[null,"claim detection"],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Strong and Light Baseline Models for Fact Checking Joint Inference","abstract_clean":"How to combine several pieces of evidence to verify a claim is an interesting semantic task. Very complex methods have been proposed, combining different evidence vectors using an evidence interaction graph. In this paper, we show that in case of inference based on transformer models, two effective approaches use either (i) a simple application of max pooling over the Transformer evidence vectors; or (ii) computing a weighted sum of the evidence vectors. Our experiments on the FEVER claim verification task show that the methods above achieve the state of the art, constituting strong baseline for much more computationally complex methods."},{"ID":"tziafas-etal-2021-fighting","methods":["bert","transformers"],"center_method":["bert","transformers"],"tasks":["misinformation detection"],"center_task":["misinformation detection"],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Fighting the COVID 19 Infodemic with a Holistic BERT Ensemble","abstract_clean":"This paper describes the TOKOFOU system, an ensemble model for misinformation detection tasks based on six different transformer based pre trained encoders, implemented in the context of the COVID 19 Infodemic Shared Task for English. We fine tune each model on each of the task's questions and aggregate their prediction scores using a majority voting approach. TOKOFOU obtains an overall F1 score of 89.7%, ranking first."},{"ID":"v-hahn-vertan-2002-architectures","methods":["architectures"],"center_method":[null],"tasks":["teaching machine translation"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Architectures of ``toy'' systems for teaching machine translation","abstract_clean":"This paper addresses the advantages of practical academic teaching of machine translation by implementations of \"toy\" systems. This is the result of experience from several semesters with different types of courses and different categories of students. In addition to describing two possible architectures for such educational toy systems, we will also discuss how to overcome misconceptions about MT and the evaluation both of the achieved systems and the learning success."},{"ID":"vaidhya-kaushal-2020-iitkgp","methods":["bio - bert"],"center_method":[null],"tasks":["named entity recognition of lab protocol"],"center_task":[null],"Goal":["Industry, Innovation and Infrastructure"],"title_clean":"IITKGP at W NUT 2020 Shared Task 1: Domain specific BERT representation for Named Entity Recognition of lab protocol","abstract_clean":"Supervised models trained to predict properties from representations, have been achieving high accuracy on a variety of tasks. For instance, the BERT family seems to work exceptionally well on the downstream task from NER tagging to the range of other linguistic tasks. But the vocabulary used in the medical field contains a lot of different tokens used only in the medical industry such as the name of different diseases, devices, organisms, medicines, etc. that makes it difficult for traditional BERT model to create contextualized embedding. In this paper, we are going to illustrate the System for Named Entity Tagging based on Bio Bert. Experimental results show that our model gives substantial improvements over the baseline and stood the fourth runner up in terms of F1 score, and first runner up in terms of Recall with just 2.21 F1 score behind the best one. 1"},{"ID":"van-deemter-gatt-2007-content","methods":["evaluation measures"],"center_method":[null],"tasks":["content determination"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Content determination in GRE: evaluating the evaluator","abstract_clean":"In this paper, we discuss the evaluation measures proposed in a number of recent papers associated with the TUNA project 1 , and which have become an important component of the First NLG Shared Task and Evaluation Campaign (STEC) on attribute selection for referring expressions generation. Focusing on reference to individual objects, we discuss what such evaluation measures should be expected to achieve, and what alternative measures merit consideration."},{"ID":"vanderwende-etal-2013-annotating","methods":["annotation"],"center_method":["annotation"],"tasks":["identification of phenotypes"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Annotating Change of State for Clinical Events","abstract_clean":"Understanding the event structure of sentences and whole documents is an important step in being able to extract meaningful information from the text. Our task is the identification of phenotypes, specifically, pneumonia, from clinical narratives. In this paper, we consider the importance of identifying the change of state for events, in particular, events that measure and compare multiple states across time. Change of state is important to the clinical diagnosis of pneumonia; in the example \"there are bibasilar opacities that are unchanged\", the presence of bibasilar opacities alone may suggest pneumonia, but not when they are unchanged, which suggests the need to modify events with change of state information. Our corpus is comprised of chest Xray reports, where we find many descriptions of change of state comparing the volume and density of the lungs and surrounding areas. We propose an annotation schema to capture this information as a tuple of <location, attribute, value, change of state, time reference>."},{"ID":"vashisth-etal-2019-exploring","methods":["distributed concept representations"],"center_method":[null],"tasks":["exploring diachronic changes of biomedical knowledge"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Exploring Diachronic Changes of Biomedical Knowledge using Distributed Concept Representations","abstract_clean":"In research best practices can change over time as new discoveries are made and novel methods are implemented. Scientific publications reporting about the latest facts and current state of the art can be possibly outdated after some years or even proved to be false. A publication usually sheds light only on the knowledge of the period it has been published. Thus, the aspect of time can play an essential role in the reliability of the presented information. In Natural Language Processing many methods focus on information extraction from text, such as detecting entities and their relationship to each other. Those methods mostly focus on the facts presented in the text itself and not on the aspects of knowledge which changes over time. This work instead examines the evolution in biomedical knowledge over time using scientific literature in terms of diachronic change. Mainly the usage of temporal and distributional concept representations are explored and evaluated by a proof of concept."},{"ID":"vlad-etal-2019-sentence","methods":["transfer learning","bert","lstm","capsule"],"center_method":["transfer learning","bert","lstm",null],"tasks":["propaganda detection"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Sentence Level Propaganda Detection in News Articles with Transfer Learning and BERT BiLSTM Capsule Model","abstract_clean":"In recent years, the need for communication increased in online social media. Propaganda is a mechanism which was used throughout history to influence public opinion and it is gaining a new dimension with the rising interest of online social media. This paper presents our submission to NLP4IF 2019 Shared Task SLC: Sentence level Propaganda Detection in news articles. The challenge of this task is to build a robust binary classifier able to provide corresponding propaganda labels, propaganda or non propaganda. Our model relies on a unified neural network, which consists of several deep leaning modules, namely BERT, BiLSTM and Capsule, to solve the sentencelevel propaganda classification problem. In addition, we take a pre training approach on a somewhat similar task (i.e., emotion classification) improving results against the coldstart model. Among the 26 participant teams in the NLP4IF 2019 Task SLC, our solution ranked 12th with an F 1 score 0.5868 on the official test data. Our proposed solution indicates promising results since our system significantly exceeds the baseline approach of the task organizers by 0.1521 and is slightly lower than the winning system by 0.0454."},{"ID":"wahlgren-1961-linguistic","methods":["linguistic analysis"],"center_method":[null],"tasks":["linguistic research"],"center_task":[null],"Goal":["Industry, Innovation and Infrastructure"],"title_clean":"Linguistic analysis or Russian chemical terminology","abstract_clean":"THIS paper is a discussion of a specialized phase of linguistic research being carried on in the Machine Translation Project at the University of California in Berkeley. The material presented here is intended to illustrate in some detail the application of linguistic analysis to a particular problem. The fundamental approach upon which this work, is based has been described in a paper by Sydney M. Lamb. 1 The first part of the following discussion deals with theoretical considerations underlying linguistic research into scientific terminology, with special reference to chemical terminology. The second part of the paper provides material which is illustrative of a linguistic description of chemical nomenclature. Examples are drawn from a detailed grammatical analysis of chemical terminology which is being conducted. Ultimately the results of this analysis will be incorporated into the total grammatical description of Russian which is to be employed in the machine translation process.\nRelatively little attention is devoted here to the machine translation process, inasmuch as the application of the results of linguistic analysis constitutes a separate operation in the California Project. Some general comment on this aspect of the problem, however, will be made where necessary."},{"ID":"wandji-tchami-grabar-2014-towards","methods":["contrastive automatic analysis"],"center_method":[null],"tasks":["automatic distinction between specialized and non - specialized occurrences of verbs"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Towards Automatic Distinction between Specialized and Non Specialized Occurrences of Verbs in Medical Corpora","abstract_clean":"The medical field gathers people of different social statuses, such as students, pharmacists, managers, biologists, nurses and mainly medical doctors and patients, who represent the main actors. Despite their different levels of expertise, these actors need to interact and understand each other but the communication is not always easy and effective. This paper describes a method for a contrastive automatic analysis of verbs in medical corpora, based on the semantic annotation of the verbs nominal co occurents. The corpora used are specialized in cardiology and distinguished according to their levels of expertise (high and low). The semantic annotation of these corpora is performed by using an existing medical terminology. The results indicate that the same verbs occurring in the two corpora show different specialization levels, which are indicated by the words (nouns and adjectives derived from medical terms) they occur with."},{"ID":"wang-etal-2017-statistical","methods":["statistical framework"],"center_method":[null],"tasks":["product description generation"],"center_task":[null],"Goal":["Industry, Innovation and Infrastructure"],"title_clean":"A Statistical Framework for Product Description Generation","abstract_clean":"We present in this paper a statistical framework that generates accurate and fluent product description from product attributes. Specifically, after extracting templates and learning writing knowledge from attribute description parallel data, we use the learned knowledge to decide what to say and how to say for product description generation. To evaluate accuracy and fluency for the generated descriptions, in addition to BLEU and Recall, we propose to measure what to say (in terms of attribute coverage) and to measure how to say (by attribute specified generation) separately. Experimental results show that our framework is effective."},{"ID":"wang-etal-2019-bigodm","methods":["ensemble methods","support vector machine"],"center_method":["ensemble methods","support vector machine"],"tasks":["social media mining"],"center_task":["social media mining"],"Goal":["Good Health and Well-Being"],"title_clean":"BIGODM System in the Social Media Mining for Health Applications Shared Task 2019","abstract_clean":"In this study, we describe our methods to automatically classify Twitter posts conveying events of adverse drug reaction (ADR). Based on our previous experience in tackling the ADR classification task, we empirically applied the vote based undersampling ensemble approach along with linear support vector machine (SVM) to develop our classifiers as part of our participation in ACL 2019 Social Media Mining for Health Applications (SMM4H) shared task 1. The best performed model on the test sets were trained on a merged corpus consisting of the datasets released by SMM4H 2017 and 2019. By using VUE, the corpus was randomly under sampled with 2:1 ratio between the negative and positive classes to create an ensemble using the linear kernel trained with features including bag of word, domain knowledge, negation and word embedding. The best performing model achieved an F measure of 0.551 which is about 5% higher than the average F scores of 16 teams."},{"ID":"wang-etal-2020-evaluating","methods":["language models","bert"],"center_method":["language models","bert"],"tasks":["clinical semantic textual similarity"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Evaluating the Utility of Model Configurations and Data Augmentation on Clinical Semantic Textual Similarity","abstract_clean":"In this paper, we apply pre trained language models to the Semantic Textual Similarity (STS) task, with a specific focus on the clinical domain. In low resource setting of clinical STS, these large models tend to be impractical and prone to overfitting. Building on BERT, we study the impact of a number of model design choices, namely different fine tuning and pooling strategies. We observe that the impact of domain specific fine tuning on clinical STS is much less than that in the general domain, likely due to the concept richness of the domain. Based on this, we propose two data augmentation techniques. Experimental results on N2C2 STS 1 demonstrate substantial improvements, validating the utility of the proposed methods."},{"ID":"wang-etal-2020-rationalizing","methods":["framework"],"center_method":[null],"tasks":["relation extraction"],"center_task":["relation extraction"],"Goal":["Good Health and Well-Being"],"title_clean":"Rationalizing Medical Relation Prediction from Corpus level Statistics","abstract_clean":"Nowadays, the interpretability of machine learning models is becoming increasingly important, especially in the medical domain. Aiming to shed some light on how to rationalize medical relation prediction, we present a new interpretable framework inspired by existing theories on how human memory works, e.g., theories of recall and recognition. Given the corpus level statistics, i.e., a global cooccurrence graph of a clinical text corpus, to predict the relations between two entities, we first recall rich contexts associated with the target entities, and then recognize relational interactions between these contexts to form model rationales, which will contribute to the final prediction. We conduct experiments on a real world public clinical dataset and show that our framework can not only achieve competitive predictive performance against a comprehensive list of neural baseline models, but also present rationales to justify its prediction. We further collaborate with medical experts deeply to verify the usefulness of our model rationales for clinical decision making 1 ."},{"ID":"wibberley-etal-2014-method51","methods":["software platform"],"center_method":[null],"tasks":["mining insight from social media datasets"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Method51 for Mining Insight from Social Media Datasets","abstract_clean":"We present Method51, a social media analysis software platform with a set of accompanying methodologies. We discuss a series of case studies illustrating the platform's application, and motivating our methodological proposals."},{"ID":"wilson-wun-2020-automatic","methods":["classification","profile - based features"],"center_method":["classification",null],"tasks":["automatic classification of students"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Automatic Classification of Students on Twitter Using Simple Profile Information","abstract_clean":"Obtaining social media demographic information using machine learning is important for efficient computational social science research. Automatic age classification has been accomplished with relative success and allows for the study of youth populations, but student classification determining which users are currently attending an academic institution has not been thoroughly studied. Previous work (He et al., 2016) proposes a model which utilizes 3 tweet content features to classify users as students or non students. This model achieves an accuracy of 84%, but is restrictive and time intensive because it requires accessing and processing many user tweets. In this study, we propose classification models which use 7 numerical features and 10 text based features drawn from simple profile information. These profile based features allow for faster, more accessible data collection and enable the classification of users without needing access to their tweets. Compared to previous models, our models identify students with greater accuracy; our best model obtains an accuracy of 88.1% and an F1 score of .704. This improved student identification tool has the potential to facilitate research on topics ranging from professional networking to the impact of education on Twitter behaviors."},{"ID":"wu-etal-2019-wtmed","methods":["text encoder","syntax encoder","feature encoder"],"center_method":[null,null,null],"tasks":["inference"],"center_task":["inference"],"Goal":["Good Health and Well-Being"],"title_clean":"WTMED at MEDIQA 2019: A Hybrid Approach to Biomedical Natural Language Inference","abstract_clean":"Natural language inference (NLI) is challenging, especially when it is applied to technical domains such as biomedical settings. In this paper, we propose a hybrid approach to biomedical NLI where different types of information are exploited for this task. Our base model includes a pre trained text encoder as the core component, and a syntax encoder and a feature encoder to capture syntactic and domain specific information. Then we combine the output of different base models to form more powerful ensemble models. Finally, we design two conflict resolution strategies when the test data contain multiple (premise, hypothesis) pairs with the same premise. We train our models on the MedNLI dataset, yielding the best performance on the test set of the MEDIQA 2019 Task 1."},{"ID":"wu-etal-2021-multimodal","methods":["co - attention networks"],"center_method":[null],"tasks":["fake news detection"],"center_task":["fake news detection"],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Multimodal Fusion with Co Attention Networks for Fake News Detection","abstract_clean":"Fake news with textual and visual contents has a better story telling ability than text only contents, and can be spread quickly with social media. People can be easily deceived by such fake news, and traditional expert identification is labor intensive. Therefore, automatic detection of multimodal fake news has become a new hot spot issue. A shortcoming of existing approaches is their inability to fuse multimodality features effectively. They simply concatenate unimodal features without considering inter modality relations. Inspired by the way people read news with image and text, we propose a novel Multimodal Co Attention Networks (MCAN) to better fuse textual and visual features for fake news detection. Extensive experiments conducted on two realworld datasets demonstrate that MCAN can learn inter dependencies among multimodal features and outperforms state of the art methods."},{"ID":"xie-etal-2021-humorhunter","methods":["disentangled attention","deberta","bert"],"center_method":[null,null,"bert"],"tasks":["humor and offense recognition"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"HumorHunter at SemEval 2021 Task 7: Humor and Offense Recognition with Disentangled Attention","abstract_clean":"In this paper, we describe our system submitted to SemEval 2021 Task 7: HaHackathon: Detecting and Rating Humor and Offense. The task aims at predicting whether the given text is humorous, the average humor rating given by the annotators, and whether the humor rating is controversial. In addition, the task also involves predicting how offensive the text is. Our approach adopts the DeBERTa architecture with disentangled attention mechanism, where the attention scores between words are calculated based on their content vectors and relative position vectors. We also took advantage of the pre trained language models and fine tuned the DeBERTa model on all the four subtasks. We experimented with several BERT like structures and found that the large DeBERTa model generally performs better. During the evaluation phase, our system achieved an F score of 0.9480 on subtask 1a, an RMSE of 0.5510 on subtask 1b, an F score of 0.4764 on subtask 1c, and an RMSE of 0.4230 on subtask 2a (rank 3 on the leaderboard)."},{"ID":"xiong-litman-2011-understanding","methods":["feature engineering"],"center_method":["feature engineering"],"tasks":["identifying peer - review helpfulness"],"center_task":[null],"Goal":["Quality Education"],"title_clean":"Understanding Differences in Perceived Peer Review Helpfulness using Natural Language Processing","abstract_clean":"Identifying peer review helpfulness is an important task for improving the quality of feedback received by students, as well as for helping students write better reviews. As we tailor standard product review analysis techniques to our peer review domain, we notice that peerreview helpfulness differs not only between students and experts but also between types of experts. In this paper, we investigate how different types of perceived helpfulness might influence the utility of features for automatic prediction. Our feature selection results show that certain low level linguistic features are more useful for predicting student perceived helpfulness, while high level cognitive constructs are more effective in modeling experts' perceived helpfulness."},{"ID":"xu-etal-2013-examination","methods":["corpus of bullying tweets","exploratory analysis"],"center_method":[null,null],"tasks":["examination of regret"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"An Examination of Regret in Bullying Tweets","abstract_clean":"Social media users who post bullying related tweets may later experience regret, potentially causing them to delete their posts. In this paper, we construct a corpus of bullying tweets and periodically check the existence of each tweet in order to infer if and when it becomes deleted. We then conduct exploratory analysis in order to isolate factors associated with deleted posts. Finally, we propose the construction of a regrettable posts predictor to warn users if a tweet might cause regret."},{"ID":"xu-zhao-2012-using","methods":["deep linguistic features","syntactic dependency parsing tree"],"center_method":[null,null],"tasks":["finding deceptive opinion spam"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Using Deep Linguistic Features for Finding Deceptive Opinion Spam","abstract_clean":"While most recent work has focused on instances of opinion spam which are manually identifiable or deceptive opinion spam which are written by paid writers separately, in this work we study both of these interesting topics and propose an effective framework which has good performance on both datasets. Based on the golden standard opinion spam dataset, we propose a novel model which integrates some deep linguistic features derived from a syntactic dependency parsing tree to discriminate deceptive opinions from normal ones. On a background of multiple language tasks, our model is evaluated on both English (gold standard) and Chinese (non gold) datasets. The experimental results show that our model produces state of the art results on both of the topics."},{"ID":"yang-etal-2001-towards","methods":["usercentered approach"],"center_method":[null],"tasks":["sign translation"],"center_task":[null],"Goal":["Reduced Inequalities"],"title_clean":"Towards Automatic Sign Translation","abstract_clean":"Signs are everywhere in our lives. They make our lives easier when we are familiar with them. But sometimes they also pose problems. For example, a tourist might not be able to understand signs in a foreign country. In this paper, we present our efforts towards automatic sign translation. We discuss methods for automatic sign detection. We describe sign translation using example based machine translation technology. We use a usercentered approach in developing an automatic sign translation system. The approach takes advantage of human intelligence in selecting an area of interest and domain for translation if needed. A user can determine which sign is to be translated if multiple signs have been detected within the image. The selected part of the image is then processed, recognized, and translated. We have developed a prototype system that can recognize Chinese signs input from a video camera which is a common gadget for a tourist, and translate them into English text or voice stream."},{"ID":"yang-etal-2015-sampling","methods":["bilingual hierarchical sub - sentential alignment","sampling - based multilingual alignment"],"center_method":[null,null],"tasks":["machine translation"],"center_task":["machine translation"],"Goal":["Industry, Innovation and Infrastructure"],"title_clean":"Sampling based Alignment and Hierarchical Sub sentential Alignment in Chinese Japanese Translation of Patents","abstract_clean":"This paper describes Chinese Japanese translation systems based on different alignment methods using the JPO corpus and our submission (ID: WASUIPS) to the subtask of the 2015 Workshop on Asian Translation. One of the alignment methods used is bilingual hierarchical sub sentential alignment combined with sampling based multilingual alignment. We also accelerated this method and in this paper, we evaluate the translation results and time spent on several machine translation tasks. The training time is much faster than the standard baseline pipeline (GIZA++\/Moses) and MGIZA\/Moses."},{"ID":"yang-heeman-2007-avoiding","methods":["empirical study"],"center_method":[null],"tasks":["avoiding and resolving initiative conflicts in dialogue"],"center_task":[null],"Goal":["Partnership for the Goals"],"title_clean":"Avoiding and Resolving Initiative Conflicts in Dialogue","abstract_clean":"In this paper, we report on an empirical study on initiative conflicts in human human conversation. We examined these conflicts in two corpora of task oriented dialogues. The results show that conversants try to avoid initiative conflicts, but when these conflicts occur, they are efficiently resolved by linguistic devices, such as volume."},{"ID":"yasaswini-etal-2021-iiitt","methods":["transfer learning"],"center_method":["transfer learning"],"tasks":["toxicity detection"],"center_task":["toxicity detection"],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"IIITT@DravidianLangTech EACL2021: Transfer Learning for Offensive Language Detection in Dravidian Languages","abstract_clean":"This paper demonstrates our work for the shared task on Offensive Language Identification in Dravidian Languages EACL 2021. Offensive language detection in the various social media platforms was identified previously. However, with the increase in the diversity of users, there is a need to identify the offensive language in multilingual posts which are largely code mixed or written in a non native script. We approach this challenge with various transfer learning based models to classify a given post or comment in Dravidian languages (Malayalam, Tamil and Kannada) into 6 categories. The source codes for our systems are published 1 ."},{"ID":"yu-etal-2021-interpretable","methods":["interpretability"],"center_method":[null],"tasks":["propaganda detection"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Interpretable Propaganda Detection in News Articles","abstract_clean":"Online users today are exposed to misleading and propagandistic news articles and media posts on a daily basis. To counter thus, a number of approaches have been designed aiming to achieve a healthier and safer online news and media consumption. Automatic systems are able to support humans in detecting such content; yet, a major impediment to their broad adoption is that besides being accurate, the decisions of such systems need also to be interpretable in order to be trusted and widely adopted by users. Since misleading and propagandistic content influences readers through the use of a number of deception techniques, we propose to detect and to show the use of such techniques as a way to offer interpretability. In particular, we define qualitatively descriptive features and we analyze their suitability for detecting deception techniques. We further show that our interpretable features can be easily combined with pre trained language models, yielding state of the art results."},{"ID":"yu-hatzivassiloglou-2003-towards","methods":["bayesian classifier"],"center_method":[null],"tasks":["opinion question answering"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"title_clean":"Towards Answering Opinion Questions: Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences","abstract_clean":"Opinion question answering is a challenging task for natural language processing. In this paper, we discuss a necessary component for an opinion question answering system: separating opinions from fact, at both the document and sentence level. We present a Bayesian classifier for discriminating between documents with a preponderance of opinions such as editorials from regular news stories, and describe three unsupervised, statistical techniques for the significantly harder task of detecting opinions at the sentence level. We also present a first model for classifying opinion sentences as positive or negative in terms of the main perspective being expressed in the opinion. Results from a large collection of news stories and a human evaluation of 400 sentences are reported, indicating that we achieve very high performance in document classification (upwards of 97% precision and recall), and respectable performance in detecting opinions and classifying them at the sentence level as positive, negative, or neutral (up to 91% accuracy)."},{"ID":"zeng-etal-2019-faceted","methods":["hierarchy growth algorithm"],"center_method":[null],"tasks":["organize scientific concepts"],"center_task":[null],"Goal":["Industry, Innovation and Infrastructure"],"title_clean":"Faceted Hierarchy: A New Graph Type to Organize Scientific Concepts and a Construction Method","abstract_clean":"On a scientific concept hierarchy, a parent concept may have a few attributes, each of which has multiple values being a group of child concepts. We call these attributes facets: classification has a few facets such as application (e.g., face recognition), model (e.g., svm, knn), and metric (e.g., precision). In this work, we aim at building faceted concept hierarchies from scientific literature. Hierarchy construction methods heavily rely on hypernym detection, however, the faceted relations are parent to child links but the hypernym relation is a multi hop, i.e., ancestor todescendent link with a specific facet \"type of\". We use information extraction techniques to find synonyms, sibling concepts, and ancestordescendent relations from a data science corpus. And we propose a hierarchy growth algorithm to infer the parent child links from the three types of relationships. It resolves conflicts by maintaining the acyclic structure of a hierarchy."},{"ID":"zhang-ma-2020-dual","methods":["dual attention model","embedding - based neural network"],"center_method":[null,null],"tasks":["citation recommendation"],"center_task":[null],"Goal":["Industry, Innovation and Infrastructure"],"title_clean":"Dual Attention Model for Citation Recommendation","abstract_clean":"Based on an exponentially increasing number of academic articles, discovering and citing comprehensive and appropriate resources has become a non trivial task. Conventional citation recommender methods suffer from severe information loss. For example, they do not consider the section of the paper that the user is writing and for which they need to find a citation, the relatedness between the words in the local context (the text span that describes a citation), or the importance on each word from the local context. These shortcomings make such methods insufficient for recommending adequate citations to academic manuscripts. In this study, we propose a novel embedding based neural network called \"dual attention model for citation recommendation (DACR)\" to recommend citations during manuscript preparation. Our method adapts embedding of three semantic information: words in the local context, structural contexts 1 , and the section on which a user is working. A neural network model is designed to maximize the similarity between the embedding of the three input (local context words, section and structural contexts) and the target citation appearing in the context. The core of the neural network model is composed of self attention and additive attention, where the former aims to capture the relatedness between the contextual words and structural context, and the latter aims to learn the importance of them. The experiments on real world datasets demonstrate the effectiveness of the proposed approach."},{"ID":"zhang-patrick-2006-extracting","methods":["markup tag set","manually annotated corpus"],"center_method":[null,null],"tasks":["extracting patient clinical profiles"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Extracting Patient Clinical Profiles from Case Reports","abstract_clean":"This research aims to extract detailed clinical profiles, such as signs and symptoms, and important laboratory test results of the patient from descriptions of the diagnostic and treatment procedures in journal articles. This paper proposes a novel markup tag set to cover a wide variety of semantics in the description of clinical case studies in the clinical literature. A manually annotated corpus which consists of 75 clinical reports with 5,117 sentences has been created and a sentence classification system is reported as the preliminary attempt to exploit the fast growing online repositories of clinical case reports."},{"ID":"zheng-etal-2021-duo","methods":["multimodal value","multimodal representation discreteness"],"center_method":[null,null],"tasks":["language measurement","analysis and measurement of multimodal discourse"],"center_task":[null,null],"Goal":["Quality Education"],"title_clean":"\u591a\u6a21\u6001\u8868\u8ff0\u89c6\u57df\u4e0b\u7684\u5c0f\u5b66\u6570\u5b66\u8bfe\u5802\u8bed\u8a00\u8ba1\u91cf\u521d\u63a2(A preliminary study of language measurement in elementary school mathematics classrooms from the perspective of multimodal representation)","abstract_clean":"This paper focuses on the analysis and measurement of multimodal discourse in elementary school mathematics classroom. Based on a high quality mathematics class, this paper explores the processing and annotation of a multimodal corpus, proposes two multimodal language measurement methods: multimodal value and multimodal representation discreteness, and analyzes the results of quantified multimodal language sampling data. The results show that teachers can better transfer abstract knowledge with the help of multimodal languages, and the measurement results can reflect the cooperative representation relationship between modes and the appropriateness of multimodal language deduction in classroom teaching."},{"ID":"zheng-yu-2015-identifying","methods":["domain adaption"],"center_method":["domain adaption"],"tasks":["identifying key concepts"],"center_task":[null],"Goal":["Good Health and Well-Being"],"title_clean":"Identifying Key Concepts from EHR Notes Using Domain Adaptation","abstract_clean":"Linking electronic health records (EHRs) to relevant education materials can provide patient centered tailored education which can potentially improve patients' medical knowledge, self management and clinical outcome. It is shown that EHR query generation using key concept identification improves retrieval of education materials. In this study, we explored domain adaptation approaches to improve key concept identification. Our experiments show that a 20.7% improvement in the F1 measure can be achieved by leveraging data from Wikipedia. Queries generated from the best performing approach achieved a 20.6% and 27.8% improvement over the queries generated from the baseline approach."}]