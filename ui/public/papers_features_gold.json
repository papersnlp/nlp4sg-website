[{"ID":"abadeer-2020-assessment","methods":["lightweight deep learning model","bert","phi","transformers"],"center_method":[null,"bert",null,"transformers"],"tasks":["detection of medical concepts","detection of protected health information","named entity recognition","nlp applications","inference"],"center_task":[null,null,"named entity recognition","nlp applications","inference"],"Goal":["Good Health and Well-Being"],"text":"Assessment of DistilBERT performance on Named Entity Recognition task for the detection of Protected Health Information and medical concepts. Bidirectional Encoder Representations from Transformers (BERT) models achieve state-ofthe-art performance on a number of Natural Language Processing tasks. However, their model size on disk often exceeds 1 GB and the process of fine-tuning them and using them to run inference consumes significant hardware resources and runtime. This makes them hard to deploy to production environments. This paper fine-tunes DistilBERT, a lightweight deep learning model, on medical text for the named entity recognition task of Protected Health Information (PHI) and medical concepts. This work provides a full assessment of the performance of DistilBERT in comparison with BERT models that were pre-trained on medical text. For Named Entity Recognition task of PHI, DistilBERT achieved almost the same results as medical versions of BERT in terms of F 1 score at almost half the runtime and consuming approximately half the disk space. On the other hand, for the detection of medical concepts, DistilBERT's F 1 score was lower by 4 points on average than medical BERT variants.","label":1,"title_clean":"Assessment of DistilBERT performance on Named Entity Recognition task for the detection of Protected Health Information and medical concepts","abstract_clean":"Bidirectional Encoder Representations from Transformers (BERT) models achieve state ofthe art performance on a number of Natural Language Processing tasks. However, their model size on disk often exceeds 1 GB and the process of fine tuning them and using them to run inference consumes significant hardware resources and runtime. This makes them hard to deploy to production environments. This paper fine tunes DistilBERT, a lightweight deep learning model, on medical text for the named entity recognition task of Protected Health Information (PHI) and medical concepts. This work provides a full assessment of the performance of DistilBERT in comparison with BERT models that were pre trained on medical text. For Named Entity Recognition task of PHI, DistilBERT achieved almost the same results as medical versions of BERT in terms of F 1 score at almost half the runtime and consuming approximately half the disk space. On the other hand, for the detection of medical concepts, DistilBERT's F 1 score was lower by 4 points on average than medical BERT variants.","url":"https:\/\/aclanthology.org\/2020.clinicalnlp-1.18"},{"ID":"abercrombie-batista-navarro-2020-parlvote","methods":["linear classifier","transformers","neural classifier","neural network","transformer word embedding model","classification","bag of words text representation"],"center_method":[null,"transformers",null,"neural network",null,"classification",null],"tasks":["sentiment analysis of political debates","classification of sentiment polarity"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"ParlVote: A Corpus for Sentiment Analysis of Political Debates. Debate transcripts from the UK Parliament contain information about the positions taken by politicians towards important topics, but are difficult for people to process manually. While sentiment analysis of debate speeches could facilitate understanding of the speakers' stated opinions, datasets currently available for this task are small when compared to the benchmark corpora in other domains. We present ParlVote, a new, larger corpus of parliamentary debate speeches for use in the evaluation of sentiment analysis systems for the political domain. We also perform a number of initial experiments on this dataset, testing a variety of approaches to the classification of sentiment polarity in debate speeches. These include a linear classifier as well as a neural network trained using a transformer word embedding model (BERT), and fine-tuned on the parliamentary speeches. We find that in many scenarios, a linear classifier trained on a bag-of-words text representation achieves the best results. However, with the largest dataset, the transformer-based model combined with a neural classifier provides the best performance. We suggest that further experimentation with classification models and observations of the debate content and structure are required, and that there remains much room for improvement in parliamentary sentiment analysis.","label":1,"title_clean":"ParlVote: A Corpus for Sentiment Analysis of Political Debates","abstract_clean":"Debate transcripts from the UK Parliament contain information about the positions taken by politicians towards important topics, but are difficult for people to process manually. While sentiment analysis of debate speeches could facilitate understanding of the speakers' stated opinions, datasets currently available for this task are small when compared to the benchmark corpora in other domains. We present ParlVote, a new, larger corpus of parliamentary debate speeches for use in the evaluation of sentiment analysis systems for the political domain. We also perform a number of initial experiments on this dataset, testing a variety of approaches to the classification of sentiment polarity in debate speeches. These include a linear classifier as well as a neural network trained using a transformer word embedding model (BERT), and fine tuned on the parliamentary speeches. We find that in many scenarios, a linear classifier trained on a bag of words text representation achieves the best results. However, with the largest dataset, the transformer based model combined with a neural classifier provides the best performance. We suggest that further experimentation with classification models and observations of the debate content and structure are required, and that there remains much room for improvement in parliamentary sentiment analysis.","url":"https:\/\/aclanthology.org\/2020.lrec-1.624"},{"ID":"abnar-etal-2018-experiential","methods":["distributional and dependency - based word embeddings"],"center_method":[null],"tasks":["decoding brain"],"center_task":[null],"Goal":["Good Health and Well-Being"],"text":"Experiential, Distributional and Dependency-based Word Embeddings have Complementary Roles in Decoding Brain Activity. ","label":1,"title_clean":"Experiential, Distributional and Dependency based Word Embeddings have Complementary Roles in Decoding Brain Activity","abstract_clean":"","url":"https:\/\/aclanthology.org\/W18-0107"},{"ID":"adam-etal-2017-zikahack","methods":["digital disease detection solutions","surveillance methods"],"center_method":[null,null],"tasks":["rapid and early detection of those outbreaks","rapid surveillance and emergency response programs","digital disease detection competition","zik ahack competition","infectious diseases outbreaks"],"center_task":[null,null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"ZikaHack 2016: A digital disease detection competition. Effective response to infectious diseases outbreaks relies on the rapid and early detection of those outbreaks. Invalidated, yet timely and openly available digital information can be used for the early detection of outbreaks. Public health surveillance authorities can exploit these early warnings to plan and coordinate rapid surveillance and emergency response programs. In 2016, a digital disease detection competition named ZikaHack was launched. The objective of the competition was for multidisciplinary teams to design, develop and demonstrate innovative digital disease detection solutions to retrospectively detect the 2015-16 Brazilian Zika virus outbreak earlier than traditional surveillance methods. In this paper, an overview of the Zik-aHack competition is provided. The challenges and lessons learned in organizing this competition are also discussed for use by other researchers interested in organizing similar competitions.","label":1,"title_clean":"ZikaHack 2016: A digital disease detection competition","abstract_clean":"Effective response to infectious diseases outbreaks relies on the rapid and early detection of those outbreaks. Invalidated, yet timely and openly available digital information can be used for the early detection of outbreaks. Public health surveillance authorities can exploit these early warnings to plan and coordinate rapid surveillance and emergency response programs. In 2016, a digital disease detection competition named ZikaHack was launched. The objective of the competition was for multidisciplinary teams to design, develop and demonstrate innovative digital disease detection solutions to retrospectively detect the 2015 16 Brazilian Zika virus outbreak earlier than traditional surveillance methods. In this paper, an overview of the Zik aHack competition is provided. The challenges and lessons learned in organizing this competition are also discussed for use by other researchers interested in organizing similar competitions.","url":"https:\/\/aclanthology.org\/W17-5806"},{"ID":"adams-etal-2016-distributed","methods":["distributional text representations","scoring scheme","latent semantic analysis","vector space methods","unsupervised algorithm","distributed vector representations"],"center_method":[null,null,null,null,"unsupervised algorithm",null],"tasks":["unsupervised automatic short answer grading","english reading comprehension","computer science","one to many alignment of word vectors","determining text similarity"],"center_task":[null,null,null,null,null],"Goal":["Quality Education"],"text":"Distributed Vector Representations for Unsupervised Automatic Short Answer Grading. We address the problem of automatic short answer grading, evaluating a collection of approaches inspired by recent advances in distributional text representations. In addition, we propose an unsupervised approach for determining text similarity using one-to-many alignment of word vectors. We evaluate the proposed technique across two datasets from different domains, namely, computer science and English reading comprehension, that additionally vary between highschool level and undergraduate students. Experiments demonstrate that the proposed technique often outperforms other compositional distributional semantics approaches as well as vector space methods such as latent semantic analysis. When combined with a scoring scheme, the proposed technique provides a powerful tool for tackling the complex problem of short answer grading. We also discuss a number of other key points worthy of consideration in preparing viable, easy-to-deploy automatic short-answer grading systems for the real-world.","label":1,"title_clean":"Distributed Vector Representations for Unsupervised Automatic Short Answer Grading","abstract_clean":"We address the problem of automatic short answer grading, evaluating a collection of approaches inspired by recent advances in distributional text representations. In addition, we propose an unsupervised approach for determining text similarity using one to many alignment of word vectors. We evaluate the proposed technique across two datasets from different domains, namely, computer science and English reading comprehension, that additionally vary between highschool level and undergraduate students. Experiments demonstrate that the proposed technique often outperforms other compositional distributional semantics approaches as well as vector space methods such as latent semantic analysis. When combined with a scoring scheme, the proposed technique provides a powerful tool for tackling the complex problem of short answer grading. We also discuss a number of other key points worthy of consideration in preparing viable, easy to deploy automatic short answer grading systems for the real world.","url":"https:\/\/aclanthology.org\/W16-4904"},{"ID":"afzal-etal-2020-cora","methods":["deep active learning covid 19"],"center_method":[null],"tasks":["covid - 19"],"center_task":[null],"Goal":["Good Health and Well-Being","Industry, Innovation and Infrastrucure"],"text":"CORA: A Deep Active Learning Covid-19 Relevancy Algorithm to Identify Core Scientific Articles. Ever since the COVID-19 pandemic broke out, the academic and scientific research community, as well as industry and governments around the world have joined forces in an unprecedented manner to fight the threat. Clinicians, biologists, chemists, bioinformaticians, nurses, data scientists, and all of the affiliated relevant disciplines have been mobilized to help discover efficient treatments for the infected population, as well as a vaccine solution to prevent further the virus' spread. In this combat against the virus responsible for the pandemic, key for any advancements is the timely, accurate, peer-reviewed, and efficient communication of any novel research findings. In this paper we present a novel framework to address the information need of filtering efficiently the scientific bibliography for relevant literature around COVID-19. The contributions of the paper are summarized in the following: we define and describe the information need that encompasses the major requirements for COVID-19 articles' relevancy, we present and release an expert-curated benchmark set for the task, and we analyze the performance of several state-of-the-art machine learning classifiers that may distinguish the relevant from the non-relevant COVID-19 literature. 1 https:\/\/covid19.who.int\/","label":1,"title_clean":"CORA: A Deep Active Learning Covid 19 Relevancy Algorithm to Identify Core Scientific Articles","abstract_clean":"Ever since the COVID 19 pandemic broke out, the academic and scientific research community, as well as industry and governments around the world have joined forces in an unprecedented manner to fight the threat. Clinicians, biologists, chemists, bioinformaticians, nurses, data scientists, and all of the affiliated relevant disciplines have been mobilized to help discover efficient treatments for the infected population, as well as a vaccine solution to prevent further the virus' spread. In this combat against the virus responsible for the pandemic, key for any advancements is the timely, accurate, peer reviewed, and efficient communication of any novel research findings. In this paper we present a novel framework to address the information need of filtering efficiently the scientific bibliography for relevant literature around COVID 19. The contributions of the paper are summarized in the following: we define and describe the information need that encompasses the major requirements for COVID 19 articles' relevancy, we present and release an expert curated benchmark set for the task, and we analyze the performance of several state of the art machine learning classifiers that may distinguish the relevant from the non relevant COVID 19 literature. 1 https:\/\/covid19.who.int\/","url":"https:\/\/aclanthology.org\/2020.nlpcovid19-2.2"},{"ID":"aggarwal-etal-2019-ltl","methods":["two vote classification","bert","ltl udes systems","minority fallback","multi layer perceptron","embedding representation of postings"],"center_method":[null,"bert",null,null,null,null],"tasks":["categorizing offensiveness"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions","Reduced Inequalities"],"text":"LTL-UDE at SemEval-2019 Task 6: BERT and Two-Vote Classification for Categorizing Offensiveness. This paper describes LTL-UDE's systems for the SemEval 2019 Shared Task 6. We present results for Subtask A and C. In Subtask A, we experiment with an embedding representation of postings and use a Multi-Layer Perceptron and BERT to categorize postings. Our best result reaches the 10th place (out of 103) using BERT. In Subtask C, we applied a two-vote classification approach with minority fallback, which is placed on the 19th rank (out of 65).","label":1,"title_clean":"LTL UDE at SemEval 2019 Task 6: BERT and Two Vote Classification for Categorizing Offensiveness","abstract_clean":"This paper describes LTL UDE's systems for the SemEval 2019 Shared Task 6. We present results for Subtask A and C. In Subtask A, we experiment with an embedding representation of postings and use a Multi Layer Perceptron and BERT to categorize postings. Our best result reaches the 10th place (out of 103) using BERT. In Subtask C, we applied a two vote classification approach with minority fallback, which is placed on the 19th rank (out of 65).","url":"https:\/\/aclanthology.org\/S19-2121.pdf"},{"ID":"agrawal-carpuat-2020-generating","methods":["feature rich binary classifier","filtering hypotheses"],"center_method":[null,null],"tasks":["machine translation","duolingo staple task"],"center_task":["machine translation",null],"Goal":["Quality Education"],"text":"Generating Diverse Translations via Weighted Fine-tuning and Hypotheses Filtering for the Duolingo STAPLE Task. This paper describes the University of Maryland's submission to the Duolingo Shared Task on Simultaneous Translation And Paraphrase for Language Education (STAPLE). Unlike the standard machine translation task, STAPLE requires generating a set of outputs for a given input sequence, aiming to cover the space of translations produced by language learners. We adapt neural machine translation models to this requirement by (a) generating n-best translation hypotheses from a model fine-tuned on learner translations, oversampled to reflect the distribution of learner responses, and (b) filtering hypotheses using a feature-rich binary classifier that directly optimizes a close approximation of the official evaluation metric. Combination of systems that use these two strategies achieves F1 scores of 53.9% and 52.5% on Vietnamese and Portuguese, respectively ranking 2 nd and 4 th on the leaderboard.","label":1,"title_clean":"Generating Diverse Translations via Weighted Fine tuning and Hypotheses Filtering for the Duolingo STAPLE Task","abstract_clean":"This paper describes the University of Maryland's submission to the Duolingo Shared Task on Simultaneous Translation And Paraphrase for Language Education (STAPLE). Unlike the standard machine translation task, STAPLE requires generating a set of outputs for a given input sequence, aiming to cover the space of translations produced by language learners. We adapt neural machine translation models to this requirement by (a) generating n best translation hypotheses from a model fine tuned on learner translations, oversampled to reflect the distribution of learner responses, and (b) filtering hypotheses using a feature rich binary classifier that directly optimizes a close approximation of the official evaluation metric. Combination of systems that use these two strategies achieves F1 scores of 53.9% and 52.5% on Vietnamese and Portuguese, respectively ranking 2 nd and 4 th on the leaderboard.","url":"https:\/\/aclanthology.org\/2020.ngt-1.21"},{"ID":"akasaki-kaji-2019-conversation","methods":["conversation systems","generation based models","information retrieval"],"center_method":[null,null,"information retrieval"],"tasks":["conversation initiation","open domain nontask oriented conversations"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"Conversation Initiation by Diverse News Contents Introduction. In our everyday chitchat , there is a conversation initiator, who proactively casts an initial utterance to start chatting. However, most existing conversation systems cannot play this role. Previous studies on conversation systems assume that the user always initiates conversation, and have placed emphasis on how to respond to the given user's utterance. As a result, existing conversation systems become passive. Namely they continue waiting until being spoken to by the users. In this paper, we consider the system as a conversation initiator and propose a novel task of generating the initial utterance in open-domain non-task-oriented conversation. Here, in order not to make users bored, it is necessary to generate diverse utterances to initiate conversation without relying on boilerplate utterances like greetings. To this end, we propose to generate initial utterance by summarizing and chatting about news articles, which provide fresh and various contents everyday. To address the lack of the training data for this task, we constructed a novel largescale dataset through crowd-sourcing. We also analyzed the dataset in detail to examine how humans initiate conversations (the dataset will be released to facilitate future research activities). We present several approaches to conversation initiation including information retrieval based and generation based models. Experimental results showed that the proposed models trained on our dataset performed reasonably well and outperformed baselines that utilize automatically collected training data in both automatic and manual evaluation. * This work was done during research internship at Yahoo Japan Corporation. 1 \"Conversation\" in this paper refers to open-domain nontask-oriented conversations and chitchat .","label":1,"title_clean":"Conversation Initiation by Diverse News Contents Introduction","abstract_clean":"In our everyday chitchat , there is a conversation initiator, who proactively casts an initial utterance to start chatting. However, most existing conversation systems cannot play this role. Previous studies on conversation systems assume that the user always initiates conversation, and have placed emphasis on how to respond to the given user's utterance. As a result, existing conversation systems become passive. Namely they continue waiting until being spoken to by the users. In this paper, we consider the system as a conversation initiator and propose a novel task of generating the initial utterance in open domain non task oriented conversation. Here, in order not to make users bored, it is necessary to generate diverse utterances to initiate conversation without relying on boilerplate utterances like greetings. To this end, we propose to generate initial utterance by summarizing and chatting about news articles, which provide fresh and various contents everyday. To address the lack of the training data for this task, we constructed a novel largescale dataset through crowd sourcing. We also analyzed the dataset in detail to examine how humans initiate conversations (the dataset will be released to facilitate future research activities). We present several approaches to conversation initiation including information retrieval based and generation based models. Experimental results showed that the proposed models trained on our dataset performed reasonably well and outperformed baselines that utilize automatically collected training data in both automatic and manual evaluation. * This work was done during research internship at Yahoo Japan Corporation. 1 \"Conversation\" in this paper refers to open domain nontask oriented conversations and chitchat .","url":"https:\/\/aclanthology.org\/N19-1400.pdf"},{"ID":"akhlaghi-etal-2020-constructing","methods":["lara","crowdsourcing techniques"],"center_method":[null,null],"tasks":["conversion task","multimodal language learner texts","reading and listening","multimodal online versions"],"center_task":[null,null,null,null],"Goal":["Quality Education"],"text":"Constructing Multimodal Language Learner Texts Using LARA: Experiences with Nine Languages. LARA (Learning and Reading Assistant) is an open source platform whose purpose is to support easy conversion of plain texts into multimodal online versions suitable for use by language learners. This involves semi-automatically tagging the text, adding other annotations and recording audio. The platform is suitable for creating texts in multiple languages via crowdsourcing techniques that can be used for teaching a language via reading and listening. We present results of initial experiments by various collaborators where we measure the time required to produce substantial LARA resources, up to the length of short novels, in Dutch, English, Farsi, French, German, Icelandic, Irish, Swedish and Turkish. The first results are encouraging. Although there are some startup problems, the conversion task seems manageable for the languages tested so far. The resulting enriched texts are posted online and are freely available in both source and compiled form.","label":1,"title_clean":"Constructing Multimodal Language Learner Texts Using LARA: Experiences with Nine Languages","abstract_clean":"LARA (Learning and Reading Assistant) is an open source platform whose purpose is to support easy conversion of plain texts into multimodal online versions suitable for use by language learners. This involves semi automatically tagging the text, adding other annotations and recording audio. The platform is suitable for creating texts in multiple languages via crowdsourcing techniques that can be used for teaching a language via reading and listening. We present results of initial experiments by various collaborators where we measure the time required to produce substantial LARA resources, up to the length of short novels, in Dutch, English, Farsi, French, German, Icelandic, Irish, Swedish and Turkish. The first results are encouraging. Although there are some startup problems, the conversion task seems manageable for the languages tested so far. The resulting enriched texts are posted online and are freely available in both source and compiled form.","url":"https:\/\/aclanthology.org\/2020.lrec-1.40.pdf"},{"ID":"akhtar-etal-2017-multilayer","methods":["multilayer perceptron based ensemble technique","deep neural network","support vector regression","supervised model","autoencoder based financial word embeddings","gru","lstm","feature based models","convolutional neural network"],"center_method":[null,"deep neural network",null,null,null,"gru","lstm",null,"convolutional neural network"],"tasks":["fine grained financial sentiment analysis"],"center_task":[null],"Goal":["Decent Work and Economic Growth"],"text":"A Multilayer Perceptron based Ensemble Technique for Fine-grained Financial Sentiment Analysis. In this paper, we propose a novel method for combining deep learning and classical feature based models using a Multi-Layer Perceptron (MLP) network for financial sentiment analysis. We develop various deep learning models based on Convolutional Neural Network (CNN), Long Short Term Memory (LSTM) and Gated Recurrent Unit (GRU). These are trained on top of pre-trained, autoencoder-based, financial word embeddings and lexicon features. An ensemble is constructed by combining these deep learning models and a classical supervised model based on Support Vector Regression (SVR). We evaluate our proposed technique on a benchmark dataset of SemEval-2017 shared task on financial sentiment analysis. The propose model shows impressive results on two datasets, i.e. microblogs and news headlines datasets. Comparisons show that our proposed model performs better than the existing state-of-the-art systems for the above two datasets by 2.0 and 4.1 cosine points, respectively.","label":1,"title_clean":"A Multilayer Perceptron based Ensemble Technique for Fine grained Financial Sentiment Analysis","abstract_clean":"In this paper, we propose a novel method for combining deep learning and classical feature based models using a Multi Layer Perceptron (MLP) network for financial sentiment analysis. We develop various deep learning models based on Convolutional Neural Network (CNN), Long Short Term Memory (LSTM) and Gated Recurrent Unit (GRU). These are trained on top of pre trained, autoencoder based, financial word embeddings and lexicon features. An ensemble is constructed by combining these deep learning models and a classical supervised model based on Support Vector Regression (SVR). We evaluate our proposed technique on a benchmark dataset of SemEval 2017 shared task on financial sentiment analysis. The propose model shows impressive results on two datasets, i.e. microblogs and news headlines datasets. Comparisons show that our proposed model performs better than the existing state of the art systems for the above two datasets by 2.0 and 4.1 cosine points, respectively.","url":"https:\/\/aclanthology.org\/D17-1057"},{"ID":"alexeeva-etal-2020-mathalign","methods":["mathalign","rule based system"],"center_method":[null,"rule based system"],"tasks":["linking formula identifiers","mathematical information retrieval","accessibility of scientific documents"],"center_task":[null,null,null],"Goal":["Industry, Innovation and Infrastrucure","Quality Education"],"text":"MathAlign: Linking Formula Identifiers to their Contextual Natural Language Descriptions. Extending machine reading approaches to extract mathematical concepts and their descriptions is useful for a variety of tasks, ranging from mathematical information retrieval to increasing accessibility of scientific documents for the visually impaired. This entails segmenting mathematical formulae into identifiers and linking them to their natural language descriptions. We propose a rule-based approach for this task, which extracts L A T E X representations of formula identifiers and links them to their in-text descriptions, given only the original PDF and the location of the formula of interest. We also present a novel evaluation dataset for this task, as well as the tool used to create it.","label":1,"title_clean":"MathAlign: Linking Formula Identifiers to their Contextual Natural Language Descriptions","abstract_clean":"Extending machine reading approaches to extract mathematical concepts and their descriptions is useful for a variety of tasks, ranging from mathematical information retrieval to increasing accessibility of scientific documents for the visually impaired. This entails segmenting mathematical formulae into identifiers and linking them to their natural language descriptions. We propose a rule based approach for this task, which extracts L A T E X representations of formula identifiers and links them to their in text descriptions, given only the original PDF and the location of the formula of interest. We also present a novel evaluation dataset for this task, as well as the tool used to create it.","url":"https:\/\/aclanthology.org\/2020.lrec-1.269.pdf"},{"ID":"ali-etal-2013-hear","methods":["machine learning methods"],"center_method":["machine learning methods"],"tasks":["text mining studies","annotation","sentiment analysis"],"center_task":[null,"annotation","sentiment analysis"],"Goal":["Good Health and Well-Being"],"text":"Can I Hear You? Sentiment Analysis on Medical Forums. Text mining studies have started to investigae relations between positive and negative opinions and patients' physical health. Several studies linked the personal lexicon with health and the health-related behavior of the individual. However, few text mining studies were performed to analyze opinions expressed in a large volume of user-written Web content. Our current study focused on performing sentiment analysis on several medical forums dedicated to Hearing Loss (HL). We categorized messages posted on the forums as positive, negative and neutral. Our study had two stages: first, we applied manual annotation of the posts with two annotators and have 82.01% overall agreement with kappa 0.65 and then we applied Machine Learning techniques to classify the posts.","label":1,"title_clean":"Can I Hear You? Sentiment Analysis on Medical Forums","abstract_clean":"Text mining studies have started to investigae relations between positive and negative opinions and patients' physical health. Several studies linked the personal lexicon with health and the health related behavior of the individual. However, few text mining studies were performed to analyze opinions expressed in a large volume of user written Web content. Our current study focused on performing sentiment analysis on several medical forums dedicated to Hearing Loss (HL). We categorized messages posted on the forums as positive, negative and neutral. Our study had two stages: first, we applied manual annotation of the posts with two annotators and have 82.01% overall agreement with kappa 0.65 and then we applied Machine Learning techniques to classify the posts.","url":"https:\/\/aclanthology.org\/I13-1077"},{"ID":"aly-etal-2021-fact","methods":["feverous"],"center_method":[null],"tasks":["fact checking","shared task","claim detection","evidence retrieval","fever 2018 shared task"],"center_task":["fact checking",null,"claim detection",null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"The Fact Extraction and VERification Over Unstructured and Structured information (FEVEROUS) Shared Task. The Fact Extraction and VERification Over Unstructured and Structured information (FEVEROUS) shared task, asks participating systems to determine whether human-authored claims are SUPPORTED or REFUTED based on evidence retrieved from Wikipedia (or NOTENOUGHINFO if the claim cannot be verified). Compared to the FEVER 2018 shared task, the main challenge is the addition of structured data (tables and lists) as a source of evidence. The claims in the FEVEROUS dataset can be verified using only structured evidence, only unstructured evidence, or a mixture of both. Submissions are evaluated using the FEVEROUS score that combines label accuracy and evidence retrieval. Unlike FEVER 2018 (Thorne et al., 2018a), FEVEROUS requires partial evidence to be returned for NOTENOUGHINFO claims, and the claims are longer and thus more complex. The shared task received 13 entries, six of which were able to beat the baseline system. The winning team was \"Bust a move!\", achieving a FEVEROUS score of 27% (+9% compared to the baseline). In this paper we describe the shared task, present the full results and highlight commonalities and innovations among the participating systems. Claim: In the 2018 Naples general election, Roberto Fico, an Italian politician and member of the Five Star Movement, received 57,119 votes with 57.6 percent of the total votes.","label":1,"title_clean":"The Fact Extraction and VERification Over Unstructured and Structured information (FEVEROUS) Shared Task","abstract_clean":"The Fact Extraction and VERification Over Unstructured and Structured information (FEVEROUS) shared task, asks participating systems to determine whether human authored claims are SUPPORTED or REFUTED based on evidence retrieved from Wikipedia (or NOTENOUGHINFO if the claim cannot be verified). Compared to the FEVER 2018 shared task, the main challenge is the addition of structured data (tables and lists) as a source of evidence. The claims in the FEVEROUS dataset can be verified using only structured evidence, only unstructured evidence, or a mixture of both. Submissions are evaluated using the FEVEROUS score that combines label accuracy and evidence retrieval. Unlike FEVER 2018 (Thorne et al., 2018a), FEVEROUS requires partial evidence to be returned for NOTENOUGHINFO claims, and the claims are longer and thus more complex. The shared task received 13 entries, six of which were able to beat the baseline system. The winning team was \"Bust a move!\", achieving a FEVEROUS score of 27% (+9% compared to the baseline). In this paper we describe the shared task, present the full results and highlight commonalities and innovations among the participating systems. Claim: In the 2018 Naples general election, Roberto Fico, an Italian politician and member of the Five Star Movement, received 57,119 votes with 57.6 percent of the total votes.","url":"https:\/\/aclanthology.org\/2021.fever-1.1"},{"ID":"amason-etal-2019-harvey","methods":["hyperpartisan news detector","naive bayes"],"center_method":[null,"naive bayes"],"tasks":["hyperpartisan news detection task"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Harvey Mudd College at SemEval-2019 Task 4: The D.X. Beaumont Hyperpartisan News Detector. We use the 600 hand-labelled articles from Se-mEval Task 4 (Kiesel et al., 2019) to handtune a classifier with 3000 features for the Hyperpartisan News Detection task. Our final system uses features based on bag-of-words (BoW), analysis of the article title, language complexity, and simple sentiment analysis in a naive Bayes classifier. We trained our final system on the 600,000 articles labelled by publisher. Our final system has an accuracy of 0.653 on the hand-labeled test set. The most effective features are the Automated Readability Index and the presence of certain words in the title. This suggests that hyperpartisan writing uses a distinct writing style, especially in the title.","label":1,"title_clean":"Harvey Mudd College at SemEval 2019 Task 4: The D.X. Beaumont Hyperpartisan News Detector","abstract_clean":"We use the 600 hand labelled articles from Se mEval Task 4 (Kiesel et al., 2019) to handtune a classifier with 3000 features for the Hyperpartisan News Detection task. Our final system uses features based on bag of words (BoW), analysis of the article title, language complexity, and simple sentiment analysis in a naive Bayes classifier. We trained our final system on the 600,000 articles labelled by publisher. Our final system has an accuracy of 0.653 on the hand labeled test set. The most effective features are the Automated Readability Index and the presence of certain words in the title. This suggests that hyperpartisan writing uses a distinct writing style, especially in the title.","url":"https:\/\/aclanthology.org\/S19-2166"},{"ID":"amin-etal-2020-data","methods":["data encoding scheme","entity enriched relation classification bert model","data driven approach"],"center_method":[null,null,null],"tasks":["relation extraction","knowledge graph completion","joint learning","relation learning","multiple instance learning","distant supervision","noise reduction"],"center_task":["relation extraction",null,null,null,null,"distant supervision",null],"Goal":["Good Health and Well-Being"],"text":"A Data-driven Approach for Noise Reduction in Distantly Supervised Biomedical Relation Extraction. Fact triples are a common form of structured knowledge used within the biomedical domain. As the amount of unstructured scientific texts continues to grow, manual annotation of these texts for the task of relation extraction becomes increasingly expensive. Distant supervision offers a viable approach to combat this by quickly producing large amounts of labeled, but considerably noisy, data. We aim to reduce such noise by extending an entity-enriched relation classification BERT model to the problem of multiple instance learning, and defining a simple data encoding scheme that significantly reduces noise, reaching state-of-the-art performance for distantly-supervised biomedical relation extraction. Our approach further encodes knowledge about the direction of relation triples, allowing for increased focus on relation learning by reducing noise and alleviating the need for joint learning with knowledge graph completion.","label":1,"title_clean":"A Data driven Approach for Noise Reduction in Distantly Supervised Biomedical Relation Extraction","abstract_clean":"Fact triples are a common form of structured knowledge used within the biomedical domain. As the amount of unstructured scientific texts continues to grow, manual annotation of these texts for the task of relation extraction becomes increasingly expensive. Distant supervision offers a viable approach to combat this by quickly producing large amounts of labeled, but considerably noisy, data. We aim to reduce such noise by extending an entity enriched relation classification BERT model to the problem of multiple instance learning, and defining a simple data encoding scheme that significantly reduces noise, reaching state of the art performance for distantly supervised biomedical relation extraction. Our approach further encodes knowledge about the direction of relation triples, allowing for increased focus on relation learning by reducing noise and alleviating the need for joint learning with knowledge graph completion.","url":"https:\/\/aclanthology.org\/2020.bionlp-1.20"},{"ID":"amin-etal-2022-using","methods":["word2vec based word embeddings","bert","correlation analysis","word importance models"],"center_method":[null,"bert",null,null],"tasks":["6 class word importance classification task","live tv captioning"],"center_task":[null,null],"Goal":["Reduced Inequalities"],"text":"Using BERT Embeddings to Model Word Importance in Conversational Transcripts for Deaf and Hard of Hearing Users. Deaf and hard of hearing individuals regularly rely on captioning while watching live TV. Live TV captioning is evaluated by regulatory agencies using various caption evaluation metrics. However, caption evaluation metrics are often not informed by preferences of DHH users or how meaningful the captions are. There is a need to construct caption evaluation metrics that take the relative importance of words in a transcript into account. We conducted correlation analysis between two types of word embeddings and human-annotated labeled wordimportance scores in existing corpus. We found that normalized contextualized word embeddings generated using BERT correlated better with manually annotated importance scores than word2vec-based word embeddings. We make available a pairing of word embeddings and their human-annotated importance scores. We also provide proof-of-concept utility by training word importance models, achieving an F1-score of 0.57 in the 6-class word importance classification task.","label":1,"title_clean":"Using BERT Embeddings to Model Word Importance in Conversational Transcripts for Deaf and Hard of Hearing Users","abstract_clean":"Deaf and hard of hearing individuals regularly rely on captioning while watching live TV. Live TV captioning is evaluated by regulatory agencies using various caption evaluation metrics. However, caption evaluation metrics are often not informed by preferences of DHH users or how meaningful the captions are. There is a need to construct caption evaluation metrics that take the relative importance of words in a transcript into account. We conducted correlation analysis between two types of word embeddings and human annotated labeled wordimportance scores in existing corpus. We found that normalized contextualized word embeddings generated using BERT correlated better with manually annotated importance scores than word2vec based word embeddings. We make available a pairing of word embeddings and their human annotated importance scores. We also provide proof of concept utility by training word importance models, achieving an F1 score of 0.57 in the 6 class word importance classification task.","url":"https:\/\/aclanthology.org\/2022.ltedi-1.5"},{"ID":"ananiadou-etal-2010-evaluating","methods":["search engine","text mining based search engine","text mining features","text mining based educational search portal","user centred framework"],"center_method":["search engine",null,null,null,null],"tasks":["classification of search results","evaluation methods","text mining","automatic clustering of search results"],"center_task":[null,"evaluation methods","text mining",null],"Goal":["Good Health and Well-Being","Quality Education"],"text":"Evaluating a Text Mining Based Educational Search Portal. In this paper, we present the main features of a text mining based search engine for the UK Educational Evidence Portal available at the UK National Centre for Text Mining (NaCTeM), together with a user-centred framework for the evaluation of the search engine. The framework is adapted from an existing proposal by the ISLE (EAGLES) Evaluation Working group. We introduce the metrics employed for the evaluation, and explain how these relate to the text mining based search engine. Following this, we describe how we applied the framework to the evaluation of a number of key text mining features of the search engine, namely the automatic clustering of search results, classification of search results according to a taxonomy, and identification of topics and other documents that are related to a chosen document. Finally, we present the results of the evaluation in terms of the strengths, weaknesses and improvements identified for each of these features.","label":1,"title_clean":"Evaluating a Text Mining Based Educational Search Portal","abstract_clean":"In this paper, we present the main features of a text mining based search engine for the UK Educational Evidence Portal available at the UK National Centre for Text Mining (NaCTeM), together with a user centred framework for the evaluation of the search engine. The framework is adapted from an existing proposal by the ISLE (EAGLES) Evaluation Working group. We introduce the metrics employed for the evaluation, and explain how these relate to the text mining based search engine. Following this, we describe how we applied the framework to the evaluation of a number of key text mining features of the search engine, namely the automatic clustering of search results, classification of search results according to a taxonomy, and identification of topics and other documents that are related to a chosen document. Finally, we present the results of the evaluation in terms of the strengths, weaknesses and improvements identified for each of these features.","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2010\/pdf\/121_Paper.pdf"},{"ID":"anastasopoulos-etal-2020-tico","methods":["tico - 19"],"center_method":[null],"tasks":["covid 19","development","translation initiative","sars cov 2 virus","artificial intelligence"],"center_task":["covid 19",null,null,null,"artificial intelligence"],"Goal":["Good Health and Well-Being"],"text":"TICO-19: the Translation Initiative for COvid-19. The COVID-19 pandemic is the worst pandemic to strike the world in over a century. Crucial to stemming the tide of the SARS-CoV-2 virus is communicating to vulnerable populations the means by which they can protect themselves. To this end, the collaborators forming the Translation Initiative for COvid-19 (TICO-19) 1 have made test and development data available to AI and MT researchers in 35 different languages in order to foster the development of tools and resources for improving access to information about COVID-19 in these languages. In addition to 9 highresourced, \"pivot\" languages, the team is targeting 26 lesser resourced languages, in particular languages of Africa, South Asia and SouthEast Asia, whose populations may be the most vulnerable to the spread of the virus. The same data is translated into all of the languages represented, meaning that testing or development can be done for any pairing of languages in the set. Further, the team is converting the test and development data into translation memories (TMXs) that can be used by localizers from and to any of the languages. 2","label":1,"title_clean":"TICO 19: the Translation Initiative for COvid 19","abstract_clean":"The COVID 19 pandemic is the worst pandemic to strike the world in over a century. Crucial to stemming the tide of the SARS CoV 2 virus is communicating to vulnerable populations the means by which they can protect themselves. To this end, the collaborators forming the Translation Initiative for COvid 19 (TICO 19) 1 have made test and development data available to AI and MT researchers in 35 different languages in order to foster the development of tools and resources for improving access to information about COVID 19 in these languages. In addition to 9 highresourced, \"pivot\" languages, the team is targeting 26 lesser resourced languages, in particular languages of Africa, South Asia and SouthEast Asia, whose populations may be the most vulnerable to the spread of the virus. The same data is translated into all of the languages represented, meaning that testing or development can be done for any pairing of languages in the set. Further, the team is converting the test and development data into translation memories (TMXs) that can be used by localizers from and to any of the languages. 2","url":"https:\/\/aclanthology.org\/2020.nlpcovid19-2.5"},{"ID":"andersen-etal-2013-developing","methods":["self assessment and tutoring system","language learner"],"center_method":[null,null],"tasks":["detection and correction of frequent errors","automated feedback","writing"],"center_task":[null,null,null],"Goal":["Quality Education"],"text":"Developing and testing a self-assessment and tutoring system. Automated feedback on writing may be a useful complement to teacher comments in the process of learning a foreign language. This paper presents a self-assessment and tutoring system which combines an holistic score with detection and correction of frequent errors and furthermore provides a qualitative assessment of each individual sentence, thus making the language learner aware of potentially problematic areas rather than providing a panacea. The system has been tested by learners in a range of educational institutions, and their feedback has guided its development.","label":1,"title_clean":"Developing and testing a self assessment and tutoring system","abstract_clean":"Automated feedback on writing may be a useful complement to teacher comments in the process of learning a foreign language. This paper presents a self assessment and tutoring system which combines an holistic score with detection and correction of frequent errors and furthermore provides a qualitative assessment of each individual sentence, thus making the language learner aware of potentially problematic areas rather than providing a panacea. The system has been tested by learners in a range of educational institutions, and their feedback has guided its development.","url":"https:\/\/aclanthology.org\/W13-1704"},{"ID":"arsenos-siolas-2020-ntuaails","methods":["lstm","propaganda technique","word embeddings","elmo","deep neural network","bilstms"],"center_method":["lstm",null,"word embeddings","elmo","deep neural network",null],"tasks":["technique classification","ntuaails","tc sub task","si","detection of propaganda techniques","semeval","span identification"],"center_task":[null,null,null,null,null,"semeval",null],"Goal":["Peace, Justice and Strong Institutions"],"text":"NTUAAILS at SemEval-2020 Task 11: Propaganda Detection and Classification with biLSTMs and ELMo. This paper describes the NTUAAILS submission for SemEval 2020 Task 11 Detection of Propaganda Techniques in News Articles. This task comprises of two different sub-tasks, namely A: Span Identification (SI), B: Technique Classification (TC). The goal for the SI sub-task is to identify specific fragments, in a given plain text, containing at least one propaganda technique. The TC sub-task aims to identify the applied propaganda technique in a given text fragment. A different model was trained for each sub-task. Our best performing system for the SI task consists of pre-trained ELMo word embeddings followed by residual bidirectional LSTM network. For the TC sub-task pre-trained word embeddings from GloVe fed to a bidirectional LSTM neural network. The models achieved rank 28 among 36 teams with F1 score of 0.335 and rank 25 among 31 teams with 0.463 F1 score for SI and TC sub-tasks respectively. Our results indicate that the proposed deep learning models, although relatively simple in architecture and fast to train, achieve satisfactory results in the tasks on hand.","label":1,"title_clean":"NTUAAILS at SemEval 2020 Task 11: Propaganda Detection and Classification with biLSTMs and ELMo","abstract_clean":"This paper describes the NTUAAILS submission for SemEval 2020 Task 11 Detection of Propaganda Techniques in News Articles. This task comprises of two different sub tasks, namely A: Span Identification (SI), B: Technique Classification (TC). The goal for the SI sub task is to identify specific fragments, in a given plain text, containing at least one propaganda technique. The TC sub task aims to identify the applied propaganda technique in a given text fragment. A different model was trained for each sub task. Our best performing system for the SI task consists of pre trained ELMo word embeddings followed by residual bidirectional LSTM network. For the TC sub task pre trained word embeddings from GloVe fed to a bidirectional LSTM neural network. The models achieved rank 28 among 36 teams with F1 score of 0.335 and rank 25 among 31 teams with 0.463 F1 score for SI and TC sub tasks respectively. Our results indicate that the proposed deep learning models, although relatively simple in architecture and fast to train, achieve satisfactory results in the tasks on hand.","url":"https:\/\/aclanthology.org\/2020.semeval-1.195"},{"ID":"arslan-etal-2020-modeling","methods":["modeling","annotation","framenet","framenet frames","berkeley framenet"],"center_method":[null,"annotation",null,null,null],"tasks":["modeling factual claims","matching claims"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Modeling Factual Claims with Semantic Frames. In this paper, we introduce an extension of the Berkeley FrameNet for the structured and semantic modeling of factual claims. Modeling is a robust tool that can be leveraged in many different tasks such as matching claims to existing fact-checks and translating claims to structured queries. Our work introduces 11 new manually crafted frames along with 9 existing FrameNet frames, all of which have been selected with fact-checking in mind. Along with these frames, we are also providing 2, 540 fully annotated sentences, which can be used to understand how these frames are intended to work and to train machine learning models. Finally, we are also releasing our annotation tool to facilitate other researchers to make their own local extensions to FrameNet.","label":1,"title_clean":"Modeling Factual Claims with Semantic Frames","abstract_clean":"In this paper, we introduce an extension of the Berkeley FrameNet for the structured and semantic modeling of factual claims. Modeling is a robust tool that can be leveraged in many different tasks such as matching claims to existing fact checks and translating claims to structured queries. Our work introduces 11 new manually crafted frames along with 9 existing FrameNet frames, all of which have been selected with fact checking in mind. Along with these frames, we are also providing 2, 540 fully annotated sentences, which can be used to understand how these frames are intended to work and to train machine learning models. Finally, we are also releasing our annotation tool to facilitate other researchers to make their own local extensions to FrameNet.","url":"https:\/\/aclanthology.org\/2020.lrec-1.306"},{"ID":"asgari-etal-2020-topic","methods":["conversation based measures","computational method"],"center_method":[null,null],"tasks":["topic based measures of conversation","mci"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"Topic-Based Measures of Conversation for Detecting Mild CognitiveImpairment. Conversation is a complex cognitive task that engages multiple aspects of cognitive functions to remember the discussed topics, monitor the semantic and linguistic elements, and recognize others' emotions. In this paper, we propose a computational method based on the lexical coherence of consecutive utterances to quantify topical variations in semistructured conversations of older adults with cognitive impairments. Extracting the lexical knowledge of conversational utterances, our method generates a set of novel conversational measures that indicate underlying cognitive deficits among subjects with mild cognitive impairment (MCI). Our preliminary results verify the utility of the proposed conversation-based measures in distinguishing MCI from healthy controls.","label":1,"title_clean":"Topic Based Measures of Conversation for Detecting Mild CognitiveImpairment","abstract_clean":"Conversation is a complex cognitive task that engages multiple aspects of cognitive functions to remember the discussed topics, monitor the semantic and linguistic elements, and recognize others' emotions. In this paper, we propose a computational method based on the lexical coherence of consecutive utterances to quantify topical variations in semistructured conversations of older adults with cognitive impairments. Extracting the lexical knowledge of conversational utterances, our method generates a set of novel conversational measures that indicate underlying cognitive deficits among subjects with mild cognitive impairment (MCI). Our preliminary results verify the utility of the proposed conversation based measures in distinguishing MCI from healthy controls.","url":"https:\/\/aclanthology.org\/2020.nlpmc-1.9.pdf"},{"ID":"atanasov-etal-2019-predicting","methods":["machine learning methods","learned representations"],"center_method":["machine learning methods",null],"tasks":["supervised learning scenario"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Predicting the Role of Political Trolls in Social Media. We investigate the political roles of \"Internet trolls\" in social media. Political trolls, such as the ones linked to the Russian Internet Research Agency (IRA), have recently gained enormous attention for their ability to sway public opinion and even influence elections. Analysis of the online traces of trolls has shown different behavioral patterns, which target different slices of the population. However, this analysis is manual and laborintensive, thus making it impractical as a firstresponse tool for newly-discovered troll farms. In this paper, we show how to automate this analysis by using machine learning in a realistic setting. In particular, we show how to classify trolls according to their political role-left, news feed, right-by using features extracted from social media, i.e., Twitter, in two scenarios: (i) in a traditional supervised learning scenario, where labels for trolls are available, and (ii) in a distant supervision scenario, where labels for trolls are not available, and we rely on more-commonly-available labels for news outlets mentioned by the trolls. Technically, we leverage the community structure and the text of the messages in the online social network of trolls represented as a graph, from which we extract several types of learned representations, i.e., embeddings, for the trolls. Experiments on the \"IRA Russian Troll\" dataset show that our methodology improves over the state-of-the-art in the first scenario, while providing a compelling case for the second scenario, which has not been explored in the literature thus far.","label":1,"title_clean":"Predicting the Role of Political Trolls in Social Media","abstract_clean":"We investigate the political roles of \"Internet trolls\" in social media. Political trolls, such as the ones linked to the Russian Internet Research Agency (IRA), have recently gained enormous attention for their ability to sway public opinion and even influence elections. Analysis of the online traces of trolls has shown different behavioral patterns, which target different slices of the population. However, this analysis is manual and laborintensive, thus making it impractical as a firstresponse tool for newly discovered troll farms. In this paper, we show how to automate this analysis by using machine learning in a realistic setting. In particular, we show how to classify trolls according to their political role left, news feed, right by using features extracted from social media, i.e., Twitter, in two scenarios: (i) in a traditional supervised learning scenario, where labels for trolls are available, and (ii) in a distant supervision scenario, where labels for trolls are not available, and we rely on more commonly available labels for news outlets mentioned by the trolls. Technically, we leverage the community structure and the text of the messages in the online social network of trolls represented as a graph, from which we extract several types of learned representations, i.e., embeddings, for the trolls. Experiments on the \"IRA Russian Troll\" dataset show that our methodology improves over the state of the art in the first scenario, while providing a compelling case for the second scenario, which has not been explored in the literature thus far.","url":"https:\/\/aclanthology.org\/K19-1096"},{"ID":"azab-etal-2013-nlp","methods":["text reading tool","web based tool","nlp applications"],"center_method":[null,null,"nlp applications"],"tasks":["non - native english readers"],"center_task":[null],"Goal":["Reduced Inequalities"],"text":"An NLP-based Reading Tool for Aiding Non-native English Readers. This paper describes a text-reading tool that makes extensive use of widelyavailable NLP tools and resources to aid non-native English speakers overcome language related hindrances while reading a text. It is a web-based tool, that can be accessed from browsers running on PCs or tablets, and provides the reader with an intelligent e-book functionality.","label":1,"title_clean":"An NLP based Reading Tool for Aiding Non native English Readers","abstract_clean":"This paper describes a text reading tool that makes extensive use of widelyavailable NLP tools and resources to aid non native English speakers overcome language related hindrances while reading a text. It is a web based tool, that can be accessed from browsers running on PCs or tablets, and provides the reader with an intelligent e book functionality.","url":"https:\/\/aclanthology.org\/R13-1006"},{"ID":"bagheri-garakani-etal-2022-improving","methods":["high precision crossencoder bert model"],"center_method":[null],"tasks":["ranking","high precision query product semantic similarity","e commerce setting","optimization","product search"],"center_task":[null,null,null,null,null],"Goal":["Decent Work and Economic Growth","Industry, Innovation and Infrastrucure"],"text":"Improving Relevance Quality in Product Search using High-Precision Query-Product Semantic Similarity. Ensuring relevance quality in product search is a critical task as it impacts the customer's ability to find intended products in the short-term as well as the general perception and trust of the e-commerce system in the long term. In this work we leverage a high-precision crossencoder BERT model for semantic similarity between customer query and products and survey its effectiveness for three ranking applications where offline-generated scores could be used: (1) as an offline metric for estimating relevance quality impact, (2) as a re-ranking feature covering head\/torso queries, and (3) as a training objective for optimization. We present results on effectiveness of this strategy for the large e-commerce setting, which has general applicability for choice of other high-precision models and tasks in ranking.","label":1,"title_clean":"Improving Relevance Quality in Product Search using High Precision Query Product Semantic Similarity","abstract_clean":"Ensuring relevance quality in product search is a critical task as it impacts the customer's ability to find intended products in the short term as well as the general perception and trust of the e commerce system in the long term. In this work we leverage a high precision crossencoder BERT model for semantic similarity between customer query and products and survey its effectiveness for three ranking applications where offline generated scores could be used: (1) as an offline metric for estimating relevance quality impact, (2) as a re ranking feature covering head\/torso queries, and (3) as a training objective for optimization. We present results on effectiveness of this strategy for the large e commerce setting, which has general applicability for choice of other high precision models and tasks in ranking.","url":"https:\/\/aclanthology.org\/2022.ecnlp-1.6.pdf"},{"ID":"bajaj-etal-2022-evaluating","methods":["bert","umls metathesaurus","biowordvec embeddings","feature extraction methods","siamese network","word embeddings"],"center_method":["bert",null,null,null,null,"word embeddings"],"tasks":["vocabulary alignment","synonymy prediction","umls unified medical language system metathesaurus construction process"],"center_task":[null,null,null],"Goal":["Good Health and Well-Being"],"text":"Evaluating Biomedical Word Embeddings for Vocabulary Alignment at Scale in the UMLS Metathesaurus Using Siamese Networks. Recent work uses a Siamese Network, initialized with BioWordVec embeddings (distributed word embeddings), for predicting synonymy among biomedical terms to automate a part of the UMLS (Unified Medical Language System) Metathesaurus construction process. We evaluate the use of contextualized word embeddings extracted from nine different biomedical BERT-based models for synonymy prediction in the UMLS by replacing BioWordVec embeddings with embeddings extracted from each biomedical BERT model using different feature extraction methods. Surprisingly, we find that Siamese Networks initialized with BioWordVec embeddings still outperform the Siamese Networks initialized with embedding extracted from biomedical BERT model.","label":1,"title_clean":"Evaluating Biomedical Word Embeddings for Vocabulary Alignment at Scale in the UMLS Metathesaurus Using Siamese Networks","abstract_clean":"Recent work uses a Siamese Network, initialized with BioWordVec embeddings (distributed word embeddings), for predicting synonymy among biomedical terms to automate a part of the UMLS (Unified Medical Language System) Metathesaurus construction process. We evaluate the use of contextualized word embeddings extracted from nine different biomedical BERT based models for synonymy prediction in the UMLS by replacing BioWordVec embeddings with embeddings extracted from each biomedical BERT model using different feature extraction methods. Surprisingly, we find that Siamese Networks initialized with BioWordVec embeddings still outperform the Siamese Networks initialized with embedding extracted from biomedical BERT model.","url":"https:\/\/aclanthology.org\/2022.insights-1.11"},{"ID":"baldwin-etal-2003-alias","methods":["threattrackers","alias i","information retrieval search engines","information access application"],"center_method":[null,null,null,null],"tasks":["named entity detection","information extraction","total information awareness program","threat trackers"],"center_task":[null,"information extraction",null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Alias-i Threat Trackers. Alias-i ThreatTrackers are an advanced information access application designed around the needs of analysts working through a large daily data feed. ThreatTrackers help analysts decompose an information gathering topic like the unfolding political situation in Iraq into specifications including people, places, organizations and relationships. These specifications are then used to collect and browse information on a daily basis. The nearest related technologies are information retrieval (search engines), document categorization, information extraction and named entity detection.ThreatTrackers are currently being used in the Total Information Awareness program.","label":1,"title_clean":"Alias i Threat Trackers","abstract_clean":"Alias i ThreatTrackers are an advanced information access application designed around the needs of analysts working through a large daily data feed. ThreatTrackers help analysts decompose an information gathering topic like the unfolding political situation in Iraq into specifications including people, places, organizations and relationships. These specifications are then used to collect and browse information on a daily basis. The nearest related technologies are information retrieval (search engines), document categorization, information extraction and named entity detection.ThreatTrackers are currently being used in the Total Information Awareness program.","url":"https:\/\/aclanthology.org\/N03-4002"},{"ID":"balouchzahi-etal-2021-mucs","methods":["mucsdravidianlangtech eacl2021","cooli ensemble","kn en","cooli keras"],"center_method":[null,null,null,null],"tasks":["hate speech","code mixed texts","dravidian languages"],"center_task":["hate speech",null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"MUCS@DravidianLangTech-EACL2021:COOLI-Code-Mixing Offensive Language Identification. This paper describes the models submitted by the team MUCS for Offensive Language Identification in Dravidian Languages-EACL 2021 shared task that aims at identifying and classifying code-mixed texts of three language pairs namely, Kannada-English (Kn-En), Malayalam-English (Ma-En), and Tamil-English (Ta-En) into six predefined categories (5 categories in Ma-En language pair). Two models, namely, COOLI-Ensemble and COOLI-Keras are trained with the char sequences extracted from the sentences combined with words in the sentences as features. Out of the two proposed models, COOLI-Ensemble model (best among our models) obtained first rank for Ma-En language pair with 0.97 weighted F1-score and fourth and sixth ranks with 0.75 and 0.69 weighted F1-score for Ta-En and Kn-En language pairs respectively.","label":1,"title_clean":"MUCS@DravidianLangTech EACL2021:COOLI Code Mixing Offensive Language Identification","abstract_clean":"This paper describes the models submitted by the team MUCS for Offensive Language Identification in Dravidian Languages EACL 2021 shared task that aims at identifying and classifying code mixed texts of three language pairs namely, Kannada English (Kn En), Malayalam English (Ma En), and Tamil English (Ta En) into six predefined categories (5 categories in Ma En language pair). Two models, namely, COOLI Ensemble and COOLI Keras are trained with the char sequences extracted from the sentences combined with words in the sentences as features. Out of the two proposed models, COOLI Ensemble model (best among our models) obtained first rank for Ma En language pair with 0.97 weighted F1 score and fourth and sixth ranks with 0.75 and 0.69 weighted F1 score for Ta En and Kn En language pairs respectively.","url":"https:\/\/aclanthology.org\/2021.dravidianlangtech-1.47"},{"ID":"bartolini-etal-2004-semantic","methods":["nlp based techniques","ilc","salem"],"center_method":[null,null,null],"tasks":["semantic mark up","semantic annotation and indexing of italian legislative texts"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Semantic Mark-up of Italian Legal Texts Through NLP-based Techniques. In this paper we illustrate an approach to information extraction from legal texts using SALEM. SALEM is an NLP architecture for semantic annotation and indexing of Italian legislative texts, developed by ILC in close collaboration with ITTIG-CNR, Florence. Results of SALEM performance on a test sample of about 500 Italian law paragraphs are provided.","label":1,"title_clean":"Semantic Mark up of Italian Legal Texts Through NLP based Techniques","abstract_clean":"In this paper we illustrate an approach to information extraction from legal texts using SALEM. SALEM is an NLP architecture for semantic annotation and indexing of Italian legislative texts, developed by ILC in close collaboration with ITTIG CNR, Florence. Results of SALEM performance on a test sample of about 500 Italian law paragraphs are provided.","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2004\/pdf\/709.pdf"},{"ID":"basta-etal-2019-evaluating","methods":["word embedding techniques","word embeddings","word vector representations"],"center_method":[null,"word embeddings",null],"tasks":["word embedding computation","nlp applications","gender bias mitigation"],"center_task":[null,"nlp applications","gender bias mitigation"],"Goal":["Gender Equality"],"text":"Evaluating the Underlying Gender Bias in Contextualized Word Embeddings. Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.","label":1,"title_clean":"Evaluating the Underlying Gender Bias in Contextualized Word Embeddings","abstract_clean":"Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.","url":"https:\/\/aclanthology.org\/W19-3805"},{"ID":"batista-navarro-ananiadou-2011-building","methods":["annotation"],"center_method":["annotation"],"tasks":["coreference resolution"],"center_task":["coreference resolution"],"Goal":["Good Health and Well-Being","Industry, Innovation and Infrastrucure"],"text":"Building a Coreference-Annotated Corpus from the Domain of Biochemistry. One of the reasons for which the resolution of coreferences has remained a challenging information extraction task, especially in the biomedical domain, is the lack of training data in the form of annotated corpora. In order to address this issue, we developed the HANAPIN corpus. It consists of full-text articles from biochemistry literature, covering entities of several semantic types: chemical compounds, drug targets (e.g., proteins, enzymes, cell lines, pathogens), diseases, organisms and drug effects. All of the coreferring expressions pertaining to these semantic types were annotated based on the annotation scheme that we developed. We observed four general types of coreferences in the corpus: sortal, pronominal, abbreviation and numerical. Using the MASI distance metric, we obtained 84% in computing the inter-annotator agreement in terms of Krippendorff's alpha. Consisting of 20 full-text, open-access articles, the corpus will enable other researchers to use it as a resource for their own coreference resolution methodologies.","label":1,"title_clean":"Building a Coreference Annotated Corpus from the Domain of Biochemistry","abstract_clean":"One of the reasons for which the resolution of coreferences has remained a challenging information extraction task, especially in the biomedical domain, is the lack of training data in the form of annotated corpora. In order to address this issue, we developed the HANAPIN corpus. It consists of full text articles from biochemistry literature, covering entities of several semantic types: chemical compounds, drug targets (e.g., proteins, enzymes, cell lines, pathogens), diseases, organisms and drug effects. All of the coreferring expressions pertaining to these semantic types were annotated based on the annotation scheme that we developed. We observed four general types of coreferences in the corpus: sortal, pronominal, abbreviation and numerical. Using the MASI distance metric, we obtained 84% in computing the inter annotator agreement in terms of Krippendorff's alpha. Consisting of 20 full text, open access articles, the corpus will enable other researchers to use it as a resource for their own coreference resolution methodologies.","url":"https:\/\/aclanthology.org\/W11-0210"},{"ID":"beltagy-etal-2019-scibert","methods":["unsupervised algorithm","bert","language models"],"center_method":["unsupervised algorithm","bert","language models"],"tasks":["scientific nlp tasks","classification","sequence tagging","dependency parsing"],"center_task":[null,"classification",null,"dependency parsing"],"Goal":["Industry, Innovation and Infrastrucure"],"text":"SciBERT: A Pretrained Language Model for Scientific Text. Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SCIBERT, a pretrained language model based on BERT (Devlin et al., 2019) to address the lack of highquality, large-scale labeled scientific data. SCIBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https:\/\/github. com\/allenai\/scibert\/.","label":1,"title_clean":"SciBERT: A Pretrained Language Model for Scientific Text","abstract_clean":"Obtaining large scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SCIBERT, a pretrained language model based on BERT (Devlin et al., 2019) to address the lack of highquality, large scale labeled scientific data. SCIBERT leverages unsupervised pretraining on a large multi domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state of the art results on several of these tasks. The code and pretrained models are available at https:\/\/github. com\/allenai\/scibert\/.","url":"https:\/\/aclanthology.org\/D19-1371"},{"ID":"ben-abacha-zweigenbaum-2011-medical","methods":["machine learning methods","domain knowledge","hybrid approach","chunker","comparaison of semantic and statistical methods"],"center_method":["machine learning methods",null,"hybrid approach",null,null],"tasks":["medical entity recognition","classification","medical texts analysis"],"center_task":[null,"classification",null],"Goal":["Good Health and Well-Being"],"text":"Medical Entity Recognition: A Comparaison of Semantic and Statistical Methods. Medical Entity Recognition is a crucial step towards efficient medical texts analysis. In this paper we present and compare three methods based on domain-knowledge and machine-learning techniques. We study two research directions through these approaches: (i) a first direction where noun phrases are extracted in a first step with a chunker before the final classification step and (ii) a second direction where machine learning techniques are used to identify simultaneously entities boundaries and categories. Each of the presented approaches is tested on a standard corpus of clinical texts. The obtained results show that the hybrid approach based on both machine learning and domain knowledge obtains the best performance.","label":1,"title_clean":"Medical Entity Recognition: A Comparaison of Semantic and Statistical Methods","abstract_clean":"Medical Entity Recognition is a crucial step towards efficient medical texts analysis. In this paper we present and compare three methods based on domain knowledge and machine learning techniques. We study two research directions through these approaches: (i) a first direction where noun phrases are extracted in a first step with a chunker before the final classification step and (ii) a second direction where machine learning techniques are used to identify simultaneously entities boundaries and categories. Each of the presented approaches is tested on a standard corpus of clinical texts. The obtained results show that the hybrid approach based on both machine learning and domain knowledge obtains the best performance.","url":"https:\/\/aclanthology.org\/W11-0207"},{"ID":"berzak-etal-2015-contrastive","methods":["contrastive analysis","bootstrapping approach","computational framework","typology driven model","predictive power"],"center_method":[null,null,null,null,null],"tasks":["esl","linguistic inquiry","second language acquisition","typology driven estimation of grammatical error distributions","crosslinguistic transfer","grammatical errors"],"center_task":[null,null,null,null,null,null],"Goal":["Quality Education","Reduced Inequalities"],"text":"Contrastive Analysis with Predictive Power: Typology Driven Estimation of Grammatical Error Distributions in ESL. This work examines the impact of crosslinguistic transfer on grammatical errors in English as Second Language (ESL) texts. Using a computational framework that formalizes the theory of Contrastive Analysis (CA), we demonstrate that language specific error distributions in ESL writing can be predicted from the typological properties of the native language and their relation to the typology of English. Our typology driven model enables to obtain accurate estimates of such distributions without access to any ESL data for the target languages. Furthermore, we present a strategy for adjusting our method to low-resource languages that lack typological documentation using a bootstrapping approach which approximates native language typology from ESL texts. Finally, we show that our framework is instrumental for linguistic inquiry seeking to identify first language factors that contribute to a wide range of difficulties in second language acquisition.","label":1,"title_clean":"Contrastive Analysis with Predictive Power: Typology Driven Estimation of Grammatical Error Distributions in ESL","abstract_clean":"This work examines the impact of crosslinguistic transfer on grammatical errors in English as Second Language (ESL) texts. Using a computational framework that formalizes the theory of Contrastive Analysis (CA), we demonstrate that language specific error distributions in ESL writing can be predicted from the typological properties of the native language and their relation to the typology of English. Our typology driven model enables to obtain accurate estimates of such distributions without access to any ESL data for the target languages. Furthermore, we present a strategy for adjusting our method to low resource languages that lack typological documentation using a bootstrapping approach which approximates native language typology from ESL texts. Finally, we show that our framework is instrumental for linguistic inquiry seeking to identify first language factors that contribute to a wide range of difficulties in second language acquisition.","url":"https:\/\/aclanthology.org\/K15-1010.pdf"},{"ID":"bestgen-2019-tintin","methods":["tintin","supervised learning procedure"],"center_method":[null,null],"tasks":["detecting hyperpartisan news article"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Tintin at SemEval-2019 Task 4: Detecting Hyperpartisan News Article with only Simple Tokens. Tintin, the system proposed by the CECL for the Hyperpartisan News Detection task of Se-mEval 2019, is exclusively based on the tokens that make up the documents and a standard supervised learning procedure. It obtained very contrasting results: poor on the main task, but much more effective at distinguishing documents published by hyperpartisan media outlets from unbiased ones, as it ranked first. An analysis of the most important features highlighted the positive aspects, but also some potential limitations of the approach.","label":1,"title_clean":"Tintin at SemEval 2019 Task 4: Detecting Hyperpartisan News Article with only Simple Tokens","abstract_clean":"Tintin, the system proposed by the CECL for the Hyperpartisan News Detection task of Se mEval 2019, is exclusively based on the tokens that make up the documents and a standard supervised learning procedure. It obtained very contrasting results: poor on the main task, but much more effective at distinguishing documents published by hyperpartisan media outlets from unbiased ones, as it ranked first. An analysis of the most important features highlighted the positive aspects, but also some potential limitations of the approach.","url":"https:\/\/aclanthology.org\/S19-2186.pdf"},{"ID":"bhandwaldar-zadrozny-2018-uncc","methods":["extractive summarization techniques","named entity based method","uncc qa"],"center_method":[null,null,null],"tasks":["semantic question and answering task","ranking of entities","factoidand list type question","paragraph sized summaries"],"center_task":[null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"UNCC QA: Biomedical Question Answering system. In this paper, we detail our submission to the BioASQ competition's Biomedical Semantic Question and Answering task. Our system uses extractive summarization techniques to generate answers and has scored highest ROUGE-2 and Rogue-SU4 in all test batch sets. Our contributions are named-entity based method for answering factoid and list questions, and an extractive summarization techniques for building paragraph-sized summaries, based on lexical chains. Our system got highest ROUGE-2 and ROUGE-SU4 scores for ideal-type answers in all test batch sets. We also discuss the limitations of the described system, such lack of the evaluation on other criteria (e.g. manual). Also, for factoidand list-type question our system got low accuracy (which suggests that our algorithm needs to improve in the ranking of entities).","label":1,"title_clean":"UNCC QA: Biomedical Question Answering system","abstract_clean":"In this paper, we detail our submission to the BioASQ competition's Biomedical Semantic Question and Answering task. Our system uses extractive summarization techniques to generate answers and has scored highest ROUGE 2 and Rogue SU4 in all test batch sets. Our contributions are named entity based method for answering factoid and list questions, and an extractive summarization techniques for building paragraph sized summaries, based on lexical chains. Our system got highest ROUGE 2 and ROUGE SU4 scores for ideal type answers in all test batch sets. We also discuss the limitations of the described system, such lack of the evaluation on other criteria (e.g. manual). Also, for factoidand list type question our system got low accuracy (which suggests that our algorithm needs to improve in the ranking of entities).","url":"https:\/\/aclanthology.org\/W18-5308"},{"ID":"biatov-kohler-2002-methods","methods":["speech data acquisition","finite state automata","language models","acoustic models"],"center_method":[null,null,"language models",null],"tasks":["acoustic model training","recognition task"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Methods and Tools for Speech Data Acquisition exploiting a Database of German Parliamentary Speeches and Transcripts from the Internet. This paper describes methods that exploit stenographic transcripts of the German parliament to improve the acoustic models of a speech recognition system for this domain. The stenographic transcripts and the speech data are available on the Internet. Using data from the Internet makes it possible to avoid the costly process of the collection and annotation of a huge amount of data. The automatic data acquisition technique works using the stenographic transcripts and acoustic data from the German parliamentary speeches plus general acoustic models, trained on different data. The idea of this technique is to generate special finite state automata from the stenographic transcripts. These finite state automata simulate potential possible correspondences between the stenographic transcript and the spoken audio content, i.e. accurate transcript. The first step is the recognition of the speech data using finite state automaton as a language model. The next step is to find, to extract and to verify the match between sections of recognized words and actually spoken audio content. After this, the automatically extracted and verified data can be used for acoustic model training. Experiments show that for a given recognition task from the German Parliament domain the absolute decrease of the word error rate is 20%.","label":1,"title_clean":"Methods and Tools for Speech Data Acquisition exploiting a Database of German Parliamentary Speeches and Transcripts from the Internet","abstract_clean":"This paper describes methods that exploit stenographic transcripts of the German parliament to improve the acoustic models of a speech recognition system for this domain. The stenographic transcripts and the speech data are available on the Internet. Using data from the Internet makes it possible to avoid the costly process of the collection and annotation of a huge amount of data. The automatic data acquisition technique works using the stenographic transcripts and acoustic data from the German parliamentary speeches plus general acoustic models, trained on different data. The idea of this technique is to generate special finite state automata from the stenographic transcripts. These finite state automata simulate potential possible correspondences between the stenographic transcript and the spoken audio content, i.e. accurate transcript. The first step is the recognition of the speech data using finite state automaton as a language model. The next step is to find, to extract and to verify the match between sections of recognized words and actually spoken audio content. After this, the automatically extracted and verified data can be used for acoustic model training. Experiments show that for a given recognition task from the German Parliament domain the absolute decrease of the word error rate is 20%.","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2002\/pdf\/176.pdf"},{"ID":"bicici-van-genabith-2013-cngl-grading","methods":["rtm models","predictions","student response analysis","computational model","cngl","referential translation machines","mtpp machine translation performance predictor model"],"center_method":[null,"predictions",null,null,null,null,null],"tasks":["automatically grading student answers","question answering","machine translation","quality and semantic similarity judgments","translation acts"],"center_task":[null,"question answering","machine translation",null,null],"Goal":["Quality Education"],"text":"CNGL: Grading Student Answers by Acts of Translation. We invent referential translation machines (RTMs), a computational model for identifying the translation acts between any two data sets with respect to a reference corpus selected in the same domain, which can be used for automatically grading student answers. RTMs make quality and semantic similarity judgments possible by using retrieved relevant training data as interpretants for reaching shared semantics. An MTPP (machine translation performance predictor) model derives features measuring the closeness of the test sentences to the training data, the difficulty of translating them, and the presence of acts of translation involved. We view question answering as translation from the question to the answer, from the question to the reference answer, from the answer to the reference answer, or from the question and the answer to the reference answer. Each view is modeled by an RTM model, giving us a new perspective on the ternary relationship between the question, the answer, and the reference answer. We show that all RTM models contribute and a prediction model based on all four perspectives performs the best. Our prediction model is the 2nd best system on some tasks according to the official results of the Student Response Analysis (SRA 2013) challenge.","label":1,"title_clean":"CNGL: Grading Student Answers by Acts of Translation","abstract_clean":"We invent referential translation machines (RTMs), a computational model for identifying the translation acts between any two data sets with respect to a reference corpus selected in the same domain, which can be used for automatically grading student answers. RTMs make quality and semantic similarity judgments possible by using retrieved relevant training data as interpretants for reaching shared semantics. An MTPP (machine translation performance predictor) model derives features measuring the closeness of the test sentences to the training data, the difficulty of translating them, and the presence of acts of translation involved. We view question answering as translation from the question to the answer, from the question to the reference answer, from the answer to the reference answer, or from the question and the answer to the reference answer. Each view is modeled by an RTM model, giving us a new perspective on the ternary relationship between the question, the answer, and the reference answer. We show that all RTM models contribute and a prediction model based on all four perspectives performs the best. Our prediction model is the 2nd best system on some tasks according to the official results of the Student Response Analysis (SRA 2013) challenge.","url":"https:\/\/aclanthology.org\/S13-2098"},{"ID":"bilbao-jayo-almeida-2018-political","methods":["convolutional neural network","electronic regional manifestos project","rmp","discourse classifier","context sensitive convolutional neural networks"],"center_method":["convolutional neural network",null,null,null,null],"tasks":["classification","political discourse"],"center_task":["classification",null],"Goal":["Sustainable Cities and Communities"],"text":"Political discourse classification in social networks using context sensitive convolutional neural networks. In this study we propose a new approach to analyse the political discourse in online social networks such as Twitter. To do so, we have built a discourse classifier using Convolutional Neural Networks. Our model has been trained using election manifestos annotated manually by political scientists following the Regional Manifestos Project (RMP) methodology. In total, it has been trained with more than 88,000 sentences extracted from more that 100 annotated manifestos. Our approach takes into account the context of the phrase in order to classify it, like what was previously said and the political affiliation of the transmitter. To improve the classification results we have used a simplified political message taxonomy developed within the Electronic Regional Manifestos Project (E-RMP). Using this taxonomy, we have validated our approach analysing the Twitter activity of the main Spanish political parties during 2015 and 2016 Spanish general election and providing a study of their discourse.","label":1,"title_clean":"Political discourse classification in social networks using context sensitive convolutional neural networks","abstract_clean":"In this study we propose a new approach to analyse the political discourse in online social networks such as Twitter. To do so, we have built a discourse classifier using Convolutional Neural Networks. Our model has been trained using election manifestos annotated manually by political scientists following the Regional Manifestos Project (RMP) methodology. In total, it has been trained with more than 88,000 sentences extracted from more that 100 annotated manifestos. Our approach takes into account the context of the phrase in order to classify it, like what was previously said and the political affiliation of the transmitter. To improve the classification results we have used a simplified political message taxonomy developed within the Electronic Regional Manifestos Project (E RMP). Using this taxonomy, we have validated our approach analysing the Twitter activity of the main Spanish political parties during 2015 and 2016 Spanish general election and providing a study of their discourse.","url":"https:\/\/aclanthology.org\/W18-3513"},{"ID":"bingel-etal-2016-extracting","methods":["fmri","part of speech"],"center_method":[null,"part of speech"],"tasks":["pos induction","token level signals of syntactic processing"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"Extracting token-level signals of syntactic processing from fMRI - with an application to PoS induction. Neuro-imaging studies on reading different parts of speech (PoS) report somewhat mixed results, yet some of them indicate different activations with different PoS. This paper addresses the difficulty of using fMRI to discriminate between linguistic tokens in reading of running text because of low temporal resolution. We show that once we solve this problem, fMRI data contains a signal of PoS distinctions to the extent that it improves PoS induction with error reductions of more than 4%.","label":1,"title_clean":"Extracting token level signals of syntactic processing from fMRI  with an application to PoS induction","abstract_clean":"Neuro imaging studies on reading different parts of speech (PoS) report somewhat mixed results, yet some of them indicate different activations with different PoS. This paper addresses the difficulty of using fMRI to discriminate between linguistic tokens in reading of running text because of low temporal resolution. We show that once we solve this problem, fMRI data contains a signal of PoS distinctions to the extent that it improves PoS induction with error reductions of more than 4%.","url":"https:\/\/aclanthology.org\/P16-1071"},{"ID":"bjorne-salakoski-2011-generalizing","methods":["support vector machine","event extraction"],"center_method":["support vector machine","event extraction"],"tasks":["event extraction","bionlp11 shared task","generalization","descriptions of biomolecular interactions"],"center_task":["event extraction",null,null,null],"Goal":["Good Health and Well-Being"],"text":"Generalizing Biomedical Event Extraction. We present a system for extracting biomedical events (detailed descriptions of biomolecular interactions) from research articles. This system was developed for the BioNLP'11 Shared Task and extends our BioNLP'09 Shared Task winning Turku Event Extraction System. It uses support vector machines to first detect event-defining words, followed by detection of their relationships. The theme of the BioNLP'11 Shared Task is generalization, extending event extraction to varied biomedical domains. Our current system successfully predicts events for every domain case introduced in the BioNLP'11 Shared Task, being the only system to participate in all eight tasks and all of their subtasks, with best performance in four tasks.","label":1,"title_clean":"Generalizing Biomedical Event Extraction","abstract_clean":"We present a system for extracting biomedical events (detailed descriptions of biomolecular interactions) from research articles. This system was developed for the BioNLP'11 Shared Task and extends our BioNLP'09 Shared Task winning Turku Event Extraction System. It uses support vector machines to first detect event defining words, followed by detection of their relationships. The theme of the BioNLP'11 Shared Task is generalization, extending event extraction to varied biomedical domains. Our current system successfully predicts events for every domain case introduced in the BioNLP'11 Shared Task, being the only system to participate in all eight tasks and all of their subtasks, with best performance in four tasks.","url":"https:\/\/aclanthology.org\/W11-1828.pdf"},{"ID":"blaschke-etal-2020-cyberwalle","methods":["label post processing","ensemble models","lstm","feature engineering"],"center_method":[null,null,"lstm","feature engineering"],"tasks":["propaganda detection","si subtask","span identification","technique classification"],"center_task":[null,null,null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"CyberWallE at SemEval-2020 Task 11: An Analysis of Feature Engineering for Ensemble Models for Propaganda Detection. This paper describes our participation in the SemEval-2020 task Detection of Propaganda Techniques in News Articles. We participate in both subtasks: Span Identification (SI) and Technique Classification (TC). We use a bi-LSTM architecture in the SI subtask and train a complex ensemble model for the TC subtask. Our architectures are built using embeddings from BERT in combination with additional lexical features and extensive label post-processing. Our systems achieve a rank of 8 out of 35 teams in the SI subtask (F1-score: 43.86%) and 8 out of 31 teams in the TC subtask (F1-score: 57.37%).","label":1,"title_clean":"CyberWallE at SemEval 2020 Task 11: An Analysis of Feature Engineering for Ensemble Models for Propaganda Detection","abstract_clean":"This paper describes our participation in the SemEval 2020 task Detection of Propaganda Techniques in News Articles. We participate in both subtasks: Span Identification (SI) and Technique Classification (TC). We use a bi LSTM architecture in the SI subtask and train a complex ensemble model for the TC subtask. Our architectures are built using embeddings from BERT in combination with additional lexical features and extensive label post processing. Our systems achieve a rank of 8 out of 35 teams in the SI subtask (F1 score: 43.86%) and 8 out of 31 teams in the TC subtask (F1 score: 57.37%).","url":"https:\/\/aclanthology.org\/2020.semeval-1.192"},{"ID":"blekhman-etal-1997-pars","methods":["mt engine","dictionary updating program","lingvistica 93 co"],"center_method":[null,null,null],"tasks":["machine translation"],"center_task":["machine translation"],"Goal":["Decent Work and Economic Growth","Industry, Innovation and Infrastrucure"],"text":"PARS\/U for Windows: The World's First Commercial English-Ukrainian and Ukrainian-English Machine Translation System. The paper describes the PARS\/U Ukrainian-English bidirectional MT system by Lingvistica '93 Co. PARS\/U translates MS Word and HTML files as well as screen Helps. It features an easy-to-master dictionary updating program, which permits the user to customize the system by means of running subject-area oriented texts through the MT engine. PARS\/U is marketed in Ukraine and North America.","label":1,"title_clean":"PARS\/U for Windows: The World's First Commercial English Ukrainian and Ukrainian English Machine Translation System","abstract_clean":"The paper describes the PARS\/U Ukrainian English bidirectional MT system by Lingvistica '93 Co. PARS\/U translates MS Word and HTML files as well as screen Helps. It features an easy to master dictionary updating program, which permits the user to customize the system by means of running subject area oriented texts through the MT engine. PARS\/U is marketed in Ukraine and North America.","url":"https:\/\/aclanthology.org\/1997.mtsummit-papers.16"},{"ID":"blokker-etal-2020-swimming","methods":["classification"],"center_method":["classification"],"tasks":["party positioning","positional claim detection","cross text type setting","classification"],"center_task":[null,null,null,"classification"],"Goal":["Peace, Justice and Strong Institutions"],"text":"Swimming with the Tide? Positional Claim Detection across Political Text Types. Manifestos are official documents of political parties, providing a comprehensive topical overview of the electoral programs. Voters, however, seldom read them and often prefer other channels, such as newspaper articles, to understand the party positions on various policy issues. The natural question to ask is how compatible these two formats (manifesto and newspaper reports) are in their representation of party positioning. We address this question with an approach that combines political science (manual annotation and analysis) and natural language processing (supervised claim identification) in a cross-text type setting: we train a classifier on annotated newspaper data and test its performance on manifestos. Our findings show a) strong performance for supervised classification even across text types and b) a substantive overlap between the two formats in terms of party positioning, with differences regarding the salience of specific issues.","label":1,"title_clean":"Swimming with the Tide? Positional Claim Detection across Political Text Types","abstract_clean":"Manifestos are official documents of political parties, providing a comprehensive topical overview of the electoral programs. Voters, however, seldom read them and often prefer other channels, such as newspaper articles, to understand the party positions on various policy issues. The natural question to ask is how compatible these two formats (manifesto and newspaper reports) are in their representation of party positioning. We address this question with an approach that combines political science (manual annotation and analysis) and natural language processing (supervised claim identification) in a cross text type setting: we train a classifier on annotated newspaper data and test its performance on manifestos. Our findings show a) strong performance for supervised classification even across text types and b) a substantive overlap between the two formats in terms of party positioning, with differences regarding the salience of specific issues.","url":"https:\/\/aclanthology.org\/2020.nlpcss-1.3"},{"ID":"bobic-etal-2013-scai","methods":["scai","balancing","resampling","machine learning methods","ensemble methods"],"center_method":[null,null,null,"machine learning methods","ensemble methods"],"tasks":["relation extraction","extracting drug drug interactions"],"center_task":["relation extraction",null],"Goal":["Good Health and Well-Being"],"text":"SCAI: Extracting drug-drug interactions using a rich feature vector. Automatic relation extraction provides great support for scientists and database curators in dealing with the extensive amount of biomedical textual data. The DDIExtraction 2013 challenge poses the task of detecting drugdrug interactions and further categorizing them into one of the four relation classes. We present our machine learning system which utilizes lexical, syntactical and semantic based feature sets. Resampling, balancing and ensemble learning experiments are performed to infer the best configuration. For general drugdrug relation extraction, the system achieves 70.4% in F 1 score.","label":1,"title_clean":"SCAI: Extracting drug drug interactions using a rich feature vector","abstract_clean":"Automatic relation extraction provides great support for scientists and database curators in dealing with the extensive amount of biomedical textual data. The DDIExtraction 2013 challenge poses the task of detecting drugdrug interactions and further categorizing them into one of the four relation classes. We present our machine learning system which utilizes lexical, syntactical and semantic based feature sets. Resampling, balancing and ensemble learning experiments are performed to infer the best configuration. For general drugdrug relation extraction, the system achieves 70.4% in F 1 score.","url":"https:\/\/aclanthology.org\/S13-2111"},{"ID":"boella-etal-2012-nlp","methods":["legal knowledge management service","semi automated approach"],"center_method":[null,null],"tasks":["legal knowledge","specialist knowledge management systems","resource bottleneck problem","accuracy","construction and analysis of knowledge"],"center_task":[null,null,null,null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"NLP Challenges for Eunomos a Tool to Build and Manage Legal Knowledge. In this paper, we describe how NLP can semi-automate the construction and analysis of knowledge in Eunomos, a legal knowledge management service which enables users to view legislation from various sources and find the right definitions and explanations of legal concepts in a given context. NLP can semi-automate some routine tasks currently performed by knowledge engineers, such as classifying norms, or linking key terms within legislation to ontological concepts. This helps overcome the resource bottleneck problem of creating specialist knowledge management systems. While accuracy is of the utmost importance in the legal domain, and the information should be verified by domain experts as a matter of course, a semi-automated approach can result in considerable efficiency gains.","label":1,"title_clean":"NLP Challenges for Eunomos a Tool to Build and Manage Legal Knowledge","abstract_clean":"In this paper, we describe how NLP can semi automate the construction and analysis of knowledge in Eunomos, a legal knowledge management service which enables users to view legislation from various sources and find the right definitions and explanations of legal concepts in a given context. NLP can semi automate some routine tasks currently performed by knowledge engineers, such as classifying norms, or linking key terms within legislation to ontological concepts. This helps overcome the resource bottleneck problem of creating specialist knowledge management systems. While accuracy is of the utmost importance in the legal domain, and the information should be verified by domain experts as a matter of course, a semi automated approach can result in considerable efficiency gains.","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2012\/pdf\/1035_Paper.pdf"},{"ID":"bokaie-hosseini-etal-2020-identifying","methods":["privacy policies"],"center_method":[null],"tasks":["app developers","natural language privacy policies"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Identifying and Classifying Third-party Entities in Natural Language Privacy Policies. App developers often raise revenue by contracting with third party ad networks, which serve targeted ads to end-users. To this end, a free app may collect data about its users and share it with advertising companies for targeting purposes. Regulations such as General Data Protection Regulation (GDPR) require transparency with respect to the recipients (or categories of recipients) of user data. These regulations call for app developers to have privacy policies that disclose those third party recipients of user data. Privacy policies provide users transparency into what data an app will access, collect, shared, and retain. Given the size of app marketplaces, verifying compliance with such regulations is a tedious task. This paper aims to develop an automated approach to extract and categorize third party data recipients (i.e., entities) declared in privacy policies. We analyze 100 privacy policies associated with most downloaded apps in the Google Play Store. We crowdsource the collection and annotation of app privacy policies to establish the ground truth with respect to third party entities. From this, we train various models to extract third party entities automatically. Our best model achieves average F1 score of 66% when compared to crowdsourced annotations.","label":1,"title_clean":"Identifying and Classifying Third party Entities in Natural Language Privacy Policies","abstract_clean":"App developers often raise revenue by contracting with third party ad networks, which serve targeted ads to end users. To this end, a free app may collect data about its users and share it with advertising companies for targeting purposes. Regulations such as General Data Protection Regulation (GDPR) require transparency with respect to the recipients (or categories of recipients) of user data. These regulations call for app developers to have privacy policies that disclose those third party recipients of user data. Privacy policies provide users transparency into what data an app will access, collect, shared, and retain. Given the size of app marketplaces, verifying compliance with such regulations is a tedious task. This paper aims to develop an automated approach to extract and categorize third party data recipients (i.e., entities) declared in privacy policies. We analyze 100 privacy policies associated with most downloaded apps in the Google Play Store. We crowdsource the collection and annotation of app privacy policies to establish the ground truth with respect to third party entities. From this, we train various models to extract third party entities automatically. Our best model achieves average F1 score of 66% when compared to crowdsourced annotations.","url":"https:\/\/aclanthology.org\/2020.privatenlp-1.3"},{"ID":"boldrini-etal-2010-emotiblog","methods":["finegrained annotation scheme","emotiblog","subjectivity expression models","nlp applications"],"center_method":[null,null,null,"nlp applications"],"tasks":["sentiment analysis","opinion mining","training"],"center_task":["sentiment analysis",null,"training"],"Goal":["Peace, Justice and Strong Institutions"],"text":"EmotiBlog: A Finer-Grained and More Precise Learning of Subjectivity Expression Models. The exponential growth of the subjective information in the framework of the Web 2.0 has led to the need to create Natural Language Processing tools able to analyse and process such data for multiple practical applications. They require training on specifically annotated corpora, whose level of detail must be fine enough to capture the phenomena involved. This paper presents EmotiBlog-a finegrained annotation scheme for subjectivity. We show the manner in which it is built and demonstrate the benefits it brings to the systems using it for training, through the experiments we carried out on opinion mining and emotion detection. We employ corpora of different textual genres-a set of annotated reported speech extracted from news articles, the set of news titles annotated with polarity and emotion from the SemEval 2007 (Task 14) and ISEAR, a corpus of real-life selfexpressed emotion. We also show how the model built from the EmotiBlog annotations can be enhanced with external resources. The results demonstrate that EmotiBlog, through its structure and annotation paradigm, offers high quality training data for systems dealing both with opinion mining, as well as emotion detection.","label":1,"title_clean":"EmotiBlog: A Finer Grained and More Precise Learning of Subjectivity Expression Models","abstract_clean":"The exponential growth of the subjective information in the framework of the Web 2.0 has led to the need to create Natural Language Processing tools able to analyse and process such data for multiple practical applications. They require training on specifically annotated corpora, whose level of detail must be fine enough to capture the phenomena involved. This paper presents EmotiBlog a finegrained annotation scheme for subjectivity. We show the manner in which it is built and demonstrate the benefits it brings to the systems using it for training, through the experiments we carried out on opinion mining and emotion detection. We employ corpora of different textual genres a set of annotated reported speech extracted from news articles, the set of news titles annotated with polarity and emotion from the SemEval 2007 (Task 14) and ISEAR, a corpus of real life selfexpressed emotion. We also show how the model built from the EmotiBlog annotations can be enhanced with external resources. The results demonstrate that EmotiBlog, through its structure and annotation paradigm, offers high quality training data for systems dealing both with opinion mining, as well as emotion detection.","url":"https:\/\/aclanthology.org\/W10-1801"},{"ID":"bommadi-etal-2021-automatic","methods":["learning assistant"],"center_method":[null],"tasks":["automatic learning assistant","automated question generation","virtual assistance","research","self assessment","testing ml agents","educational applications","faqs","information websites"],"center_task":[null,null,null,null,null,null,"educational applications",null,null],"Goal":["Quality Education"],"text":"Automatic Learning Assistant in Telugu. This paper presents a learning assistant that tests one's knowledge and gives feedback that helps a person learn at a faster pace. A learning assistant (based on an automated question generation) has extensive uses in education, information websites, self-assessment, FAQs, testing ML agents, research, etc. Multiple researchers, and companies have worked on Virtual Assistance, but majorly in English. We built our learning assistant for Telugu language to help with teaching in the mother tongue, which is the most efficient way of learning 1. Our system is built primarily based on Question Generation in Telugu.","label":1,"title_clean":"Automatic Learning Assistant in Telugu","abstract_clean":"This paper presents a learning assistant that tests one's knowledge and gives feedback that helps a person learn at a faster pace. A learning assistant (based on an automated question generation) has extensive uses in education, information websites, self assessment, FAQs, testing ML agents, research, etc. Multiple researchers, and companies have worked on Virtual Assistance, but majorly in English. We built our learning assistant for Telugu language to help with teaching in the mother tongue, which is the most efficient way of learning 1. Our system is built primarily based on Question Generation in Telugu.","url":"https:\/\/aclanthology.org\/2021.dialdoc-1.4"},{"ID":"boriola-paetzold-2020-utfpr","methods":["ensemble methods","convolutional neural network","bow model"],"center_method":["ensemble methods","convolutional neural network",null],"tasks":["english sub task"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"text":"UTFPR at SemEval 2020 Task 12: Identifying Offensive Tweets with Lightweight Ensembles. Offensive language is a common issue on social media platforms nowadays. In an effort to address this issue, the SemEval 2020 event held the OffensEval 2020 shared task where the participants were challenged to develop systems that identify and classify offensive language in tweets. In this paper, we present a system that uses an Ensemble model stacking a BOW model and a CNN model that led us to place 29th in the ranking for English sub-task A.","label":1,"title_clean":"UTFPR at SemEval 2020 Task 12: Identifying Offensive Tweets with Lightweight Ensembles","abstract_clean":"Offensive language is a common issue on social media platforms nowadays. In an effort to address this issue, the SemEval 2020 event held the OffensEval 2020 shared task where the participants were challenged to develop systems that identify and classify offensive language in tweets. In this paper, we present a system that uses an Ensemble model stacking a BOW model and a CNN model that led us to place 29th in the ranking for English sub task A.","url":"https:\/\/aclanthology.org\/2020.semeval-1.297"},{"ID":"boudin-etal-2010-clinical","methods":["clinical information retrieval","location based weighting strategy","pico query formulation"],"center_method":[null,null,null],"tasks":["clinical information retrieval","evidence - based medicine"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"Clinical Information Retrieval using Document and PICO Structure. In evidence-based medicine, clinical questions involve four aspects: Patient\/Problem (P), Intervention (I), Comparison (C) and Outcome (O), known as PICO elements. In this paper we present a method that extends the language modeling approach to incorporate both document structure and PICO query formulation. We present an analysis of the distribution of PICO elements in medical abstracts that motivates the use of a location-based weighting strategy. In experiments carried out on a collection of 1.5 million abstracts, the method was found to lead to an improvement of roughly 60% in MAP and 70% in P@10 as compared to state-of-the-art methods.","label":1,"title_clean":"Clinical Information Retrieval using Document and PICO Structure","abstract_clean":"In evidence based medicine, clinical questions involve four aspects: Patient\/Problem (P), Intervention (I), Comparison (C) and Outcome (O), known as PICO elements. In this paper we present a method that extends the language modeling approach to incorporate both document structure and PICO query formulation. We present an analysis of the distribution of PICO elements in medical abstracts that motivates the use of a location based weighting strategy. In experiments carried out on a collection of 1.5 million abstracts, the method was found to lead to an improvement of roughly 60% in MAP and 70% in P@10 as compared to state of the art methods.","url":"https:\/\/aclanthology.org\/N10-1124"},{"ID":"boytcheva-etal-2009-extraction","methods":["clustering","ie"],"center_method":[null,null],"tasks":["extraction and exploration of correlations","automatic processing of hospital patient records","retrieval of status descriptions","shallow analysis"],"center_task":[null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"Extraction and Exploration of Correlations in Patient Status Data. The paper discusses an Information Extraction approach, which is applied for the automatic processing of hospital Patient Records (PRs) in Bulgarian language. The main task reported here is retrieval of status descriptions related to anatomical organs. Due to the specific telegraphic PR style, the approach is focused on shallow analysis. Missing text descriptions and default values are another obstacle. To overcome it, we propose an algorithm for exploring the correlations between patient status data and the corresponding diagnosis. Rules for interdependencies of the patient status data are generated by clustering according to chosen metrics. In this way it becomes possible to fill in status templates for each patient when explicit descriptions are unavailable in the text. The article summarises evaluation results which concern the performance of the current IE prototype.","label":1,"title_clean":"Extraction and Exploration of Correlations in Patient Status Data","abstract_clean":"The paper discusses an Information Extraction approach, which is applied for the automatic processing of hospital Patient Records (PRs) in Bulgarian language. The main task reported here is retrieval of status descriptions related to anatomical organs. Due to the specific telegraphic PR style, the approach is focused on shallow analysis. Missing text descriptions and default values are another obstacle. To overcome it, we propose an algorithm for exploring the correlations between patient status data and the corresponding diagnosis. Rules for interdependencies of the patient status data are generated by clustering according to chosen metrics. In this way it becomes possible to fill in status templates for each patient when explicit descriptions are unavailable in the text. The article summarises evaluation results which concern the performance of the current IE prototype.","url":"https:\/\/aclanthology.org\/W09-4501.pdf"},{"ID":"brekke-etal-2006-automatic","methods":["term base","lexical and statistical filtering"],"center_method":[null,null],"tasks":["norwegian to english mt","automatic term extraction","subdomain and collocations based word sense disambiguation","logon"],"center_task":[null,null,null,null],"Goal":["Decent Work and Economic Growth"],"text":"Automatic Term Extraction from Knowledge Bank of Economics. KB-N is a web-accessible searchable Knowledge Bank comprising A) a parallel corpus of quality assured and calibrated English and Norwegian text drawn from economic-administrative knowledge domains, and B) a domain-focused database representing that knowledge universe in terms of defined concepts and their respective bilingual terminological entries. A central mechanism in connecting A and B is an algorithm for the automatic extraction of term candidates from aligned translation pairs on the basis of linguistic, lexical and statistical filtering (first ever for Norwegian). The system is designed and programmed by Paul Meurer at Aksis (UiB). An important pilot application of the term base is subdomain and collocations based word-sense disambiguation for LOGON, a system for Norwegian-to-English MT currently being developed.","label":1,"title_clean":"Automatic Term Extraction from Knowledge Bank of Economics","abstract_clean":"KB N is a web accessible searchable Knowledge Bank comprising A) a parallel corpus of quality assured and calibrated English and Norwegian text drawn from economic administrative knowledge domains, and B) a domain focused database representing that knowledge universe in terms of defined concepts and their respective bilingual terminological entries. A central mechanism in connecting A and B is an algorithm for the automatic extraction of term candidates from aligned translation pairs on the basis of linguistic, lexical and statistical filtering (first ever for Norwegian). The system is designed and programmed by Paul Meurer at Aksis (UiB). An important pilot application of the term base is subdomain and collocations based word sense disambiguation for LOGON, a system for Norwegian to English MT currently being developed.","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2006\/pdf\/807_pdf.pdf"},{"ID":"brixey-etal-2017-shihbot","methods":["autonomous chatbot","npceditor","response selection platform","shihbot","facebook chatbot"],"center_method":[null,null,null,null,null],"tasks":["sexual health information"],"center_task":[null],"Goal":["Good Health and Well-Being"],"text":"SHIHbot: A Facebook chatbot for Sexual Health Information on HIV\/AIDS. We present the implementation of an autonomous chatbot, SHIHbot, deployed on Facebook, which answers a wide variety of sexual health questions on HIV\/AIDS. The chatbot's response database is compiled from professional medical and public health resources in order to provide reliable information to users. The system's backend is NPCEditor, a response selection platform trained on linked questions and answers; to our knowledge this is the first retrieval-based chatbot deployed on a large public social network.","label":1,"title_clean":"SHIHbot: A Facebook chatbot for Sexual Health Information on HIV\/AIDS","abstract_clean":"We present the implementation of an autonomous chatbot, SHIHbot, deployed on Facebook, which answers a wide variety of sexual health questions on HIV\/AIDS. The chatbot's response database is compiled from professional medical and public health resources in order to provide reliable information to users. The system's backend is NPCEditor, a response selection platform trained on linked questions and answers; to our knowledge this is the first retrieval based chatbot deployed on a large public social network.","url":"https:\/\/aclanthology.org\/W17-5544"},{"ID":"brugman-etal-2004-collaborative","methods":["peer to peer technology","multimedia annotation tool"],"center_method":[null,null],"tasks":["collaborative annotation of sign language data"],"center_task":[null],"Goal":["Reduced Inequalities"],"text":"Collaborative Annotation of Sign Language Data with Peer-to-Peer Technology. Collaboration on annotation projects is in practice mostly done by people sharing the same room. However, several models for online cooperative annotation over the internet are possible. This paper explores and evaluates these, and reports on the use of peer-to-peer technology to extend a multimedia annotation tool (ELAN) with functions that support collaborative annotation.","label":1,"title_clean":"Collaborative Annotation of Sign Language Data with Peer to Peer Technology","abstract_clean":"Collaboration on annotation projects is in practice mostly done by people sharing the same room. However, several models for online cooperative annotation over the internet are possible. This paper explores and evaluates these, and reports on the use of peer to peer technology to extend a multimedia annotation tool (ELAN) with functions that support collaborative annotation.","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2004\/pdf\/473.pdf"},{"ID":"buechel-etal-2016-enterprises","methods":["anthropomorphic perspective","rcv1"],"center_method":[null,null],"tasks":["management","organization studies"],"center_task":[null,null],"Goal":["Industry, Innovation and Infrastrucure","Decent Work and Economic Growth"],"text":"Do Enterprises Have Emotions?. Emotional language of human individuals has been studied for quite a while dealing with opinions and value judgments people have and share with others. In our work, we take a different stance and investigate whether large organizations, such as major industrial players, have and communicate emotions, as well. Such an anthropomorphic perspective has recently been advocated in management and organization studies which consider organizations as social actors. We studied this assumption by analyzing 1,676 annual business and sustainability reports from 90 top-performing enterprises in the United States, Great Britain and Germany. We compared the measurements of emotions in this homogeneous corporate text corpus with those from RCV1, a heterogeneous Reuters newswire corpus. From this, we gathered empirical evidence that business reports compare well with typical emotion-neutral economic news, whereas sustainability reports are much more emotionally loaded, similar to emotion-heavy sports and fashion news from Reuters. Furthermore, our data suggest that these emotions are distinctive and relatively stable over time per organization, thus constituting an emotional profile for enterprises.","label":1,"title_clean":"Do Enterprises Have Emotions?","abstract_clean":"Emotional language of human individuals has been studied for quite a while dealing with opinions and value judgments people have and share with others. In our work, we take a different stance and investigate whether large organizations, such as major industrial players, have and communicate emotions, as well. Such an anthropomorphic perspective has recently been advocated in management and organization studies which consider organizations as social actors. We studied this assumption by analyzing 1,676 annual business and sustainability reports from 90 top performing enterprises in the United States, Great Britain and Germany. We compared the measurements of emotions in this homogeneous corporate text corpus with those from RCV1, a heterogeneous Reuters newswire corpus. From this, we gathered empirical evidence that business reports compare well with typical emotion neutral economic news, whereas sustainability reports are much more emotionally loaded, similar to emotion heavy sports and fashion news from Reuters. Furthermore, our data suggest that these emotions are distinctive and relatively stable over time per organization, thus constituting an emotional profile for enterprises.","url":"https:\/\/aclanthology.org\/W16-0423"},{"ID":"burtenshaw-kestemont-2021-uantwerp","methods":["digital humanities","error analysis","categorical span based models","component models","stacked generalisation ensemble","lstm"],"center_method":[null,"error analysis",null,null,null,"lstm"],"tasks":["toxic spans detection"],"center_task":["toxic spans detection"],"Goal":["Peace, Justice and Strong Institutions"],"text":"UAntwerp at SemEval-2021 Task 5: Spans are Spans, stacking a binary word level approach to toxic span detection. This paper describes the system developed by the Antwerp Centre for Digital humanities and literary Criticism [UAntwerp] for toxic span detection. We used a stacked generalisation ensemble of five component models, with two distinct interpretations of the task. Two models attempted to predict binary word toxicity based on ngram sequences, whilst 3 categorical span based models were trained to predict toxic token labels based on complete sequence tokens. The five models' predictions were ensembled within an LSTM model. As well as describing the system, we perform error analysis to explore model performance in relation to textual features. The system described in this paper scored 0.6755 and ranked 26 th .","label":1,"title_clean":"UAntwerp at SemEval 2021 Task 5: Spans are Spans, stacking a binary word level approach to toxic span detection","abstract_clean":"This paper describes the system developed by the Antwerp Centre for Digital humanities and literary Criticism [UAntwerp] for toxic span detection. We used a stacked generalisation ensemble of five component models, with two distinct interpretations of the task. Two models attempted to predict binary word toxicity based on ngram sequences, whilst 3 categorical span based models were trained to predict toxic token labels based on complete sequence tokens. The five models' predictions were ensembled within an LSTM model. As well as describing the system, we perform error analysis to explore model performance in relation to textual features. The system described in this paper scored 0.6755 and ranked 26 th .","url":"https:\/\/aclanthology.org\/2021.semeval-1.121"},{"ID":"busemann-1997-automating","methods":["cosma","cooperating agent systems","german language server"],"center_method":[null,null,null],"tasks":["appointment scheduling agent systems"],"center_task":[null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Automating NL Appointment Scheduling with COSMA. Appointment scheduling is a problem faced daily by many individuals and organizations. Cooperating agent systems have been developed to partially automate this task. In order to extend the circle of participants as far as possible we advocate the use of natural language transmitted by email. We demonstrate COSMA, a fully implemented German language server for existing appointment scheduling agent systems. COSMA can cope with multiple dialogues in parallel, and accounts for differences in dialogue behaviour between human and machine agents.","label":1,"title_clean":"Automating NL Appointment Scheduling with COSMA","abstract_clean":"Appointment scheduling is a problem faced daily by many individuals and organizations. Cooperating agent systems have been developed to partially automate this task. In order to extend the circle of participants as far as possible we advocate the use of natural language transmitted by email. We demonstrate COSMA, a fully implemented German language server for existing appointment scheduling agent systems. COSMA can cope with multiple dialogues in parallel, and accounts for differences in dialogue behaviour between human and machine agents.","url":"https:\/\/aclanthology.org\/A97-2003"},{"ID":"buyukoz-etal-2020-analyzing","methods":["elmo","machine learning ml algorithms","bert","linear support vector machine","naive bayes","feed forward neural network","deep contextual language representations"],"center_method":["elmo",null,"bert",null,"naive bayes",null,null],"tasks":["sentiment analysis","binary protest news classification","generalization"],"center_task":["sentiment analysis",null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Analyzing ELMo and DistilBERT on Socio-political News Classification. This study evaluates the robustness of two state-of-the-art deep contextual language representations, ELMo and DistilBERT, on supervised learning of binary protest news classification (PC) and sentiment analysis (SA) of product reviews. A \"cross-context\" setting is enabled using test sets that are distinct from the training data. The models are fine-tuned and fed into a Feed-Forward Neural Network (FFNN) and a Bidirectional Long Short Term Memory network (BiLSTM). Multinomial Naive Bayes (MNB) and Linear Support Vector Machine (LSVM) are used as traditional baselines. The results suggest that DistilBERT can transfer generic semantic knowledge to other domains better than ELMo. DistilBERT is also 30% smaller and 83% faster than ELMo, which suggests superiority for smaller computational training budgets. When generalization is not the utmost preference and test domain is similar to the training domain, the traditional machine learning (ML) algorithms can still be considered as more economic alternatives to deep language representations.","label":1,"title_clean":"Analyzing ELMo and DistilBERT on Socio political News Classification","abstract_clean":"This study evaluates the robustness of two state of the art deep contextual language representations, ELMo and DistilBERT, on supervised learning of binary protest news classification (PC) and sentiment analysis (SA) of product reviews. A \"cross context\" setting is enabled using test sets that are distinct from the training data. The models are fine tuned and fed into a Feed Forward Neural Network (FFNN) and a Bidirectional Long Short Term Memory network (BiLSTM). Multinomial Naive Bayes (MNB) and Linear Support Vector Machine (LSVM) are used as traditional baselines. The results suggest that DistilBERT can transfer generic semantic knowledge to other domains better than ELMo. DistilBERT is also 30% smaller and 83% faster than ELMo, which suggests superiority for smaller computational training budgets. When generalization is not the utmost preference and test domain is similar to the training domain, the traditional machine learning (ML) algorithms can still be considered as more economic alternatives to deep language representations.","url":"https:\/\/aclanthology.org\/2020.aespen-1.4.pdf"},{"ID":"cadel-ledouble-2000-extraction","methods":["domain - specific information schemes"],"center_method":[null],"tasks":["classification","economics"],"center_task":["classification",null],"Goal":["Decent Work and Economic Growth"],"text":"Extraction of Concepts and Multilingual Information Schemes from French and English Economics Documents. This paper focuses on the linguistic analysis of economic information in French and English documents. Our objective is to establish domain-specific information schemes based on structural and conceptual information. At the structural level, we define linguistic triggers that take into account each language's specificity. At the conceptual level, analysis of concepts and relations between concepts result in a classification, prior to the representation of schemes. The final outcome of this study is a mapping between linguistic and conceptual structures in the field of economics.","label":1,"title_clean":"Extraction of Concepts and Multilingual Information Schemes from French and English Economics Documents","abstract_clean":"This paper focuses on the linguistic analysis of economic information in French and English documents. Our objective is to establish domain specific information schemes based on structural and conceptual information. At the structural level, we define linguistic triggers that take into account each language's specificity. At the conceptual level, analysis of concepts and relations between concepts result in a classification, prior to the representation of schemes. The final outcome of this study is a mapping between linguistic and conceptual structures in the field of economics.","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2000\/pdf\/202.pdf"},{"ID":"caines-etal-2018-aggressive","methods":["binary classifier"],"center_method":[null],"tasks":["inter domain classification experiments","abuse detection"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Aggressive language in an online hacking forum. We probe the heterogeneity in levels of abusive language in different sections of the Internet, using an annotated corpus of Wikipedia page edit comments to train a binary classifier for abuse detection. Our test data come from the CrimeBB Corpus of hacking-related forum posts and we find that (a) forum interactions are rarely abusive, (b) the abusive language which does exist tends to be relatively mild compared to that found in the Wikipedia comments domain, and tends to involve aggressive posturing rather than hate speech or threats of violence. We observe that the purpose of conversations in online forums tend to be more constructive and informative than those in Wikipedia page edit comments which are geared more towards adversarial interactions, and that this may explain the lower levels of abuse found in our forum data than in Wikipedia comments. Further work remains to be done to compare these results with other inter-domain classification experiments, and to understand the impact of aggressive language in forum conversations.","label":1,"title_clean":"Aggressive language in an online hacking forum","abstract_clean":"We probe the heterogeneity in levels of abusive language in different sections of the Internet, using an annotated corpus of Wikipedia page edit comments to train a binary classifier for abuse detection. Our test data come from the CrimeBB Corpus of hacking related forum posts and we find that (a) forum interactions are rarely abusive, (b) the abusive language which does exist tends to be relatively mild compared to that found in the Wikipedia comments domain, and tends to involve aggressive posturing rather than hate speech or threats of violence. We observe that the purpose of conversations in online forums tend to be more constructive and informative than those in Wikipedia page edit comments which are geared more towards adversarial interactions, and that this may explain the lower levels of abuse found in our forum data than in Wikipedia comments. Further work remains to be done to compare these results with other inter domain classification experiments, and to understand the impact of aggressive language in forum conversations.","url":"https:\/\/aclanthology.org\/W18-5109"},{"ID":"calixto-etal-2017-human","methods":["pbsmt system","multi modal neural machine translation","attention based nmt","multi modal nmt","text only nmt","text only approaches"],"center_method":[null,null,null,null,null,null],"tasks":["usergenerated product listings","human evaluation","e commerce listing titles"],"center_task":[null,"human evaluation",null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Human Evaluation of Multi-modal Neural Machine Translation: A Case-Study on E-Commerce Listing Titles. In this paper, we study how humans perceive the use of images as an additional knowledge source to machine-translate usergenerated product listings in an e-commerce company. We conduct a human evaluation where we assess how a multi-modal neural machine translation (NMT) model compares to two text-only approaches: a conventional state-of-the-art attention-based NMT and a phrase-based statistical machine translation (PBSMT) model. We evaluate translations obtained with different systems and also discuss the data set of user-generated product listings, which in our case comprises both product listings and associated images. We found that humans preferred translations obtained with a PBSMT system to both text-only and multi-modal NMT over 56% of the time. Nonetheless, human evaluators ranked translations from a multi-modal NMT model as better than those of a text-only NMT over 88% of the time, which suggests that images do help NMT in this use-case.","label":1,"title_clean":"Human Evaluation of Multi modal Neural Machine Translation: A Case Study on E Commerce Listing Titles","abstract_clean":"In this paper, we study how humans perceive the use of images as an additional knowledge source to machine translate usergenerated product listings in an e commerce company. We conduct a human evaluation where we assess how a multi modal neural machine translation (NMT) model compares to two text only approaches: a conventional state of the art attention based NMT and a phrase based statistical machine translation (PBSMT) model. We evaluate translations obtained with different systems and also discuss the data set of user generated product listings, which in our case comprises both product listings and associated images. We found that humans preferred translations obtained with a PBSMT system to both text only and multi modal NMT over 56% of the time. Nonetheless, human evaluators ranked translations from a multi modal NMT model as better than those of a text only NMT over 88% of the time, which suggests that images do help NMT in this use case.","url":"https:\/\/aclanthology.org\/W17-2004"},{"ID":"calzolari-etal-2004-enabler","methods":["lrs","enabler thematic network"],"center_method":[null,null],"tasks":["horizontal technology","language technology","evaluation methods"],"center_task":[null,"language technology","evaluation methods"],"Goal":["Industry, Innovation and Infrastrucure","Peace, Justice and Strong Institutions"],"text":"ENABLER Thematic Network of National Projects: Technical, Strategic and Political Issues of LRs. In this paper we present general strategies concerning Language Resources (LRs)-Written, Spoken and, recently, Multimodal-as developed within the ENABLER Thematic Network. LRs are a central component of the so-called \"linguistic infrastructure\" (the other key element being Evaluation), necessary for the development of any Human Language Technology (HLT) application. They play a critical role, as horizontal technology, in different emerging areas of FP6, and have been recognized as a priority within a number of national projects around Europe and worldwide. The availability of LRs is also a \"sensitive\" issue, touching directly the sphere of linguistic and cultural identity, but also with economical, societal and political implications. This is going to be even more true in the new Europe with 25 languages on a par.","label":1,"title_clean":"ENABLER Thematic Network of National Projects: Technical, Strategic and Political Issues of LRs","abstract_clean":"In this paper we present general strategies concerning Language Resources (LRs) Written, Spoken and, recently, Multimodal as developed within the ENABLER Thematic Network. LRs are a central component of the so called \"linguistic infrastructure\" (the other key element being Evaluation), necessary for the development of any Human Language Technology (HLT) application. They play a critical role, as horizontal technology, in different emerging areas of FP6, and have been recognized as a priority within a number of national projects around Europe and worldwide. The availability of LRs is also a \"sensitive\" issue, touching directly the sphere of linguistic and cultural identity, but also with economical, societal and political implications. This is going to be even more true in the new Europe with 25 languages on a par.","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2004\/pdf\/545.pdf"},{"ID":"candido-etal-2009-supporting","methods":["text simplification editor","line authoring system"],"center_method":[null,null],"tasks":["text simplification","educational applications"],"center_task":[null,"educational applications"],"Goal":["Reduced Inequalities","Quality Education"],"text":"Supporting the Adaptation of Texts for Poor Literacy Readers: a Text Simplification Editor for Brazilian Portuguese. In this paper we investigate the task of text simplification for Brazilian Portuguese. Our purpose is threefold: to introduce a simplification tool for such language and its underlying development methodology, to present an on-line authoring system of simplified text based on the previous tool, and finally to discuss the potentialities of such technology for education. The resources and tools we present are new for Portuguese and innovative in many aspects with respect to previous initiatives for other languages.","label":1,"title_clean":"Supporting the Adaptation of Texts for Poor Literacy Readers: a Text Simplification Editor for Brazilian Portuguese","abstract_clean":"In this paper we investigate the task of text simplification for Brazilian Portuguese. Our purpose is threefold: to introduce a simplification tool for such language and its underlying development methodology, to present an on line authoring system of simplified text based on the previous tool, and finally to discuss the potentialities of such technology for education. The resources and tools we present are new for Portuguese and innovative in many aspects with respect to previous initiatives for other languages.","url":"https:\/\/aclanthology.org\/W09-2105"},{"ID":"cardellino-etal-2017-legal","methods":["legal nerc","lkif","curriculum learning","named entity recognizer and classifier"],"center_method":[null,null,null,null],"tasks":["annotation","preprocess"],"center_task":["annotation",null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Legal NERC with ontologies, Wikipedia and curriculum learning. In this paper, we present a Wikipediabased approach to develop resources for the legal domain. We establish a mapping between a legal domain ontology, LKIF (Hoekstra et al., 2007), and a Wikipediabased ontology, YAGO (Suchanek et al., 2007), and through that we populate LKIF. Moreover, we use the mentions of those entities in Wikipedia text to train a specific Named Entity Recognizer and Classifier. We find that this classifier works well in the Wikipedia, but, as could be expected, performance decreases in a corpus of judgments of the European Court of Human Rights. However, this tool will be used as a preprocess for human annotation. We resort to a technique called curriculum learning aimed to overcome problems of overfitting by learning increasingly more complex concepts. However, we find that in this particular setting, the method works best by learning from most specific to most general concepts, not the other way round.","label":1,"title_clean":"Legal NERC with ontologies, Wikipedia and curriculum learning","abstract_clean":"In this paper, we present a Wikipediabased approach to develop resources for the legal domain. We establish a mapping between a legal domain ontology, LKIF (Hoekstra et al., 2007), and a Wikipediabased ontology, YAGO (Suchanek et al., 2007), and through that we populate LKIF. Moreover, we use the mentions of those entities in Wikipedia text to train a specific Named Entity Recognizer and Classifier. We find that this classifier works well in the Wikipedia, but, as could be expected, performance decreases in a corpus of judgments of the European Court of Human Rights. However, this tool will be used as a preprocess for human annotation. We resort to a technique called curriculum learning aimed to overcome problems of overfitting by learning increasingly more complex concepts. However, we find that in this particular setting, the method works best by learning from most specific to most general concepts, not the other way round.","url":"https:\/\/aclanthology.org\/E17-2041.pdf"},{"ID":"caselli-etal-2021-dalc","methods":["explicitness layer","multi layer annotation scheme"],"center_method":[null,null],"tasks":["classification","annotation","automatic content moderation"],"center_task":["classification","annotation",null],"Goal":["Peace, Justice and Strong Institutions"],"text":"DALC: the Dutch Abusive Language Corpus. As socially unacceptable language become pervasive in social media platforms, the need for automatic content moderation become more pressing. This contribution introduces the Dutch Abusive Language Corpus (DALC v1.0), a new dataset with tweets manually annotated for abusive language. The resource address a gap in language resources for Dutch and adopts a multi-layer annotation scheme modeling the explicitness and the target of the abusive messages. Baselines experiments on all annotation layers have been conducted, achieving a macro F1 score of 0.748 for binary classification of the explicitness layer and .489 for target classification.","label":1,"title_clean":"DALC: the Dutch Abusive Language Corpus","abstract_clean":"As socially unacceptable language become pervasive in social media platforms, the need for automatic content moderation become more pressing. This contribution introduces the Dutch Abusive Language Corpus (DALC v1.0), a new dataset with tweets manually annotated for abusive language. The resource address a gap in language resources for Dutch and adopts a multi layer annotation scheme modeling the explicitness and the target of the abusive messages. Baselines experiments on all annotation layers have been conducted, achieving a macro F1 score of 0.748 for binary classification of the explicitness layer and .489 for target classification.","url":"https:\/\/aclanthology.org\/2021.woah-1.6"},{"ID":"cavicchio-2009-modulation","methods":["multimodal annotation scheme","logistic regression"],"center_method":[null,"logistic regression"],"tasks":["cooperation","sentiment analysis","annotation","dialogue tasks"],"center_task":[null,"sentiment analysis","annotation",null],"Goal":["Partnership for the Goals"],"text":"The Modulation of Cooperation and Emotion in Dialogue: The REC Corpus. In this paper we describe the Rovereto Emotive Corpus (REC) which we collected to investigate the relationship between emotion and cooperation in dialogue tasks. It is an area where still many unsolved questions are present. One of the main open issues is the annotation of the socalled \"blended\" emotions and their recognition. Usually, there is a low agreement among raters in annotating emotions and, surprisingly, emotion recognition is higher in a condition of modality deprivation (i. e. only acoustic or only visual modality vs. bimodal display of emotion). Because of these previous results, we collected a corpus in which \"emotive\" tokens are pointed out during the recordings by psychophysiological indexes (ElectroCardioGram, and Galvanic Skin Conductance). From the output values of these indexes a general recognition of each emotion arousal is allowed. After this selection we will annotate emotive interactions with our multimodal annotation scheme, performing a kappa statistic on annotation results to validate our coding scheme. In the near future, a logistic regression on annotated data will be performed to find out correlations between cooperation and negative emotions. A final step will be an fMRI experiment on emotion recognition of blended emotions from face displays.","label":1,"title_clean":"The Modulation of Cooperation and Emotion in Dialogue: The REC Corpus","abstract_clean":"In this paper we describe the Rovereto Emotive Corpus (REC) which we collected to investigate the relationship between emotion and cooperation in dialogue tasks. It is an area where still many unsolved questions are present. One of the main open issues is the annotation of the socalled \"blended\" emotions and their recognition. Usually, there is a low agreement among raters in annotating emotions and, surprisingly, emotion recognition is higher in a condition of modality deprivation (i. e. only acoustic or only visual modality vs. bimodal display of emotion). Because of these previous results, we collected a corpus in which \"emotive\" tokens are pointed out during the recordings by psychophysiological indexes (ElectroCardioGram, and Galvanic Skin Conductance). From the output values of these indexes a general recognition of each emotion arousal is allowed. After this selection we will annotate emotive interactions with our multimodal annotation scheme, performing a kappa statistic on annotation results to validate our coding scheme. In the near future, a logistic regression on annotated data will be performed to find out correlations between cooperation and negative emotions. A final step will be an fMRI experiment on emotion recognition of blended emotions from face displays.","url":"https:\/\/aclanthology.org\/P09-3010.pdf"},{"ID":"cercas-curry-etal-2021-convabuse","methods":["nuanced abuse detection","task based system","opendomain social bot"],"center_method":[null,null,null],"tasks":["nuanced abuse detection","conversational ai"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"ConvAbuse: Data, Analysis, and Benchmarks for Nuanced Abuse Detection in Conversational AI. We present the first English corpus study on abusive language towards three conversational AI systems gathered 'in the wild': an opendomain social bot, a rule-based chatbot, and a task-based system. To account for the complexity of the task, we take a more 'nuanced' approach where our ConvAI dataset reflects fine-grained notions of abuse, as well as views from multiple expert annotators. We find that the distribution of abuse is vastly different compared to other commonly used datasets, with more sexually tinted aggression towards the virtual persona of these systems. Finally, we report results from bench-marking existing models against this data. Unsurprisingly, we find that there is substantial room for improvement with F1 scores below 90%. Warning: This paper contains examples of language that some people may find offensive or upsetting.","label":1,"title_clean":"ConvAbuse: Data, Analysis, and Benchmarks for Nuanced Abuse Detection in Conversational AI","abstract_clean":"We present the first English corpus study on abusive language towards three conversational AI systems gathered 'in the wild': an opendomain social bot, a rule based chatbot, and a task based system. To account for the complexity of the task, we take a more 'nuanced' approach where our ConvAI dataset reflects fine grained notions of abuse, as well as views from multiple expert annotators. We find that the distribution of abuse is vastly different compared to other commonly used datasets, with more sexually tinted aggression towards the virtual persona of these systems. Finally, we report results from bench marking existing models against this data. Unsurprisingly, we find that there is substantial room for improvement with F1 scores below 90%. Warning: This paper contains examples of language that some people may find offensive or upsetting.","url":"https:\/\/aclanthology.org\/2021.emnlp-main.587"},{"ID":"cercel-etal-2017-oiqa","methods":["opinion influence oriented question answering framework","oiqa"],"center_method":[null,null],"tasks":["marketing"],"center_task":[null],"Goal":["Decent Work and Economic Growth","Industry, Innovation and Infrastrucure"],"text":"oIQa: An Opinion Influence Oriented Question Answering Framework with Applications to Marketing Domain. ","label":1,"title_clean":"oIQa: An Opinion Influence Oriented Question Answering Framework with Applications to Marketing Domain","abstract_clean":"","url":"https:\/\/doi.org\/10.26615\/978-954-452-038-0_002.pdf"},{"ID":"ceska-fox-2009-influence","methods":["text pre processing techniques","word generalization","lemmatization","text pre processing"],"center_method":[null,null,null,null],"tasks":["plagiarism detection","synonymy recognition","number replacement","stop word removal"],"center_task":[null,null,null,null],"Goal":["Quality Education"],"text":"The Influence of Text Pre-processing on Plagiarism Detection. This paper explores the influence of text preprocessing techniques on plagiarism detection. We examine stop-word removal, lemmatization, number replacement, synonymy recognition, and word generalization. We also look into the influence of punctuation and word-order within N-grams. All these techniques are evaluated according to their impact on F1-measure and speed of execution. Our experiments were performed on a Czech corpus of plagiarized documents about politics. At the end of this paper, we propose what we consider to be the best combination of text pre-processing techniques.","label":1,"title_clean":"The Influence of Text Pre processing on Plagiarism Detection","abstract_clean":"This paper explores the influence of text preprocessing techniques on plagiarism detection. We examine stop word removal, lemmatization, number replacement, synonymy recognition, and word generalization. We also look into the influence of punctuation and word order within N grams. All these techniques are evaluated according to their impact on F1 measure and speed of execution. Our experiments were performed on a Czech corpus of plagiarized documents about politics. At the end of this paper, we propose what we consider to be the best combination of text pre processing techniques.","url":"https:\/\/aclanthology.org\/R09-1011"},{"ID":"ch-wang-jurgens-2021-using","methods":["quasi causal analysis"],"center_method":[null],"tasks":["linguistic change"],"center_task":[null],"Goal":["Gender Equality"],"text":"Using Sociolinguistic Variables to Reveal Changing Attitudes Towards Sexuality and Gender. Individuals signal aspects of their identity and beliefs through linguistic choices. Studying these choices in aggregate allows us to examine large-scale attitude shifts within a population. Here, we develop computational methods to study word choice within a sociolinguistic lexical variable-alternate words used to express the same concept-in order to test for change in the United States towards sexuality and gender. We examine two variables: i) referents to significant others, such as the word \"partner\" and ii) referents to an indefinite person, both of which could optionally be marked with gender. The linguistic choices in each variable allow us to study increased rates of acceptances of gay marriage and gender equality, respectively. In longitudinal analyses across Twitter and Reddit over 87M messages, we demonstrate that attitudes are changing but that these changes are driven by specific demographics within the United States. Further, in a quasi-causal analysis, we show that passages of Marriage Equality Acts in different states are drivers of linguistic change.","label":1,"title_clean":"Using Sociolinguistic Variables to Reveal Changing Attitudes Towards Sexuality and Gender","abstract_clean":"Individuals signal aspects of their identity and beliefs through linguistic choices. Studying these choices in aggregate allows us to examine large scale attitude shifts within a population. Here, we develop computational methods to study word choice within a sociolinguistic lexical variable alternate words used to express the same concept in order to test for change in the United States towards sexuality and gender. We examine two variables: i) referents to significant others, such as the word \"partner\" and ii) referents to an indefinite person, both of which could optionally be marked with gender. The linguistic choices in each variable allow us to study increased rates of acceptances of gay marriage and gender equality, respectively. In longitudinal analyses across Twitter and Reddit over 87M messages, we demonstrate that attitudes are changing but that these changes are driven by specific demographics within the United States. Further, in a quasi causal analysis, we show that passages of Marriage Equality Acts in different states are drivers of linguistic change.","url":"https:\/\/aclanthology.org\/2021.emnlp-main.782"},{"ID":"chalapathy-etal-2016-investigation","methods":["recurrent neural architectures","conditional random field","hand crafted systems","lstm","elman and jordan networks","dnr","dnr approaches"],"center_method":[null,"conditional random field",null,"lstm",null,null,null],"tasks":["drug name recognition","pharmacovigilance pv pipeline"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"An Investigation of Recurrent Neural Architectures for Drug Name Recognition. Drug name recognition (DNR) is an essential step in the Pharmacovigilance (PV) pipeline. DNR aims to find drug name mentions in unstructured biomedical texts and classify them into predefined categories. State-of-the-art DNR approaches heavily rely on hand-crafted features and domain-specific resources which are difficult to collect and tune. For this reason, this paper investigates the effectiveness of contemporary recurrent neural architecturesthe Elman and Jordan networks and the bidirectional LSTM with CRF decoding-at performing DNR straight from the text. The experimental results achieved on the authoritative SemEval-2013 Task 9.1 benchmarks show that the bidirectional LSTM-CRF ranks closely to highly-dedicated, hand-crafted systems.","label":1,"title_clean":"An Investigation of Recurrent Neural Architectures for Drug Name Recognition","abstract_clean":"Drug name recognition (DNR) is an essential step in the Pharmacovigilance (PV) pipeline. DNR aims to find drug name mentions in unstructured biomedical texts and classify them into predefined categories. State of the art DNR approaches heavily rely on hand crafted features and domain specific resources which are difficult to collect and tune. For this reason, this paper investigates the effectiveness of contemporary recurrent neural architecturesthe Elman and Jordan networks and the bidirectional LSTM with CRF decoding at performing DNR straight from the text. The experimental results achieved on the authoritative SemEval 2013 Task 9.1 benchmarks show that the bidirectional LSTM CRF ranks closely to highly dedicated, hand crafted systems.","url":"https:\/\/aclanthology.org\/W16-6101"},{"ID":"chalkidis-etal-2021-paragraph","methods":["regularization"],"center_method":[null],"tasks":["paragraph level rationale extraction"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Paragraph-level Rationale Extraction through Regularization: A case study on European Court of Human Rights Cases. Interpretability or explainability is an emerging research field in NLP. From a user-centric point of view, the goal is to build models that provide proper justification for their decisions, similar to those of humans, by requiring the models to satisfy additional constraints. To this end, we introduce a new application on legal text where, contrary to mainstream literature targeting word-level rationales, we conceive rationales as selected paragraphs in multi-paragraph structured court cases. We also release a new dataset comprising European Court of Human Rights cases, including annotations for paragraph-level rationales. We use this dataset to study the effect of already proposed rationale constraints, i.e., sparsity, continuity, and comprehensiveness, formulated as regularizers. Our findings indicate that some of these constraints are not beneficial in paragraph-level rationale extraction, while others need re-formulation to better handle the multi-label nature of the task we consider. We also introduce a new constraint, singularity, which further improves the quality of rationales, even compared with noisy rationale supervision. Experimental results indicate that the newly introduced task is very challenging and there is a large scope for further research.","label":1,"title_clean":"Paragraph level Rationale Extraction through Regularization: A case study on European Court of Human Rights Cases","abstract_clean":"Interpretability or explainability is an emerging research field in NLP. From a user centric point of view, the goal is to build models that provide proper justification for their decisions, similar to those of humans, by requiring the models to satisfy additional constraints. To this end, we introduce a new application on legal text where, contrary to mainstream literature targeting word level rationales, we conceive rationales as selected paragraphs in multi paragraph structured court cases. We also release a new dataset comprising European Court of Human Rights cases, including annotations for paragraph level rationales. We use this dataset to study the effect of already proposed rationale constraints, i.e., sparsity, continuity, and comprehensiveness, formulated as regularizers. Our findings indicate that some of these constraints are not beneficial in paragraph level rationale extraction, while others need re formulation to better handle the multi label nature of the task we consider. We also introduce a new constraint, singularity, which further improves the quality of rationales, even compared with noisy rationale supervision. Experimental results indicate that the newly introduced task is very challenging and there is a large scope for further research.","url":"https:\/\/aclanthology.org\/2021.naacl-main.22"},{"ID":"chandu-etal-2017-tackling","methods":["maximum marginal relevance","agglomerative clustering","word embedding based tf idf similarity metric","sentence compression"],"center_method":[null,null,null,null],"tasks":["oaqa","text summarization"],"center_task":[null,"text summarization"],"Goal":["Good Health and Well-Being"],"text":"Tackling Biomedical Text Summarization: OAQA at BioASQ 5B. In this paper, we describe our participation in phase B of task 5b of the fifth edition of the annual BioASQ challenge, which includes answering factoid, list, yes-no and summary questions from biomedical data. We describe our techniques with an emphasis on ideal answer generation, where the goal is to produce a relevant, precise, non-redundant, query-oriented summary from multiple relevant documents. We make use of extractive summarization techniques to address this task and experiment with different biomedical ontologies and various algorithms including agglomerative clustering, Maximum Marginal Relevance (MMR) and sentence compression. We propose a novel word embedding based tf-idf similarity metric and a soft positional constraint which improve our system performance. We evaluate our techniques on test batch 4 from the fourth edition of the challenge. Our best system achieves a ROUGE-2 score of 0.6534 and ROUGE-SU4 score of 0.6536.","label":1,"title_clean":"Tackling Biomedical Text Summarization: OAQA at BioASQ 5B","abstract_clean":"In this paper, we describe our participation in phase B of task 5b of the fifth edition of the annual BioASQ challenge, which includes answering factoid, list, yes no and summary questions from biomedical data. We describe our techniques with an emphasis on ideal answer generation, where the goal is to produce a relevant, precise, non redundant, query oriented summary from multiple relevant documents. We make use of extractive summarization techniques to address this task and experiment with different biomedical ontologies and various algorithms including agglomerative clustering, Maximum Marginal Relevance (MMR) and sentence compression. We propose a novel word embedding based tf idf similarity metric and a soft positional constraint which improve our system performance. We evaluate our techniques on test batch 4 from the fourth edition of the challenge. Our best system achieves a ROUGE 2 score of 0.6534 and ROUGE SU4 score of 0.6536.","url":"https:\/\/aclanthology.org\/W17-2307"},{"ID":"chang-etal-2021-nao","methods":["labeling system"],"center_method":[null],"tasks":["labeling of named entity and entity relations","electronic medical records of stroke disease","proofreading","manual tagging","named entity and entity relations"],"center_task":[null,null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"\u8111\u5352\u4e2d\u75be\u75c5\u7535\u5b50\u75c5\u5386\u5b9e\u4f53\u53ca\u5b9e\u4f53\u5173\u7cfb\u6807\u6ce8\u8bed\u6599\u5e93\u6784\u5efa(Corpus Construction for Named-Entity and Entity Relations for Electronic Medical Records of Stroke Disease). This paper discussed the labeling of Named-Entity and Entity Relations in Chinese electronic medical records of stroke disease, and proposes a system and norms for labeling entity and entity relations that are suitable for content and characteristics of electronic medical records of stroke disease. Based on the guidance of the labeling system and norms, this carried out several rounds of manual tagging and proofreading and completed the labeling of entities and relationships more than 1.5 million words. The entity and entity relationship tagging corpus of stroke electronic medical record(Stroke Electronic Medical Record entity and entity related Corpus, SEMRC)is fromed. The constructed corpus contains 10,594 named entities and 14,457 entity relationships. The consistency of named entity reached 85.16%, and that of entity relationship reached 94.16%.","label":1,"title_clean":"\u8111\u5352\u4e2d\u75be\u75c5\u7535\u5b50\u75c5\u5386\u5b9e\u4f53\u53ca\u5b9e\u4f53\u5173\u7cfb\u6807\u6ce8\u8bed\u6599\u5e93\u6784\u5efa(Corpus Construction for Named Entity and Entity Relations for Electronic Medical Records of Stroke Disease)","abstract_clean":"This paper discussed the labeling of Named Entity and Entity Relations in Chinese electronic medical records of stroke disease, and proposes a system and norms for labeling entity and entity relations that are suitable for content and characteristics of electronic medical records of stroke disease. Based on the guidance of the labeling system and norms, this carried out several rounds of manual tagging and proofreading and completed the labeling of entities and relationships more than 1.5 million words. The entity and entity relationship tagging corpus of stroke electronic medical record(Stroke Electronic Medical Record entity and entity related Corpus, SEMRC)is fromed. The constructed corpus contains 10,594 named entities and 14,457 entity relationships. The consistency of named entity reached 85.16%, and that of entity relationship reached 94.16%.","url":"https:\/\/aclanthology.org\/2021.ccl-1.57"},{"ID":"chathuranga-etal-2017-opinion","methods":["conditional random field","classification"],"center_method":["conditional random field","classification"],"tasks":["opinion target extraction","student feedback summarization","student feedback","information extraction task"],"center_task":[null,null,null,null],"Goal":["Quality Education"],"text":"Opinion Target Extraction for Student Course Feedback. Student feedback is an essential part of the instructor-student relationship. Traditionally student feedback is manually summarized by instructors, which is time consuming. Automatic student feedback summarization provides a potential solution to this. For summarizing student feedback, first, the opinion targets should be identified and extracted. In this context, opinion targets such as \"lecture slides\", \"teaching style\" are the important key points in the feedback that the students have shown their sentiment towards. In this paper, we focus on the opinion target extraction task of general student feedback. We model this problem as an information extraction task and extract opinion targets using a Conditional Random Fields (CRF) classifier. Our results show that this classifier outperforms the state-of-the-art techniques for student feedback summarization.","label":1,"title_clean":"Opinion Target Extraction for Student Course Feedback","abstract_clean":"Student feedback is an essential part of the instructor student relationship. Traditionally student feedback is manually summarized by instructors, which is time consuming. Automatic student feedback summarization provides a potential solution to this. For summarizing student feedback, first, the opinion targets should be identified and extracted. In this context, opinion targets such as \"lecture slides\", \"teaching style\" are the important key points in the feedback that the students have shown their sentiment towards. In this paper, we focus on the opinion target extraction task of general student feedback. We model this problem as an information extraction task and extract opinion targets using a Conditional Random Fields (CRF) classifier. Our results show that this classifier outperforms the state of the art techniques for student feedback summarization.","url":"https:\/\/aclanthology.org\/O17-1028"},{"ID":"chatzichrisafis-etal-2006-evaluating","methods":["medslt","french to english version"],"center_method":[null,null],"tasks":["evaluating task performance","doctor patient diagnosis interviews","unidirectional controlled language medical speech translation system"],"center_task":[null,null,null],"Goal":["Good Health and Well-Being"],"text":"Evaluating Task Performance for a Unidirectional Controlled Language Medical Speech Translation System. We present a task-level evaluation of the French to English version of MedSLT, a medium-vocabulary unidirectional controlled language medical speech translation system designed for doctor-patient diagnosis interviews. Our main goal was to establish task performance levels of novice users and compare them to expert users. Tests were carried out on eight medical students with no previous exposure to the system, with each student using the system for a total of three sessions. By the end of the third session, all the students were able to use the system confidently, with an average task completion time of about 4 minutes.","label":1,"title_clean":"Evaluating Task Performance for a Unidirectional Controlled Language Medical Speech Translation System","abstract_clean":"We present a task level evaluation of the French to English version of MedSLT, a medium vocabulary unidirectional controlled language medical speech translation system designed for doctor patient diagnosis interviews. Our main goal was to establish task performance levels of novice users and compare them to expert users. Tests were carried out on eight medical students with no previous exposure to the system, with each student using the system for a total of three sessions. By the end of the third session, all the students were able to use the system confidently, with an average task completion time of about 4 minutes.","url":"https:\/\/aclanthology.org\/W06-3702"},{"ID":"chauhan-etal-2020-one","methods":["inter class relationship module","deep attentive multi task learning framework","itrm","multi task learning","attention like mechanisms","multi modal deep learning framework"],"center_method":[null,null,null,"multi task learning",null,null],"tasks":["offensive content detection","humour detection","multi tasking","sentiment analysis","motivation","sarcasm detection"],"center_task":[null,null,null,"sentiment analysis",null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"All-in-One: A Deep Attentive Multi-task Learning Framework for Humour, Sarcasm, Offensive, Motivation, and Sentiment on Memes. In this paper, we aim at learning the relationships and similarities of a variety of tasks, such as humour detection, sarcasm detection, offensive content detection, motivational content detection and sentiment analysis on a somewhat complicated form of information, i.e., memes. We propose a multi-task, multi-modal deep learning framework to solve multiple tasks simultaneously. For multi-tasking, we propose two attention-like mechanisms viz., Inter-task Relationship Module (iTRM) and Inter-class Relationship Module (iCRM). The main motivation of iTRM is to learn the relationship between the tasks to realize how they help each other. In contrast, iCRM develops relations between the different classes of tasks. Finally, representations from both the attentions are concatenated and shared across the five tasks (i.e., humour, sarcasm, offensive, motivational, and sentiment) for multi-tasking. We use the recently released dataset in the Memotion Analysis task @ SemEval 2020, which consists of memes annotated for the classes as mentioned above. Empirical results on Memotion dataset show the efficacy of our proposed approach over the existing state-of-theart systems (Baseline and SemEval 2020 winner). The evaluation also indicates that the proposed multi-task framework yields better performance over the single-task learning.","label":1,"title_clean":"All in One: A Deep Attentive Multi task Learning Framework for Humour, Sarcasm, Offensive, Motivation, and Sentiment on Memes","abstract_clean":"In this paper, we aim at learning the relationships and similarities of a variety of tasks, such as humour detection, sarcasm detection, offensive content detection, motivational content detection and sentiment analysis on a somewhat complicated form of information, i.e., memes. We propose a multi task, multi modal deep learning framework to solve multiple tasks simultaneously. For multi tasking, we propose two attention like mechanisms viz., Inter task Relationship Module (iTRM) and Inter class Relationship Module (iCRM). The main motivation of iTRM is to learn the relationship between the tasks to realize how they help each other. In contrast, iCRM develops relations between the different classes of tasks. Finally, representations from both the attentions are concatenated and shared across the five tasks (i.e., humour, sarcasm, offensive, motivational, and sentiment) for multi tasking. We use the recently released dataset in the Memotion Analysis task @ SemEval 2020, which consists of memes annotated for the classes as mentioned above. Empirical results on Memotion dataset show the efficacy of our proposed approach over the existing state of theart systems (Baseline and SemEval 2020 winner). The evaluation also indicates that the proposed multi task framework yields better performance over the single task learning.","url":"https:\/\/aclanthology.org\/2020.aacl-main.31"},{"ID":"chen-etal-2020-analyzing","methods":["neural models"],"center_method":["neural models"],"tasks":["analyzing political bias","automatic detection of bias"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Analyzing Political Bias and Unfairness in News Articles at Different Levels of Granularity. Media organizations bear great reponsibility because of their considerable influence on shaping beliefs and positions of our society. Any form of media can contain overly biased content, e.g., by reporting on political events in a selective or incomplete manner. A relevant question hence is whether and how such form of imbalanced news coverage can be exposed. The research presented in this paper addresses not only the automatic detection of bias but goes one step further in that it explores how political bias and unfairness are manifested linguistically. In this regard we utilize a new corpus of 6964 news articles with labels derived from adfontesmedia.com and develop a neural model for bias assessment. By analyzing this model on article excerpts, we find insightful bias patterns at different levels of text granularity, from single words to the whole article discourse.","label":1,"title_clean":"Analyzing Political Bias and Unfairness in News Articles at Different Levels of Granularity","abstract_clean":"Media organizations bear great reponsibility because of their considerable influence on shaping beliefs and positions of our society. Any form of media can contain overly biased content, e.g., by reporting on political events in a selective or incomplete manner. A relevant question hence is whether and how such form of imbalanced news coverage can be exposed. The research presented in this paper addresses not only the automatic detection of bias but goes one step further in that it explores how political bias and unfairness are manifested linguistically. In this regard we utilize a new corpus of 6964 news articles with labels derived from adfontesmedia.com and develop a neural model for bias assessment. By analyzing this model on article excerpts, we find insightful bias patterns at different levels of text granularity, from single words to the whole article discourse.","url":"https:\/\/aclanthology.org\/2020.nlpcss-1.16.pdf"},{"ID":"chen-kageura-2020-multilingualization","methods":["automatic multilingual term extraction methods","graph neural networks","semantic and structural embedding approaches"],"center_method":[null,"graph neural networks",null],"tasks":["term translation","multilingualization of terminology","transfer of domain specific concepts"],"center_task":[null,null,null],"Goal":["Good Health and Well-Being"],"text":"Multilingualization of Medical Terminology: Semantic and Structural Embedding Approaches. The multilingualization of terminology is an essential step in the translation pipeline, to ensure the correct transfer of domain-specific concepts. Many institutions and language service providers construct and maintain multilingual terminologies, which constitute important assets. However, the curation of such multilingual resources requires significant human effort; though automatic multilingual term extraction methods have been proposed so far, they are of limited success as term translation cannot be satisfied by simply conveying meaning, but requires the terminologists and domain experts' knowledge to fit the term within the existing terminology. Here we propose a method to encode the structural properties of terms by aligning their embeddings using graph convolutional networks trained from separate languages. The results show that the structural information can augment the standard bilingual lexicon induction methods, and that taking into account the structural nature of terminologies allows our method to produce better results.","label":1,"title_clean":"Multilingualization of Medical Terminology: Semantic and Structural Embedding Approaches","abstract_clean":"The multilingualization of terminology is an essential step in the translation pipeline, to ensure the correct transfer of domain specific concepts. Many institutions and language service providers construct and maintain multilingual terminologies, which constitute important assets. However, the curation of such multilingual resources requires significant human effort; though automatic multilingual term extraction methods have been proposed so far, they are of limited success as term translation cannot be satisfied by simply conveying meaning, but requires the terminologists and domain experts' knowledge to fit the term within the existing terminology. Here we propose a method to encode the structural properties of terms by aligning their embeddings using graph convolutional networks trained from separate languages. The results show that the structural information can augment the standard bilingual lexicon induction methods, and that taking into account the structural nature of terminologies allows our method to produce better results.","url":"https:\/\/aclanthology.org\/2020.lrec-1.512"},{"ID":"chiang-etal-2021-improved","methods":["tf idf","naive bayes","logistic regression"],"center_method":["tf idf","naive bayes","logistic regression"],"tasks":["classification","long term care","long term care related applications","identification","text classification of long term care materials aging populations"],"center_task":["classification",null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"Improved Text Classification of Long-term Care Materials. Aging populations have posed a challenge to many countries including Taiwan, and with them come the issue of long-term care. Given the current context, the aim of this study was to explore the hotly-discussed subtopics in the field of long-term care, and identify its features through NLP. Texts from forums and websites were utilized for data collection and analysis. The study applied TF-IDF, the logistic regression model, and the naive Bayes classifier to process data. In sum, the results showed that it reached a F1-score of 0.92 in identification, and a best accuracy of 0.71 in classification. Results of the study found that apart from TF-IDF features, certain words could be elicited as favorable features in classification. The results of this study could be used as a reference for future long-term care related applications.","label":1,"title_clean":"Improved Text Classification of Long term Care Materials","abstract_clean":"Aging populations have posed a challenge to many countries including Taiwan, and with them come the issue of long term care. Given the current context, the aim of this study was to explore the hotly discussed subtopics in the field of long term care, and identify its features through NLP. Texts from forums and websites were utilized for data collection and analysis. The study applied TF IDF, the logistic regression model, and the naive Bayes classifier to process data. In sum, the results showed that it reached a F1 score of 0.92 in identification, and a best accuracy of 0.71 in classification. Results of the study found that apart from TF IDF features, certain words could be elicited as favorable features in classification. The results of this study could be used as a reference for future long term care related applications.","url":"https:\/\/aclanthology.org\/2021.rocling-1.38"},{"ID":"chikka-2016-cde","methods":["machine learning methods","deep neural network","conditional random field","cde iiith","support vector machine"],"center_method":["machine learning methods","deep neural network","conditional random field",null,"support vector machine"],"tasks":["identification of events","extraction of temporal information"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"CDE-IIITH at SemEval-2016 Task 12: Extraction of Temporal Information from Clinical documents using Machine Learning techniques. In this paper, we demonstrate our approach for identification of events, time expressions and temporal relations among them. This work was carried out as part of SemEval-2016 Challenge Task 12: Clinical TempEval. The task comprises six sub-tasks: identification of event spans, time spans and their attributes, document time relation and the narrative container relations among events and time expressions. We have participated in all six subtasks. We have provided with a manually annotated dataset which comprises of training dataset (293 documents), development dataset (147 documents) and 151 documents as test dataset. We have submitted our work as two systems for the challenge. One system is developed using machine learning techniques, Conditional Random Fields (CRF) and Support Vector machines (SVM) and the other system is developed using deep neural network (DNN) techniques. The results show that both systems have given relatively same performance on these tasks.","label":1,"title_clean":"CDE IIITH at SemEval 2016 Task 12: Extraction of Temporal Information from Clinical documents using Machine Learning techniques","abstract_clean":"In this paper, we demonstrate our approach for identification of events, time expressions and temporal relations among them. This work was carried out as part of SemEval 2016 Challenge Task 12: Clinical TempEval. The task comprises six sub tasks: identification of event spans, time spans and their attributes, document time relation and the narrative container relations among events and time expressions. We have participated in all six subtasks. We have provided with a manually annotated dataset which comprises of training dataset (293 documents), development dataset (147 documents) and 151 documents as test dataset. We have submitted our work as two systems for the challenge. One system is developed using machine learning techniques, Conditional Random Fields (CRF) and Support Vector machines (SVM) and the other system is developed using deep neural network (DNN) techniques. The results show that both systems have given relatively same performance on these tasks.","url":"https:\/\/aclanthology.org\/S16-1192"},{"ID":"chiril-etal-2020-annotated","methods":["annotation","deep neural network"],"center_method":["annotation","deep neural network"],"tasks":["sexism detection"],"center_task":[null],"Goal":["Gender Equality"],"text":"An Annotated Corpus for Sexism Detection in French Tweets. Social media networks have become a space where users are free to relate their opinions and sentiments which may lead to a large spreading of hatred or abusive messages which have to be moderated. This paper presents the first French corpus annotated for sexism detection composed of about 12,000 tweets. In a context of offensive content mediation on social media now regulated by European laws, we think that it is important to be able to detect automatically not only sexist content but also to identify if a message with a sexist content is really sexist (i.e. addressed to a woman or describing a woman or women in general) or is a story of sexism experienced by a woman. This point is the novelty of our annotation scheme. We also propose some preliminary results for sexism detection obtained with a deep learning approach. Our experiments show encouraging results.","label":1,"title_clean":"An Annotated Corpus for Sexism Detection in French Tweets","abstract_clean":"Social media networks have become a space where users are free to relate their opinions and sentiments which may lead to a large spreading of hatred or abusive messages which have to be moderated. This paper presents the first French corpus annotated for sexism detection composed of about 12,000 tweets. In a context of offensive content mediation on social media now regulated by European laws, we think that it is important to be able to detect automatically not only sexist content but also to identify if a message with a sexist content is really sexist (i.e. addressed to a woman or describing a woman or women in general) or is a story of sexism experienced by a woman. This point is the novelty of our annotation scheme. We also propose some preliminary results for sexism detection obtained with a deep learning approach. Our experiments show encouraging results.","url":"https:\/\/aclanthology.org\/2020.lrec-1.175"},{"ID":"chiril-etal-2020-said","methods":["speech acts theory","tweets vectorial representations","deep learning experiments","generalization strategies"],"center_method":[null,null,null,null],"tasks":["offensive content moderation","sexism detection"],"center_task":[null,null],"Goal":["Gender Equality"],"text":"He said ``who's gonna take care of your children when you are at ACL?'': Reported Sexist Acts are Not Sexist. In a context of offensive content mediation on social media now regulated by European laws, it is important not only to be able to automatically detect sexist content but also to identify if a message with a sexist content is really sexist or is a story of sexism experienced by a woman. We propose: (1) a new characterization of sexist content inspired by speech acts theory and discourse analysis studies, (2) the first French dataset annotated for sexism detection, and (3) a set of deep learning experiments trained on top of a combination of several tweet's vectorial representations (word embeddings, linguistic features, and various generalization strategies). Our results are encouraging and constitute a first step towards offensive content moderation.","label":1,"title_clean":"He said ``who's gonna take care of your children when you are at ACL?'': Reported Sexist Acts are Not Sexist","abstract_clean":"In a context of offensive content mediation on social media now regulated by European laws, it is important not only to be able to automatically detect sexist content but also to identify if a message with a sexist content is really sexist or is a story of sexism experienced by a woman. We propose: (1) a new characterization of sexist content inspired by speech acts theory and discourse analysis studies, (2) the first French dataset annotated for sexism detection, and (3) a set of deep learning experiments trained on top of a combination of several tweet's vectorial representations (word embeddings, linguistic features, and various generalization strategies). Our results are encouraging and constitute a first step towards offensive content moderation.","url":"https:\/\/aclanthology.org\/2020.acl-main.373"},{"ID":"cieri-dipersio-2014-intellectual","methods":["web services","us language application"],"center_method":[null,null],"tasks":["licensing challenges","web service configurations","intellectual property rights management"],"center_task":[null,null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Intellectual Property Rights Management with Web Service Grids. This paper enumerates the ways in which configurations of web services may complicate issues of licensing language resources, whether data or tools. It details specific licensing challenges within the context of the US Language Application (LAPPS) Grid, sketches a solution under development and highlights ways in which that approach may be extended for other web service configurations.","label":1,"title_clean":"Intellectual Property Rights Management with Web Service Grids","abstract_clean":"This paper enumerates the ways in which configurations of web services may complicate issues of licensing language resources, whether data or tools. It details specific licensing challenges within the context of the US Language Application (LAPPS) Grid, sketches a solution under development and highlights ways in which that approach may be extended for other web service configurations.","url":"https:\/\/aclanthology.org\/W14-5211.pdf"},{"ID":"cohan-goharian-2016-revisiting","methods":["relevance analysis","sera"],"center_method":[null,null],"tasks":["text summarization","evaluation methods","evaluating scientific summaries"],"center_task":["text summarization","evaluation methods",null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Revisiting Summarization Evaluation for Scientific Articles. Evaluation of text summarization approaches have been mostly based on metrics that measure similarities of system generated summaries with a set of human written gold-standard summaries. The most widely used metric in summarization evaluation has been the ROUGE family. ROUGE solely relies on lexical overlaps between the terms and phrases in the sentences; therefore, in cases of terminology variations and paraphrasing, ROUGE is not as effective. Scientific article summarization is one such case that is different from general domain summarization (e.g. newswire data). We provide an extensive analysis of ROUGE's effectiveness as an evaluation metric for scientific summarization; we show that, contrary to the common belief, ROUGE is not much reliable in evaluating scientific summaries. We furthermore show how different variants of ROUGE result in very different correlations with the manual Pyramid scores. Finally, we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries. We call our metric SERA (Summarization Evaluation by Relevance Analysis). Unlike ROUGE, SERA consistently achieves high correlations with manual scores which shows its effectiveness in evaluation of scientific article summarization.","label":1,"title_clean":"Revisiting Summarization Evaluation for Scientific Articles","abstract_clean":"Evaluation of text summarization approaches have been mostly based on metrics that measure similarities of system generated summaries with a set of human written gold standard summaries. The most widely used metric in summarization evaluation has been the ROUGE family. ROUGE solely relies on lexical overlaps between the terms and phrases in the sentences; therefore, in cases of terminology variations and paraphrasing, ROUGE is not as effective. Scientific article summarization is one such case that is different from general domain summarization (e.g. newswire data). We provide an extensive analysis of ROUGE's effectiveness as an evaluation metric for scientific summarization; we show that, contrary to the common belief, ROUGE is not much reliable in evaluating scientific summaries. We furthermore show how different variants of ROUGE result in very different correlations with the manual Pyramid scores. Finally, we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries. We call our metric SERA (Summarization Evaluation by Relevance Analysis). Unlike ROUGE, SERA consistently achieves high correlations with manual scores which shows its effectiveness in evaluation of scientific article summarization.","url":"https:\/\/aclanthology.org\/L16-1130"},{"ID":"collier-etal-1999-genia","methods":["language engineering","genia"],"center_method":[null,null],"tasks":["genome information acquisition","automatically extracting biochemical information","visualisation","information retrieval","genia project"],"center_task":[null,null,null,"information retrieval",null],"Goal":["Good Health and Well-Being","Industry, Innovation and Infrastrucure"],"text":"The GENIA project: corpus-based knowledge acquisition and information extraction from genome research papers. We present an outline of the genome information acquisition (GENIA) project for automatically extracting biochemical information from journal papers and abstracts. GENIA will be available over the Internet and is designed to aid in information extraction, retrieval and visualisation and to help reduce information overload on researchers. The vast repository of papers available online in databases such as MEDLINE is a natural environment in which to develop language engineering methods and tools and is an opportunity to show how language engineering can play a key role on the Internet.","label":1,"title_clean":"The GENIA project: corpus based knowledge acquisition and information extraction from genome research papers","abstract_clean":"We present an outline of the genome information acquisition (GENIA) project for automatically extracting biochemical information from journal papers and abstracts. GENIA will be available over the Internet and is designed to aid in information extraction, retrieval and visualisation and to help reduce information overload on researchers. The vast repository of papers available online in databases such as MEDLINE is a natural environment in which to develop language engineering methods and tools and is an opportunity to show how language engineering can play a key role on the Internet.","url":"https:\/\/aclanthology.org\/E99-1043"},{"ID":"collier-etal-2014-impact","methods":["near domain transfer","feature augmentation","pooling of data","stacking","class re labeling","transference"],"center_method":[null,null,null,null,null,null],"tasks":["near domain transference","learning","named entity recognition"],"center_task":[null,"learning","named entity recognition"],"Goal":["Good Health and Well-Being"],"text":"The impact of near domain transfer on biomedical named entity recognition. Current research in fully supervised biomedical named entity recognition (bioNER) is often conducted in a setting of low sample sizes. Whilst experimental results show strong performance in-domain it has been recognised that quality suffers when models are applied to heterogeneous text collections. However the causal factors have until now been uncertain. In this paper we describe a controlled experiment into near domain bias for two Medline corpora on hereditary diseases. Five strategies are employed for mitigating the impact of near domain transference including simple transference, pooling, stacking, class re-labeling and feature augmentation. We measure their effect on f-score performance against an in domain baseline. Stacking and feature augmentation mitigate f-score loss but do not necessarily result in superior performance except for selected classes. Simple pooling of data across domains failed to exploit size effects for most classes. We conclude that we can expect lower performance and higher annotation costs if we do not adequately compensate for the distributional dissimilarities of domains during learning.","label":1,"title_clean":"The impact of near domain transfer on biomedical named entity recognition","abstract_clean":"Current research in fully supervised biomedical named entity recognition (bioNER) is often conducted in a setting of low sample sizes. Whilst experimental results show strong performance in domain it has been recognised that quality suffers when models are applied to heterogeneous text collections. However the causal factors have until now been uncertain. In this paper we describe a controlled experiment into near domain bias for two Medline corpora on hereditary diseases. Five strategies are employed for mitigating the impact of near domain transference including simple transference, pooling, stacking, class re labeling and feature augmentation. We measure their effect on f score performance against an in domain baseline. Stacking and feature augmentation mitigate f score loss but do not necessarily result in superior performance except for selected classes. Simple pooling of data across domains failed to exploit size effects for most classes. We conclude that we can expect lower performance and higher annotation costs if we do not adequately compensate for the distributional dissimilarities of domains during learning.","url":"https:\/\/aclanthology.org\/W14-1103"},{"ID":"concordia-etal-2020-store","methods":["scientific workflows","cloud repository","sshoc repository","software framework"],"center_method":[null,null,null,null],"tasks":["re usability","social sciences and humanities open cloud","workflow description"],"center_task":[null,null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Store Scientific Workflows Data in SSHOC Repository. Today scientific workflows are used by scientists as a way to define automated, scalable, and portable in-silico experiments. Having a formal description of an experiment can improve replicability and reproducibility of the experiment. However, simply publishing the workflow may be not enough to achieve reproducibility and re-usability, in particular workflow description should be enriched with provenance data generated during the workflow life cycle. This paper presents a software framework being designed and developed in the context of the Social Sciences and Humanities Open Cloud (SSHOC) project, whose overall objective is to realise the social sciences and humanities' part of European Open Science Cloud initiative. The framework will implement functionalities to use the SSHOC Repository service as a cloud repository for scientific workflows.","label":1,"title_clean":"Store Scientific Workflows Data in SSHOC Repository","abstract_clean":"Today scientific workflows are used by scientists as a way to define automated, scalable, and portable in silico experiments. Having a formal description of an experiment can improve replicability and reproducibility of the experiment. However, simply publishing the workflow may be not enough to achieve reproducibility and re usability, in particular workflow description should be enriched with provenance data generated during the workflow life cycle. This paper presents a software framework being designed and developed in the context of the Social Sciences and Humanities Open Cloud (SSHOC) project, whose overall objective is to realise the social sciences and humanities' part of European Open Science Cloud initiative. The framework will implement functionalities to use the SSHOC Repository service as a cloud repository for scientific workflows.","url":"https:\/\/aclanthology.org\/2020.lr4sshoc-1.1"},{"ID":"cook-etal-2016-dictionary","methods":["dictionary and rule based system","tagit","rule based post processing steps","dictionary based named entity tagger"],"center_method":[null,null,null,null],"tasks":["automated information extraction","identification of bacteria and habitats"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"A dictionary- and rule-based system for identification of bacteria and habitats in text. The number of scientific papers published each year is growing exponentially and given the rate of this growth, automated information extraction is needed to efficiently extract information from this corpus. A critical first step in this process is to accurately recognize the names of entities in text. Previous efforts, such as SPECIES, have identified bacteria strain names, among other taxonomic groups, but have been limited to those names present in NCBI taxonomy. We have implemented a dictionary-based named entity tagger, TagIt, that is followed by a rule based expansion system to identify bacteria strain names and habitats and resolve them to the closest match possible in the NCBI taxonomy and the OntoBiotope ontology respectively. The rule based post processing steps expand acronyms, and extend strain names according to a set of rules, which captures additional aliases and strains that are not present in the dictionary. TagIt has the best performance out of three entries to BioNLP-ST BB3 cat+ner, with an overall SER of 0.628 on the independent test set.","label":1,"title_clean":"A dictionary and rule based system for identification of bacteria and habitats in text","abstract_clean":"The number of scientific papers published each year is growing exponentially and given the rate of this growth, automated information extraction is needed to efficiently extract information from this corpus. A critical first step in this process is to accurately recognize the names of entities in text. Previous efforts, such as SPECIES, have identified bacteria strain names, among other taxonomic groups, but have been limited to those names present in NCBI taxonomy. We have implemented a dictionary based named entity tagger, TagIt, that is followed by a rule based expansion system to identify bacteria strain names and habitats and resolve them to the closest match possible in the NCBI taxonomy and the OntoBiotope ontology respectively. The rule based post processing steps expand acronyms, and extend strain names according to a set of rules, which captures additional aliases and strains that are not present in the dictionary. TagIt has the best performance out of three entries to BioNLP ST BB3 cat+ner, with an overall SER of 0.628 on the independent test set.","url":"https:\/\/aclanthology.org\/W16-3006"},{"ID":"coppersmith-etal-2015-adhd","methods":["linguistic components"],"center_method":[null],"tasks":["mental health","language of mental health"],"center_task":["mental health",null],"Goal":["Good Health and Well-Being"],"text":"From ADHD to SAD: Analyzing the Language of Mental Health on Twitter through Self-Reported Diagnoses. Many significant challenges exist for the mental health field, but one in particular is a lack of data available to guide research. Language provides a natural lens for studying mental health-much existing work and therapy have strong linguistic components, so the creation of a large, varied, language-centric dataset could provide significant grist for the field of mental health research. We examine a broad range of mental health conditions in Twitter data by identifying self-reported statements of diagnosis. We systematically explore language differences between ten conditions with respect to the general population, and to each other. Our aim is to provide guidance and a roadmap for where deeper exploration is likely to be fruitful.","label":1,"title_clean":"From ADHD to SAD: Analyzing the Language of Mental Health on Twitter through Self Reported Diagnoses","abstract_clean":"Many significant challenges exist for the mental health field, but one in particular is a lack of data available to guide research. Language provides a natural lens for studying mental health much existing work and therapy have strong linguistic components, so the creation of a large, varied, language centric dataset could provide significant grist for the field of mental health research. We examine a broad range of mental health conditions in Twitter data by identifying self reported statements of diagnosis. We systematically explore language differences between ten conditions with respect to the general population, and to each other. Our aim is to provide guidance and a roadmap for where deeper exploration is likely to be fruitful.","url":"https:\/\/aclanthology.org\/W15-1201.pdf"},{"ID":"da-san-martino-etal-2020-prta","methods":["analysis of propaganda techniques","prta","propaganda techniques"],"center_method":[null,null,null],"tasks":["factchecking and disinformation detection"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Prta: A System to Support the Analysis of Propaganda Techniques in the News. Recent events, such as the 2016 US Presidential Campaign, Brexit and the COVID-19 \"infodemic\", have brought into the spotlight the dangers of online disinformation. There has been a lot of research focusing on factchecking and disinformation detection. However, little attention has been paid to the specific rhetorical and psychological techniques used to convey propaganda messages. Revealing the use of such techniques can help promote media literacy and critical thinking, and eventually contribute to limiting the impact of \"fake news\" and disinformation campaigns. Prta (Propaganda Persuasion Techniques Analyzer) allows users to explore the articles crawled on a regular basis by highlighting the spans in which propaganda techniques occur and to compare them on the basis of their use of propaganda techniques. The system further reports statistics about the use of such techniques, overall and over time, or according to filtering criteria specified by the user based on time interval, keywords, and\/or political orientation of the media. Moreover, it allows users to analyze any text or URL through a dedicated interface or via an API. The system is available online: https:\/\/www.tanbih.org\/prta.","label":1,"title_clean":"Prta: A System to Support the Analysis of Propaganda Techniques in the News","abstract_clean":"Recent events, such as the 2016 US Presidential Campaign, Brexit and the COVID 19 \"infodemic\", have brought into the spotlight the dangers of online disinformation. There has been a lot of research focusing on factchecking and disinformation detection. However, little attention has been paid to the specific rhetorical and psychological techniques used to convey propaganda messages. Revealing the use of such techniques can help promote media literacy and critical thinking, and eventually contribute to limiting the impact of \"fake news\" and disinformation campaigns. Prta (Propaganda Persuasion Techniques Analyzer) allows users to explore the articles crawled on a regular basis by highlighting the spans in which propaganda techniques occur and to compare them on the basis of their use of propaganda techniques. The system further reports statistics about the use of such techniques, overall and over time, or according to filtering criteria specified by the user based on time interval, keywords, and\/or political orientation of the media. Moreover, it allows users to analyze any text or URL through a dedicated interface or via an API. The system is available online: https:\/\/www.tanbih.org\/prta.","url":"https:\/\/aclanthology.org\/2020.acl-demos.32.pdf"},{"ID":"das-kannan-2014-discovering","methods":["em algorithm","probabilistic model"],"center_method":[null,null],"tasks":["trend detection","business analytics"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Discovering Topical Aspects in Microblogs. We address the problem of discovering topical phrases or \"aspects\" from microblogging sites like Twitter, that correspond to key talking points or buzz around a particular topic or entity of interest. Inferring such topical aspects enables various applications such as trend detection and opinion mining for business analytics. However, mining high-volume microblog streams for aspects poses unique challenges due to the inherent noise, redundancy and ambiguity in users' social posts. We address these challenges by using a probabilistic model that incorporates various global and local indicators such as \"uniqueness\", \"diversity\" and \"burstiness\" of phrases, to infer relevant aspects. Our model is learned using an EM algorithm that uses automatically generated noisy labels, without requiring manual effort or domain knowledge. We present results on three months of Twitter data across different types of entities to validate our approach.","label":1,"title_clean":"Discovering Topical Aspects in Microblogs","abstract_clean":"We address the problem of discovering topical phrases or \"aspects\" from microblogging sites like Twitter, that correspond to key talking points or buzz around a particular topic or entity of interest. Inferring such topical aspects enables various applications such as trend detection and opinion mining for business analytics. However, mining high volume microblog streams for aspects poses unique challenges due to the inherent noise, redundancy and ambiguity in users' social posts. We address these challenges by using a probabilistic model that incorporates various global and local indicators such as \"uniqueness\", \"diversity\" and \"burstiness\" of phrases, to infer relevant aspects. Our model is learned using an EM algorithm that uses automatically generated noisy labels, without requiring manual effort or domain knowledge. We present results on three months of Twitter data across different types of entities to validate our approach.","url":"https:\/\/aclanthology.org\/C14-1082"},{"ID":"davoodi-etal-2022-modeling","methods":["health care and education access","textual graphbased model"],"center_method":[null,null],"tasks":["legislative process"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Modeling U.S. State-Level Policies by Extracting Winners and Losers from Legislative Texts. Decisions on state-level policies have a deep effect on many aspects of our everyday life, such as health-care and education access. However, there is little understanding of how these policies and decisions are being formed in the legislative process. We take a datadriven approach by decoding the impact of legislation on relevant stakeholders (e.g., teachers in education bills) to understand legislators' decision-making process and votes. We build a new dataset for multiple US states that interconnects multiple sources of data including bills, stakeholders, legislators, and money donors. Next, we develop a textual graphbased model to embed and analyze state bills. Our model predicts winners\/losers of bills and then utilizes them to better determine the legislative body's vote breakdown according to demographic\/ideological criteria, e.g., gender.","label":1,"title_clean":"Modeling U.S. State Level Policies by Extracting Winners and Losers from Legislative Texts","abstract_clean":"Decisions on state level policies have a deep effect on many aspects of our everyday life, such as health care and education access. However, there is little understanding of how these policies and decisions are being formed in the legislative process. We take a datadriven approach by decoding the impact of legislation on relevant stakeholders (e.g., teachers in education bills) to understand legislators' decision making process and votes. We build a new dataset for multiple US states that interconnects multiple sources of data including bills, stakeholders, legislators, and money donors. Next, we develop a textual graphbased model to embed and analyze state bills. Our model predicts winners\/losers of bills and then utilizes them to better determine the legislative body's vote breakdown according to demographic\/ideological criteria, e.g., gender.","url":"https:\/\/aclanthology.org\/2022.acl-long.22"},{"ID":"dcosta-etal-2020-multiple","methods":["snorkel","msbc","word2vec cnn","ms bert","rule based approaches","classification","transformers","keyword searches","expanded disability status scale"],"center_method":[null,null,null,null,null,"classification","transformers",null,null],"tasks":["prediction tasks","multiple sclerosis severity classification","edss measurement","chronic inflammatory and degenerative neurological disease","predicting edss"],"center_task":[null,null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"Multiple Sclerosis Severity Classification From Clinical Text. Multiple Sclerosis (MS) is a chronic, inflammatory and degenerative neurological disease, which is monitored by a specialist using the Expanded Disability Status Scale (EDSS) and recorded in unstructured text in the form of a neurology consult note. An EDSS measurement contains an overall 'EDSS' score and several functional subscores. Typically, expert knowledge is required to interpret consult notes and generate these scores. Previous approaches used limited context length Word2Vec embeddings and keyword searches to predict scores given a consult note, but often failed when scores were not explicitly stated. In this work, we present MS-BERT, the first publicly available transformer model trained on real clinical data other than MIMIC. Next, we present MSBC, a classifier that applies MS-BERT to generate embeddings and predict EDSS and functional subscores. Lastly, we explore combining MSBC with other models through the use of Snorkel to generate scores for unlabelled consult notes. MSBC achieves state-of-the-art performance on all metrics and prediction tasks and outperforms the models generated from the Snorkel ensemble. We improve Macro-F1 by 0.12 (to 0.88) for predicting EDSS and on average by 0.29 (to 0.63) for predicting functional subscores over previous Word2Vec CNN and rule-based approaches.","label":1,"title_clean":"Multiple Sclerosis Severity Classification From Clinical Text","abstract_clean":"Multiple Sclerosis (MS) is a chronic, inflammatory and degenerative neurological disease, which is monitored by a specialist using the Expanded Disability Status Scale (EDSS) and recorded in unstructured text in the form of a neurology consult note. An EDSS measurement contains an overall 'EDSS' score and several functional subscores. Typically, expert knowledge is required to interpret consult notes and generate these scores. Previous approaches used limited context length Word2Vec embeddings and keyword searches to predict scores given a consult note, but often failed when scores were not explicitly stated. In this work, we present MS BERT, the first publicly available transformer model trained on real clinical data other than MIMIC. Next, we present MSBC, a classifier that applies MS BERT to generate embeddings and predict EDSS and functional subscores. Lastly, we explore combining MSBC with other models through the use of Snorkel to generate scores for unlabelled consult notes. MSBC achieves state of the art performance on all metrics and prediction tasks and outperforms the models generated from the Snorkel ensemble. We improve Macro F1 by 0.12 (to 0.88) for predicting EDSS and on average by 0.29 (to 0.63) for predicting functional subscores over previous Word2Vec CNN and rule based approaches.","url":"https:\/\/aclanthology.org\/2020.clinicalnlp-1.2"},{"ID":"del-tredici-fernandez-2020-words","methods":["language based user representations"],"center_method":[null],"tasks":["fake news detection"],"center_task":["fake news detection"],"Goal":["Peace, Justice and Strong Institutions"],"text":"Words are the Window to the Soul: Language-based User Representations for Fake News Detection. Cognitive and social traits of individuals are reflected in language use. Moreover, individuals who are prone to spread fake news online often share common traits. Building on these ideas, we introduce a model that creates representations of individuals on social media based only on the language they produce, and use them to detect fake news. We show that language-based user representations are beneficial for this task. We also present an extended analysis of the language of fake news spreaders, showing that its main features are mostly domain independent and consistent across two English datasets. Finally, we exploit the relation between language use and connections in the social graph to assess the presence of the Echo Chamber effect in our data.","label":1,"title_clean":"Words are the Window to the Soul: Language based User Representations for Fake News Detection","abstract_clean":"Cognitive and social traits of individuals are reflected in language use. Moreover, individuals who are prone to spread fake news online often share common traits. Building on these ideas, we introduce a model that creates representations of individuals on social media based only on the language they produce, and use them to detect fake news. We show that language based user representations are beneficial for this task. We also present an extended analysis of the language of fake news spreaders, showing that its main features are mostly domain independent and consistent across two English datasets. Finally, we exploit the relation between language use and connections in the social graph to assess the presence of the Echo Chamber effect in our data.","url":"https:\/\/aclanthology.org\/2020.coling-main.477.pdf"},{"ID":"deleger-zweigenbaum-2010-identifying","methods":["extraction rules"],"center_method":[null],"tasks":["identifying paraphrases"],"center_task":[null],"Goal":["Quality Education"],"text":"Identifying Paraphrases between Technical and Lay Corpora. In previous work, we presented a preliminary study to identify paraphrases between technical and lay discourse types from medical corpora dedicated to the French language. In this paper, we test the hypothesis that the same kinds of paraphrases as for French can be detected between English technical and lay discourse types and report the adaptation of our method from French to English. Starting from the constitution of monolingual comparable corpora, we extract two kinds of paraphrases: paraphrases between nominalizations and verbal constructions and paraphrases between neo-classical compounds and modern-language phrases. We do this relying on morphological resources and a set of extraction rules we adapt from the original approach for French. Results show that paraphrases could be identified with a rather good precision, and that these types of paraphrase are relevant in the context of the opposition between technical and lay discourse types. These observations are consistent with the results obtained for French, which demonstrates the portability of the approach as well as the similarity of the two languages as regards the use of those kinds of expressions in technical and lay discourse types.","label":1,"title_clean":"Identifying Paraphrases between Technical and Lay Corpora","abstract_clean":"In previous work, we presented a preliminary study to identify paraphrases between technical and lay discourse types from medical corpora dedicated to the French language. In this paper, we test the hypothesis that the same kinds of paraphrases as for French can be detected between English technical and lay discourse types and report the adaptation of our method from French to English. Starting from the constitution of monolingual comparable corpora, we extract two kinds of paraphrases: paraphrases between nominalizations and verbal constructions and paraphrases between neo classical compounds and modern language phrases. We do this relying on morphological resources and a set of extraction rules we adapt from the original approach for French. Results show that paraphrases could be identified with a rather good precision, and that these types of paraphrase are relevant in the context of the opposition between technical and lay discourse types. These observations are consistent with the results obtained for French, which demonstrates the portability of the approach as well as the similarity of the two languages as regards the use of those kinds of expressions in technical and lay discourse types.","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2010\/pdf\/472_Paper.pdf"},{"ID":"dennis-henderson-etal-2020-life","methods":["distant reading","topic modelling"],"center_method":[null,null],"tasks":["extracting dates","sentiment analysis"],"center_task":[null,"sentiment analysis"],"Goal":["Peace, Justice and Strong Institutions"],"text":"Life still goes on: Analysing Australian WW1 Diaries through Distant Reading. An increasing amount of historic data is now available in digital (text) formats. This gives quantitative researchers an opportunity to use distant reading techniques, as opposed to traditional close reading, in order to analyse larger quantities of historic data. Distant reading allows researchers to view overall patterns within the data and reduce researcher bias. One such data set that has recently been transcribed is a collection of over 500 Australian World War I (WW1) diaries held by the State Library of New South Wales. Here we apply distant reading techniques to this corpus to understand what soldiers wrote about and how they felt over the course of the war. Extracting dates accurately is important as it allows us to perform our analysis over time, however, it is very challenging due to the variety of date formats and abbreviations diarists use. But with that data, topic modelling and sentiment analysis can then be applied to show trends, for instance, that despite the horrors of war, Australians in WW1 primarily wrote about their everyday routines and experiences. Our results detail some of the challenges likely to be encountered by quantitative researchers intending to analyse historical texts, and provide some approaches to these issues.","label":1,"title_clean":"Life still goes on: Analysing Australian WW1 Diaries through Distant Reading","abstract_clean":"An increasing amount of historic data is now available in digital (text) formats. This gives quantitative researchers an opportunity to use distant reading techniques, as opposed to traditional close reading, in order to analyse larger quantities of historic data. Distant reading allows researchers to view overall patterns within the data and reduce researcher bias. One such data set that has recently been transcribed is a collection of over 500 Australian World War I (WW1) diaries held by the State Library of New South Wales. Here we apply distant reading techniques to this corpus to understand what soldiers wrote about and how they felt over the course of the war. Extracting dates accurately is important as it allows us to perform our analysis over time, however, it is very challenging due to the variety of date formats and abbreviations diarists use. But with that data, topic modelling and sentiment analysis can then be applied to show trends, for instance, that despite the horrors of war, Australians in WW1 primarily wrote about their everyday routines and experiences. Our results detail some of the challenges likely to be encountered by quantitative researchers intending to analyse historical texts, and provide some approaches to these issues.","url":"https:\/\/aclanthology.org\/2020.latechclfl-1.11"},{"ID":"dernoncourt-etal-2017-neural","methods":["ann architecture","artificial neural networks","conditional random field","sentence classification approaches"],"center_method":[null,null,"conditional random field",null],"tasks":["sequential sentence classification","structured prediction"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"Neural Networks for Joint Sentence Classification in Medical Paper Abstracts. Existing models based on artificial neural networks (ANNs) for sentence classification often do not incorporate the context in which sentences appear, and classify sentences individually. However, traditional sentence classification approaches have been shown to greatly benefit from jointly classifying subsequent sentences, such as with conditional random fields. In this work, we present an ANN architecture that combines the effectiveness of typical ANN models to classify sentences in isolation, with the strength of structured prediction. Our model outperforms the state-ofthe-art results on two different datasets for sequential sentence classification in medical abstracts.","label":1,"title_clean":"Neural Networks for Joint Sentence Classification in Medical Paper Abstracts","abstract_clean":"Existing models based on artificial neural networks (ANNs) for sentence classification often do not incorporate the context in which sentences appear, and classify sentences individually. However, traditional sentence classification approaches have been shown to greatly benefit from jointly classifying subsequent sentences, such as with conditional random fields. In this work, we present an ANN architecture that combines the effectiveness of typical ANN models to classify sentences in isolation, with the strength of structured prediction. Our model outperforms the state ofthe art results on two different datasets for sequential sentence classification in medical abstracts.","url":"https:\/\/aclanthology.org\/E17-2110.pdf"},{"ID":"desilets-etal-2008-evaluating","methods":["machine translation","integration approaches","automatic speech recognition","dictaphone","manufacturer recommended procedure","hybrid technology","hybrid asr mt systems","asr language model","human translator"],"center_method":["machine translation",null,"automatic speech recognition",null,null,null,null,null,null],"tasks":["translation dictation","error correction","automatic speech recognition","machine translation"],"center_task":[null,null,"automatic speech recognition","machine translation"],"Goal":["Decent Work and Economic Growth"],"text":"Evaluating productivity gains of hybrid ASR-MT systems for translation dictation.. This paper is about Translation Dictation with ASR, that is, the use of Automatic Speech Recognition (ASR) by human translators, in order to dictate translations. We are particularly interested in the productivity gains that this could provide over conventional keyboard input, and ways in which such gains might be increased through a combination of ASR and Statistical Machine Translation (SMT). In this hybrid technology, the source language text is presented to both the human translator and a SMT system. The latter produces Nbest translations hypotheses, which are then used to fine tune the ASR language model and vocabulary towards utterances which are probable translations of source text sentences. We conducted an ergonomic experiment with eight professional translators dictating into French, using a top of the line offthe-shelf ASR system (Dragon NatuallySpeaking 8). We found that the ASR system had an average Word Error Rate (WER) of 11.7%, and that translation using this system did not provide statistically significant productivity increases over keyboard input, when following the manufacturer recommended procedure for error correction. However, we found indications that, even in its current imperfect state, French ASR might be beneficial to translators who are already used to dictation (either with ASR or a dictaphone), but more focused experiments are needed to confirm this. We also found that dictation using an ASR with WER of 4% or less would have resulted in statistically significant (p < 0.6) productivity gains in the order of 25.1% to 44.9% Translated Words Per Minute. We also evaluated the extent to which the limited manufacturer provided Domain Adaptation features could be used to positively bias the ASR using SMT hypotheses. We found that the relative gains in WER were much lower than has been reported in the literature for tighter integration of SMT with ASR, pointing the advantages of tight integration approaches and the need for more research in that area.","label":1,"title_clean":"Evaluating productivity gains of hybrid ASR MT systems for translation dictation.","abstract_clean":"This paper is about Translation Dictation with ASR, that is, the use of Automatic Speech Recognition (ASR) by human translators, in order to dictate translations. We are particularly interested in the productivity gains that this could provide over conventional keyboard input, and ways in which such gains might be increased through a combination of ASR and Statistical Machine Translation (SMT). In this hybrid technology, the source language text is presented to both the human translator and a SMT system. The latter produces Nbest translations hypotheses, which are then used to fine tune the ASR language model and vocabulary towards utterances which are probable translations of source text sentences. We conducted an ergonomic experiment with eight professional translators dictating into French, using a top of the line offthe shelf ASR system (Dragon NatuallySpeaking 8). We found that the ASR system had an average Word Error Rate (WER) of 11.7%, and that translation using this system did not provide statistically significant productivity increases over keyboard input, when following the manufacturer recommended procedure for error correction. However, we found indications that, even in its current imperfect state, French ASR might be beneficial to translators who are already used to dictation (either with ASR or a dictaphone), but more focused experiments are needed to confirm this. We also found that dictation using an ASR with WER of 4% or less would have resulted in statistically significant (p < 0.6) productivity gains in the order of 25.1% to 44.9% Translated Words Per Minute. We also evaluated the extent to which the limited manufacturer provided Domain Adaptation features could be used to positively bias the ASR using SMT hypotheses. We found that the relative gains in WER were much lower than has been reported in the literature for tighter integration of SMT with ASR, pointing the advantages of tight integration approaches and the need for more research in that area.","url":"https:\/\/aclanthology.org\/2008.iwslt-papers.3.pdf"},{"ID":"deville-etal-1996-anthem","methods":["natural language interface"],"center_method":[null],"tasks":["multilingual text generation"],"center_task":[null],"Goal":["Good Health and Well-Being"],"text":"ANTHEM: advanced natural language interface for multilingual text generation in healthcare (LRE 62-007). ","label":1,"title_clean":"ANTHEM: advanced natural language interface for multilingual text generation in healthcare (LRE 62 007)","abstract_clean":"","url":"https:\/\/aclanthology.org\/1996.amta-1.27"},{"ID":"diaz-etal-2010-development","methods":["personalisation of digital newspapers","personalization system"],"center_method":[null,null],"tasks":["personalisation of digital newspapers","representation of information needs"],"center_task":[null,null],"Goal":["Quality Education"],"text":"Development and Use of an Evaluation Collection for Personalisation of Digital Newspapers. This paper presents the process of development and the characteristics of an evaluation collection for a personalisation system for digital newspapers. This system selects, adapts and presents contents according to a user model that define information needs. The collection presented here contains data that are cross-related over four different axes: a set of news items from an electronic newspaper, collected into subsets corresponding to a particular sequence of days, packaged together and cross-indexed with a set of user profiles that represent the particular evolution of interests of a set of real users over the given days, expressed in each case according to four different representation frameworks: newspaper sections, Yahoo categories, keywords, and relevance feedback over the set of news items for the previous day. This information provides a minimum starting material over which one can evaluate for a given system how it addresses the first two observations-adapting to different users and adapting to particular users over time-providing that the particular system implements the representation of information needs according to the four frameworks employed in the collection. This collection has been successfully used to perform some different experiments to determine the effectiveness of the personalization system presented.","label":1,"title_clean":"Development and Use of an Evaluation Collection for Personalisation of Digital Newspapers","abstract_clean":"This paper presents the process of development and the characteristics of an evaluation collection for a personalisation system for digital newspapers. This system selects, adapts and presents contents according to a user model that define information needs. The collection presented here contains data that are cross related over four different axes: a set of news items from an electronic newspaper, collected into subsets corresponding to a particular sequence of days, packaged together and cross indexed with a set of user profiles that represent the particular evolution of interests of a set of real users over the given days, expressed in each case according to four different representation frameworks: newspaper sections, Yahoo categories, keywords, and relevance feedback over the set of news items for the previous day. This information provides a minimum starting material over which one can evaluate for a given system how it addresses the first two observations adapting to different users and adapting to particular users over time providing that the particular system implements the representation of information needs according to the four frameworks employed in the collection. This collection has been successfully used to perform some different experiments to determine the effectiveness of the personalization system presented.","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2010\/pdf\/569_Paper.pdf"},{"ID":"dillinger-seligman-2006-conversertm","methods":["translation shortcuts facility","interactive system"],"center_method":[null,null],"tasks":["text entry","interactive speech to speech translation","healthcare"],"center_task":[null,null,"healthcare"],"Goal":["Good Health and Well-Being"],"text":"Converser\\mbox$^\\mboxTM$: Highly Interactive Speech-to-Speech Translation for Healthcare. We describe a highly interactive system for bidirectional, broad-coverage spoken language communication in the healthcare area. The paper briefly reviews the system's interactive foundations, and then goes on to discuss in greater depth our Translation Shortcuts facility, which minimizes the need for interactive verification of sentences after they have been vetted. This facility also considerably speeds throughput while maintaining accuracy, and allows use by minimally literate patients for whom any mode of text entry might be difficult.","label":1,"title_clean":"Converser\\mbox$^\\mboxTM$: Highly Interactive Speech to Speech Translation for Healthcare","abstract_clean":"We describe a highly interactive system for bidirectional, broad coverage spoken language communication in the healthcare area. The paper briefly reviews the system's interactive foundations, and then goes on to discuss in greater depth our Translation Shortcuts facility, which minimizes the need for interactive verification of sentences after they have been vetted. This facility also considerably speeds throughput while maintaining accuracy, and allows use by minimally literate patients for whom any mode of text entry might be difficult.","url":"https:\/\/aclanthology.org\/W06-3706"},{"ID":"dilsizian-etal-2014-new","methods":["tracking method","2d modeling","linguistic modeling"],"center_method":[null,null,null],"tasks":["sign language recognition","3d handshape identification","sign production","recognition of 3d hand configuration","computer"],"center_task":["sign language recognition",null,null,null,null],"Goal":["Reduced Inequalities"],"text":"A New Framework for Sign Language Recognition based on 3D Handshape Identification and Linguistic Modeling. Current approaches to sign recognition by computer generally have at least some of the following limitations: they rely on laboratory conditions for sign production, are limited to a small vocabulary, rely on 2D modeling (and therefore cannot deal with occlusions and off-plane rotations), and\/or achieve limited success. Here we propose a new framework that (1) provides a new tracking method less dependent than others on laboratory conditions and able to deal with variations in background and skin regions (such as the face, forearms, or other hands); (2) allows for identification of 3D hand configurations that are linguistically important in American Sign Language (ASL); and (3) incorporates statistical information reflecting linguistic constraints in sign production. For purposes of large-scale computer-based sign language recognition from video, the ability to distinguish hand configurations accurately is critical. Our current method estimates the 3D hand configuration to distinguish among 77 hand configurations linguistically relevant for ASL. Constraining the problem in this way makes recognition of 3D hand configuration more tractable and provides the information specifically needed for sign recognition. Further improvements are obtained by incorporation of statistical information about linguistic dependencies among handshapes within a sign derived from an annotated corpus of almost 10,000 sign tokens.","label":1,"title_clean":"A New Framework for Sign Language Recognition based on 3D Handshape Identification and Linguistic Modeling","abstract_clean":"Current approaches to sign recognition by computer generally have at least some of the following limitations: they rely on laboratory conditions for sign production, are limited to a small vocabulary, rely on 2D modeling (and therefore cannot deal with occlusions and off plane rotations), and\/or achieve limited success. Here we propose a new framework that (1) provides a new tracking method less dependent than others on laboratory conditions and able to deal with variations in background and skin regions (such as the face, forearms, or other hands); (2) allows for identification of 3D hand configurations that are linguistically important in American Sign Language (ASL); and (3) incorporates statistical information reflecting linguistic constraints in sign production. For purposes of large scale computer based sign language recognition from video, the ability to distinguish hand configurations accurately is critical. Our current method estimates the 3D hand configuration to distinguish among 77 hand configurations linguistically relevant for ASL. Constraining the problem in this way makes recognition of 3D hand configuration more tractable and provides the information specifically needed for sign recognition. Further improvements are obtained by incorporation of statistical information about linguistic dependencies among handshapes within a sign derived from an annotated corpus of almost 10,000 sign tokens.","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2014\/pdf\/1138_Paper.pdf"},{"ID":"dimov-etal-2020-nopropaganda","methods":["autoregressive transformer decoder","lstm baselines"],"center_method":[null,null],"tasks":["sequence tagging","classification","relation extraction","detection of propaganda techniques"],"center_task":[null,"classification","relation extraction",null],"Goal":["Peace, Justice and Strong Institutions"],"text":"NoPropaganda at SemEval-2020 Task 11: A Borrowed Approach to Sequence Tagging and Text Classification. This paper describes our contribution to SemEval-2020 Task 11: Detection Of Propaganda Techniques In News Articles. We start with simple LSTM baselines and move to an autoregressive transformer decoder to predict long continuous propaganda spans for the first subtask. We also adopt an approach from relation extraction by enveloping spans mentioned above with special tokens for the second subtask of propaganda technique classification. Our models report an F-score of 44.6% and a micro-averaged F-score of 58.2% for those tasks accordingly.","label":1,"title_clean":"NoPropaganda at SemEval 2020 Task 11: A Borrowed Approach to Sequence Tagging and Text Classification","abstract_clean":"This paper describes our contribution to SemEval 2020 Task 11: Detection Of Propaganda Techniques In News Articles. We start with simple LSTM baselines and move to an autoregressive transformer decoder to predict long continuous propaganda spans for the first subtask. We also adopt an approach from relation extraction by enveloping spans mentioned above with special tokens for the second subtask of propaganda technique classification. Our models report an F score of 44.6% and a micro averaged F score of 58.2% for those tasks accordingly.","url":"https:\/\/aclanthology.org\/2020.semeval-1.194.pdf"},{"ID":"ding-etal-2013-detecting","methods":["text - based predictor"],"center_method":[null],"tasks":["cqa sites","community question answering","detecting spammers","optimization problem"],"center_task":[null,null,null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Detecting Spammers in Community Question Answering. As the popularity of Community Question Answering(CQA) increases, spamming activities also picked up in numbers and variety. On CQA sites, spammers often pretend to ask questions, and select answers which were published by their partners or themselves as the best answers. These fake best answers cannot be easily detected by neither existing methods nor common users. In this paper, we address the issue of detecting spammers on CQA sites. We formulate the task as an optimization problem. Social information is incorporated by adding graph regularization constraints to the text-based predictor. To evaluate the proposed approach, we crawled a data set from a CQA portal. Experimental results demonstrate that the proposed method can achieve better performance than some state-of-the-art methods.","label":1,"title_clean":"Detecting Spammers in Community Question Answering","abstract_clean":"As the popularity of Community Question Answering(CQA) increases, spamming activities also picked up in numbers and variety. On CQA sites, spammers often pretend to ask questions, and select answers which were published by their partners or themselves as the best answers. These fake best answers cannot be easily detected by neither existing methods nor common users. In this paper, we address the issue of detecting spammers on CQA sites. We formulate the task as an optimization problem. Social information is incorporated by adding graph regularization constraints to the text based predictor. To evaluate the proposed approach, we crawled a data set from a CQA portal. Experimental results demonstrate that the proposed method can achieve better performance than some state of the art methods.","url":"https:\/\/aclanthology.org\/I13-1014"},{"ID":"ding-etal-2014-using","methods":["open ie technology","bags of words based baselines","linear and nonlinear models","event based system","open information extraction"],"center_method":[null,null,null,null,null],"tasks":["individual stock prediction","event based stock price movement prediction","extracting structured events"],"center_task":[null,null,null],"Goal":["Decent Work and Economic Growth"],"text":"Using Structured Events to Predict Stock Price Movement: An Empirical Investigation. It has been shown that news events influence the trends of stock price movements. However, previous work on news-driven stock market prediction rely on shallow features (such as bags-of-words, named entities and noun phrases), which do not capture structured entity-relation information, and hence cannot represent complete and exact events. Recent advances in Open Information Extraction (Open IE) techniques enable the extraction of structured events from web-scale data. We propose to adapt Open IE technology for event-based stock price movement prediction, extracting structured events from large-scale public news without manual efforts. Both linear and nonlinear models are employed to empirically investigate the hidden and complex relationships between events and the stock market. Largescale experiments show that the accuracy of S&P 500 index prediction is 60%, and that of individual stock prediction can be over 70%. Our event-based system outperforms bags-of-words-based baselines, and previously reported systems trained on S&P 500 stock historical data.","label":1,"title_clean":"Using Structured Events to Predict Stock Price Movement: An Empirical Investigation","abstract_clean":"It has been shown that news events influence the trends of stock price movements. However, previous work on news driven stock market prediction rely on shallow features (such as bags of words, named entities and noun phrases), which do not capture structured entity relation information, and hence cannot represent complete and exact events. Recent advances in Open Information Extraction (Open IE) techniques enable the extraction of structured events from web scale data. We propose to adapt Open IE technology for event based stock price movement prediction, extracting structured events from large scale public news without manual efforts. Both linear and nonlinear models are employed to empirically investigate the hidden and complex relationships between events and the stock market. Largescale experiments show that the accuracy of S&P 500 index prediction is 60%, and that of individual stock prediction can be over 70%. Our event based system outperforms bags of words based baselines, and previously reported systems trained on S&P 500 stock historical data.","url":"https:\/\/aclanthology.org\/D14-1148"},{"ID":"ding-feng-2020-learning","methods":["classify events","reinforced selection algorithm","event classifier"],"center_method":[null,null,null],"tasks":["physiological needs description","human needs categorization task","instantiation"],"center_task":[null,null,null],"Goal":["Sustainable Cities and Communities"],"text":"Learning to Classify Events from Human Needs Category Descriptions. We study the problem of learning an event classifier from human needs category descriptions, which is challenging due to: (1) the use of highly abstract concepts in natural language descriptions, (2) the difficulty of choosing key concepts. To tackle these two challenges, we propose LEAPI, a zero-shot learning method that first automatically generate weak labels by instantiating high-level concepts with prototypical instances and then trains a human needs classifier with the weakly labeled data. To filter noisy concepts, we design a reinforced selection algorithm to choose high-quality concepts for instantiation. Experimental results on the human needs categorization task show that our method outperforms baseline methods, producing substantially better precision. Physiological Needs Description: the need for a person to obtain food, to have meals \u2026 food\u00e0fruit, vegetable, meat, egg, fish,\u2026 \"I bought fruits\", \"I had eggs this morning\" Concept\u00e0Instances Labeled Events Leisure Needs Description: the need for a person to have leisure activities, to enjoy art \u2026 leisure activities\u00e0fishing, shopping, golf \"I went to fishing\", \"Dad went to play golf\" Concept\u00e0Instances Labeled Events","label":1,"title_clean":"Learning to Classify Events from Human Needs Category Descriptions","abstract_clean":"We study the problem of learning an event classifier from human needs category descriptions, which is challenging due to: (1) the use of highly abstract concepts in natural language descriptions, (2) the difficulty of choosing key concepts. To tackle these two challenges, we propose LEAPI, a zero shot learning method that first automatically generate weak labels by instantiating high level concepts with prototypical instances and then trains a human needs classifier with the weakly labeled data. To filter noisy concepts, we design a reinforced selection algorithm to choose high quality concepts for instantiation. Experimental results on the human needs categorization task show that our method outperforms baseline methods, producing substantially better precision. Physiological Needs Description: the need for a person to obtain food, to have meals \u2026 food\u00e0fruit, vegetable, meat, egg, fish,\u2026 \"I bought fruits\", \"I had eggs this morning\" Concept\u00e0Instances Labeled Events Leisure Needs Description: the need for a person to have leisure activities, to enjoy art \u2026 leisure activities\u00e0fishing, shopping, golf \"I went to fishing\", \"Dad went to play golf\" Concept\u00e0Instances Labeled Events","url":"https:\/\/aclanthology.org\/2020.findings-emnlp.421"},{"ID":"dinu-moldovan-2021-automatic","methods":["deep neural network","eating disorder classifier","depression classification","xlnet","roberta","bert"],"center_method":["deep neural network",null,"depression classification","xlnet","roberta","bert"],"tasks":["automatically detecting mental disorders","classification"],"center_task":[null,"classification"],"Goal":["Good Health and Well-Being"],"text":"Automatic Detection and Classification of Mental Illnesses from General Social Media Texts. Mental health is getting more and more attention recently, depression being a very common illness nowadays, but also other disorders like anxiety, obsessive-compulsive disorders, feeding disorders, autism, or attention-deficit\/hyperactivity disorders. The huge amount of data from social media and the recent advances of deep learning models provide valuable means to automatically detecting mental disorders from plain text. In this article, we experiment with state-of-the-art methods on the SMHD mental health conditions dataset from Reddit (Cohan et al., 2018). Our contribution is threefold: using a dataset consisting of more illnesses than most studies, focusing on general text rather than mental health support groups and classification by posts rather than individuals or groups. For the automatic classification of the diseases, we employ three deep learning models: BERT, RoBERTa and XLNET. We double the baseline established by Cohan et al. (2018), on just a sample of their dataset. We improve the results obtained by Jiang et al. (2020) on post-level classification. The accuracy obtained by the eating disorder classifier is the highest due to the pregnant presence of discussions related to calories, diets, recipes etc., whereas depression had the lowest F1 score, probably because depression is more difficult to identify in linguistic acts.","label":1,"title_clean":"Automatic Detection and Classification of Mental Illnesses from General Social Media Texts","abstract_clean":"Mental health is getting more and more attention recently, depression being a very common illness nowadays, but also other disorders like anxiety, obsessive compulsive disorders, feeding disorders, autism, or attention deficit\/hyperactivity disorders. The huge amount of data from social media and the recent advances of deep learning models provide valuable means to automatically detecting mental disorders from plain text. In this article, we experiment with state of the art methods on the SMHD mental health conditions dataset from Reddit (Cohan et al., 2018). Our contribution is threefold: using a dataset consisting of more illnesses than most studies, focusing on general text rather than mental health support groups and classification by posts rather than individuals or groups. For the automatic classification of the diseases, we employ three deep learning models: BERT, RoBERTa and XLNET. We double the baseline established by Cohan et al. (2018), on just a sample of their dataset. We improve the results obtained by Jiang et al. (2020) on post level classification. The accuracy obtained by the eating disorder classifier is the highest due to the pregnant presence of discussions related to calories, diets, recipes etc., whereas depression had the lowest F1 score, probably because depression is more difficult to identify in linguistic acts.","url":"https:\/\/aclanthology.org\/2021.ranlp-1.41"},{"ID":"dirkson-etal-2021-fuzzybio","methods":["fuzzybio","biohd","bio representation scheme"],"center_method":[null,null,null],"tasks":["named entity recognition","normalization","adverse drug response extraction","discontinuous entities","fuzzy representation of discontinuous entities"],"center_task":["named entity recognition","normalization",null,null,null],"Goal":["Good Health and Well-Being"],"text":"FuzzyBIO: A Proposal for Fuzzy Representation of Discontinuous Entities. Discontinuous entities pose a challenge to named entity recognition (NER). These phenomena occur commonly in the biomedical domain. As a solution, expansions of the BIO representation scheme that can handle these entity types are commonly used (i.e. BIOHD). However, the extra tag types make the NER task more difficult to learn. In this paper we propose an alternative; a fuzzy continuous BIO scheme (FuzzyBIO). We focus on the task of Adverse Drug Response extraction and normalization to compare FuzzyBIO to BIOHD. We find that FuzzyBIO improves recall of NER for two of three data sets and results in a higher percentage of correctly identified disjoint and composite entities for all data sets. Using FuzzyBIO also improves end-toend performance for continuous and composite entities in two of three data sets. Since FuzzyBIO improves performance for some data sets and the conversion from BIOHD to FuzzyBIO is straightforward, we recommend investigating which is more effective for any data set containing discontinuous entities.","label":1,"title_clean":"FuzzyBIO: A Proposal for Fuzzy Representation of Discontinuous Entities","abstract_clean":"Discontinuous entities pose a challenge to named entity recognition (NER). These phenomena occur commonly in the biomedical domain. As a solution, expansions of the BIO representation scheme that can handle these entity types are commonly used (i.e. BIOHD). However, the extra tag types make the NER task more difficult to learn. In this paper we propose an alternative; a fuzzy continuous BIO scheme (FuzzyBIO). We focus on the task of Adverse Drug Response extraction and normalization to compare FuzzyBIO to BIOHD. We find that FuzzyBIO improves recall of NER for two of three data sets and results in a higher percentage of correctly identified disjoint and composite entities for all data sets. Using FuzzyBIO also improves end toend performance for continuous and composite entities in two of three data sets. Since FuzzyBIO improves performance for some data sets and the conversion from BIOHD to FuzzyBIO is straightforward, we recommend investigating which is more effective for any data set containing discontinuous entities.","url":"https:\/\/aclanthology.org\/2021.louhi-1.9"},{"ID":"dong-etal-2021-discourse","methods":["supervised approaches","unsupervised baselines","level hierarchical graph representation","unsupervised graph based ranking model"],"center_method":[null,null,null,null],"tasks":["discourse aware unsupervised summarization","determining importance"],"center_task":[null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Discourse-Aware Unsupervised Summarization for Long Scientific Documents. We propose an unsupervised graph-based ranking model for extractive summarization of long scientific documents. Our method assumes a two-level hierarchical graph representation of the source document, and exploits asymmetrical positional cues to determine sentence importance. Results on the PubMed and arXiv datasets show that our approach 1 outperforms strong unsupervised baselines by wide margins in automatic metrics and human evaluation. In addition, it achieves performance comparable to many state-of-the-art supervised approaches which are trained on hundreds of thousands of examples. These results suggest that patterns in the discourse structure are a strong signal for determining importance in scientific articles. * Equal contribution. 1 Link to our code: https:\/\/github.com\/ mirandrom\/HipoRank. Introduction anxiety affects quality of life in those living with parkinson's disease (pd) more so than overall cognitive status, motor deficits, apathy, and depression.","label":1,"title_clean":"Discourse Aware Unsupervised Summarization for Long Scientific Documents","abstract_clean":"We propose an unsupervised graph based ranking model for extractive summarization of long scientific documents. Our method assumes a two level hierarchical graph representation of the source document, and exploits asymmetrical positional cues to determine sentence importance. Results on the PubMed and arXiv datasets show that our approach 1 outperforms strong unsupervised baselines by wide margins in automatic metrics and human evaluation. In addition, it achieves performance comparable to many state of the art supervised approaches which are trained on hundreds of thousands of examples. These results suggest that patterns in the discourse structure are a strong signal for determining importance in scientific articles. * Equal contribution. 1 Link to our code: https:\/\/github.com\/ mirandrom\/HipoRank. Introduction anxiety affects quality of life in those living with parkinson's disease (pd) more so than overall cognitive status, motor deficits, apathy, and depression.","url":"https:\/\/aclanthology.org\/2021.eacl-main.93"},{"ID":"dragoni-2018-neurosent","methods":["inferring polarity","neurosent system","neural network","word embeddings","supervised approach","vector representation"],"center_method":[null,null,"neural network","word embeddings",null,null],"tasks":["sentiment analysis tasks"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"text":"NEUROSENT-PDI at SemEval-2018 Task 1: Leveraging a Multi-Domain Sentiment Model for Inferring Polarity in Micro-blog Text. This paper describes the NeuroSent system that participated in SemEval 2018 Task 1. Our system takes a supervised approach that builds on neural networks and word embeddings. Word embeddings were built by starting from a repository of user generated reviews. Thus, they are specific for sentiment analysis tasks. Then, tweets are converted in the corresponding vector representation and given as input to the neural network with the aim of learning the different semantics contained in each emotion taken into account by the SemEval task. The output layer has been adapted based on the characteristics of each subtask. Preliminary results obtained on the provided training set are encouraging for pursuing the investigation into this direction.","label":1,"title_clean":"NEUROSENT PDI at SemEval 2018 Task 1: Leveraging a Multi Domain Sentiment Model for Inferring Polarity in Micro blog Text","abstract_clean":"This paper describes the NeuroSent system that participated in SemEval 2018 Task 1. Our system takes a supervised approach that builds on neural networks and word embeddings. Word embeddings were built by starting from a repository of user generated reviews. Thus, they are specific for sentiment analysis tasks. Then, tweets are converted in the corresponding vector representation and given as input to the neural network with the aim of learning the different semantics contained in each emotion taken into account by the SemEval task. The output layer has been adapted based on the characteristics of each subtask. Preliminary results obtained on the provided training set are encouraging for pursuing the investigation into this direction.","url":"https:\/\/aclanthology.org\/S18-1013"},{"ID":"dsouza-etal-2021-semeval","methods":["nlpcontri"],"center_method":[null],"tasks":["shared task","knowledge graph kg building"],"center_task":[null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"SemEval-2021 Task 11: NLPContributionGraph - Structuring Scholarly NLP Contributions for a Research Knowledge Graph. There is currently a gap between the natural language expression of scholarly publications and their structured semantic content modeling to enable intelligent content search. With the volume of research growing exponentially every year, a search feature operating over semantically structured content is compelling. The SemEval-2021 Shared Task NLPCONTRI-BUTIONGRAPH (a.k.a. 'the NCG task') tasks participants to develop automated systems that structure contributions from NLP scholarly articles in the English language. Being the firstof-its-kind in the SemEval series, the task released structured data from NLP scholarly articles at three levels of information granularity, i.e. at sentence-level, phrase-level, and phrases organized as triples toward Knowledge Graph (KG) building. The sentencelevel annotations comprised the few sentences about the article's contribution. The phraselevel annotations were scientific term and predicate phrases from the contribution sentences. Finally, the triples constituted the research overview KG. For the Shared Task, participating systems were then expected to automatically classify contribution sentences, extract scientific terms and relations from the sentences, and organize them as KG triples. Overall, the task drew a strong participation demographic of seven teams and 27 participants. The best end-to-end task system classified contribution sentences at 57.27% F1, phrases at 46.41% F1, and triples at 22.28% F1. While the absolute performance to generate triples remains low, in the conclusion of this article, the difficulty of producing such data and as a consequence of modeling it is highlighted.","label":1,"title_clean":"SemEval 2021 Task 11: NLPContributionGraph  Structuring Scholarly NLP Contributions for a Research Knowledge Graph","abstract_clean":"There is currently a gap between the natural language expression of scholarly publications and their structured semantic content modeling to enable intelligent content search. With the volume of research growing exponentially every year, a search feature operating over semantically structured content is compelling. The SemEval 2021 Shared Task NLPCONTRI BUTIONGRAPH (a.k.a. 'the NCG task') tasks participants to develop automated systems that structure contributions from NLP scholarly articles in the English language. Being the firstof its kind in the SemEval series, the task released structured data from NLP scholarly articles at three levels of information granularity, i.e. at sentence level, phrase level, and phrases organized as triples toward Knowledge Graph (KG) building. The sentencelevel annotations comprised the few sentences about the article's contribution. The phraselevel annotations were scientific term and predicate phrases from the contribution sentences. Finally, the triples constituted the research overview KG. For the Shared Task, participating systems were then expected to automatically classify contribution sentences, extract scientific terms and relations from the sentences, and organize them as KG triples. Overall, the task drew a strong participation demographic of seven teams and 27 participants. The best end to end task system classified contribution sentences at 57.27% F1, phrases at 46.41% F1, and triples at 22.28% F1. While the absolute performance to generate triples remains low, in the conclusion of this article, the difficulty of producing such data and as a consequence of modeling it is highlighted.","url":"https:\/\/aclanthology.org\/2021.semeval-1.44"},{"ID":"dsouza-ng-2014-ensemble","methods":["ensemble methods","distant supervision approaches"],"center_method":["ensemble methods",null],"tasks":["ensemble based medical relation classification","relation extraction"],"center_task":[null,"relation extraction"],"Goal":["Good Health and Well-Being"],"text":"Ensemble-Based Medical Relation Classification. Despite the successes of distant supervision approaches to relation extraction in the news domain, the lack of a comprehensive ontology of medical relations makes it difficult to apply such approaches to relation classification in the medical domain. In light of this difficulty, we propose an ensemble approach to this task where we exploit human-supplied knowledge to guide the design of members of the ensemble. Results on the 2010 i2b2\/VA Challenge corpus show that our ensemble approach yields a 19.8% relative error reduction over a state-of-the-art baseline.","label":1,"title_clean":"Ensemble Based Medical Relation Classification","abstract_clean":"Despite the successes of distant supervision approaches to relation extraction in the news domain, the lack of a comprehensive ontology of medical relations makes it difficult to apply such approaches to relation classification in the medical domain. In light of this difficulty, we propose an ensemble approach to this task where we exploit human supplied knowledge to guide the design of members of the ensemble. Results on the 2010 i2b2\/VA Challenge corpus show that our ensemble approach yields a 19.8% relative error reduction over a state of the art baseline.","url":"https:\/\/aclanthology.org\/C14-1159"},{"ID":"du-etal-2019-extracting","methods":["extracting symptoms","deep neural network","sequence to sequence model"],"center_method":[null,"deep neural network",null],"tasks":["medical providers"],"center_task":[null],"Goal":["Good Health and Well-Being"],"text":"Extracting Symptoms and their Status from Clinical Conversations. This paper describes novel models tailored for a new application, that of extracting the symptoms mentioned in clinical conversations along with their status. Lack of any publicly available corpus in this privacy-sensitive domain led us to develop our own corpus, consisting of about 3K conversations annotated by professional medical scribes. We propose two novel deep learning approaches to infer the symptom names and their status: (1) a new hierarchical span-attribute tagging (SA-T) model, trained using curriculum learning, and (2) a variant of sequence-to-sequence model which decodes the symptoms and their status from a few speaker turns within a sliding window over the conversation. This task stems from a realistic application of assisting medical providers in capturing symptoms mentioned by patients from their clinical conversations. To reflect this application, we define multiple metrics. From inter-rater agreement, we find that the task is inherently difficult. We conduct comprehensive evaluations on several contrasting conditions and observe that the performance of the models range from an F-score of 0.5 to 0.8 depending on the condition. Our analysis not only reveals the inherent challenges of the task, but also provides useful directions to improve the models.","label":1,"title_clean":"Extracting Symptoms and their Status from Clinical Conversations","abstract_clean":"This paper describes novel models tailored for a new application, that of extracting the symptoms mentioned in clinical conversations along with their status. Lack of any publicly available corpus in this privacy sensitive domain led us to develop our own corpus, consisting of about 3K conversations annotated by professional medical scribes. We propose two novel deep learning approaches to infer the symptom names and their status: (1) a new hierarchical span attribute tagging (SA T) model, trained using curriculum learning, and (2) a variant of sequence to sequence model which decodes the symptoms and their status from a few speaker turns within a sliding window over the conversation. This task stems from a realistic application of assisting medical providers in capturing symptoms mentioned by patients from their clinical conversations. To reflect this application, we define multiple metrics. From inter rater agreement, we find that the task is inherently difficult. We conduct comprehensive evaluations on several contrasting conditions and observe that the performance of the models range from an F score of 0.5 to 0.8 depending on the condition. Our analysis not only reveals the inherent challenges of the task, but also provides useful directions to improve the models.","url":"https:\/\/aclanthology.org\/P19-1087.pdf"},{"ID":"duan-etal-2012-twitter","methods":["unified mutual reinforcement graph","time line based framework"],"center_method":[null,null],"tasks":["twitter topic summarization","ranking tweets","ranking sentences"],"center_task":[null,null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Twitter Topic Summarization by Ranking Tweets using Social Influence and Content Quality. In this paper, we propose a time-line based framework for topic summarization in Twitter. We summarize topics by sub-topics along time line to fully capture rapid topic evolution in Twitter. Specifically, we rank and select salient and diversified tweets as a summary of each sub-topic. We have observed that ranking tweets is significantly different from ranking sentences in traditional extractive document summarization. We model and formulate the tweet ranking in a unified mutual reinforcement graph, where the social influence of users and the content quality of tweets are taken into consideration simultaneously in a mutually reinforcing manner. Extensive experiments are conducted on 3.9 million tweets. The results show that the proposed approach outperforms previous approaches by 14% improvement on average ROUGE-1. Moreover, we show how the content quality of tweets and the social influence of users effectively improve the performance of measuring the salience of tweets.","label":1,"title_clean":"Twitter Topic Summarization by Ranking Tweets using Social Influence and Content Quality","abstract_clean":"In this paper, we propose a time line based framework for topic summarization in Twitter. We summarize topics by sub topics along time line to fully capture rapid topic evolution in Twitter. Specifically, we rank and select salient and diversified tweets as a summary of each sub topic. We have observed that ranking tweets is significantly different from ranking sentences in traditional extractive document summarization. We model and formulate the tweet ranking in a unified mutual reinforcement graph, where the social influence of users and the content quality of tweets are taken into consideration simultaneously in a mutually reinforcing manner. Extensive experiments are conducted on 3.9 million tweets. The results show that the proposed approach outperforms previous approaches by 14% improvement on average ROUGE 1. Moreover, we show how the content quality of tweets and the social influence of users effectively improve the performance of measuring the salience of tweets.","url":"https:\/\/aclanthology.org\/C12-1047"},{"ID":"dugan-etal-2022-feasibility","methods":["answer - agnostic question generation models"],"center_method":[null],"tasks":["answer unaware question generation","text summarization"],"center_task":[null,"text summarization"],"Goal":["Quality Education"],"text":"A Feasibility Study of Answer-Unaware Question Generation for Education. We conduct a feasibility study into the applicability of answer-agnostic question generation models to textbook passages. We show that a significant portion of errors in such systems arise from asking irrelevant or uninterpretable questions and that such errors can be ameliorated by providing summarized input. We find that giving these models human-written summaries instead of the original text results in a significant increase in acceptability of generated questions (33% \u2192 83%) as determined by expert annotators. We also find that, in the absence of human-written summaries, automatic summarization can serve as a good middle ground.","label":1,"title_clean":"A Feasibility Study of Answer Unaware Question Generation for Education","abstract_clean":"We conduct a feasibility study into the applicability of answer agnostic question generation models to textbook passages. We show that a significant portion of errors in such systems arise from asking irrelevant or uninterpretable questions and that such errors can be ameliorated by providing summarized input. We find that giving these models human written summaries instead of the original text results in a significant increase in acceptability of generated questions (33% \u2192 83%) as determined by expert annotators. We also find that, in the absence of human written summaries, automatic summarization can serve as a good middle ground.","url":"https:\/\/aclanthology.org\/2022.findings-acl.151"},{"ID":"economou-etal-2000-lexiploigissi","methods":["educational platform","lexiploigissi "],"center_method":[null,null],"tasks":["language and speech processing","exploratory learning environments","upper secondary education","school curriculum","pilot program","reading and writing abilities","teaching of terminology","educational purposes"],"center_task":[null,null,null,null,null,null,null,null],"Goal":["Quality Education"],"text":"LEXIPLOIGISSI: An Educational Platform for the Teaching of Terminology in Greece. This paper introduces a project, LEXIPLOIGISSI * , which involves use of language resources for educational purposes. More particularly, the aim of the project is to develop written corpora, electronic dictionaries and exercises to enhance students' reading and writing abilities in six different school subjects. It is the product of a small-scale pilot program that will be part of the school curriculum in the three grades of Upper Secondary Education in Greece. The application seeks to create exploratory learning environments in which digital sound, image, text and video are fully integrated through the educational platform and placed under the direct control of users who are able to follow individual pathways through data stores. * The Institute for Language and Speech Processing has undertaken this project as the leading contractor and Kastaniotis Publications as a subcontractor. The first partner was responsible for the design, development and implementation of the educational platform, as well as for the provision of pedagogic scenarios of use; the second partner provided the resources (texts and multimedia material). The starting date of the project was June 1999, the development of the software and the collection of material lasted nine months.","label":1,"title_clean":"LEXIPLOIGISSI: An Educational Platform for the Teaching of Terminology in Greece","abstract_clean":"This paper introduces a project, LEXIPLOIGISSI * , which involves use of language resources for educational purposes. More particularly, the aim of the project is to develop written corpora, electronic dictionaries and exercises to enhance students' reading and writing abilities in six different school subjects. It is the product of a small scale pilot program that will be part of the school curriculum in the three grades of Upper Secondary Education in Greece. The application seeks to create exploratory learning environments in which digital sound, image, text and video are fully integrated through the educational platform and placed under the direct control of users who are able to follow individual pathways through data stores. * The Institute for Language and Speech Processing has undertaken this project as the leading contractor and Kastaniotis Publications as a subcontractor. The first partner was responsible for the design, development and implementation of the educational platform, as well as for the provision of pedagogic scenarios of use; the second partner provided the resources (texts and multimedia material). The starting date of the project was June 1999, the development of the software and the collection of material lasted nine months.","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2000\/pdf\/271.pdf"},{"ID":"el-haj-etal-2018-profiling","methods":["retrieval methods","lexicon","gene ontology semantic tagger"],"center_method":[null,null,null],"tasks":["academic publishing","corpus linguistics"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"Profiling Medical Journal Articles Using a Gene Ontology Semantic Tagger. In many areas of academic publishing, there is an explosion of literature, and subdivision of fields into subfields, leading to stove-piping where sub-communities of expertise become disconnected from each other. This is especially true in the genetics literature over the last 10 years where researchers are no longer able to maintain knowledge of previously related areas. This paper extends several approaches based on natural language processing and corpus linguistics which allow us to examine corpora derived from bodies of genetics literature and will help to make comparisons and improve retrieval methods using domain knowledge via an existing gene ontology. We derived two open access medical journal corpora from PubMed related to psychiatric genetics and immune disorder genetics. We created a novel Gene Ontology Semantic Tagger (GOST) and lexicon to annotate the corpora and are then able to compare subsets of literature to understand the relative distributions of genetic terminology, thereby enabling researchers to make improved connections between them.","label":1,"title_clean":"Profiling Medical Journal Articles Using a Gene Ontology Semantic Tagger","abstract_clean":"In many areas of academic publishing, there is an explosion of literature, and subdivision of fields into subfields, leading to stove piping where sub communities of expertise become disconnected from each other. This is especially true in the genetics literature over the last 10 years where researchers are no longer able to maintain knowledge of previously related areas. This paper extends several approaches based on natural language processing and corpus linguistics which allow us to examine corpora derived from bodies of genetics literature and will help to make comparisons and improve retrieval methods using domain knowledge via an existing gene ontology. We derived two open access medical journal corpora from PubMed related to psychiatric genetics and immune disorder genetics. We created a novel Gene Ontology Semantic Tagger (GOST) and lexicon to annotate the corpora and are then able to compare subsets of literature to understand the relative distributions of genetic terminology, thereby enabling researchers to make improved connections between them.","url":"https:\/\/aclanthology.org\/L18-1726"},{"ID":"elkaref-hassan-2021-joint","methods":["bert","joint training approach"],"center_method":["bert",null],"tasks":["normalization","classification","extraction","social media mining for health","smm4h 2021","normalization of adverse drug effect","adverse effect extraction"],"center_task":["normalization","classification",null,null,null,null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"A Joint Training Approach to Tweet Classification and Adverse Effect Extraction and Normalization for SMM4H 2021. In this work we describe our submissions to the Social Media Mining for Health (SMM4H) 2021 Shared Task (Magge et al., 2021). We investigated the effectiveness of a joint training approach to Task 1, specifically classification, extraction and normalization of Adverse Drug Effect (ADE) mentions in English tweets. Our approach performed well on the normalization task, achieving an above average f1 score of 24%, but less so on classification and extraction, with f1 scores of 22% and 37% respectively. Our experiments also showed that a larger dataset with more negative results led to stronger results than a smaller more balanced dataset, even when both datasets have the same positive examples. Finally we also submitted a tuned BERT model for Task 6: Classification of Covid-19 tweets containing symptoms, which achieved an above average f1 score of 96%.","label":1,"title_clean":"A Joint Training Approach to Tweet Classification and Adverse Effect Extraction and Normalization for SMM4H 2021","abstract_clean":"In this work we describe our submissions to the Social Media Mining for Health (SMM4H) 2021 Shared Task (Magge et al., 2021). We investigated the effectiveness of a joint training approach to Task 1, specifically classification, extraction and normalization of Adverse Drug Effect (ADE) mentions in English tweets. Our approach performed well on the normalization task, achieving an above average f1 score of 24%, but less so on classification and extraction, with f1 scores of 22% and 37% respectively. Our experiments also showed that a larger dataset with more negative results led to stronger results than a smaller more balanced dataset, even when both datasets have the same positive examples. Finally we also submitted a tuned BERT model for Task 6: Classification of Covid 19 tweets containing symptoms, which achieved an above average f1 score of 96%.","url":"https:\/\/aclanthology.org\/2021.smm4h-1.16"},{"ID":"erjavec-etal-2004-making","methods":["mixture of encodings","xml based japanese slovene learners dictionary"],"center_method":[null,null],"tasks":["inflected parts of speech","full text search","reading tutor program"],"center_task":[null,null,null],"Goal":["Quality Education"],"text":"Making an XML-based Japanese-Slovene Learners' Dictionary. In this paper we present a hypertext dictionary of Japanese lexical units for Slovene students of Japanese at the Faculty of Arts of Ljubljana University. The dictionary is planned as a long-term project in which a simple dictionary is to be gradually enlarged and enhanced, taking into account the needs of the students. Initially, the dictionary was encoded in a tabular format, in a mixture of encodings, and subsequently rendered in HTML. The paper first discusses the conversion of the dictionary into XML, into an encoding that complies with the Text Encoding Initiative (TEI) Guidelines. The conversion into such an encoding validates, enriches, explicates and standardises the structure of the dictionary, thus making it more usable for further development and linguistically oriented research. We also present the current Web implementation of the dictionary, which offers full text search and a tool for practising inflected parts of speech. The paper gives an overview of related research, i.e. other XML oriented Web dictionaries of Slovene and East Asian languages and presents planned developments, i.e. the inclusion of the dictionary into the Reading Tutor program.","label":1,"title_clean":"Making an XML based Japanese Slovene Learners' Dictionary","abstract_clean":"In this paper we present a hypertext dictionary of Japanese lexical units for Slovene students of Japanese at the Faculty of Arts of Ljubljana University. The dictionary is planned as a long term project in which a simple dictionary is to be gradually enlarged and enhanced, taking into account the needs of the students. Initially, the dictionary was encoded in a tabular format, in a mixture of encodings, and subsequently rendered in HTML. The paper first discusses the conversion of the dictionary into XML, into an encoding that complies with the Text Encoding Initiative (TEI) Guidelines. The conversion into such an encoding validates, enriches, explicates and standardises the structure of the dictionary, thus making it more usable for further development and linguistically oriented research. We also present the current Web implementation of the dictionary, which offers full text search and a tool for practising inflected parts of speech. The paper gives an overview of related research, i.e. other XML oriented Web dictionaries of Slovene and East Asian languages and presents planned developments, i.e. the inclusion of the dictionary into the Reading Tutor program.","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2004\/pdf\/107.pdf"},{"ID":"escoter-etal-2017-grouping","methods":["grouping algorithm","salience"],"center_method":[null,null],"tasks":["news aggregation systems"],"center_task":[null],"Goal":["Decent Work and Economic Growth","Industry, Innovation and Infrastrucure"],"text":"Grouping business news stories based on salience of named entities. In news aggregation systems focused on broad news domains, certain stories may appear in multiple articles. Depending on the relative importance of the story, the number of versions can reach dozens or hundreds within a day. The text in these versions may be nearly identical or quite different. Linking multiple versions of a story into a single group brings several important benefits to the end-user-reducing the cognitive load on the reader, as well as signaling the relative importance of the story. We present a grouping algorithm, and explore several vector-based representations of input documents: from a baseline using keywords, to a method using salience-a measure of importance of named entities in the text. We demonstrate that features beyond keywords yield substantial improvements, verified on a manually-annotated corpus of business news stories.","label":1,"title_clean":"Grouping business news stories based on salience of named entities","abstract_clean":"In news aggregation systems focused on broad news domains, certain stories may appear in multiple articles. Depending on the relative importance of the story, the number of versions can reach dozens or hundreds within a day. The text in these versions may be nearly identical or quite different. Linking multiple versions of a story into a single group brings several important benefits to the end user reducing the cognitive load on the reader, as well as signaling the relative importance of the story. We present a grouping algorithm, and explore several vector based representations of input documents: from a baseline using keywords, to a method using salience a measure of importance of named entities in the text. We demonstrate that features beyond keywords yield substantial improvements, verified on a manually annotated corpus of business news stories.","url":"https:\/\/aclanthology.org\/E17-1103"},{"ID":"falis-etal-2019-ontological","methods":["ontological attention ensembles","multilevel attention","label dependent ensembles","semantically interpretable system"],"center_method":[null,null,null,null],"tasks":["icd code prediction","memory usage","learning","icd","capturing semantic concepts"],"center_task":[null,null,"learning",null,null],"Goal":["Good Health and Well-Being"],"text":"Ontological attention ensembles for capturing semantic concepts in ICD code prediction from clinical text. We present a semantically interpretable system for automated ICD coding of clinical text documents. Our contribution is an ontological attention mechanism which matches the structure of the ICD ontology, in which shared attention vectors are learned at each level of the hierarchy, and combined into label-dependent ensembles. Analysis of the attention heads shows that shared concepts are learned by the lowest common denominator node. This allows child nodes to focus on the differentiating concepts, leading to efficient learning and memory usage. Visualisation of the multilevel attention on the original text allows explanation of the code predictions according to the semantics of the ICD ontology. On the MIMIC-III dataset we achieve a 2.7% absolute (11% relative) improvement from 0.218 to 0.245 macro-F1 score compared to the previous state of the art across 3,912 codes. Finally, we analyse the labelling inconsistencies arising from different coding practices which limit performance on this task.","label":1,"title_clean":"Ontological attention ensembles for capturing semantic concepts in ICD code prediction from clinical text","abstract_clean":"We present a semantically interpretable system for automated ICD coding of clinical text documents. Our contribution is an ontological attention mechanism which matches the structure of the ICD ontology, in which shared attention vectors are learned at each level of the hierarchy, and combined into label dependent ensembles. Analysis of the attention heads shows that shared concepts are learned by the lowest common denominator node. This allows child nodes to focus on the differentiating concepts, leading to efficient learning and memory usage. Visualisation of the multilevel attention on the original text allows explanation of the code predictions according to the semantics of the ICD ontology. On the MIMIC III dataset we achieve a 2.7% absolute (11% relative) improvement from 0.218 to 0.245 macro F1 score compared to the previous state of the art across 3,912 codes. Finally, we analyse the labelling inconsistencies arising from different coding practices which limit performance on this task.","url":"https:\/\/aclanthology.org\/D19-6220"},{"ID":"feng-etal-2021-alpha","methods":["multimodal transformers","transformer based","ernie vil","focal","pretrained object detector","cross entropy loss","steam crossattended transformers model"],"center_method":[null,null,null,null,null,null,null],"tasks":["multimodal propaganda technique classification"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Alpha at SemEval-2021 Task 6: Transformer Based Propaganda Classification. This paper describes our system participated in Task 6 of SemEval-2021: this task focuses on multimodal propaganda technique classification and it aims to classify given image and text into 22 classes. In this paper, we propose to use transformer-based (Vaswani et al., 2017) architecture to fuse the clues from both image and text. We explore two branches of techniques including fine-tuning the text pre-trained transformer with extended visual features and fine-tuning the multimodal pre-trained transformers. For the visual features, we experiment with both grid features extracted from ResNet(He et al., 2016) network and salient region features from a pretrained object detector. Among the pre-trained multimodal transformers, we choose ERNIE-ViL (Yu et al., 2020), a two-steam crossattended transformers model pre-trained on large-scale image-caption aligned data. Finetuning ERNIE-ViL for our task produces a better performance due to general joint multimodal representation for text and image learned by ERNIE-ViL. Besides, as the distribution of the classification labels is extremely unbalanced, we also make a further attempt on the loss function and the experiment results show that focal loss would perform better than cross-entropy loss. Lastly, we ranked first place at sub-task C in the final competition. * indicates equal contribution.","label":1,"title_clean":"Alpha at SemEval 2021 Task 6: Transformer Based Propaganda Classification","abstract_clean":"This paper describes our system participated in Task 6 of SemEval 2021: this task focuses on multimodal propaganda technique classification and it aims to classify given image and text into 22 classes. In this paper, we propose to use transformer based (Vaswani et al., 2017) architecture to fuse the clues from both image and text. We explore two branches of techniques including fine tuning the text pre trained transformer with extended visual features and fine tuning the multimodal pre trained transformers. For the visual features, we experiment with both grid features extracted from ResNet(He et al., 2016) network and salient region features from a pretrained object detector. Among the pre trained multimodal transformers, we choose ERNIE ViL (Yu et al., 2020), a two steam crossattended transformers model pre trained on large scale image caption aligned data. Finetuning ERNIE ViL for our task produces a better performance due to general joint multimodal representation for text and image learned by ERNIE ViL. Besides, as the distribution of the classification labels is extremely unbalanced, we also make a further attempt on the loss function and the experiment results show that focal loss would perform better than cross entropy loss. Lastly, we ranked first place at sub task C in the final competition. * indicates equal contribution.","url":"https:\/\/aclanthology.org\/2021.semeval-1.8"},{"ID":"fierro-etal-2017-200k","methods":["three part argumentation model"],"center_method":[null],"tasks":["classification","tagging arguments"],"center_task":["classification",null],"Goal":["Peace, Justice and Strong Institutions"],"text":"200K+ Crowdsourced Political Arguments for a New Chilean Constitution. In this paper we present the dataset of 200,000+ political arguments produced in the local phase of the 2016 Chilean constitutional process. We describe the human processing of this data by the government officials, and the manual tagging of arguments performed by members of our research group. Afterwards we focus on classification tasks that mimic the human processes, comparing linear methods with neural network architectures. The experiments show that some of the manual tasks are suitable for automatization. In particular, the best methods achieve a 90% top-5 accuracy in a multiclass classification of arguments, and 65% macro-averaged F1-score for tagging arguments according to a three-part argumentation model.","label":1,"title_clean":"200K+ Crowdsourced Political Arguments for a New Chilean Constitution","abstract_clean":"In this paper we present the dataset of 200,000+ political arguments produced in the local phase of the 2016 Chilean constitutional process. We describe the human processing of this data by the government officials, and the manual tagging of arguments performed by members of our research group. Afterwards we focus on classification tasks that mimic the human processes, comparing linear methods with neural network architectures. The experiments show that some of the manual tasks are suitable for automatization. In particular, the best methods achieve a 90% top 5 accuracy in a multiclass classification of arguments, and 65% macro averaged F1 score for tagging arguments according to a three part argumentation model.","url":"https:\/\/aclanthology.org\/W17-5101"},{"ID":"finegan-dollak-verma-2020-layout","methods":["layout aware transformer","layoutlm","regular bert","bert","lay outlm","layout aware text representations"],"center_method":[null,null,null,"bert",null,null],"tasks":["clustering documents","document type classification","organizing large collections of document scans"],"center_task":[null,null,null],"Goal":["Good Health and Well-Being"],"text":"Layout-Aware Text Representations Harm Clustering Documents by Type. Clustering documents by type-grouping invoices with invoices and articles with articles-is a desirable first step for organizing large collections of document scans. Humans approaching this task use both the semantics of the text and the document layout to assist in grouping like documents. Lay-outLM (Xu et al., 2019), a layout-aware transformer built on top of BERT with state-of-theart performance on document-type classification, could reasonably be expected to outperform regular BERT (Devlin et al., 2018) for document-type clustering. However, we find experimentally that BERT significantly outperforms LayoutLM on this task (p < 0.001). We analyze clusters to show where layout awareness is an asset and where it is a liability.","label":1,"title_clean":"Layout Aware Text Representations Harm Clustering Documents by Type","abstract_clean":"Clustering documents by type grouping invoices with invoices and articles with articles is a desirable first step for organizing large collections of document scans. Humans approaching this task use both the semantics of the text and the document layout to assist in grouping like documents. Lay outLM (Xu et al., 2019), a layout aware transformer built on top of BERT with state of theart performance on document type classification, could reasonably be expected to outperform regular BERT (Devlin et al., 2018) for document type clustering. However, we find experimentally that BERT significantly outperforms LayoutLM on this task (p < 0.001). We analyze clusters to show where layout awareness is an asset and where it is a liability.","url":"https:\/\/aclanthology.org\/2020.insights-1.9"},{"ID":"finzel-etal-2021-conversational","methods":["conversational agent","cadlac","web frameworks","gpt","rule based natural language generation nlg module","bidirectional long short term memory topic tracker","mind meld platform","open domain conversational sub agent","multi modal conversational agent system","dialogue manager"],"center_method":[null,null,null,"gpt",null,null,null,null,null,null],"tasks":["daily living assessment coaching demo","conversation turns","mobility","conversational ai","professional assessors"],"center_task":[null,null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"Conversational Agent for Daily Living Assessment Coaching Demo. Conversational Agent for Daily Living Assessment Coaching (CADLAC) is a multi-modal conversational agent system designed to impersonate \"individuals\" with various levels of ability in activities of daily living (ADLs: e.g., dressing, bathing, mobility, etc.) for use in training professional assessors how to conduct interviews to determine one's level of functioning. The system is implemented on the Mind-Meld platform for conversational AI and features a Bidirectional Long Short-Term Memory topic tracker that allows the agent to navigate conversations spanning 18 different ADL domains, a dialogue manager that interfaces with a database of over 10,000 historical ADL assessments, a rule-based Natural Language Generation (NLG) module, and a pre-trained open-domain conversational sub-agent (based on GPT-2) for handling conversation turns outside of the 18 ADL domains. CADLAC is delivered via state-of-the-art web frameworks to handle multiple conversations and users simultaneously and is enabled with voice interface. The paper includes a description of the system design and evaluation of individual components followed by a brief discussion of current limitations and next steps.","label":1,"title_clean":"Conversational Agent for Daily Living Assessment Coaching Demo","abstract_clean":"Conversational Agent for Daily Living Assessment Coaching (CADLAC) is a multi modal conversational agent system designed to impersonate \"individuals\" with various levels of ability in activities of daily living (ADLs: e.g., dressing, bathing, mobility, etc.) for use in training professional assessors how to conduct interviews to determine one's level of functioning. The system is implemented on the Mind Meld platform for conversational AI and features a Bidirectional Long Short Term Memory topic tracker that allows the agent to navigate conversations spanning 18 different ADL domains, a dialogue manager that interfaces with a database of over 10,000 historical ADL assessments, a rule based Natural Language Generation (NLG) module, and a pre trained open domain conversational sub agent (based on GPT 2) for handling conversation turns outside of the 18 ADL domains. CADLAC is delivered via state of the art web frameworks to handle multiple conversations and users simultaneously and is enabled with voice interface. The paper includes a description of the system design and evaluation of individual components followed by a brief discussion of current limitations and next steps.","url":"https:\/\/aclanthology.org\/2021.eacl-demos.38"},{"ID":"flekova-etal-2016-exploring","methods":["nlp applications"],"center_method":["nlp applications"],"tasks":["social media","regression task"],"center_task":[null,null],"Goal":["Decent Work and Economic Growth","Reduced Inequalities"],"text":"Exploring Stylistic Variation with Age and Income on Twitter. Writing style allows NLP tools to adjust to the traits of an author. In this paper, we explore the relation between stylistic and syntactic features and authors' age and income. We confirm our hypothesis that for numerous feature types writing style is predictive of income even beyond age. We analyze the predictive power of writing style features in a regression task on two data sets of around 5,000 Twitter users each. Additionally, we use our validated features to study daily variations in writing style of users from distinct income groups. Temporal stylistic patterns not only provide novel psychological insight into user behavior, but are useful for future research and applications in social media.","label":1,"title_clean":"Exploring Stylistic Variation with Age and Income on Twitter","abstract_clean":"Writing style allows NLP tools to adjust to the traits of an author. In this paper, we explore the relation between stylistic and syntactic features and authors' age and income. We confirm our hypothesis that for numerous feature types writing style is predictive of income even beyond age. We analyze the predictive power of writing style features in a regression task on two data sets of around 5,000 Twitter users each. Additionally, we use our validated features to study daily variations in writing style of users from distinct income groups. Temporal stylistic patterns not only provide novel psychological insight into user behavior, but are useful for future research and applications in social media.","url":"https:\/\/aclanthology.org\/P16-2051.pdf"},{"ID":"fortuna-etal-2020-toxic","methods":["automatic detection of hate speech"],"center_method":[null],"tasks":["annotation"],"center_task":["annotation"],"Goal":["Peace, Justice and Strong Institutions"],"text":"Toxic, Hateful, Offensive or Abusive? What Are We Really Classifying? An Empirical Analysis of Hate Speech Datasets. The field of the automatic detection of hate speech and related concepts has raised a lot of interest in the last years. Different datasets were annotated and classified by means of applying different machine learning algorithms. However, few efforts were done in order to clarify the applied categories and homogenize different datasets. Our study takes up this demand. We analyze six different publicly available datasets in this field with respect to their similarity and compatibility. We conduct two different experiments. First, we try to make the datasets compatible and represent the dataset classes as Fast Text word vectors analyzing the similarity between different classes in a intra and inter dataset manner. Second, we submit the chosen datasets to the Perspective API Toxicity classifier, achieving different performances depending on the categories and datasets. One of the main conclusions of these experiments is that many different definitions are being used for equivalent concepts, which makes most of the publicly available datasets incompatible. Grounded in our analysis, we provide guidelines for future dataset collection and annotation.","label":1,"title_clean":"Toxic, Hateful, Offensive or Abusive? What Are We Really Classifying? An Empirical Analysis of Hate Speech Datasets","abstract_clean":"The field of the automatic detection of hate speech and related concepts has raised a lot of interest in the last years. Different datasets were annotated and classified by means of applying different machine learning algorithms. However, few efforts were done in order to clarify the applied categories and homogenize different datasets. Our study takes up this demand. We analyze six different publicly available datasets in this field with respect to their similarity and compatibility. We conduct two different experiments. First, we try to make the datasets compatible and represent the dataset classes as Fast Text word vectors analyzing the similarity between different classes in a intra and inter dataset manner. Second, we submit the chosen datasets to the Perspective API Toxicity classifier, achieving different performances depending on the categories and datasets. One of the main conclusions of these experiments is that many different definitions are being used for equivalent concepts, which makes most of the publicly available datasets incompatible. Grounded in our analysis, we provide guidelines for future dataset collection and annotation.","url":"https:\/\/aclanthology.org\/2020.lrec-1.838.pdf"},{"ID":"fourtassi-etal-2019-development","methods":["evolving network","lexical networks"],"center_method":[null,null],"tasks":["abstract concepts"],"center_task":[null],"Goal":["Quality Education"],"text":"The Development of Abstract Concepts in Children's Early Lexical Networks. How do children learn abstract concepts such as animal vs. artifact? Previous research has suggested that such concepts can partly be derived using cues from the language children hear around them. Following this suggestion, we propose a model where we represent the children's developing lexicon as an evolving network. The nodes of this network are based on vocabulary knowledge as reported by parents, and the edges between pairs of nodes are based on the probability of their co-occurrence in a corpus of child-directed speech. We found that several abstract categories can be identified as the dense regions in such networks. In addition, our simulations suggest that these categories develop simultaneously, rather than sequentially, thanks to the children's word learning trajectory which favors the exploration of the global conceptual space.","label":1,"title_clean":"The Development of Abstract Concepts in Children's Early Lexical Networks","abstract_clean":"How do children learn abstract concepts such as animal vs. artifact? Previous research has suggested that such concepts can partly be derived using cues from the language children hear around them. Following this suggestion, we propose a model where we represent the children's developing lexicon as an evolving network. The nodes of this network are based on vocabulary knowledge as reported by parents, and the edges between pairs of nodes are based on the probability of their co occurrence in a corpus of child directed speech. We found that several abstract categories can be identified as the dense regions in such networks. In addition, our simulations suggest that these categories develop simultaneously, rather than sequentially, thanks to the children's word learning trajectory which favors the exploration of the global conceptual space.","url":"https:\/\/aclanthology.org\/W19-2914"},{"ID":"freibott-1992-computer","methods":["organisational and technical solutions"],"center_method":[null],"tasks":["document production process","computer aided translation","information technology","intemationalisation of markets","machine translation"],"center_task":[null,null,null,null,"machine translation"],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Computer aided translation in an integrated document production process: Tools and applications. The intemationalisation of markets, the ever shortening life cycles of products as well as the increasing importance of information technology all demand a change in technical equipment, the software used on it and the organisational structures and processes in our working environment. Translation as a whole, but in particular as an integral part of the document production process, has to cope with these changes and with new and additional requirements. This paper describes the organisational and technical solutions developed and implemented in an industrial company for a number of computer aided translation applications integrated in the document production process to meet these requirements and to ensure high-quality mono and multilingual documentation on restricted budgetary grounds.","label":1,"title_clean":"Computer aided translation in an integrated document production process: Tools and applications","abstract_clean":"The intemationalisation of markets, the ever shortening life cycles of products as well as the increasing importance of information technology all demand a change in technical equipment, the software used on it and the organisational structures and processes in our working environment. Translation as a whole, but in particular as an integral part of the document production process, has to cope with these changes and with new and additional requirements. This paper describes the organisational and technical solutions developed and implemented in an industrial company for a number of computer aided translation applications integrated in the document production process to meet these requirements and to ensure high quality mono and multilingual documentation on restricted budgetary grounds.","url":"https:\/\/aclanthology.org\/1992.tc-1.5"},{"ID":"friedberg-2011-turn","methods":["spoken dialogue systems"],"center_method":[null],"tasks":["turn taking cues"],"center_task":[null],"Goal":["Quality Education"],"text":"Turn-Taking Cues in a Human Tutoring Corpus. Most spoken dialogue systems are still lacking in their ability to accurately model the complex process that is human turntaking. This research analyzes a humanhuman tutoring corpus in order to identify prosodic turn-taking cues, with the hopes that they can be used by intelligent tutoring systems to predict student turn boundaries. Results show that while there was variation between subjects, three features were significant turn-yielding cues overall. In addition, a positive relationship between the number of cues present and the probability of a turn yield was demonstrated.","label":1,"title_clean":"Turn Taking Cues in a Human Tutoring Corpus","abstract_clean":"Most spoken dialogue systems are still lacking in their ability to accurately model the complex process that is human turntaking. This research analyzes a humanhuman tutoring corpus in order to identify prosodic turn taking cues, with the hopes that they can be used by intelligent tutoring systems to predict student turn boundaries. Results show that while there was variation between subjects, three features were significant turn yielding cues overall. In addition, a positive relationship between the number of cues present and the probability of a turn yield was demonstrated.","url":"https:\/\/aclanthology.org\/P11-3017"},{"ID":"fudholi-suominen-2018-importance","methods":["guided learning","recommender system","feedback system","artificial intelligence system"],"center_method":[null,null,null,null],"tasks":["pronunciation","language learners","automatic speech recognition","accent acquisition","speech replay","recommendations","verbal communication","reinforcement learning","speech analytics","information retrieval"],"center_task":[null,null,"automatic speech recognition",null,null,null,null,null,null,"information retrieval"],"Goal":["Quality Education"],"text":"The Importance of Recommender and Feedback Features in a Pronunciation Learning Aid. Verbal communication-and pronunciation as its part-is a core skill that can be developed through guided learning. An artificial intelligence system can take a role in these guided learning approaches as an enabler of an application for pronunciation learning with a recommender system to guide language learners through exercises and feedback system to correct their pronunciation. In this paper, we report on a user study on language learners' perceived usefulness of the application. 16 international students who spoke non-native English and lived in Australia participated. 13 of them said they need to improve their pronunciation skills in English because of their foreign accent. The feedback system with features for pronunciation scoring, speech replay, and giving a pronunciation example was deemed essential by most of the respondents. In contrast, a clear dichotomy between the recommender system perceived as useful or useless existed; the system had features to prompt new common words or old poorly-scored words. These results can be used to target research and development from information retrieval and reinforcement learning for better and better recommendations to speech recognition and speech analytics for accent acquisition.","label":1,"title_clean":"The Importance of Recommender and Feedback Features in a Pronunciation Learning Aid","abstract_clean":"Verbal communication and pronunciation as its part is a core skill that can be developed through guided learning. An artificial intelligence system can take a role in these guided learning approaches as an enabler of an application for pronunciation learning with a recommender system to guide language learners through exercises and feedback system to correct their pronunciation. In this paper, we report on a user study on language learners' perceived usefulness of the application. 16 international students who spoke non native English and lived in Australia participated. 13 of them said they need to improve their pronunciation skills in English because of their foreign accent. The feedback system with features for pronunciation scoring, speech replay, and giving a pronunciation example was deemed essential by most of the respondents. In contrast, a clear dichotomy between the recommender system perceived as useful or useless existed; the system had features to prompt new common words or old poorly scored words. These results can be used to target research and development from information retrieval and reinforcement learning for better and better recommendations to speech recognition and speech analytics for accent acquisition.","url":"https:\/\/aclanthology.org\/W18-3711"},{"ID":"gamoran-etal-2021-using","methods":["theory guided approach","bayesian modeling","psychologically informed priors"],"center_method":[null,null,null],"tasks":["clpsych","suicide prediction","prediction tasks","over fitting"],"center_task":["clpsych",null,null,null],"Goal":["Good Health and Well-Being"],"text":"Using Psychologically-Informed Priors for Suicide Prediction in the CLPsych 2021 Shared Task. This paper describes our approach to the CLPsych 2021 Shared Task, in which we aimed to predict suicide attempts based on Twitter feed data. We addressed this challenge by emphasizing reliance on prior domain knowledge. We engineered novel theorydriven features, and integrated prior knowledge with empirical evidence in a principled manner using Bayesian modeling. While this theory-guided approach increases bias and lowers accuracy on the training set, it was successful in preventing over-fitting. The models provided reasonable classification accuracy on unseen test data (0.68 \u2264 AU C \u2264 0.84). Our approach may be particularly useful in prediction tasks trained on a relatively small data set.","label":1,"title_clean":"Using Psychologically Informed Priors for Suicide Prediction in the CLPsych 2021 Shared Task","abstract_clean":"This paper describes our approach to the CLPsych 2021 Shared Task, in which we aimed to predict suicide attempts based on Twitter feed data. We addressed this challenge by emphasizing reliance on prior domain knowledge. We engineered novel theorydriven features, and integrated prior knowledge with empirical evidence in a principled manner using Bayesian modeling. While this theory guided approach increases bias and lowers accuracy on the training set, it was successful in preventing over fitting. The models provided reasonable classification accuracy on unseen test data (0.68 \u2264 AU C \u2264 0.84). Our approach may be particularly useful in prediction tasks trained on a relatively small data set.","url":"https:\/\/aclanthology.org\/2021.clpsych-1.12"},{"ID":"garimella-etal-2021-intelligent","methods":["encoder","bert","post training bias mitigation","contextual language model","context free representations","lexical co occurrence based bias penalization","language models"],"center_method":["encoder","bert",null,null,null,null,"language models"],"tasks":["natural language generation","text summarization","fill in the blank sentence completion","bias mitigation","word and sentence level classification tasks","language models","generation frameworks","auto completion","language generation tasks"],"center_task":["natural language generation","text summarization",null,null,null,"language models",null,null,null],"Goal":["Reduced Inequalities","Gender Equality"],"text":"He is very intelligent, she is very beautiful? On Mitigating Social Biases in Language Modelling and Generation. Social biases with respect to demographics (e.g., gender, age, race) in datasets are often encoded in the large pre-trained language models trained on them. Prior works have largely focused on mitigating biases in context-free representations, with recent shift to contextual ones. While this is useful for several word and sentence-level classification tasks, mitigating biases in only the representations may not suffice to use these models for language generation tasks, such as auto-completion, summarization, or dialogue generation. In this paper, we propose an approach to mitigate social biases in BERT, a large pre-trained contextual language model, and show its effectiveness in fill-in-the-blank sentence completion and summarization tasks. In addition to mitigating biases in BERT, which in general acts as an encoder, we propose lexical co-occurrence-based bias penalization in the decoder units in generation frameworks, and show bias mitigation in summarization. Finally, our approach results in better debiasing of BERT-based representations compared to post training bias mitigation, thus illustrating the efficacy of our approach to not just mitigate biases in representations, but also generate text with reduced biases.","label":1,"title_clean":"He is very intelligent, she is very beautiful? On Mitigating Social Biases in Language Modelling and Generation","abstract_clean":"Social biases with respect to demographics (e.g., gender, age, race) in datasets are often encoded in the large pre trained language models trained on them. Prior works have largely focused on mitigating biases in context free representations, with recent shift to contextual ones. While this is useful for several word and sentence level classification tasks, mitigating biases in only the representations may not suffice to use these models for language generation tasks, such as auto completion, summarization, or dialogue generation. In this paper, we propose an approach to mitigate social biases in BERT, a large pre trained contextual language model, and show its effectiveness in fill in the blank sentence completion and summarization tasks. In addition to mitigating biases in BERT, which in general acts as an encoder, we propose lexical co occurrence based bias penalization in the decoder units in generation frameworks, and show bias mitigation in summarization. Finally, our approach results in better debiasing of BERT based representations compared to post training bias mitigation, thus illustrating the efficacy of our approach to not just mitigate biases in representations, but also generate text with reduced biases.","url":"https:\/\/aclanthology.org\/2021.findings-acl.397"},{"ID":"gasperin-briscoe-2008-statistical","methods":["probabilistic model"],"center_method":[null],"tasks":["statistical anaphora resolution"],"center_task":[null],"Goal":["Good Health and Well-Being"],"text":"Statistical Anaphora Resolution in Biomedical Texts. This paper presents a probabilistic model for resolution of non-pronominal anaphora in biomedical texts. The model seeks to find the antecedents of anaphoric expressions, both coreferent and associative ones, and also to identify discourse-new expressions. We consider only the noun phrases referring to biomedical entities. The model reaches state-of-the art performance: 56-69% precision and 54-67% recall on coreferent cases, and reasonable performance on different classes of associative cases.","label":1,"title_clean":"Statistical Anaphora Resolution in Biomedical Texts","abstract_clean":"This paper presents a probabilistic model for resolution of non pronominal anaphora in biomedical texts. The model seeks to find the antecedents of anaphoric expressions, both coreferent and associative ones, and also to identify discourse new expressions. We consider only the noun phrases referring to biomedical entities. The model reaches state of the art performance: 56 69% precision and 54 67% recall on coreferent cases, and reasonable performance on different classes of associative cases.","url":"https:\/\/aclanthology.org\/C08-1033"},{"ID":"gavankar-etal-2012-enriching","methods":["academic knowledge base","bootstrapping paradigm"],"center_method":[null,null],"tasks":["semantic web context","ontology mapping","inter - operation of heterogeneous resources"],"center_task":[null,null,null],"Goal":["Quality Education","Industry, Innovation and Infrastrucure"],"text":"Enriching An Academic knowledge base using Linked Open Data. In this paper we present work done towards populating a domain ontology using a public knowledge base like DBpedia. Using an academic ontology as our target we identify mappings between a subset of its predicates and those in DBpedia and other linked datasets. In the semantic web context, ontology mapping allows linking of independently developed ontologies and inter-operation of heterogeneous resources. Linked open data is an initiative in this direction. We populate our ontology by querying the linked open datasets for extracting instances from these resources. We show how these along with semantic web standards and tools enable us to populate the academic ontology. Resulting instances could then be used as seeds in spirit of the typical bootstrapping paradigm.","label":1,"title_clean":"Enriching An Academic knowledge base using Linked Open Data","abstract_clean":"In this paper we present work done towards populating a domain ontology using a public knowledge base like DBpedia. Using an academic ontology as our target we identify mappings between a subset of its predicates and those in DBpedia and other linked datasets. In the semantic web context, ontology mapping allows linking of independently developed ontologies and inter operation of heterogeneous resources. Linked open data is an initiative in this direction. We populate our ontology by querying the linked open datasets for extracting instances from these resources. We show how these along with semantic web standards and tools enable us to populate the academic ontology. Resulting instances could then be used as seeds in spirit of the typical bootstrapping paradigm.","url":"https:\/\/aclanthology.org\/W12-5807.pdf"},{"ID":"gemes-recski-2021-tuw","methods":["tuw inf","rule based and hybrid methods","bert"],"center_method":[null,null,"bert"],"tasks":["identifying toxic","semi automatic generation"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"TUW-Inf at GermEval2021: Rule-based and Hybrid Methods for Detecting Toxic, Engaging, and Fact-Claiming Comments. This paper describes our methods submitted for the GermEval 2021 shared task on identifying toxic, engaging and factclaiming comments in social media texts (Risch et al., 2021). We explore simple strategies for semi-automatic generation of rule-based systems with high precision and low recall, and use them to achieve slight overall improvements over a standard BERT-based classifier.","label":1,"title_clean":"TUW Inf at GermEval2021: Rule based and Hybrid Methods for Detecting Toxic, Engaging, and Fact Claiming Comments","abstract_clean":"This paper describes our methods submitted for the GermEval 2021 shared task on identifying toxic, engaging and factclaiming comments in social media texts (Risch et al., 2021). We explore simple strategies for semi automatic generation of rule based systems with high precision and low recall, and use them to achieve slight overall improvements over a standard BERT based classifier.","url":"https:\/\/aclanthology.org\/2021.germeval-1.10"},{"ID":"georgiev-etal-2009-joint","methods":["joint classifier","averaged versions","mira","perceptron","maximum entropy","nave bayes","learning methods"],"center_method":[null,null,null,null,null,null,null],"tasks":["gene mention normalization"],"center_task":[null],"Goal":["Good Health and Well-Being"],"text":"A Joint Model for Normalizing Gene and Organism Mentions in Text. The aim of gene mention normalization is to propose an appropriate canonical name, or an identifier from a popular database, for a gene or a gene product mentioned in a given piece of text. The task has attracted a lot of research attention for several organisms under the assumption that both the mention boundaries and the target organism are known. Here we extend the task to also recognizing whether the gene mention is valid and to finding the organism it is from. We solve this extended task using a joint model for gene and organism name normalization which allows for instances from different organisms to share features, thus achieving sizable performance gains with different learning methods: Na\u00efve Bayes, Maximum Entropy, Perceptron and mira, as well as averaged versions of the last two. The evaluation results for our joint classifier show F1 score of over 97%, which proves the potential of the approach.","label":1,"title_clean":"A Joint Model for Normalizing Gene and Organism Mentions in Text","abstract_clean":"The aim of gene mention normalization is to propose an appropriate canonical name, or an identifier from a popular database, for a gene or a gene product mentioned in a given piece of text. The task has attracted a lot of research attention for several organisms under the assumption that both the mention boundaries and the target organism are known. Here we extend the task to also recognizing whether the gene mention is valid and to finding the organism it is from. We solve this extended task using a joint model for gene and organism name normalization which allows for instances from different organisms to share features, thus achieving sizable performance gains with different learning methods: Na\u00efve Bayes, Maximum Entropy, Perceptron and mira, as well as averaged versions of the last two. The evaluation results for our joint classifier show F1 score of over 97%, which proves the potential of the approach.","url":"https:\/\/aclanthology.org\/W09-4503"},{"ID":"gero-etal-2022-sparks","methods":["language models","sparks","large scale language models"],"center_method":["language models",null,null],"tasks":["science writing","inspiration","machine translation","writing task"],"center_task":[null,null,"machine translation",null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Sparks: Inspiration for Science Writing using Language Models. Large-scale language models are rapidly improving, performing well on a variety of tasks with little to no customization. In this work we investigate how language models can support science writing, a challenging writing task that is both open-ended and highly constrained. We present a system for generating \"sparks\", sentences related to a scientific concept intended to inspire writers. We run a user study with 13 STEM graduate students and find three main use cases of sparks-inspiration, translation, and perspective-each of which correlates with a unique interaction pattern. We also find that while participants were more likely to select higher quality sparks, the overall quality of sparks seen by a given participant did not correlate with their satisfaction with the tool. 1","label":1,"title_clean":"Sparks: Inspiration for Science Writing using Language Models","abstract_clean":"Large scale language models are rapidly improving, performing well on a variety of tasks with little to no customization. In this work we investigate how language models can support science writing, a challenging writing task that is both open ended and highly constrained. We present a system for generating \"sparks\", sentences related to a scientific concept intended to inspire writers. We run a user study with 13 STEM graduate students and find three main use cases of sparks inspiration, translation, and perspective each of which correlates with a unique interaction pattern. We also find that while participants were more likely to select higher quality sparks, the overall quality of sparks seen by a given participant did not correlate with their satisfaction with the tool. 1","url":"https:\/\/aclanthology.org\/2022.in2writing-1.12"},{"ID":"ghosh-etal-2020-cease","methods":["gru","lstm","deep neural network","ensemble methods"],"center_method":["gru","lstm","deep neural network","ensemble methods"],"tasks":["suicide prevention","sentiment analysis"],"center_task":[null,"sentiment analysis"],"Goal":["Good Health and Well-Being"],"text":"CEASE, a Corpus of Emotion Annotated Suicide notes in English. A suicide note is usually written shortly before the suicide, and it provides a chance to comprehend the self-destructive state of mind of the deceased. From a psychological point of view, suicide notes have been utilized for recognizing the motive behind the suicide. To the best of our knowledge, there are no openly accessible suicide note corpus at present, making it challenging for the researchers and developers to deep dive into the area of mental health assessment and suicide prevention. In this paper, we create a fine-grained emotion annotated corpus (CEASE) of suicide notes in English, and develop various deep learning models to perform emotion detection on the curated dataset. The corpus consists of 2393 sentences from around 205 suicide notes collected from various sources. Each sentence is annotated with a particular emotion class from a set of 15 fine-grained emotion labels, namely (forgiveness, happiness peacefulness, love, pride, hopefulness, thankfulness, blame, anger, fear, abuse, sorrow, hopelessness, guilt, information, instructions). For the evaluation, we develop an ensemble architecture, where the base models correspond to three supervised deep learning models, namely Convolutional Neural Network (CNN), Gated Recurrent Unit (GRU) and Long Short Term Memory (LSTM).We obtain the highest test accuracy of 60.17%, and cross-validation accuracy of 60.32%.","label":1,"title_clean":"CEASE, a Corpus of Emotion Annotated Suicide notes in English","abstract_clean":"A suicide note is usually written shortly before the suicide, and it provides a chance to comprehend the self destructive state of mind of the deceased. From a psychological point of view, suicide notes have been utilized for recognizing the motive behind the suicide. To the best of our knowledge, there are no openly accessible suicide note corpus at present, making it challenging for the researchers and developers to deep dive into the area of mental health assessment and suicide prevention. In this paper, we create a fine grained emotion annotated corpus (CEASE) of suicide notes in English, and develop various deep learning models to perform emotion detection on the curated dataset. The corpus consists of 2393 sentences from around 205 suicide notes collected from various sources. Each sentence is annotated with a particular emotion class from a set of 15 fine grained emotion labels, namely (forgiveness, happiness peacefulness, love, pride, hopefulness, thankfulness, blame, anger, fear, abuse, sorrow, hopelessness, guilt, information, instructions). For the evaluation, we develop an ensemble architecture, where the base models correspond to three supervised deep learning models, namely Convolutional Neural Network (CNN), Gated Recurrent Unit (GRU) and Long Short Term Memory (LSTM).We obtain the highest test accuracy of 60.17%, and cross validation accuracy of 60.32%.","url":"https:\/\/aclanthology.org\/2020.lrec-1.201"},{"ID":"gi-etal-2021-verdict","methods":["roberta","feverous baseline"],"center_method":["roberta",null],"tasks":["fact checking","fever ous shared task","document retrieval","verdict inference"],"center_task":["fact checking",null,null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Verdict Inference with Claim and Retrieved Elements Using RoBERTa. Automatic fact verification has attracted recent research attention as the increasing dissemination of disinformation on social media platforms. The FEVEROUS shared task introduces a benchmark for fact verification, in which a system is challenged to verify the given claim using the extracted evidential elements from Wikipedia documents. In this paper, we propose our 3 rd place three-stage system consisting of document retrieval, element retrieval, and verdict inference for the FEVER-OUS shared task. By considering the context relevance in the fact extraction and verification task, our system achieves 0.29 FEVER-OUS score on the development set and 0.25 FEVEROUS score on the blind test set, both outperforming the FEVEROUS baseline.","label":1,"title_clean":"Verdict Inference with Claim and Retrieved Elements Using RoBERTa","abstract_clean":"Automatic fact verification has attracted recent research attention as the increasing dissemination of disinformation on social media platforms. The FEVEROUS shared task introduces a benchmark for fact verification, in which a system is challenged to verify the given claim using the extracted evidential elements from Wikipedia documents. In this paper, we propose our 3 rd place three stage system consisting of document retrieval, element retrieval, and verdict inference for the FEVER OUS shared task. By considering the context relevance in the fact extraction and verification task, our system achieves 0.29 FEVER OUS score on the development set and 0.25 FEVEROUS score on the blind test set, both outperforming the FEVEROUS baseline.","url":"https:\/\/aclanthology.org\/2021.fever-1.7"},{"ID":"giovanni-moller-etal-2020-nlp","methods":["bert","classification","ensembling"],"center_method":["bert","classification",null],"tasks":["covid 19","monitoring systems","wnut 2020 task","nlp north","domain and task related pre training"],"center_task":["covid 19",null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"NLP North at WNUT-2020 Task 2: Pre-training versus Ensembling for Detection of Informative COVID-19 English Tweets. With the COVID-19 pandemic raging worldwide since the beginning of the 2020 decade, the need for monitoring systems to track relevant information on social media is vitally important. This paper describes our submission to the WNUT-2020 Task 2: Identification of informative COVID-19 English Tweets. We investigate the effectiveness for a variety of classification models, and found that domainspecific pre-trained BERT models lead to the best performance. On top of this, we attempt a variety of ensembling strategies, but these attempts did not lead to further improvements. Our final best model, the standalone CT-BERT model, proved to be highly competitive, leading to a shared first place in the shared task. Our results emphasize the importance of domain and task-related pre-training. 1","label":1,"title_clean":"NLP North at WNUT 2020 Task 2: Pre training versus Ensembling for Detection of Informative COVID 19 English Tweets","abstract_clean":"With the COVID 19 pandemic raging worldwide since the beginning of the 2020 decade, the need for monitoring systems to track relevant information on social media is vitally important. This paper describes our submission to the WNUT 2020 Task 2: Identification of informative COVID 19 English Tweets. We investigate the effectiveness for a variety of classification models, and found that domainspecific pre trained BERT models lead to the best performance. On top of this, we attempt a variety of ensembling strategies, but these attempts did not lead to further improvements. Our final best model, the standalone CT BERT model, proved to be highly competitive, leading to a shared first place in the shared task. Our results emphasize the importance of domain and task related pre training. 1","url":"https:\/\/aclanthology.org\/2020.wnut-1.44"},{"ID":"gnehm-clematide-2020-text","methods":["multi tasking bert model","contextualized in domain embeddings","conditional random field","lstm","transfer approaches"],"center_method":[null,null,"conditional random field","lstm",null],"tasks":["job advertisements","classification","text zoning"],"center_task":[null,"classification",null],"Goal":["Decent Work and Economic Growth"],"text":"Text Zoning and Classification for Job Advertisements in German, French and English. We present experiments to structure job ads into text zones and classify them into professions, industries and management functions, thereby facilitating social science analyses on labor marked demand. Our main contribution are empirical findings on the benefits of contextualized embeddings and the potential of multi-task models for this purpose. With contextualized in-domain embeddings in BiLSTM-CRF models, we reach an accuracy of 91% for token-level text zoning and outperform previous approaches. A multi-tasking BERT model performs well for our classification tasks. We further compare transfer approaches for our multilingual data.","label":1,"title_clean":"Text Zoning and Classification for Job Advertisements in German, French and English","abstract_clean":"We present experiments to structure job ads into text zones and classify them into professions, industries and management functions, thereby facilitating social science analyses on labor marked demand. Our main contribution are empirical findings on the benefits of contextualized embeddings and the potential of multi task models for this purpose. With contextualized in domain embeddings in BiLSTM CRF models, we reach an accuracy of 91% for token level text zoning and outperform previous approaches. A multi tasking BERT model performs well for our classification tasks. We further compare transfer approaches for our multilingual data.","url":"https:\/\/aclanthology.org\/2020.nlpcss-1.10"},{"ID":"goel-sharma-2019-usf","methods":["keras tokenizer","word embeddings","lstm","data pre processing techniques"],"center_method":[null,"word embeddings","lstm",null],"tasks":["classification","feature engineering","hate speech"],"center_task":["classification","feature engineering","hate speech"],"Goal":["Peace, Justice and Strong Institutions"],"text":"USF at SemEval-2019 Task 6: Offensive Language Detection Using LSTM With Word Embeddings. In this paper, we present a system description for the SemEval-2019 Task 6 submitted by our team. For the task, our system takes tweet as an input and determine if the tweet is offensive or non-offensive (Sub-task A). In case a tweet is offensive, our system identifies if a tweet is targeted (insult or threat) or nontargeted like swearing (Sub-task B). In targeted tweets, our system identifies the target as an individual or group (Sub-task C). We used data pre-processing techniques like splitting hashtags into words, removing special characters, stop-word removal, stemming, lemmatization, capitalization, and offensive word dictionary. Later, we used keras tokenizer and word embeddings for feature extraction. For classification, we used the LSTM (Long shortterm memory) model of keras framework. Our accuracy scores for Sub-task A, B and C are 0.8128, 0.8167 and 0.3662 respectively. Our results indicate that fine-grained classification to identify offense target was difficult for the system. Lastly, in the future scope section, we will discuss the ways to improve system performance.","label":1,"title_clean":"USF at SemEval 2019 Task 6: Offensive Language Detection Using LSTM With Word Embeddings","abstract_clean":"In this paper, we present a system description for the SemEval 2019 Task 6 submitted by our team. For the task, our system takes tweet as an input and determine if the tweet is offensive or non offensive (Sub task A). In case a tweet is offensive, our system identifies if a tweet is targeted (insult or threat) or nontargeted like swearing (Sub task B). In targeted tweets, our system identifies the target as an individual or group (Sub task C). We used data pre processing techniques like splitting hashtags into words, removing special characters, stop word removal, stemming, lemmatization, capitalization, and offensive word dictionary. Later, we used keras tokenizer and word embeddings for feature extraction. For classification, we used the LSTM (Long shortterm memory) model of keras framework. Our accuracy scores for Sub task A, B and C are 0.8128, 0.8167 and 0.3662 respectively. Our results indicate that fine grained classification to identify offense target was difficult for the system. Lastly, in the future scope section, we will discuss the ways to improve system performance.","url":"https:\/\/aclanthology.org\/S19-2139"},{"ID":"gokce-etal-2020-embedding","methods":["search engine","text editor application","nearest neighbor search","boolean keyword filtering","text embeddings","web application"],"center_method":["search engine",null,null,null,null,null],"tasks":["discovery experience","embedding based scientific literature discovery","text editing","reading papers","search queries","bibliography management"],"center_task":[null,null,null,null,null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Embedding-based Scientific Literature Discovery in a Text Editor Application. Each claim in a research paper requires all relevant prior knowledge to be discovered, assimilated, and appropriately cited. However, despite the availability of powerful search engines and sophisticated text editing software, discovering relevant papers and integrating the knowledge into a manuscript remain complex tasks associated with high cognitive load. To define comprehensive search queries requires strong motivation from authors, irrespective of their familiarity with the research field. Moreover, switching between independent applications for literature discovery, bibliography management, reading papers, and writing text burdens authors further and interrupts their creative process. Here, we present a web application that combines text editing and literature discovery in an interactive user interface. The application is equipped with a search engine that couples Boolean keyword filtering with nearest neighbor search over text embeddings, providing a discovery experience tuned to an author's manuscript and his interests. Our application aims to take a step towards more enjoyable and effortless academic writing. The demo of the application 1 and a short video tutorial 2 are available online.","label":1,"title_clean":"Embedding based Scientific Literature Discovery in a Text Editor Application","abstract_clean":"Each claim in a research paper requires all relevant prior knowledge to be discovered, assimilated, and appropriately cited. However, despite the availability of powerful search engines and sophisticated text editing software, discovering relevant papers and integrating the knowledge into a manuscript remain complex tasks associated with high cognitive load. To define comprehensive search queries requires strong motivation from authors, irrespective of their familiarity with the research field. Moreover, switching between independent applications for literature discovery, bibliography management, reading papers, and writing text burdens authors further and interrupts their creative process. Here, we present a web application that combines text editing and literature discovery in an interactive user interface. The application is equipped with a search engine that couples Boolean keyword filtering with nearest neighbor search over text embeddings, providing a discovery experience tuned to an author's manuscript and his interests. Our application aims to take a step towards more enjoyable and effortless academic writing. The demo of the application 1 and a short video tutorial 2 are available online.","url":"https:\/\/aclanthology.org\/2020.acl-demos.36.pdf"},{"ID":"gokhman-etal-2012-search","methods":["crowdsourcing technique","non gold standard creation approaches"],"center_method":[null,null],"tasks":["webbased deception detection","deception research"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"In Search of a Gold Standard in Studies of Deception. In this study, we explore several popular techniques for obtaining corpora for deception research. Through a survey of traditional as well as non-gold standard creation approaches, we identify advantages and limitations of these techniques for webbased deception detection and offer crowdsourcing as a novel avenue toward achieving a gold standard corpus. Through an indepth case study of online hotel reviews, we demonstrate the implementation of this crowdsourcing technique and illustrate its applicability to a broad array of online reviews.","label":1,"title_clean":"In Search of a Gold Standard in Studies of Deception","abstract_clean":"In this study, we explore several popular techniques for obtaining corpora for deception research. Through a survey of traditional as well as non gold standard creation approaches, we identify advantages and limitations of these techniques for webbased deception detection and offer crowdsourcing as a novel avenue toward achieving a gold standard corpus. Through an indepth case study of online hotel reviews, we demonstrate the implementation of this crowdsourcing technique and illustrate its applicability to a broad array of online reviews.","url":"https:\/\/aclanthology.org\/W12-0404"},{"ID":"gonzalez-etal-2021-interaction","methods":["gradientbased explainability"],"center_method":[null],"tasks":["nlp practitioners","human evaluation"],"center_task":[null,"human evaluation"],"Goal":["Peace, Justice and Strong Institutions"],"text":"On the Interaction of Belief Bias and Explanations. A myriad of explainability methods have been proposed in recent years, but there is little consensus on how to evaluate them. While automatic metrics allow for quick benchmarking, it isn't clear how such metrics reflect human interaction with explanations. Human evaluation is of paramount importance, but previous protocols fail to account for belief biases affecting human performance, which may lead to misleading conclusions. We provide an overview of belief bias, its role in human evaluation, and ideas for NLP practitioners on how to account for it. For two experimental paradigms, we present a case study of gradientbased explainability introducing simple ways to account for humans' prior beliefs: models of varying quality and adversarial examples. We show that conclusions about the highest performing methods change when introducing such controls, pointing to the importance of accounting for belief bias in evaluation.","label":1,"title_clean":"On the Interaction of Belief Bias and Explanations","abstract_clean":"A myriad of explainability methods have been proposed in recent years, but there is little consensus on how to evaluate them. While automatic metrics allow for quick benchmarking, it isn't clear how such metrics reflect human interaction with explanations. Human evaluation is of paramount importance, but previous protocols fail to account for belief biases affecting human performance, which may lead to misleading conclusions. We provide an overview of belief bias, its role in human evaluation, and ideas for NLP practitioners on how to account for it. For two experimental paradigms, we present a case study of gradientbased explainability introducing simple ways to account for humans' prior beliefs: models of varying quality and adversarial examples. We show that conclusions about the highest performing methods change when introducing such controls, pointing to the importance of accounting for belief bias in evaluation.","url":"https:\/\/aclanthology.org\/2021.findings-acl.259"},{"ID":"gorrell-etal-2013-finding","methods":["rule based and machine learning approaches","rule based system"],"center_method":[null,"rule based system"],"tasks":["automatic extraction of eleven negative symptoms of schizophrenia","system development"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"Finding Negative Symptoms of Schizophrenia in Patient Records. This paper reports the automatic extraction of eleven negative symptoms of schizophrenia from patient medical records. The task offers a range of difficulties depending on the consistency and complexity with which mental health professionals describe each. In order to reduce the cost of system development, rapid prototypes are built with minimal adaptation and configuration of existing software, and additional training data is obtained by annotating automatically extracted symptoms for which the system has low confidence. The system was further improved by the addition of a manually engineered rule based approach. Rule-based and machine learning approaches are combined in various ways to achieve the optimal result for each symptom. Precisions in the range of 0.8 to 0.99 have been obtained.","label":1,"title_clean":"Finding Negative Symptoms of Schizophrenia in Patient Records","abstract_clean":"This paper reports the automatic extraction of eleven negative symptoms of schizophrenia from patient medical records. The task offers a range of difficulties depending on the consistency and complexity with which mental health professionals describe each. In order to reduce the cost of system development, rapid prototypes are built with minimal adaptation and configuration of existing software, and additional training data is obtained by annotating automatically extracted symptoms for which the system has low confidence. The system was further improved by the addition of a manually engineered rule based approach. Rule based and machine learning approaches are combined in various ways to achieve the optimal result for each symptom. Precisions in the range of 0.8 to 0.99 have been obtained.","url":"https:\/\/aclanthology.org\/W13-5102"},{"ID":"gorrell-etal-2016-identifying","methods":["machine learning methods","automated approaches"],"center_method":["machine learning methods",null],"tasks":["finding cases of first episode psychosis","medical research"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"Identifying First Episodes of Psychosis in Psychiatric Patient Records using Machine Learning. Natural language processing is being pressed into use to facilitate the selection of cases for medical research in electronic health record databases, though study inclusion criteria may be complex, and the linguistic cues indicating eligibility may be subtle. Finding cases of first episode psychosis raised a number of problems for automated approaches, providing an opportunity to explore how machine learning technologies might be used to overcome them. A system was delivered that achieved an AUC of 0.85, enabling 95% of relevant cases to be identified whilst halving the work required in manually reviewing cases. The techniques that made this possible are presented.","label":1,"title_clean":"Identifying First Episodes of Psychosis in Psychiatric Patient Records using Machine Learning","abstract_clean":"Natural language processing is being pressed into use to facilitate the selection of cases for medical research in electronic health record databases, though study inclusion criteria may be complex, and the linguistic cues indicating eligibility may be subtle. Finding cases of first episode psychosis raised a number of problems for automated approaches, providing an opportunity to explore how machine learning technologies might be used to overcome them. A system was delivered that achieved an AUC of 0.85, enabling 95% of relevant cases to be identified whilst halving the work required in manually reviewing cases. The techniques that made this possible are presented.","url":"https:\/\/aclanthology.org\/W16-2927"},{"ID":"gosangi-etal-2021-use","methods":["error analysis","hierarchical bilstm model","document level traintest splits"],"center_method":["error analysis",null,null],"tasks":["citation worthiness","predicting citation worthiness","modeling process","sequence labeling task"],"center_task":[null,null,null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"On the Use of Context for Predicting Citation Worthiness of Sentences in Scholarly Articles. In this paper, we study the importance of context in predicting the citation worthiness of sentences in scholarly articles. We formulate this problem as a sequence labeling task solved using a hierarchical BiLSTM model. We contribute a new benchmark dataset containing over two million sentences and their corresponding labels. We preserve the sentence order in this dataset and perform document-level train\/test splits, which importantly allows incorporating contextual information in the modeling process. We evaluate the proposed approach on three benchmark datasets. Our results quantify the benefits of using context and contextual embeddings for citation worthiness. Lastly, through error analysis, we provide insights into cases where context plays an essential role in predicting citation worthiness. Conclusion 0.","label":1,"title_clean":"On the Use of Context for Predicting Citation Worthiness of Sentences in Scholarly Articles","abstract_clean":"In this paper, we study the importance of context in predicting the citation worthiness of sentences in scholarly articles. We formulate this problem as a sequence labeling task solved using a hierarchical BiLSTM model. We contribute a new benchmark dataset containing over two million sentences and their corresponding labels. We preserve the sentence order in this dataset and perform document level train\/test splits, which importantly allows incorporating contextual information in the modeling process. We evaluate the proposed approach on three benchmark datasets. Our results quantify the benefits of using context and contextual embeddings for citation worthiness. Lastly, through error analysis, we provide insights into cases where context plays an essential role in predicting citation worthiness. Conclusion 0.","url":"https:\/\/aclanthology.org\/2021.naacl-main.359"},{"ID":"grandeit-etal-2020-using","methods":["bert","machine learning based text classification models","human coders"],"center_method":["bert",null,null],"tasks":["automated coding","psychosocial online counseling","qualitative content analysis"],"center_task":[null,null,null],"Goal":["Good Health and Well-Being"],"text":"Using BERT for Qualitative Content Analysis in Psychosocial Online Counseling. Qualitative content analysis is a systematic method commonly used in the social sciences to analyze textual data from interviews or online discussions. However, this method usually requires high expertise and manual effort because human coders need to read, interpret, and manually annotate text passages. This is especially true if the system of categories used for annotation is complex and semantically rich. Therefore, qualitative content analysis could benefit greatly from automated coding. In this work, we investigate the usage of machine learning-based text classification models for automatic coding in the area of psychosocial online counseling. We developed a system of over 50 categories to analyze counseling conversations, labeled over 10.000 text passages manually, and evaluated the performance of different machine learning-based classifiers against human coders.","label":1,"title_clean":"Using BERT for Qualitative Content Analysis in Psychosocial Online Counseling","abstract_clean":"Qualitative content analysis is a systematic method commonly used in the social sciences to analyze textual data from interviews or online discussions. However, this method usually requires high expertise and manual effort because human coders need to read, interpret, and manually annotate text passages. This is especially true if the system of categories used for annotation is complex and semantically rich. Therefore, qualitative content analysis could benefit greatly from automated coding. In this work, we investigate the usage of machine learning based text classification models for automatic coding in the area of psychosocial online counseling. We developed a system of over 50 categories to analyze counseling conversations, labeled over 10.000 text passages manually, and evaluated the performance of different machine learning based classifiers against human coders.","url":"https:\/\/aclanthology.org\/2020.nlpcss-1.2"},{"ID":"granfeldt-etal-2006-cefle","methods":["cefle","machine learning methods","direkt profil","pedagogical tools"],"center_method":[null,"machine learning methods",null,null],"tasks":["sentence analysis","second language acquisition","definition and detection of learner profiles"],"center_task":[null,null,null],"Goal":["Quality Education"],"text":"CEFLE and Direkt Profil: a New Computer Learner Corpus in French L2 and a System for Grammatical Profiling. The importance of computer learner corpora for research in both second language acquisition and foreign language teaching is rapidly increasing. Computer learner corpora can provide us with data to describe the learner's interlanguage system at different points of its development and they can be used to create pedagogical tools. In this paper, we first present a new computer learner corpora in French. We then describe an analyzer called Direkt Profil, that we have developed using this corpus. The system carries out a sentence analysis based on developmental sequences, i.e. local morphosyntactic phenomena linked to a development in the acquisition of French as a foreign language. We present a brief introduction to developmental sequences and some examples in French. In the final section, we introduce and evaluate a method to optimize the definition and detection of learner profiles using machine-learning techniques.","label":1,"title_clean":"CEFLE and Direkt Profil: a New Computer Learner Corpus in French L2 and a System for Grammatical Profiling","abstract_clean":"The importance of computer learner corpora for research in both second language acquisition and foreign language teaching is rapidly increasing. Computer learner corpora can provide us with data to describe the learner's interlanguage system at different points of its development and they can be used to create pedagogical tools. In this paper, we first present a new computer learner corpora in French. We then describe an analyzer called Direkt Profil, that we have developed using this corpus. The system carries out a sentence analysis based on developmental sequences, i.e. local morphosyntactic phenomena linked to a development in the acquisition of French as a foreign language. We present a brief introduction to developmental sequences and some examples in French. In the final section, we introduce and evaluate a method to optimize the definition and detection of learner profiles using machine learning techniques.","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2006\/pdf\/246_pdf.pdf"},{"ID":"green-2018-proposed","methods":["argument schemes"],"center_method":[null],"tasks":["annotation of scientific arguments"],"center_task":[null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Proposed Method for Annotation of Scientific Arguments in Terms of Semantic Relations and Argument Schemes. This paper presents a proposed method for annotation of scientific arguments in biological\/biomedical journal articles. Semantic entities and relations are used to represent the propositional content of arguments in instances of argument schemes. We describe an experiment in which we encoded the arguments in a journal article to identify issues in this approach. Our catalogue of argument schemes and a copy of the annotated article are now publically available.","label":1,"title_clean":"Proposed Method for Annotation of Scientific Arguments in Terms of Semantic Relations and Argument Schemes","abstract_clean":"This paper presents a proposed method for annotation of scientific arguments in biological\/biomedical journal articles. Semantic entities and relations are used to represent the propositional content of arguments in instances of argument schemes. We describe an experiment in which we encoded the arguments in a journal article to identify issues in this approach. Our catalogue of argument schemes and a copy of the annotated article are now publically available.","url":"https:\/\/aclanthology.org\/W18-5213.pdf"},{"ID":"grover-etal-2012-aspects","methods":["legal framework","language resources management agency"],"center_method":[null,null],"tasks":["language resource management"],"center_task":[null],"Goal":["Industry, Innovation and Infrastrucure","Peace, Justice and Strong Institutions"],"text":"Aspects of a Legal Framework for Language Resource Management. The management of language resources requires several legal aspects to be taken into consideration. In this paper we discuss a number of these aspects which lead towards the formation of a legal framework for a language resources management agency. The legal framework entails examination of the agency's stakeholders and the relationships that exist amongst them, the privacy and intellectual property rights that exist around the language resources offered by the agency, and the external (e.g. laws, acts, policies) and internal legal instruments (e.g. end user licence agreements) required for the agency's operation.","label":1,"title_clean":"Aspects of a Legal Framework for Language Resource Management","abstract_clean":"The management of language resources requires several legal aspects to be taken into consideration. In this paper we discuss a number of these aspects which lead towards the formation of a legal framework for a language resources management agency. The legal framework entails examination of the agency's stakeholders and the relationships that exist amongst them, the privacy and intellectual property rights that exist around the language resources offered by the agency, and the external (e.g. laws, acts, policies) and internal legal instruments (e.g. end user licence agreements) required for the agency's operation.","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2012\/pdf\/226_Paper.pdf"},{"ID":"guo-etal-2014-crab","methods":["crab 2 0","text mining tool"],"center_method":[null,null],"tasks":["literature review","chemical cancer risk assessment","semantic classification","in browser application","statistical analysis"],"center_task":[null,null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"CRAB 2.0: A text mining tool for supporting literature review in chemical cancer risk assessment. Chemical cancer risk assessment is a literature-dependent task which could greatly benefit from text mining support. In this paper we describe CRAB-the first publicly available tool for supporting the risk assessment workflow. CRAB, currently at version 2.0, facilitates the gathering of relevant literature via PubMed queries as well as semantic classification, statistical analysis and efficient study of the literature. The tool is freely available as an in-browser application.","label":1,"title_clean":"CRAB 2.0: A text mining tool for supporting literature review in chemical cancer risk assessment","abstract_clean":"Chemical cancer risk assessment is a literature dependent task which could greatly benefit from text mining support. In this paper we describe CRAB the first publicly available tool for supporting the risk assessment workflow. CRAB, currently at version 2.0, facilitates the gathering of relevant literature via PubMed queries as well as semantic classification, statistical analysis and efficient study of the literature. The tool is freely available as an in browser application.","url":"https:\/\/aclanthology.org\/C14-2017"},{"ID":"guo-etal-2017-effective","methods":["information retrieval","deep neural networksnn approach","hybrid method"],"center_method":["information retrieval",null,null],"tasks":["real world tests"],"center_task":[null],"Goal":["Quality Education"],"text":"Which is the Effective Way for Gaokao: Information Retrieval or Neural Networks?. As one of the most important test of China, Gaokao is designed to be difficult enough to distinguish the excellent high school students. In this work, we detailed the Gaokao History Multiple Choice Questions(GKHMC) and proposed two different approaches to address them using various resources. One approach is based on entity search technique (IR approach), the other is based on text entailment approach where we specifically employ deep neural networks(NN approach). The result of experiment on our collected real Gaokao questions showed that they are good at different categories of questions, i.e. IR approach performs much better at entity questions(EQs) while NN approach shows its advantage on sentence questions(SQs). Our new method achieves state-of-the-art performance and show that it's indispensable to apply hybrid method when participating in the real-world tests.","label":1,"title_clean":"Which is the Effective Way for Gaokao: Information Retrieval or Neural Networks?","abstract_clean":"As one of the most important test of China, Gaokao is designed to be difficult enough to distinguish the excellent high school students. In this work, we detailed the Gaokao History Multiple Choice Questions(GKHMC) and proposed two different approaches to address them using various resources. One approach is based on entity search technique (IR approach), the other is based on text entailment approach where we specifically employ deep neural networks(NN approach). The result of experiment on our collected real Gaokao questions showed that they are good at different categories of questions, i.e. IR approach performs much better at entity questions(EQs) while NN approach shows its advantage on sentence questions(SQs). Our new method achieves state of the art performance and show that it's indispensable to apply hybrid method when participating in the real world tests.","url":"https:\/\/aclanthology.org\/E17-1011"},{"ID":"guo-etal-2020-text","methods":["bert","cross lingual data augmentation method","contrastive learning","text representations","autoencoders"],"center_method":["bert",null,null,null,null],"tasks":["automatic alzheimers disease","cross lingual data augmentation","classification","data scarcity"],"center_task":[null,null,"classification",null],"Goal":["Good Health and Well-Being"],"text":"Text Classification by Contrastive Learning and Cross-lingual Data Augmentation for Alzheimer's Disease Detection. Data scarcity is always a constraint on analyzing speech transcriptions for automatic Alzheimer's disease (AD) detection, especially when the subjects are non-English speakers. To deal with this issue, this paper first proposes a contrastive learning method to obtain effective representations for text classification based on monolingual embeddings of BERT. Furthermore, a cross-lingual data augmentation method is designed by building autoencoders to learn the text representations shared by both languages. Experiments on a Mandarin AD corpus show that the contrastive learning method can achieve better detection accuracy than conventional CNN-based and BERTbased methods. Our cross-lingual data augmentation method also outperforms other compared methods when using another English AD corpus for augmentation. Finally, a best detection accuracy of 81.6% is obtained by our proposed methods on the Mandarin AD corpus.","label":1,"title_clean":"Text Classification by Contrastive Learning and Cross lingual Data Augmentation for Alzheimer's Disease Detection","abstract_clean":"Data scarcity is always a constraint on analyzing speech transcriptions for automatic Alzheimer's disease (AD) detection, especially when the subjects are non English speakers. To deal with this issue, this paper first proposes a contrastive learning method to obtain effective representations for text classification based on monolingual embeddings of BERT. Furthermore, a cross lingual data augmentation method is designed by building autoencoders to learn the text representations shared by both languages. Experiments on a Mandarin AD corpus show that the contrastive learning method can achieve better detection accuracy than conventional CNN based and BERTbased methods. Our cross lingual data augmentation method also outperforms other compared methods when using another English AD corpus for augmentation. Finally, a best detection accuracy of 81.6% is obtained by our proposed methods on the Mandarin AD corpus.","url":"https:\/\/aclanthology.org\/2020.coling-main.542"},{"ID":"guo-etal-2021-pre","methods":["transformer based classification and span detection models","oversampling","transformers"],"center_method":[null,null,"transformers"],"tasks":["classification","span detection task","social media health applications"],"center_task":["classification",null,null],"Goal":["Good Health and Well-Being"],"text":"Pre-trained Transformer-based Classification and Span Detection Models for Social Media Health Applications. This paper describes our approach for six classification tasks (Tasks 1a, 3a, 3b, 4 and 5) and one span detection task (Task 1b) from the Social Media Mining for Health (SMM4H) 2021 shared tasks. We developed two separate systems for classification and span detection, both based on pre-trained Transformer-based models. In addition, we applied oversampling and classifier ensembling in the classification tasks. The results of our submissions are over the median scores in all tasks except for Task 1a. Furthermore, our model achieved first place in Task 4 and obtained a 7% higher F 1-score than the median in Task 1b.","label":1,"title_clean":"Pre trained Transformer based Classification and Span Detection Models for Social Media Health Applications","abstract_clean":"This paper describes our approach for six classification tasks (Tasks 1a, 3a, 3b, 4 and 5) and one span detection task (Task 1b) from the Social Media Mining for Health (SMM4H) 2021 shared tasks. We developed two separate systems for classification and span detection, both based on pre trained Transformer based models. In addition, we applied oversampling and classifier ensembling in the classification tasks. The results of our submissions are over the median scores in all tasks except for Task 1a. Furthermore, our model achieved first place in Task 4 and obtained a 7% higher F 1 score than the median in Task 1b.","url":"https:\/\/aclanthology.org\/2021.smm4h-1.8.pdf"},{"ID":"gupta-2020-finlp","methods":["mixture of berts","cost sensitive learning","data augmentation","generic and domain specific bert","under sampling"],"center_method":[null,null,"data augmentation",null,null],"tasks":["causal sentence identification","fnp fns workshop","fincausal shared task"],"center_task":[null,null,null],"Goal":["Decent Work and Economic Growth"],"text":"FiNLP at FinCausal 2020 Task 1: Mixture of BERTs for Causal Sentence Identification in Financial Texts. This paper describes our system developed for the sub-task 1 of the FinCausal shared task in the FNP-FNS workshop held in conjunction with COLING-2020. The system classifies whether a financial news text segment contains causality or not. To address this task, we fine-tune and ensemble the generic and domain-specific BERT language models pre-trained on financial text corpora. The task data is highly imbalanced with the majority non-causal class; therefore, we train the models using strategies such as under-sampling, cost-sensitive learning, and data augmentation. Our best system achieves a weighted F1-score of 96.98 securing 4 th position on the evaluation leaderboard. The code is available at https:","label":1,"title_clean":"FiNLP at FinCausal 2020 Task 1: Mixture of BERTs for Causal Sentence Identification in Financial Texts","abstract_clean":"This paper describes our system developed for the sub task 1 of the FinCausal shared task in the FNP FNS workshop held in conjunction with COLING 2020. The system classifies whether a financial news text segment contains causality or not. To address this task, we fine tune and ensemble the generic and domain specific BERT language models pre trained on financial text corpora. The task data is highly imbalanced with the majority non causal class; therefore, we train the models using strategies such as under sampling, cost sensitive learning, and data augmentation. Our best system achieves a weighted F1 score of 96.98 securing 4 th position on the evaluation leaderboard. The code is available at https:","url":"https:\/\/aclanthology.org\/2020.fnp-1.12"},{"ID":"gupta-etal-2020-human","methods":["annotation schemas"],"center_method":[null],"tasks":["human human health coaching","annotation","annotating","information exchange","conversation analysis","data collection","automatically extracting activity goals"],"center_task":[null,"annotation",null,null,null,"data collection",null],"Goal":["Good Health and Well-Being"],"text":"Human-Human Health Coaching via Text Messages: Corpus, Annotation, and Analysis. Our goal is to develop and deploy a virtual assistant health coach that can help patients set realistic physical activity goals and live a more active lifestyle. Since there is no publicly shared dataset of health coaching dialogues, the first phase of our research focused on data collection. We hired a certified health coach and 28 patients to collect the first round of human-human health coaching interaction which took place via text messages. This resulted in 2853 messages. The data collection phase was followed by conversation analysis to gain insight into the way information exchange takes place between a health coach and a patient. This was formalized using two annotation schemas: one that focuses on the goals the patient is setting and another that models the higher-level structure of the interactions. In this paper, we discuss these schemas and briefly talk about their application for automatically extracting activity goals and annotating the second round of data, collected with different health coaches and patients. Given the resource-intensive nature of data annotation, successfully annotating a new dataset automatically is key to answer the need for high quality, large datasets.","label":1,"title_clean":"Human Human Health Coaching via Text Messages: Corpus, Annotation, and Analysis","abstract_clean":"Our goal is to develop and deploy a virtual assistant health coach that can help patients set realistic physical activity goals and live a more active lifestyle. Since there is no publicly shared dataset of health coaching dialogues, the first phase of our research focused on data collection. We hired a certified health coach and 28 patients to collect the first round of human human health coaching interaction which took place via text messages. This resulted in 2853 messages. The data collection phase was followed by conversation analysis to gain insight into the way information exchange takes place between a health coach and a patient. This was formalized using two annotation schemas: one that focuses on the goals the patient is setting and another that models the higher level structure of the interactions. In this paper, we discuss these schemas and briefly talk about their application for automatically extracting activity goals and annotating the second round of data, collected with different health coaches and patients. Given the resource intensive nature of data annotation, successfully annotating a new dataset automatically is key to answer the need for high quality, large datasets.","url":"https:\/\/aclanthology.org\/2020.sigdial-1.30"},{"ID":"gupta-etal-2021-sumpubmed","methods":["seq2seq models"],"center_method":[null],"tasks":["text summarization","summary generation"],"center_task":["text summarization",null],"Goal":["Good Health and Well-Being","Industry, Innovation and Infrastrucure"],"text":"SumPubMed: Summarization Dataset of PubMed Scientific Articles. Most earlier work on text summarization is carried out on news article datasets. The summary in these datasets is naturally located at the beginning of the text. Hence, a model can spuriously utilize this correlation for summary generation instead of truly learning to summarize. To address this issue, we constructed a new dataset, SUMPUBMED, using scientific articles from the PubMed archive. We conducted a human analysis of summary coverage, redundancy, readability, coherence, and informativeness on SUMPUBMED. SUMPUBMED is challenging because (a) the summary is distributed throughout the text (not-localized on top), and (b) it contains rare domain-specific scientific terms. We observe that seq2seq models that adequately summarize news articles struggle to summarize SUMPUBMED. Thus, SUMPUBMED opens new avenues for the future improvement of models as well as the development of new evaluation metrics.","label":1,"title_clean":"SumPubMed: Summarization Dataset of PubMed Scientific Articles","abstract_clean":"Most earlier work on text summarization is carried out on news article datasets. The summary in these datasets is naturally located at the beginning of the text. Hence, a model can spuriously utilize this correlation for summary generation instead of truly learning to summarize. To address this issue, we constructed a new dataset, SUMPUBMED, using scientific articles from the PubMed archive. We conducted a human analysis of summary coverage, redundancy, readability, coherence, and informativeness on SUMPUBMED. SUMPUBMED is challenging because (a) the summary is distributed throughout the text (not localized on top), and (b) it contains rare domain specific scientific terms. We observe that seq2seq models that adequately summarize news articles struggle to summarize SUMPUBMED. Thus, SUMPUBMED opens new avenues for the future improvement of models as well as the development of new evaluation metrics.","url":"https:\/\/aclanthology.org\/2021.acl-srw.30"},{"ID":"ha-yaneva-2018-automatic","methods":["information retrieval","concept embeddings","automatic method"],"center_method":["information retrieval",null,null],"tasks":["automatic distractor suggestion","multiple choice questions","predictions","high stakes medical exams","item writing process"],"center_task":[null,null,"predictions",null,null],"Goal":["Quality Education"],"text":"Automatic Distractor Suggestion for Multiple-Choice Tests Using Concept Embeddings and Information Retrieval. Developing plausible distractors (wrong answer options) when writing multiple-choice questions has been described as one of the most challenging and time-consuming parts of the item-writing process. In this paper we propose a fully automatic method for generating distractor suggestions for multiple-choice questions used in high-stakes medical exams. The system uses a question stem and the correct answer as an input and produces a list of suggested distractors ranked based on their similarity to the stem and the correct answer. To do this we use a novel approach of combining concept embeddings with information retrieval methods. We frame the evaluation as a prediction task where we aim to \"predict\" the human-produced distractors used in large sets of medical questions, i.e. if a distractor generated by our system is good enough it is likely to feature among the list of distractors produced by the human item-writers. The results reveal that combining concept embeddings with information retrieval approaches significantly improves the generation of plausible distractors and enables us to match around 1 in 5 of the human-produced distractors. The approach proposed in this paper is generalisable to all scenarios where the distractors refer to concepts.","label":1,"title_clean":"Automatic Distractor Suggestion for Multiple Choice Tests Using Concept Embeddings and Information Retrieval","abstract_clean":"Developing plausible distractors (wrong answer options) when writing multiple choice questions has been described as one of the most challenging and time consuming parts of the item writing process. In this paper we propose a fully automatic method for generating distractor suggestions for multiple choice questions used in high stakes medical exams. The system uses a question stem and the correct answer as an input and produces a list of suggested distractors ranked based on their similarity to the stem and the correct answer. To do this we use a novel approach of combining concept embeddings with information retrieval methods. We frame the evaluation as a prediction task where we aim to \"predict\" the human produced distractors used in large sets of medical questions, i.e. if a distractor generated by our system is good enough it is likely to feature among the list of distractors produced by the human item writers. The results reveal that combining concept embeddings with information retrieval approaches significantly improves the generation of plausible distractors and enables us to match around 1 in 5 of the human produced distractors. The approach proposed in this paper is generalisable to all scenarios where the distractors refer to concepts.","url":"https:\/\/aclanthology.org\/W18-0548"},{"ID":"hahn-powell-etal-2017-swanson","methods":["machine reading and knowledge assembly component","swanson linking","network graph visualization","search engine","modular approach"],"center_method":[null,null,null,"search engine",null],"tasks":["domain specific settings","reading","literature based discovery"],"center_task":[null,null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Swanson linking revisited: Accelerating literature-based discovery across domains using a conceptual influence graph. We introduce a modular approach for literature-based discovery consisting of a machine reading and knowledge assembly component that together produce a graph of influence relations (e.g., \"A promotes B\") from a collection of publications. A search engine is used to explore direct and indirect influence chains. Query results are substantiated with textual evidence, ranked according to their relevance, and presented in both a table-based view, as well as a network graph visualization. Our approach operates in both domain-specific settings, where there are knowledge bases and ontologies available to guide reading, and in multi-domain settings where such resources are absent. We demonstrate that this deep reading and search system reduces the effort needed to uncover \"undiscovered public knowledge\", and that with the aid of this tool a domain expert was able to drastically reduce her model building time from months to two days.","label":1,"title_clean":"Swanson linking revisited: Accelerating literature based discovery across domains using a conceptual influence graph","abstract_clean":"We introduce a modular approach for literature based discovery consisting of a machine reading and knowledge assembly component that together produce a graph of influence relations (e.g., \"A promotes B\") from a collection of publications. A search engine is used to explore direct and indirect influence chains. Query results are substantiated with textual evidence, ranked according to their relevance, and presented in both a table based view, as well as a network graph visualization. Our approach operates in both domain specific settings, where there are knowledge bases and ontologies available to guide reading, and in multi domain settings where such resources are absent. We demonstrate that this deep reading and search system reduces the effort needed to uncover \"undiscovered public knowledge\", and that with the aid of this tool a domain expert was able to drastically reduce her model building time from months to two days.","url":"https:\/\/aclanthology.org\/P17-4018"},{"ID":"hahn-schulz-2002-towards","methods":["ontology engineering methodology","evolving knowledge base","terminological classifier"],"center_method":[null,null,null],"tasks":["integrity checking","medical language processing"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"Towards Very Large Ontologies for Medical Language Processing. We describe an ontology engineering methodology by which conceptual knowledge is extracted from an informal medical thesaurus (UMLS) and automatically converted into a formal description logics system. Our approach consists of four steps: concept definitions are automatically generated from the UMLS source, integrity checking of taxonomic and partonomic hierarchies is performed by the terminological classifier, cycles and inconsistencies are eliminated, and incremental refinement of the evolving knowledge base is performed by a domain expert. We report on experiments with a knowledge base composed of 164,000 concepts and 76,000 relations.","label":1,"title_clean":"Towards Very Large Ontologies for Medical Language Processing","abstract_clean":"We describe an ontology engineering methodology by which conceptual knowledge is extracted from an informal medical thesaurus (UMLS) and automatically converted into a formal description logics system. Our approach consists of four steps: concept definitions are automatically generated from the UMLS source, integrity checking of taxonomic and partonomic hierarchies is performed by the terminological classifier, cycles and inconsistencies are eliminated, and incremental refinement of the evolving knowledge base is performed by a domain expert. We report on experiments with a knowledge base composed of 164,000 concepts and 76,000 relations.","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2002\/pdf\/36.pdf"},{"ID":"haldar-etal-2021-dsc","methods":["transformers","bio scheme","part of speech","fincausal 2021","attention based contextual representations","dsc iitism"],"center_method":["transformers",null,"part of speech",null,null,null],"tasks":["identifying causal relationships","cause effect pairs","question answering","market research","financial analysis","event prediction","information retrieval","financial documents","linguistics research","causality detection"],"center_task":[null,null,"question answering",null,null,null,"information retrieval",null,null,null],"Goal":["Decent Work and Economic Growth"],"text":"DSC-IITISM at FinCausal 2021: Combining POS tagging with Attention-based Contextual Representations for Identifying Causal Relationships in Financial Documents. Causality detection draws plenty of attention in the field of Natural Language Processing and linguistics research. It has essential applications in information retrieval, event prediction, question answering, financial analysis, and market research. In this study, we explore several methods to identify and extract cause-effect pairs in financial documents using transformers. For this purpose, we propose an approach that combines POS tagging with the BIO scheme, which can be integrated with modern transformer models to address this challenge of identifying causality in a given text. Our best methodology achieves an F1-Score of 0.9551, and an Exact Match Score of 0.8777 on the blind test in the FinCausal-2021 Shared Task at the FinCausal 2021 Workshop.","label":1,"title_clean":"DSC IITISM at FinCausal 2021: Combining POS tagging with Attention based Contextual Representations for Identifying Causal Relationships in Financial Documents","abstract_clean":"Causality detection draws plenty of attention in the field of Natural Language Processing and linguistics research. It has essential applications in information retrieval, event prediction, question answering, financial analysis, and market research. In this study, we explore several methods to identify and extract cause effect pairs in financial documents using transformers. For this purpose, we propose an approach that combines POS tagging with the BIO scheme, which can be integrated with modern transformer models to address this challenge of identifying causality in a given text. Our best methodology achieves an F1 Score of 0.9551, and an Exact Match Score of 0.8777 on the blind test in the FinCausal 2021 Shared Task at the FinCausal 2021 Workshop.","url":"https:\/\/aclanthology.org\/2021.fnp-1.8"},{"ID":"hamalainen-etal-2021-detecting","methods":["lstm"],"center_method":["lstm"],"tasks":["thai embeddings","detecting depression"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"Detecting Depression in Thai Blog Posts: a Dataset and a Baseline. We present the first openly available corpus for detecting depression in Thai. Our corpus is compiled by expert verified cases of depression in several online blogs. We experiment with two different LSTM based models and two different BERT based models. We achieve a 77.53% accuracy with a Thai BERT model in detecting depression. This establishes a good baseline for future researcher on the same corpus. Furthermore, we identify a need for Thai embeddings that have been trained on a more varied corpus than Wikipedia. Our corpus, code and trained models have been released openly on Zenodo.","label":1,"title_clean":"Detecting Depression in Thai Blog Posts: a Dataset and a Baseline","abstract_clean":"We present the first openly available corpus for detecting depression in Thai. Our corpus is compiled by expert verified cases of depression in several online blogs. We experiment with two different LSTM based models and two different BERT based models. We achieve a 77.53% accuracy with a Thai BERT model in detecting depression. This establishes a good baseline for future researcher on the same corpus. Furthermore, we identify a need for Thai embeddings that have been trained on a more varied corpus than Wikipedia. Our corpus, code and trained models have been released openly on Zenodo.","url":"https:\/\/aclanthology.org\/2021.wnut-1.3"},{"ID":"hamazono-etal-2021-unpredictable","methods":["text generating model"],"center_method":[null],"tasks":["data totext","market comment generation"],"center_task":[null,null],"Goal":["Decent Work and Economic Growth"],"text":"Unpredictable Attributes in Market Comment Generation. There are two types of datasets for data-totext: one uses raw data obtained in the real world, and the other is constructed artificially for a controlled task. A straightforwardly output text is generated from its paired input data for a manually constructed dataset because the dataset is well constructed without any excess or deficiencies. However, it may not be possible to generate a correct output text from the input data for a dataset constructed with realworld data and text. In such cases, we have to provide additional data, for example, data or text attribute labels, in order to generate the expected output text from the paired input. This paper discusses the importance of additional input labels in data-to-text for real-world data. The content and style of a market comment change depending on its medium, the market situation, and the time of day. However, as the stock price, which is the input data, does not contain any such aforementioned information, it cannot generate comments appropriately from the data alone. Therefore, we analyse the dataset and provide additional labels which are unpredictable with input data for the appropriate parts in the model. Thus, the accuracy of sentence generation is greatly improved compared to the case without the labels.The result suggests unpredictable attributes should be given as a part of the input in the training of the text generating model.","label":1,"title_clean":"Unpredictable Attributes in Market Comment Generation","abstract_clean":"There are two types of datasets for data totext: one uses raw data obtained in the real world, and the other is constructed artificially for a controlled task. A straightforwardly output text is generated from its paired input data for a manually constructed dataset because the dataset is well constructed without any excess or deficiencies. However, it may not be possible to generate a correct output text from the input data for a dataset constructed with realworld data and text. In such cases, we have to provide additional data, for example, data or text attribute labels, in order to generate the expected output text from the paired input. This paper discusses the importance of additional input labels in data to text for real world data. The content and style of a market comment change depending on its medium, the market situation, and the time of day. However, as the stock price, which is the input data, does not contain any such aforementioned information, it cannot generate comments appropriately from the data alone. Therefore, we analyse the dataset and provide additional labels which are unpredictable with input data for the appropriate parts in the model. Thus, the accuracy of sentence generation is greatly improved compared to the case without the labels.The result suggests unpredictable attributes should be given as a part of the input in the training of the text generating model.","url":"https:\/\/aclanthology.org\/2021.paclic-1.23"},{"ID":"hamon-etal-1998-step","methods":["exception rules"],"center_method":[null],"tasks":["detection of semantic variants of terms","structuration of terminologies"],"center_task":[null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"A step towards the detection of semantic variants of terms in technical documents. This paper reports the results of a preliminary experiment on the detection of semantic variants of terms in a French technical document. The general goal of our work is to help the structuration of terminologies. Two kinds of semantic variants can be found in traditional terminologies : strict synonymy links and fuzzier relations like see-also. We have designed three rules which exploit general dictionary information to infer synonymy relations between complex candidate terms. The results have been examined by a human terminologist. The expert has judged that half of the overall pairs of terms are relevant for the semantic variation. He validated an important part of the detected links as synonymy. Moreover, it appeared that numerous errors are due to few misinterpreted links: they could be eliminated by few exception rules.","label":1,"title_clean":"A step towards the detection of semantic variants of terms in technical documents","abstract_clean":"This paper reports the results of a preliminary experiment on the detection of semantic variants of terms in a French technical document. The general goal of our work is to help the structuration of terminologies. Two kinds of semantic variants can be found in traditional terminologies : strict synonymy links and fuzzier relations like see also. We have designed three rules which exploit general dictionary information to infer synonymy relations between complex candidate terms. The results have been examined by a human terminologist. The expert has judged that half of the overall pairs of terms are relevant for the semantic variation. He validated an important part of the detected links as synonymy. Moreover, it appeared that numerous errors are due to few misinterpreted links: they could be eliminated by few exception rules.","url":"https:\/\/aclanthology.org\/C98-1079"},{"ID":"hamoui-etal-2020-flodusta","methods":["event detection system","flodusta"],"center_method":[null,null],"tasks":["flood dust storm","classification","arabic event detection","annotation"],"center_task":[null,"classification",null,"annotation"],"Goal":["Peace, Justice and Strong Institutions","Climate Action"],"text":"FloDusTA: Saudi Tweets Dataset for Flood, Dust Storm, and Traffic Accident Events. The rise of social media platforms makes it a valuable information source of recent events and users' perspective towards them. Twitter has been one of the most important communication platforms in recent years. Event detection, one of the information extraction aspects, involves identifying specified types of events in the text. Detecting events from tweets can help to predict real-world events precisely. A serious challenge that faces Arabic event detection is the lack of Arabic datasets that can be exploited in detecting events. This paper will describe FloDusTA, which is a dataset of tweets that we have built for the purpose of developing an event detection system. The dataset contains tweets written in both Modern Standard Arabic and Saudi dialect. The process of building the dataset starting from tweets collection to annotation by human annotators will be present. The tweets are labeled with four labels: flood, dust storm, traffic accident, and non-event. The dataset was tested for classification and the result was strongly encouraging.","label":1,"title_clean":"FloDusTA: Saudi Tweets Dataset for Flood, Dust Storm, and Traffic Accident Events","abstract_clean":"The rise of social media platforms makes it a valuable information source of recent events and users' perspective towards them. Twitter has been one of the most important communication platforms in recent years. Event detection, one of the information extraction aspects, involves identifying specified types of events in the text. Detecting events from tweets can help to predict real world events precisely. A serious challenge that faces Arabic event detection is the lack of Arabic datasets that can be exploited in detecting events. This paper will describe FloDusTA, which is a dataset of tweets that we have built for the purpose of developing an event detection system. The dataset contains tweets written in both Modern Standard Arabic and Saudi dialect. The process of building the dataset starting from tweets collection to annotation by human annotators will be present. The tweets are labeled with four labels: flood, dust storm, traffic accident, and non event. The dataset was tested for classification and the result was strongly encouraging.","url":"https:\/\/aclanthology.org\/2020.lrec-1.174"},{"ID":"hardalov-etal-2021-cross","methods":["domain adaption","label embeddings","mixture of experts","domain adversarial training","end to end unsupervised framework","annotation protocol"],"center_method":["domain adaption",null,null,null,null,null],"tasks":["stance detection","cross domain learning","classification","outof domain prediction of unseen user defined labels","cross domain studies","cross domain label adaptive stance detection"],"center_task":["stance detection",null,"classification",null,null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Cross-Domain Label-Adaptive Stance Detection. Stance detection concerns the classification of a writer's viewpoint towards a target. There are different task variants, e.g., stance of a tweet vs. a full article, or stance with respect to a claim vs. an (implicit) topic. Moreover, task definitions vary, which includes the label inventory, the data collection, and the annotation protocol. All these aspects hinder cross-domain studies, as they require changes to standard domain adaptation approaches. In this paper, we perform an in-depth analysis of 16 stance detection datasets, and we explore the possibility for cross-domain learning from them. Moreover, we propose an end-to-end unsupervised framework for outof-domain prediction of unseen, user-defined labels. In particular, we combine domain adaptation techniques such as mixture of experts and domain-adversarial training with label embeddings, and we demonstrate sizable performance gains over strong baselines, both (i) indomain, i.e., for seen targets, and (ii) out-ofdomain, i.e., for unseen targets. Finally, we perform an exhaustive analysis of the crossdomain results, and we highlight the important factors influencing the model performance.","label":1,"title_clean":"Cross Domain Label Adaptive Stance Detection","abstract_clean":"Stance detection concerns the classification of a writer's viewpoint towards a target. There are different task variants, e.g., stance of a tweet vs. a full article, or stance with respect to a claim vs. an (implicit) topic. Moreover, task definitions vary, which includes the label inventory, the data collection, and the annotation protocol. All these aspects hinder cross domain studies, as they require changes to standard domain adaptation approaches. In this paper, we perform an in depth analysis of 16 stance detection datasets, and we explore the possibility for cross domain learning from them. Moreover, we propose an end to end unsupervised framework for outof domain prediction of unseen, user defined labels. In particular, we combine domain adaptation techniques such as mixture of experts and domain adversarial training with label embeddings, and we demonstrate sizable performance gains over strong baselines, both (i) indomain, i.e., for seen targets, and (ii) out ofdomain, i.e., for unseen targets. Finally, we perform an exhaustive analysis of the crossdomain results, and we highlight the important factors influencing the model performance.","url":"https:\/\/aclanthology.org\/2021.emnlp-main.710"},{"ID":"harkema-etal-2004-large-scale","methods":["termino","finite state machines","relational database"],"center_method":[null,null,null],"tasks":["text processing","large scale terminological resource","biomedical applications","term lookup","term recognition","information extraction","language processing applications","biomedical text processing"],"center_task":[null,null,null,null,null,"information extraction",null,null],"Goal":["Good Health and Well-Being"],"text":"A Large Scale Terminology Resource for Biomedical Text Processing. In this paper we discuss the design, implementation, and use of Termino, a large scale terminological resource for text processing. Dealing with terminology is a difficult but unavoidable task for language processing applications, such as Information Extraction in technical domains. Complex, heterogeneous information must be stored about large numbers of terms. At the same time term recognition must be performed in realistic times. Termino attempts to reconcile this tension by maintaining a flexible, extensible relational database for storing terminological information and compiling finite state machines from this database to do term lookup. While Termino has been developed for biomedical applications, its general design allows it to be used for term processing in any domain.","label":1,"title_clean":"A Large Scale Terminology Resource for Biomedical Text Processing","abstract_clean":"In this paper we discuss the design, implementation, and use of Termino, a large scale terminological resource for text processing. Dealing with terminology is a difficult but unavoidable task for language processing applications, such as Information Extraction in technical domains. Complex, heterogeneous information must be stored about large numbers of terms. At the same time term recognition must be performed in realistic times. Termino attempts to reconcile this tension by maintaining a flexible, extensible relational database for storing terminological information and compiling finite state machines from this database to do term lookup. While Termino has been developed for biomedical applications, its general design allows it to be used for term processing in any domain.","url":"https:\/\/aclanthology.org\/W04-3110"},{"ID":"hartvigsen-etal-2022-toxigen","methods":["adversarial classifier in the loop decoding method","toxi gen","fine tuning","language models","toxicity classifier"],"center_method":[null,null,"fine tuning","language models",null],"tasks":["toxic spans detection","machine generated toxicity"],"center_task":["toxic spans detection",null],"Goal":["Peace, Justice and Strong Institutions"],"text":"ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection. Toxic language detection systems often falsely flag text that contains minority group mentions as toxic, as those groups are often the targets of online hate. Such over-reliance on spurious correlations also causes systems to struggle with detecting implicitly toxic language. To help mitigate these issues, we create TOXIGEN, a new large-scale and machinegenerated dataset of 274k toxic and benign statements about 13 minority groups. We develop a demonstration-based prompting framework and an adversarial classifier-in-the-loop decoding method to generate subtly toxic and benign text with a massive pretrained language model (Brown et al., 2020). Controlling machine generation in this way allows TOXIGEN to cover implicitly toxic text at a larger scale, and about more demographic groups, than previous resources of human-written text. We conduct a human evaluation on a challenging subset of TOXIGEN and find that annotators struggle to distinguish machine-generated text from human-written language. We also find that 94.5% of toxic examples are labeled as hate speech by human annotators. Using three publicly-available datasets, we show that finetuning a toxicity classifier on our data improves its performance on human-written data substantially. We also demonstrate that TOXI-GEN can be used to fight machine-generated toxicity as finetuning improves the classifier significantly on our evaluation subset.","label":1,"title_clean":"ToxiGen: A Large Scale Machine Generated Dataset for Adversarial and Implicit Hate Speech Detection","abstract_clean":"Toxic language detection systems often falsely flag text that contains minority group mentions as toxic, as those groups are often the targets of online hate. Such over reliance on spurious correlations also causes systems to struggle with detecting implicitly toxic language. To help mitigate these issues, we create TOXIGEN, a new large scale and machinegenerated dataset of 274k toxic and benign statements about 13 minority groups. We develop a demonstration based prompting framework and an adversarial classifier in the loop decoding method to generate subtly toxic and benign text with a massive pretrained language model (Brown et al., 2020). Controlling machine generation in this way allows TOXIGEN to cover implicitly toxic text at a larger scale, and about more demographic groups, than previous resources of human written text. We conduct a human evaluation on a challenging subset of TOXIGEN and find that annotators struggle to distinguish machine generated text from human written language. We also find that 94.5% of toxic examples are labeled as hate speech by human annotators. Using three publicly available datasets, we show that finetuning a toxicity classifier on our data improves its performance on human written data substantially. We also demonstrate that TOXI GEN can be used to fight machine generated toxicity as finetuning improves the classifier significantly on our evaluation subset.","url":"https:\/\/aclanthology.org\/2022.acl-long.234.pdf"},{"ID":"hasan-ng-2014-taking","methods":["computational models"],"center_method":["computational models"],"tasks":["classification","classifying reasons","stance detection"],"center_task":["classification",null,"stance detection"],"Goal":["Peace, Justice and Strong Institutions"],"text":"Why are You Taking this Stance? Identifying and Classifying Reasons in Ideological Debates. Recent years have seen a surge of interest in stance classification in online debates. Oftentimes, however, it is important to determine not only the stance expressed by an author in her debate posts, but also the reasons behind her supporting or opposing the issue under debate. We therefore examine the new task of reason classification in this paper. Given the close interplay between stance classification and reason classification, we design computational models for examining how automatically computed stance information can be profitably exploited for reason classification. Experiments on our reason-annotated corpus of ideological debate posts from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classification results than their simpler counterparts.","label":1,"title_clean":"Why are You Taking this Stance? Identifying and Classifying Reasons in Ideological Debates","abstract_clean":"Recent years have seen a surge of interest in stance classification in online debates. Oftentimes, however, it is important to determine not only the stance expressed by an author in her debate posts, but also the reasons behind her supporting or opposing the issue under debate. We therefore examine the new task of reason classification in this paper. Given the close interplay between stance classification and reason classification, we design computational models for examining how automatically computed stance information can be profitably exploited for reason classification. Experiments on our reason annotated corpus of ideological debate posts from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classification results than their simpler counterparts.","url":"https:\/\/aclanthology.org\/D14-1083"},{"ID":"hassan-etal-2008-tracking","methods":["graph based techniques"],"center_method":[null],"tasks":["dynamic evolution of participants salience"],"center_task":[null],"Goal":["Partnership for the Goals"],"text":"Tracking the Dynamic Evolution of Participants Salience in a Discussion. We introduce a technique for analyzing the temporal evolution of the salience of participants in a discussion. Our method can dynamically track how the relative importance of speakers evolve over time using graph based techniques. Speaker salience is computed based on the eigenvector centrality in a graph representation of participants in a discussion. Two participants in a discussion are linked with an edge if they use similar rhetoric. The method is dynamic in the sense that the graph evolves over time to capture the evolution inherent to the participants salience. We used our method to track the salience of members of the US Senate using data from the US Congressional Record. Our analysis investigated how the salience of speakers changes over time. Our results show that the scores can capture speaker centrality in topics as well as events that result in change of salience or influence among different participants.","label":1,"title_clean":"Tracking the Dynamic Evolution of Participants Salience in a Discussion","abstract_clean":"We introduce a technique for analyzing the temporal evolution of the salience of participants in a discussion. Our method can dynamically track how the relative importance of speakers evolve over time using graph based techniques. Speaker salience is computed based on the eigenvector centrality in a graph representation of participants in a discussion. Two participants in a discussion are linked with an edge if they use similar rhetoric. The method is dynamic in the sense that the graph evolves over time to capture the evolution inherent to the participants salience. We used our method to track the salience of members of the US Senate using data from the US Congressional Record. Our analysis investigated how the salience of speakers changes over time. Our results show that the scores can capture speaker centrality in topics as well as events that result in change of salience or influence among different participants.","url":"https:\/\/aclanthology.org\/C08-1040"},{"ID":"hassanali-liu-2011-measuring","methods":["statistical systems","rule based systems"],"center_method":[null,null],"tasks":["measuring language development","automatic grammar checking","automatic detection of 6 types of verb related grammatical errors","language sample analysis","early childhood education","child language transcripts"],"center_task":[null,null,null,null,null,null],"Goal":["Quality Education"],"text":"Measuring Language Development in Early Childhood Education: A Case Study of Grammar Checking in Child Language Transcripts. Language sample analysis is an important technique used in measuring language development. At present, measures of grammatical complexity such as the Index of Productive Syntax (Scarborough, 1990) are used to measure language development in early childhood. Although these measures depict the overall competence in the usage of language, they do not provide for an analysis of the grammatical mistakes made by the child. In this paper, we explore the use of existing Natural Language Processing (NLP) techniques to provide an insight into the processing of child language transcripts and challenges in automatic grammar checking. We explore the automatic detection of 6 types of verb related grammatical errors. We compare rule based systems to statistical systems and investigate the use of different features. We found the statistical systems performed better than the rule based systems for most of the error categories.","label":1,"title_clean":"Measuring Language Development in Early Childhood Education: A Case Study of Grammar Checking in Child Language Transcripts","abstract_clean":"Language sample analysis is an important technique used in measuring language development. At present, measures of grammatical complexity such as the Index of Productive Syntax (Scarborough, 1990) are used to measure language development in early childhood. Although these measures depict the overall competence in the usage of language, they do not provide for an analysis of the grammatical mistakes made by the child. In this paper, we explore the use of existing Natural Language Processing (NLP) techniques to provide an insight into the processing of child language transcripts and challenges in automatic grammar checking. We explore the automatic detection of 6 types of verb related grammatical errors. We compare rule based systems to statistical systems and investigate the use of different features. We found the statistical systems performed better than the rule based systems for most of the error categories.","url":"https:\/\/aclanthology.org\/W11-1411"},{"ID":"hede-etal-2021-toxicity","methods":["jigsaw perspective api","perspective","similar models"],"center_method":[null,null,null],"tasks":["detecting online incivility","error analysis"],"center_task":[null,"error analysis"],"Goal":["Peace, Justice and Strong Institutions"],"text":"From Toxicity in Online Comments to Incivility in American News: Proceed with Caution. The ability to quantify incivility online, in news and in congressional debates, is of great interest to political scientists. Computational tools for detecting online incivility for English are now fairly accessible and potentially could be applied more broadly. We test the Jigsaw Perspective API for its ability to detect the degree of incivility on a corpus that we developed, consisting of manual annotations of civility in American news. We demonstrate that toxicity models, as exemplified by Perspective, are inadequate for the analysis of incivility in news. We carry out error analysis that points to the need to develop methods to remove spurious correlations between words often mentioned in the news, especially identity descriptors and incivility. Without such improvements, applying Perspective or similar models on news is likely to lead to wrong conclusions, that are not aligned with the human perception of incivility.","label":1,"title_clean":"From Toxicity in Online Comments to Incivility in American News: Proceed with Caution","abstract_clean":"The ability to quantify incivility online, in news and in congressional debates, is of great interest to political scientists. Computational tools for detecting online incivility for English are now fairly accessible and potentially could be applied more broadly. We test the Jigsaw Perspective API for its ability to detect the degree of incivility on a corpus that we developed, consisting of manual annotations of civility in American news. We demonstrate that toxicity models, as exemplified by Perspective, are inadequate for the analysis of incivility in news. We carry out error analysis that points to the need to develop methods to remove spurious correlations between words often mentioned in the news, especially identity descriptors and incivility. Without such improvements, applying Perspective or similar models on news is likely to lead to wrong conclusions, that are not aligned with the human perception of incivility.","url":"https:\/\/aclanthology.org\/2021.eacl-main.225"},{"ID":"hempelmann-etal-2005-evaluating","methods":["parsers","treebank style parsers"],"center_method":[null,null],"tasks":["learning technology","parsing evaluation","coh metrix and other learning technology environments","measuring text cohesion"],"center_task":[null,null,null,null],"Goal":["Quality Education"],"text":"Evaluating State-of-the-Art Treebank-style Parsers for Coh-Metrix and Other Learning Technology Environments. This paper evaluates a series of freely available, state-of-the-art parsers on a standard benchmark as well as with respect to a set of data relevant for measuring text cohesion. We outline advantages and disadvantages of existing technologies and make recommendations. Our performance report uses traditional measures based on a gold standard as well as novel dimensions for parsing evaluation. To our knowledge this is the first attempt to evaluate parsers accross genres and grade levels for the implementation in learning technology.","label":1,"title_clean":"Evaluating State of the Art Treebank style Parsers for Coh Metrix and Other Learning Technology Environments","abstract_clean":"This paper evaluates a series of freely available, state of the art parsers on a standard benchmark as well as with respect to a set of data relevant for measuring text cohesion. We outline advantages and disadvantages of existing technologies and make recommendations. Our performance report uses traditional measures based on a gold standard as well as novel dimensions for parsing evaluation. To our knowledge this is the first attempt to evaluate parsers accross genres and grade levels for the implementation in learning technology.","url":"https:\/\/aclanthology.org\/W05-0211"},{"ID":"henriksson-etal-2013-corpus","methods":["distributional methods"],"center_method":[null],"tasks":["corpus - driven terminology development"],"center_task":[null],"Goal":["Good Health and Well-Being"],"text":"Corpus-Driven Terminology Development: Populating Swedish SNOMED CT with Synonyms Extracted from Electronic Health Records. The various ways in which one can refer to the same clinical concept needs to be accounted for in a semantic resource such as SNOMED CT. Developing terminological resources manually is, however, prohibitively expensive and likely to result in low coverage, especially given the high variability of language use in clinical text. To support this process, distributional methods can be employed in conjunction with a large corpus of electronic health records to extract synonym candidates for clinical terms. In this paper, we exemplify the potential of our proposed method using the Swedish version of SNOMED CT, which currently lacks synonyms. A medical expert inspects two thousand term pairs generated by two semantic spacesone of which models multiword terms in addition to single words-for one hundred preferred terms of the semantic types disorder and finding.","label":1,"title_clean":"Corpus Driven Terminology Development: Populating Swedish SNOMED CT with Synonyms Extracted from Electronic Health Records","abstract_clean":"The various ways in which one can refer to the same clinical concept needs to be accounted for in a semantic resource such as SNOMED CT. Developing terminological resources manually is, however, prohibitively expensive and likely to result in low coverage, especially given the high variability of language use in clinical text. To support this process, distributional methods can be employed in conjunction with a large corpus of electronic health records to extract synonym candidates for clinical terms. In this paper, we exemplify the potential of our proposed method using the Swedish version of SNOMED CT, which currently lacks synonyms. A medical expert inspects two thousand term pairs generated by two semantic spacesone of which models multiword terms in addition to single words for one hundred preferred terms of the semantic types disorder and finding.","url":"https:\/\/aclanthology.org\/W13-1905"},{"ID":"herzig-etal-2016-classifying","methods":["emotional techniques","service agents"],"center_method":[null,null],"tasks":["classifying emotions","customer support dialogues","social media","customer support","automatic detection and analysis of the emotions"],"center_task":[null,null,null,null,null],"Goal":["Industry, Innovation and Infrastrucure","Decent Work and Economic Growth"],"text":"Classifying Emotions in Customer Support Dialogues in Social Media. Providing customer support through social media channels is gaining increasing popularity. In such a context, automatic detection and analysis of the emotions expressed by customers is important, as is identification of the emotional techniques (e.g., apology, empathy, etc.) in the responses of customer service agents. Result of such an analysis can help assess the quality of such a service, help and inform agents about desirable responses, and help develop automated service agents for social media interactions. In this paper, we show that, in addition to text based turn features, dialogue features can significantly improve detection of emotions in social media customer service dialogues and help predict emotional techniques used by customer service agents. Got excited to pick up the latest bundle since it was on sale today, but now I can't download it at all. Bummer. =\/ Yeah, no problems there. The error is coming when I actually try to download the games. Error code: 412344 Uh oh! To check, were you able to purchase that title? Let's confirm by signing in at http:\/\/t.co\/53fsdfd real quick. Appreciate that! Let's power cycle and unplug modem\/router for 2 mins then try again. Seems to be working now. Weird. I tried that 3 different times earlier. Thanks. Odd, but glad to hear that's sorted! Happy gaming, and we'll be here to help if any other questions or concerns arise.","label":1,"title_clean":"Classifying Emotions in Customer Support Dialogues in Social Media","abstract_clean":"Providing customer support through social media channels is gaining increasing popularity. In such a context, automatic detection and analysis of the emotions expressed by customers is important, as is identification of the emotional techniques (e.g., apology, empathy, etc.) in the responses of customer service agents. Result of such an analysis can help assess the quality of such a service, help and inform agents about desirable responses, and help develop automated service agents for social media interactions. In this paper, we show that, in addition to text based turn features, dialogue features can significantly improve detection of emotions in social media customer service dialogues and help predict emotional techniques used by customer service agents. Got excited to pick up the latest bundle since it was on sale today, but now I can't download it at all. Bummer. =\/ Yeah, no problems there. The error is coming when I actually try to download the games. Error code: 412344 Uh oh! To check, were you able to purchase that title? Let's confirm by signing in at http:\/\/t.co\/53fsdfd real quick. Appreciate that! Let's power cycle and unplug modem\/router for 2 mins then try again. Seems to be working now. Weird. I tried that 3 different times earlier. Thanks. Odd, but glad to hear that's sorted! Happy gaming, and we'll be here to help if any other questions or concerns arise.","url":"https:\/\/aclanthology.org\/W16-3609"},{"ID":"hieu-etal-2020-reintel","methods":["phobert","ensemble methods"],"center_method":[null,"ensemble methods"],"tasks":["vietnamese fake news detection","vietnamese language and speech processing"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"ReINTEL Challenge 2020: Vietnamese Fake News Detection usingEnsemble Model with PhoBERT embeddings. Along with the increasing traffic of social networks in Vietnam in recent years, the number of unreliable news has also grown rapidly. As we make decisions based on the information we come across daily, fake news, depending on the severity of the matter, can lead to disastrous consequences. This paper presents our approach for the Fake News Detection on Social Network Sites (SNSs), using an ensemble method with linguistic features extracted using PhoBERT (Nguyen and Nguyen, 2020). Our method achieves AUC score of 0.9521 and got 1 st place on the private test at the 7 th International Workshop on Vietnamese Language and Speech Processing (VLSP). For reproducing the result, the code can be found at https:\/\/gitlab.com\/thuan.","label":1,"title_clean":"ReINTEL Challenge 2020: Vietnamese Fake News Detection usingEnsemble Model with PhoBERT embeddings","abstract_clean":"Along with the increasing traffic of social networks in Vietnam in recent years, the number of unreliable news has also grown rapidly. As we make decisions based on the information we come across daily, fake news, depending on the severity of the matter, can lead to disastrous consequences. This paper presents our approach for the Fake News Detection on Social Network Sites (SNSs), using an ensemble method with linguistic features extracted using PhoBERT (Nguyen and Nguyen, 2020). Our method achieves AUC score of 0.9521 and got 1 st place on the private test at the 7 th International Workshop on Vietnamese Language and Speech Processing (VLSP). For reproducing the result, the code can be found at https:\/\/gitlab.com\/thuan.","url":"https:\/\/aclanthology.org\/2020.vlsp-1.1.pdf"},{"ID":"hirschman-etal-2001-integrated","methods":["integrated feasibility experiment"],"center_method":[null],"tasks":["tides demonstration","bio security","monitoring infectious disease outbreaks","biological threats","ife bio"],"center_task":[null,null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"Integrated Feasibility Experiment for Bio-Security: IFE-Bio, A TIDES Demonstration. As part of MITRE's work under the DARPA TIDES (Translingual Information Detection, Extraction and Summarization) program, we are preparing a series of demonstrations to showcase the TIDES Integrated Feasibility Experiment on Bio-Security (IFE-Bio). The current demonstration illustrates some of the resources that can be made available to analysts tasked with monitoring infectious disease outbreaks and other biological threats.","label":1,"title_clean":"Integrated Feasibility Experiment for Bio Security: IFE Bio, A TIDES Demonstration","abstract_clean":"As part of MITRE's work under the DARPA TIDES (Translingual Information Detection, Extraction and Summarization) program, we are preparing a series of demonstrations to showcase the TIDES Integrated Feasibility Experiment on Bio Security (IFE Bio). The current demonstration illustrates some of the resources that can be made available to analysts tasked with monitoring infectious disease outbreaks and other biological threats.","url":"https:\/\/aclanthology.org\/H01-1038"},{"ID":"hoang-kan-2010-towards","methods":["multi document summarization baselines","rewos"],"center_method":[null,null],"tasks":["text summarization","human evaluation","extractive summary"],"center_task":["text summarization","human evaluation",null],"Goal":["Quality Education","Industry, Innovation and Infrastrucure"],"text":"Towards Automated Related Work Summarization. We introduce the novel problem of automatic related work summarization. Given multiple articles (e.g., conference\/journal papers) as input, a related work summarization system creates a topic-biased summary of related work specific to the target paper. Our prototype Related Work Summarization system, ReWoS, takes in set of keywords arranged in a hierarchical fashion that describes a target paper's topics, to drive the creation of an extractive summary using two different strategies for locating appropriate sentences for general topics as well as detailed ones. Our initial results show an improvement over generic multi-document summarization baselines in a human evaluation.","label":1,"title_clean":"Towards Automated Related Work Summarization","abstract_clean":"We introduce the novel problem of automatic related work summarization. Given multiple articles (e.g., conference\/journal papers) as input, a related work summarization system creates a topic biased summary of related work specific to the target paper. Our prototype Related Work Summarization system, ReWoS, takes in set of keywords arranged in a hierarchical fashion that describes a target paper's topics, to drive the creation of an extractive summary using two different strategies for locating appropriate sentences for general topics as well as detailed ones. Our initial results show an improvement over generic multi document summarization baselines in a human evaluation.","url":"https:\/\/aclanthology.org\/C10-2049.pdf"},{"ID":"holderness-etal-2018-analysis","methods":["topic extraction","topic extraction model","psychiatric readmission prediction model"],"center_method":[null,null,null],"tasks":["analysis of risk factor domains","topic extraction"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"Analysis of Risk Factor Domains in Psychosis Patient Health Records. Readmission after discharge from a hospital is disruptive and costly, regardless of the reason. However, it can be particularly problematic for psychiatric patients, so predicting which patients may be readmitted is critically important but also very difficult. Clinical narratives in psychiatric electronic health records (EHRs) span a wide range of topics and vocabulary; therefore, a psychiatric readmission prediction model must begin with a robust and interpretable topic extraction component. We created a data pipeline for using document vector similarity metrics to perform topic extraction on psychiatric EHR data in service of our long-term goal of creating a readmission risk classifier. We show initial results for our topic extraction model and identify additional features we will be incorporating in the future.","label":1,"title_clean":"Analysis of Risk Factor Domains in Psychosis Patient Health Records","abstract_clean":"Readmission after discharge from a hospital is disruptive and costly, regardless of the reason. However, it can be particularly problematic for psychiatric patients, so predicting which patients may be readmitted is critically important but also very difficult. Clinical narratives in psychiatric electronic health records (EHRs) span a wide range of topics and vocabulary; therefore, a psychiatric readmission prediction model must begin with a robust and interpretable topic extraction component. We created a data pipeline for using document vector similarity metrics to perform topic extraction on psychiatric EHR data in service of our long term goal of creating a readmission risk classifier. We show initial results for our topic extraction model and identify additional features we will be incorporating in the future.","url":"https:\/\/aclanthology.org\/W18-5615"},{"ID":"honovich-etal-2022-true","methods":["meta evaluation protocols","true"],"center_method":[null,null],"tasks":["natural language generation","text summarization","inference","model and metric developers","automatic factual consistency evaluation","evaluation cycles"],"center_task":["natural language generation","text summarization","inference",null,null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"TRUE: Re-evaluating Factual Consistency Evaluation. Grounded text generation systems often generate text that contains factual inconsistencies, hindering their real-world applicability. Automatic factual consistency evaluation may help alleviate this limitation by accelerating evaluation cycles, filtering inconsistent outputs and augmenting training data. While attracting increasing attention, such evaluation metrics are usually developed and evaluated in silo for a single task or dataset, slowing their adoption. Moreover, previous meta-evaluation protocols focused on system-level correlations with human annotations, which leave the examplelevel accuracy of such metrics unclear. In this work, we introduce TRUE: a comprehensive study of factual consistency metrics on a standardized collection of existing texts from diverse tasks, manually annotated for factual consistency. Our standardization enables an example-level meta-evaluation protocol that is more actionable and interpretable than previously reported correlations, yielding clearer quality measures. Across diverse stateof-the-art metrics and 11 datasets we find that large-scale NLI and question generation-andanswering-based approaches achieve strong and complementary results. We recommend those methods as a starting point for model and metric developers, and hope TRUE will foster progress towards even better methods. 1 * Work done during an internship at Google Research. 1 Our code will be made publicly available. Summarization (Wang et al., 2020) Input Phyllis schlafly, a leading figure in the us conservative movement, has died at her home in missouri, aged 92... Summary Us conservative activist phyllis schlafly has died at the age of 87.","label":1,"title_clean":"TRUE: Re evaluating Factual Consistency Evaluation","abstract_clean":"Grounded text generation systems often generate text that contains factual inconsistencies, hindering their real world applicability. Automatic factual consistency evaluation may help alleviate this limitation by accelerating evaluation cycles, filtering inconsistent outputs and augmenting training data. While attracting increasing attention, such evaluation metrics are usually developed and evaluated in silo for a single task or dataset, slowing their adoption. Moreover, previous meta evaluation protocols focused on system level correlations with human annotations, which leave the examplelevel accuracy of such metrics unclear. In this work, we introduce TRUE: a comprehensive study of factual consistency metrics on a standardized collection of existing texts from diverse tasks, manually annotated for factual consistency. Our standardization enables an example level meta evaluation protocol that is more actionable and interpretable than previously reported correlations, yielding clearer quality measures. Across diverse stateof the art metrics and 11 datasets we find that large scale NLI and question generation andanswering based approaches achieve strong and complementary results. We recommend those methods as a starting point for model and metric developers, and hope TRUE will foster progress towards even better methods. 1 * Work done during an internship at Google Research. 1 Our code will be made publicly available. Summarization (Wang et al., 2020) Input Phyllis schlafly, a leading figure in the us conservative movement, has died at her home in missouri, aged 92... Summary Us conservative activist phyllis schlafly has died at the age of 87.","url":"https:\/\/aclanthology.org\/2022.dialdoc-1.19"},{"ID":"hope-etal-2021-extracting","methods":["pubmed search","knowledge base of mechanisms","deep neural network","query mechanism","covid 19 containment measures","kb","unified schema"],"center_method":[null,null,"deep neural network",null,null,null,null],"tasks":["destruction of sars cov","temperature increase","antiviral candidate drugs","food price inflation","interdisciplinary scientific search","cellular processes"],"center_task":[null,null,null,null,null,null],"Goal":["Good Health and Well-Being","Industry, Innovation and Infrastrucure"],"text":"Extracting a Knowledge Base of Mechanisms from COVID-19 Papers. The COVID-19 pandemic has spawned a diverse body of scientific literature that is challenging to navigate, stimulating interest in automated tools to help find useful knowledge. We pursue the construction of a knowledge base (KB) of mechanisms-a fundamental concept across the sciences, which encompasses activities, functions and causal relations, ranging from cellular processes to economic impacts. We extract this information from the natural language of scientific papers by developing a broad, unified schema that strikes a balance between relevance and breadth. We annotate a dataset of mechanisms with our schema and train a model to extract mechanism relations from papers. Our experiments demonstrate the utility of our KB in supporting interdisciplinary scientific search over COVID-19 literature, outperforming the prominent PubMed search in a study with clinical experts. Our search engine, dataset and code are publicly available. 1 * * Equal contribution. 1 https:\/\/covidmechanisms.apps.allenai.org\/ \u2026 a deep learning framework for design of antiviral candidate drugs Temperature increase can facilitate the destruction of SARS-COV-2 gpl16 antiserum blocks binding of virions to cellular receptors ...food price inflation is an unintended consequence of COVID-19 containment measures Retrieved from CORD-19 papers Ent1: deep learning Ent2: drugs Query mechanism relations","label":1,"title_clean":"Extracting a Knowledge Base of Mechanisms from COVID 19 Papers","abstract_clean":"The COVID 19 pandemic has spawned a diverse body of scientific literature that is challenging to navigate, stimulating interest in automated tools to help find useful knowledge. We pursue the construction of a knowledge base (KB) of mechanisms a fundamental concept across the sciences, which encompasses activities, functions and causal relations, ranging from cellular processes to economic impacts. We extract this information from the natural language of scientific papers by developing a broad, unified schema that strikes a balance between relevance and breadth. We annotate a dataset of mechanisms with our schema and train a model to extract mechanism relations from papers. Our experiments demonstrate the utility of our KB in supporting interdisciplinary scientific search over COVID 19 literature, outperforming the prominent PubMed search in a study with clinical experts. Our search engine, dataset and code are publicly available. 1 * * Equal contribution. 1 https:\/\/covidmechanisms.apps.allenai.org\/ \u2026 a deep learning framework for design of antiviral candidate drugs Temperature increase can facilitate the destruction of SARS COV 2 gpl16 antiserum blocks binding of virions to cellular receptors ...food price inflation is an unintended consequence of COVID 19 containment measures Retrieved from CORD 19 papers Ent1: deep learning Ent2: drugs Query mechanism relations","url":"https:\/\/aclanthology.org\/2021.naacl-main.355.pdf"},{"ID":"hopkins-etal-2017-beyond","methods":["tree transducer cascade","eu clid","euclid","sentential semantic parsing"],"center_method":[null,null,null,null],"tasks":["verb interpretation","coreference resolution","math sat"],"center_task":[null,"coreference resolution",null],"Goal":["Quality Education"],"text":"Beyond Sentential Semantic Parsing: Tackling the Math SAT with a Cascade of Tree Transducers. We present an approach for answering questions that span multiple sentences and exhibit sophisticated cross-sentence anaphoric phenomena, evaluating on a rich source of such questions-the math portion of the Scholastic Aptitude Test (SAT). By using a tree transducer cascade as its basic architecture, our system (called EU-CLID) propagates uncertainty from multiple sources (e.g. coreference resolution or verb interpretation) until it can be confidently resolved. Experiments show the first-ever results (43% recall and 91% precision) on SAT algebra word problems. We also apply EUCLID to the public Dolphin algebra question set, and improve the state-of-the-art F 1-score from 73.9% to 77.0%.","label":1,"title_clean":"Beyond Sentential Semantic Parsing: Tackling the Math SAT with a Cascade of Tree Transducers","abstract_clean":"We present an approach for answering questions that span multiple sentences and exhibit sophisticated cross sentence anaphoric phenomena, evaluating on a rich source of such questions the math portion of the Scholastic Aptitude Test (SAT). By using a tree transducer cascade as its basic architecture, our system (called EU CLID) propagates uncertainty from multiple sources (e.g. coreference resolution or verb interpretation) until it can be confidently resolved. Experiments show the first ever results (43% recall and 91% precision) on SAT algebra word problems. We also apply EUCLID to the public Dolphin algebra question set, and improve the state of the art F 1 score from 73.9% to 77.0%.","url":"https:\/\/aclanthology.org\/D17-1083.pdf"},{"ID":"hossain-etal-2021-nlp-cuet","methods":["ensemble methods","roberta","xlnet","transformer m bert","logistic regression","vector machine","nlp cuetlt edi eacl2021","bert","machine learning methods","cross lingual representation learner","deep neural network","neural network"],"center_method":["ensemble methods","roberta","xlnet",null,"logistic regression",null,null,"bert","machine learning methods",null,"deep neural network","neural network"],"tasks":["multilingual code mixed hope speech detection"],"center_task":[null],"Goal":["Good Health and Well-Being"],"text":"NLP-CUET@LT-EDI-EACL2021: Multilingual Code-Mixed Hope Speech Detection using Cross-lingual Representation Learner. In recent years, several systems have been developed to regulate the spread of negativity and eliminate aggressive, offensive or abusive contents from the online platforms. Nevertheless, a limited number of researches carried out to identify positive, encouraging and supportive contents. In this work, our goal is to identify whether a social media post\/comment contains hope speech or not. We propose three distinct models to identify hope speech in English, Tamil and Malayalam language to serve this purpose. To attain this goal, we employed various machine learning (support vector machine, logistic regression, ensemble), deep learning (convolutional neural network + long short term memory) and transformer (m-BERT, Indic-BERT, XLNet, XLM-Roberta) based methods. Results indicate that XLM-Roberta outdoes all other techniques by gaining a weighted f 1-score of 0.93, 0.60 and 0.85 respectively for English, Tamil and Malayalam language. Our team has achieved 1 st , 2 nd and 1 st rank in these three tasks respectively.","label":1,"title_clean":"NLP CUET@LT EDI EACL2021: Multilingual Code Mixed Hope Speech Detection using Cross lingual Representation Learner","abstract_clean":"In recent years, several systems have been developed to regulate the spread of negativity and eliminate aggressive, offensive or abusive contents from the online platforms. Nevertheless, a limited number of researches carried out to identify positive, encouraging and supportive contents. In this work, our goal is to identify whether a social media post\/comment contains hope speech or not. We propose three distinct models to identify hope speech in English, Tamil and Malayalam language to serve this purpose. To attain this goal, we employed various machine learning (support vector machine, logistic regression, ensemble), deep learning (convolutional neural network + long short term memory) and transformer (m BERT, Indic BERT, XLNet, XLM Roberta) based methods. Results indicate that XLM Roberta outdoes all other techniques by gaining a weighted f 1 score of 0.93, 0.60 and 0.85 respectively for English, Tamil and Malayalam language. Our team has achieved 1 st , 2 nd and 1 st rank in these three tasks respectively.","url":"https:\/\/aclanthology.org\/2021.ltedi-1.25"},{"ID":"hsueh-moore-2007-decisions","methods":["feature types","maxent models"],"center_method":[null,null],"tasks":["classification","automatic decision detection","meeting conversations"],"center_task":["classification",null,null],"Goal":["Decent Work and Economic Growth"],"text":"What Decisions Have You Made?: Automatic Decision Detection in Meeting Conversations. This study addresses the problem of automatically detecting decisions in conversational speech. We formulate the problem as classifying decision-making units at two levels of granularity: dialogue acts and topic segments. We conduct an empirical analysis to determine the characteristic features of decision-making dialogue acts, and train MaxEnt models using these features for the classification tasks. We find that models that combine lexical, prosodic, contextual and topical features yield the best results on both tasks, achieving 72% and 86% precision, respectively. The study also provides a quantitative analysis of the relative importance of the feature types.","label":1,"title_clean":"What Decisions Have You Made?: Automatic Decision Detection in Meeting Conversations","abstract_clean":"This study addresses the problem of automatically detecting decisions in conversational speech. We formulate the problem as classifying decision making units at two levels of granularity: dialogue acts and topic segments. We conduct an empirical analysis to determine the characteristic features of decision making dialogue acts, and train MaxEnt models using these features for the classification tasks. We find that models that combine lexical, prosodic, contextual and topical features yield the best results on both tasks, achieving 72% and 86% precision, respectively. The study also provides a quantitative analysis of the relative importance of the feature types.","url":"https:\/\/aclanthology.org\/N07-1004"},{"ID":"hu-etal-2018-shot","methods":["attribute attentive charge prediction model"],"center_method":[null],"tasks":["automatic charge prediction","distinguishing confusing charges","legal assistant systems"],"center_task":[null,null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Few-Shot Charge Prediction with Discriminative Legal Attributes. Automatic charge prediction aims to predict the final charges according to the fact descriptions in criminal cases and plays a crucial role in legal assistant systems. Existing works on charge prediction perform adequately on those high-frequency charges but are not yet capable of predicting few-shot charges with limited cases. Moreover, these exist many confusing charge pairs, whose fact descriptions are fairly similar to each other. To address these issues, we introduce several discriminative attributes of charges as the internal mapping between fact descriptions and charges. These attributes provide additional information for few-shot charges, as well as effective signals for distinguishing confusing charges. More specifically, we propose an attribute-attentive charge prediction model to infer the attributes and charges simultaneously. Experimental results on real-work datasets demonstrate that our proposed model achieves significant and consistent improvements than other state-of-the-art baselines. Specifically, our model outperforms other baselines by more than 50% in the few-shot scenario. Our codes and datasets can be obtained from https:\/\/github.com\/thunlp\/attribute_charge.","label":1,"title_clean":"Few Shot Charge Prediction with Discriminative Legal Attributes","abstract_clean":"Automatic charge prediction aims to predict the final charges according to the fact descriptions in criminal cases and plays a crucial role in legal assistant systems. Existing works on charge prediction perform adequately on those high frequency charges but are not yet capable of predicting few shot charges with limited cases. Moreover, these exist many confusing charge pairs, whose fact descriptions are fairly similar to each other. To address these issues, we introduce several discriminative attributes of charges as the internal mapping between fact descriptions and charges. These attributes provide additional information for few shot charges, as well as effective signals for distinguishing confusing charges. More specifically, we propose an attribute attentive charge prediction model to infer the attributes and charges simultaneously. Experimental results on real work datasets demonstrate that our proposed model achieves significant and consistent improvements than other state of the art baselines. Specifically, our model outperforms other baselines by more than 50% in the few shot scenario. Our codes and datasets can be obtained from https:\/\/github.com\/thunlp\/attribute_charge.","url":"https:\/\/aclanthology.org\/C18-1041"},{"ID":"hu-etal-2021-collaborative","methods":["fallback skill recommendation system","cdr","ipas","collaborative data relabeling","smart assistant","voice apps","apple siri"],"center_method":[null,null,null,null,null,null,null],"tasks":["automatic speech recognition asr error","partial observation","intelligent personal assistants","robust and diverse voice apps recommendation"],"center_task":[null,null,null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Collaborative Data Relabeling for Robust and Diverse Voice Apps Recommendation in Intelligent Personal Assistants. Intelligent personal assistants (IPAs) such as Amazon Alexa, Google Assistant and Apple Siri extend their built-in capabilities by supporting voice apps developed by third-party developers. Sometimes the smart assistant is not able to successfully respond to user voice commands (aka utterances). There are many reasons including automatic speech recognition (ASR) error, natural language understanding (NLU) error, routing utterances to an irrelevant voice app or simply that the user is asking for a capability that is not supported yet. The failure to handle a voice command leads to customer frustration. In this paper, we introduce a fallback skill recommendation system to suggest a voice app to a customer for an unhandled voice command. One of the prominent challenges of developing a skill recommender system for IPAs is partial observation. To solve the partial observation problem, we propose collaborative data relabeling (CDR) method. In addition, CDR also improves the diversity of the recommended skills. We evaluate the proposed method both offline and online. The offline evaluation results show that the proposed system outperforms the baselines. The online A\/B testing results show significant gain of customer experience metrics.","label":1,"title_clean":"Collaborative Data Relabeling for Robust and Diverse Voice Apps Recommendation in Intelligent Personal Assistants","abstract_clean":"Intelligent personal assistants (IPAs) such as Amazon Alexa, Google Assistant and Apple Siri extend their built in capabilities by supporting voice apps developed by third party developers. Sometimes the smart assistant is not able to successfully respond to user voice commands (aka utterances). There are many reasons including automatic speech recognition (ASR) error, natural language understanding (NLU) error, routing utterances to an irrelevant voice app or simply that the user is asking for a capability that is not supported yet. The failure to handle a voice command leads to customer frustration. In this paper, we introduce a fallback skill recommendation system to suggest a voice app to a customer for an unhandled voice command. One of the prominent challenges of developing a skill recommender system for IPAs is partial observation. To solve the partial observation problem, we propose collaborative data relabeling (CDR) method. In addition, CDR also improves the diversity of the recommended skills. We evaluate the proposed method both offline and online. The offline evaluation results show that the proposed system outperforms the baselines. The online A\/B testing results show significant gain of customer experience metrics.","url":"https:\/\/aclanthology.org\/2021.nlp4convai-1.11.pdf"},{"ID":"hu-yang-2020-privnet","methods":["transfer learning model","privnet","target recommender system","privacyaware neural representation"],"center_method":[null,null,null,null],"tasks":["safeguarding private attributes","transfer learning","recommendation"],"center_task":[null,"transfer learning",null],"Goal":["Peace, Justice and Strong Institutions"],"text":"PrivNet: Safeguarding Private Attributes in Transfer Learning for Recommendation. Transfer learning is an effective technique to improve a target recommender system with the knowledge from a source domain. Existing research focuses on the recommendation performance of the target domain while ignores the privacy leakage of the source domain. The transferred knowledge, however, may unintendedly leak private information of the source domain. For example, an attacker can accurately infer user demographics from their historical purchase provided by a source domain data owner. This paper addresses the above privacy-preserving issue by learning a privacyaware neural representation by improving target performance while protecting source privacy. The key idea is to simulate the attacks during the training for protecting unseen users' privacy in the future, modeled by an adversarial game, so that the transfer learning model becomes robust to attacks. Experiments show that the proposed PrivNet model can successfully disentangle the knowledge benefitting the transfer from leaking the privacy.","label":1,"title_clean":"PrivNet: Safeguarding Private Attributes in Transfer Learning for Recommendation","abstract_clean":"Transfer learning is an effective technique to improve a target recommender system with the knowledge from a source domain. Existing research focuses on the recommendation performance of the target domain while ignores the privacy leakage of the source domain. The transferred knowledge, however, may unintendedly leak private information of the source domain. For example, an attacker can accurately infer user demographics from their historical purchase provided by a source domain data owner. This paper addresses the above privacy preserving issue by learning a privacyaware neural representation by improving target performance while protecting source privacy. The key idea is to simulate the attacks during the training for protecting unseen users' privacy in the future, modeled by an adversarial game, so that the transfer learning model becomes robust to attacks. Experiments show that the proposed PrivNet model can successfully disentangle the knowledge benefitting the transfer from leaking the privacy.","url":"https:\/\/aclanthology.org\/2020.findings-emnlp.404"},{"ID":"huang-bai-2021-hub","methods":["bert","fine tuning methods","hubdravidianlangtech eacl2021"],"center_method":["bert",null,null],"tasks":["hate speech","five category classification task","multilingual code mixing"],"center_task":["hate speech",null,null],"Goal":["Reduced Inequalities","Peace, Justice and Strong Institutions"],"text":"HUB@DravidianLangTech-EACL2021: Identify and Classify Offensive Text in Multilingual Code Mixing in Social Media. This paper introduces the system description of the HUB team participating in Dravidian-LangTech-EACL2021: Offensive Language Identification in Dravidian Languages. The theme of this shared task is the detection of offensive content in social media. Among the known tasks related to offensive speech detection, this is the first task to detect offensive comments posted in social media comments in the Dravidian language. The task organizer team provided us with the code-mixing task data set mainly composed of three different languages: Malayalam, Kannada, and Tamil. The tasks on the code mixed data in these three different languages can be seen as three different comment\/post-level classification tasks. The task on the Malayalam data set is a five-category classification task, and the Kannada and Tamil language data sets are two six-category classification tasks. Based on our analysis of the task description and task data set, we chose to use the multilingual BERT model to complete this task. In this paper, we will discuss our fine-tuning methods, models, experiments, and results.","label":1,"title_clean":"HUB@DravidianLangTech EACL2021: Identify and Classify Offensive Text in Multilingual Code Mixing in Social Media","abstract_clean":"This paper introduces the system description of the HUB team participating in Dravidian LangTech EACL2021: Offensive Language Identification in Dravidian Languages. The theme of this shared task is the detection of offensive content in social media. Among the known tasks related to offensive speech detection, this is the first task to detect offensive comments posted in social media comments in the Dravidian language. The task organizer team provided us with the code mixing task data set mainly composed of three different languages: Malayalam, Kannada, and Tamil. The tasks on the code mixed data in these three different languages can be seen as three different comment\/post level classification tasks. The task on the Malayalam data set is a five category classification task, and the Kannada and Tamil language data sets are two six category classification tasks. Based on our analysis of the task description and task data set, we chose to use the multilingual BERT model to complete this task. In this paper, we will discuss our fine tuning methods, models, experiments, and results.","url":"https:\/\/aclanthology.org\/2021.dravidianlangtech-1.27.pdf"},{"ID":"huang-bai-2021-team","methods":["xlm roberta pre trained language model","language models","tf idf algorithm"],"center_method":[null,"language models",null],"tasks":["hope speech detection","classification","lt edi 2021"],"center_task":["hope speech detection","classification",null],"Goal":["Good Health and Well-Being"],"text":"TEAM HUB@LT-EDI-EACL2021: Hope Speech Detection Based On Pre-trained Language Model. This article introduces the system description of TEAM HUB team participating in LT-EDI 2021: Hope Speech Detection. This shared task is the first task related to the desired voice detection. The data set in the shared task consists of three different languages (English, Tamil, and Malayalam). The task type is text classification. Based on the analysis and understanding of the task description and data set, we designed a system based on a pre-trained language model to complete this shared task. In this system, we use methods and models that combine the XLM-RoBERTa pre-trained language model and the Tf-Idf algorithm. In the final result ranking announced by the task organizer, our system obtained F1 scores of 0.93, 0.84, 0.59 on the English dataset, Malayalam dataset, and Tamil dataset. Our submission results are ranked 1, 2, and 3 respectively.","label":1,"title_clean":"TEAM HUB@LT EDI EACL2021: Hope Speech Detection Based On Pre trained Language Model","abstract_clean":"This article introduces the system description of TEAM HUB team participating in LT EDI 2021: Hope Speech Detection. This shared task is the first task related to the desired voice detection. The data set in the shared task consists of three different languages (English, Tamil, and Malayalam). The task type is text classification. Based on the analysis and understanding of the task description and data set, we designed a system based on a pre trained language model to complete this shared task. In this system, we use methods and models that combine the XLM RoBERTa pre trained language model and the Tf Idf algorithm. In the final result ranking announced by the task organizer, our system obtained F1 scores of 0.93, 0.84, 0.59 on the English dataset, Malayalam dataset, and Tamil dataset. Our submission results are ranked 1, 2, and 3 respectively.","url":"https:\/\/aclanthology.org\/2021.ltedi-1.17.pdf"},{"ID":"huang-etal-2016-well","methods":["ranking svm model"],"center_method":[null],"tasks":["large scale dataset construction","math word problems"],"center_task":[null,null],"Goal":["Quality Education"],"text":"How well do Computers Solve Math Word Problems? Large-Scale Dataset Construction and Evaluation. Recently a few systems for automatically solving math word problems have reported promising results. However, the datasets used for evaluation have limitations in both scale and diversity. In this paper, we build a large-scale dataset which is more than 9 times the size of previous ones, and contains many more problem types. Problems in the dataset are semiautomatically obtained from community question-answering (CQA) web pages. A ranking SVM model is trained to automatically extract problem answers from the answer text provided by CQA users, which significantly reduces human annotation cost. Experiments conducted on the new dataset lead to interesting and surprising results.","label":1,"title_clean":"How well do Computers Solve Math Word Problems? Large Scale Dataset Construction and Evaluation","abstract_clean":"Recently a few systems for automatically solving math word problems have reported promising results. However, the datasets used for evaluation have limitations in both scale and diversity. In this paper, we build a large scale dataset which is more than 9 times the size of previous ones, and contains many more problem types. Problems in the dataset are semiautomatically obtained from community question answering (CQA) web pages. A ranking SVM model is trained to automatically extract problem answers from the answer text provided by CQA users, which significantly reduces human annotation cost. Experiments conducted on the new dataset lead to interesting and surprising results.","url":"https:\/\/aclanthology.org\/P16-1084"},{"ID":"huang-etal-2020-coda","methods":["coda 19s","majority vote","non expert crowd"],"center_method":[null,null,null],"tasks":["ai\/nlp research"],"center_task":[null],"Goal":["Good Health and Well-Being","Industry, Innovation and Infrastrucure"],"text":"CODA-19: Using a Non-Expert Crowd to Annotate Research Aspects on 10,000+ Abstracts in the COVID-19 Open Research Dataset. This paper introduces CODA-19 1 , a humanannotated dataset that codes the Background, Purpose, Method, Finding\/Contribution, and Other sections of 10,966 English abstracts in the COVID-19 Open Research Dataset. CODA-19 was created by 248 crowd workers from Amazon Mechanical Turk within 10 days, and achieved labeling quality comparable to that of experts. Each abstract was annotated by nine different workers, and the final labels were acquired by majority vote. The inter-annotator agreement (Cohen's kappa) between the crowd and the biomedical expert (0.741) is comparable to inter-expert agreement (0.788). CODA-19's labels have an accuracy of 82.2% when compared to the biomedical expert's labels, while the accuracy between experts was 85.0%. Reliable human annotations help scientists access and integrate the rapidly accelerating coronavirus literature, and also serve as the battery of AI\/NLP research, but obtaining expert annotations can be slow. We demonstrated that a non-expert crowd can be rapidly employed at scale to join the fight against COVID-19.","label":1,"title_clean":"CODA 19: Using a Non Expert Crowd to Annotate Research Aspects on 10,000+ Abstracts in the COVID 19 Open Research Dataset","abstract_clean":"This paper introduces CODA 19 1 , a humanannotated dataset that codes the Background, Purpose, Method, Finding\/Contribution, and Other sections of 10,966 English abstracts in the COVID 19 Open Research Dataset. CODA 19 was created by 248 crowd workers from Amazon Mechanical Turk within 10 days, and achieved labeling quality comparable to that of experts. Each abstract was annotated by nine different workers, and the final labels were acquired by majority vote. The inter annotator agreement (Cohen's kappa) between the crowd and the biomedical expert (0.741) is comparable to inter expert agreement (0.788). CODA 19's labels have an accuracy of 82.2% when compared to the biomedical expert's labels, while the accuracy between experts was 85.0%. Reliable human annotations help scientists access and integrate the rapidly accelerating coronavirus literature, and also serve as the battery of AI\/NLP research, but obtaining expert annotations can be slow. We demonstrated that a non expert crowd can be rapidly employed at scale to join the fight against COVID 19.","url":"https:\/\/aclanthology.org\/2020.nlpcovid19-acl.6"},{"ID":"huang-etal-2020-texthide","methods":["tex thide","texthide","language models","encryption step","bert","eavesdropping attacker"],"center_method":[null,null,"language models",null,"bert",null],"tasks":["natural language understanding tasks","training","sentence or sentence pair task","mathematical problem","distributed or federated learning","data privacy"],"center_task":[null,"training",null,null,null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"TextHide: Tackling Data Privacy in Language Understanding Tasks. An unsolved challenge in distributed or federated learning is to effectively mitigate privacy risks without slowing down training or reducing accuracy. In this paper, we propose Tex-tHide aiming at addressing this challenge for natural language understanding tasks. It requires all participants to add a simple encryption step to prevent an eavesdropping attacker from recovering private text data. Such an encryption step is efficient and only affects the task performance slightly. In addition, Tex-tHide fits well with the popular framework of fine-tuning pre-trained language models (e.g., BERT) for any sentence or sentence-pair task. We evaluate TextHide on the GLUE benchmark, and our experiments show that TextHide can effectively defend attacks on shared gradients or representations and the averaged accuracy reduction is only 1.9%. We also present an analysis of the security of TextHide using a conjecture about the computational intractability of a mathematical problem. 1","label":1,"title_clean":"TextHide: Tackling Data Privacy in Language Understanding Tasks","abstract_clean":"An unsolved challenge in distributed or federated learning is to effectively mitigate privacy risks without slowing down training or reducing accuracy. In this paper, we propose Tex tHide aiming at addressing this challenge for natural language understanding tasks. It requires all participants to add a simple encryption step to prevent an eavesdropping attacker from recovering private text data. Such an encryption step is efficient and only affects the task performance slightly. In addition, Tex tHide fits well with the popular framework of fine tuning pre trained language models (e.g., BERT) for any sentence or sentence pair task. We evaluate TextHide on the GLUE benchmark, and our experiments show that TextHide can effectively defend attacks on shared gradients or representations and the averaged accuracy reduction is only 1.9%. We also present an analysis of the security of TextHide using a conjecture about the computational intractability of a mathematical problem. 1","url":"https:\/\/aclanthology.org\/2020.findings-emnlp.123"},{"ID":"huck-etal-2017-lmu","methods":["lmu munichs systems","neural translation engines"],"center_method":[null,null],"tasks":["machine translation","biomedical translation task","wmt17 shared task","machine translation of news"],"center_task":["machine translation",null,null,null],"Goal":["Good Health and Well-Being"],"text":"LMU Munich's Neural Machine Translation Systems for News Articles and Health Information Texts. This paper describes the LMU Munich English\u2192German machine translation systems. We participated with neural translation engines in the WMT17 shared task on machine translation of news, as well as in the biomedical translation task. LMU Munich's systems deliver competitive machine translation quality on both news articles and health information texts.","label":1,"title_clean":"LMU Munich's Neural Machine Translation Systems for News Articles and Health Information Texts","abstract_clean":"This paper describes the LMU Munich English\u2192German machine translation systems. We participated with neural translation engines in the WMT17 shared task on machine translation of news, as well as in the biomedical translation task. LMU Munich's systems deliver competitive machine translation quality on both news articles and health information texts.","url":"https:\/\/aclanthology.org\/W17-4730"},{"ID":"hur-etal-2020-domain","methods":["domain adaption","document classification approach","instance selection"],"center_method":["domain adaption",null,null],"tasks":["antibiotic administration","antimicrobial resistance","annotation effort","disease syndrome classification"],"center_task":[null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"Domain Adaptation and Instance Selection for Disease Syndrome Classification over Veterinary Clinical Notes. Identifying the reasons for antibiotic administration in veterinary records is a critical component of understanding antimicrobial usage patterns. This informs antimicrobial stewardship programs designed to fight antimicrobial resistance, a major health crisis affecting both humans and animals in which veterinarians have an important role to play. We propose a document classification approach to determine the reason for administration of a given drug, with particular focus on domain adaptation from one drug to another, and instance selection to minimize annotation effort.","label":1,"title_clean":"Domain Adaptation and Instance Selection for Disease Syndrome Classification over Veterinary Clinical Notes","abstract_clean":"Identifying the reasons for antibiotic administration in veterinary records is a critical component of understanding antimicrobial usage patterns. This informs antimicrobial stewardship programs designed to fight antimicrobial resistance, a major health crisis affecting both humans and animals in which veterinarians have an important role to play. We propose a document classification approach to determine the reason for administration of a given drug, with particular focus on domain adaptation from one drug to another, and instance selection to minimize annotation effort.","url":"https:\/\/aclanthology.org\/2020.bionlp-1.17"},{"ID":"hurriyetoglu-etal-2021-challenges","methods":["transformers","word embeddings","deep neural network","nlp applications"],"center_method":["transformers","word embeddings","deep neural network","nlp applications"],"tasks":["multilingual protest news detection","automated extraction of socio political events","multi and cross lingual machine learning","few and zero shot settings","fine grained classification of socio political events","news bias detection","automatically detecting descriptions of sociopolitical events"],"center_task":[null,null,null,null,null,null,null],"Goal":["Sustainable Cities and Communities"],"text":"Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021): Workshop and Shared Task Report. This workshop is the fourth issue of a series of workshops on automatic extraction of sociopolitical events from news, organized by the Emerging Market Welfare Project, with the support of the Joint Research Centre of the European Commission and with contributions from many other prominent scholars in this field. The purpose of this series of workshops is to foster research and development of reliable, valid, robust, and practical solutions for automatically detecting descriptions of sociopolitical events, such as protests, riots, wars and armed conflicts, in text streams. This year workshop contributors make use of the stateof-the-art NLP technologies, such as Deep Learning, Word Embeddings and Transformers and cover a wide range of topics from text classification to news bias detection. Around 40 teams have registered and 15 teams contributed to three tasks that are i) multilingual protest news detection, ii) fine-grained classification of socio-political events, and iii) discovering Black Lives Matter protest events. The workshop also highlights two keynote and four invited talks about various aspects of creating event data sets and multi-and cross-lingual machine learning in few-and zero-shot settings.","label":1,"title_clean":"Challenges and Applications of Automated Extraction of Socio political Events from Text (CASE 2021): Workshop and Shared Task Report","abstract_clean":"This workshop is the fourth issue of a series of workshops on automatic extraction of sociopolitical events from news, organized by the Emerging Market Welfare Project, with the support of the Joint Research Centre of the European Commission and with contributions from many other prominent scholars in this field. The purpose of this series of workshops is to foster research and development of reliable, valid, robust, and practical solutions for automatically detecting descriptions of sociopolitical events, such as protests, riots, wars and armed conflicts, in text streams. This year workshop contributors make use of the stateof the art NLP technologies, such as Deep Learning, Word Embeddings and Transformers and cover a wide range of topics from text classification to news bias detection. Around 40 teams have registered and 15 teams contributed to three tasks that are i) multilingual protest news detection, ii) fine grained classification of socio political events, and iii) discovering Black Lives Matter protest events. The workshop also highlights two keynote and four invited talks about various aspects of creating event data sets and multi and cross lingual machine learning in few and zero shot settings.","url":"https:\/\/aclanthology.org\/2021.case-1.1"},{"ID":"ilichev-etal-2021-multiple","methods":["natural language processing progress","computer vision model resnet","multiple teacher distillation","tinybert and dis tilbert models"],"center_method":[null,null,null,null],"tasks":["robust and greener models","natural language processing progress"],"center_task":[null,null],"Goal":["Responsible Consumption and Production"],"text":"Multiple Teacher Distillation for Robust and Greener Models. The language models nowadays are in the center of natural language processing progress. These models are mostly of significant size. There are successful attempts to reduce them, but at least some of these attempts rely on randomness. We propose a novel distillation procedure leveraging on multiple teachers usage which alleviates random seed dependency and makes the models more robust. We show that this procedure applied to TinyBERT and Dis-tilBERT models improves their worst case results up to 2% while keeping almost the same best-case ones. The latter fact keeps true with a constraint on computational time, which is important to lessen the carbon footprint. In addition, we present the results of an application of the proposed procedure to a computer vision model ResNet, which shows that the statement keeps true in this totally different domain.","label":1,"title_clean":"Multiple Teacher Distillation for Robust and Greener Models","abstract_clean":"The language models nowadays are in the center of natural language processing progress. These models are mostly of significant size. There are successful attempts to reduce them, but at least some of these attempts rely on randomness. We propose a novel distillation procedure leveraging on multiple teachers usage which alleviates random seed dependency and makes the models more robust. We show that this procedure applied to TinyBERT and Dis tilBERT models improves their worst case results up to 2% while keeping almost the same best case ones. The latter fact keeps true with a constraint on computational time, which is important to lessen the carbon footprint. In addition, we present the results of an application of the proposed procedure to a computer vision model ResNet, which shows that the statement keeps true in this totally different domain.","url":"https:\/\/aclanthology.org\/2021.ranlp-1.68"},{"ID":"inan-etal-2022-modeling","methods":["transformers","supervised intensity tagger","data driven manner","computational approach","sign language generation models"],"center_method":["transformers",null,null,null,null],"tasks":["intensification modeling","sign language generation","linguistics of sign language"],"center_task":[null,null,null],"Goal":["Reduced Inequalities"],"text":"Modeling Intensification for Sign Language Generation: A Computational Approach. End-to-end sign language generation models do not accurately represent the prosody in sign language. A lack of temporal and spatial variations leads to poor-quality generated presentations that confuse human interpreters. In this paper, we aim to improve the prosody in generated sign languages by modeling intensification in a data-driven manner. We present different strategies grounded in linguistics of sign language that inform how intensity modifiers can be represented in gloss annotations. To employ our strategies, we first annotate a subset of the benchmark PHOENIX-14T, a German Sign Language dataset, with different levels of intensification. We then use a supervised intensity tagger to extend the annotated dataset and obtain labels for the remaining portion of it. This enhanced dataset is then used to train state-of-the-art transformer models for sign language generation. We find that our efforts in intensification modeling yield better results when evaluated with automatic metrics. Human evaluation also indicates a higher preference of the videos generated using our model.","label":1,"title_clean":"Modeling Intensification for Sign Language Generation: A Computational Approach","abstract_clean":"End to end sign language generation models do not accurately represent the prosody in sign language. A lack of temporal and spatial variations leads to poor quality generated presentations that confuse human interpreters. In this paper, we aim to improve the prosody in generated sign languages by modeling intensification in a data driven manner. We present different strategies grounded in linguistics of sign language that inform how intensity modifiers can be represented in gloss annotations. To employ our strategies, we first annotate a subset of the benchmark PHOENIX 14T, a German Sign Language dataset, with different levels of intensification. We then use a supervised intensity tagger to extend the annotated dataset and obtain labels for the remaining portion of it. This enhanced dataset is then used to train state of the art transformer models for sign language generation. We find that our efforts in intensification modeling yield better results when evaluated with automatic metrics. Human evaluation also indicates a higher preference of the videos generated using our model.","url":"https:\/\/aclanthology.org\/2022.findings-acl.228.pdf"},{"ID":"indurthi-etal-2019-fermi","methods":["team fermis model","sentence embeddings","embedding ml combination algorithms","training models","svm with rbf kernel"],"center_method":[null,null,null,null,null],"tasks":["hateval","classification"],"center_task":[null,"classification"],"Goal":["Peace, Justice and Strong Institutions"],"text":"FERMI at SemEval-2019 Task 5: Using Sentence embeddings to Identify Hate Speech Against Immigrants and Women in Twitter. This paper describes our system (Fermi) for Task 5 of SemEval-2019: HatEval: Multilingual Detection of Hate Speech Against Immigrants and Women on Twitter. We participated in the subtask A for English and ranked first in the evaluation on the test set. We evaluate the quality of multiple sentence embeddings and explore multiple training models to evaluate the performance of simple yet effective embedding-ML combination algorithms. Our team-Fermi's model achieved an accuracy of 65.00% for English language in task A. Our models, which use pretrained Universal Encoder sentence embeddings for transforming the input and SVM (with RBF kernel) for classification, scored first position (among 68) in the leaderboard on the test set for Subtask A in English language. In this paper we provide a detailed description of the approach, as well as the results obtained in the task.","label":1,"title_clean":"FERMI at SemEval 2019 Task 5: Using Sentence embeddings to Identify Hate Speech Against Immigrants and Women in Twitter","abstract_clean":"This paper describes our system (Fermi) for Task 5 of SemEval 2019: HatEval: Multilingual Detection of Hate Speech Against Immigrants and Women on Twitter. We participated in the subtask A for English and ranked first in the evaluation on the test set. We evaluate the quality of multiple sentence embeddings and explore multiple training models to evaluate the performance of simple yet effective embedding ML combination algorithms. Our team Fermi's model achieved an accuracy of 65.00% for English language in task A. Our models, which use pretrained Universal Encoder sentence embeddings for transforming the input and SVM (with RBF kernel) for classification, scored first position (among 68) in the leaderboard on the test set for Subtask A in English language. In this paper we provide a detailed description of the approach, as well as the results obtained in the task.","url":"https:\/\/aclanthology.org\/S19-2009"},{"ID":"inui-1996-internet","methods":["call system","language learners","information overflow","user friendly interface","machine translation","dictionaries","browsing tools","nlp technology","electronic networks","parsing"],"center_method":[null,null,null,null,"machine translation",null,null,null,null,null],"tasks":["intel ligent dictionaries","learning","communication","language learning"],"center_task":[null,"learning","communication",null],"Goal":["Quality Education"],"text":"The Internet a ``natural'' channel for language learning. The network as a motivational source for using a foreign language. Electronic networks can be useful in many ways for language learners. First of all, network facilities (e-mail, news, WWW home-pages) minimize not only the boundaries of time and space, but they also help to break communication bar-tiers. They are a wonderful tool for USING a foreign language. E-mail, for example, can be used not only for interaction between teachers and students, but also for interaction among students (collaborative learning). Students can even ask for help from friends or \"ex-perts\" living elsewhere, on the other side of the globe. There have been quite a few attempts to introduce these new tools into the classroom. For example, there are several well established mailing lists between Japanese and foreign schools. This allows Japanese kids to practice, let's say English, by exchanging messages with students from \"abroad\", chatting about their favorite topics like music, sport or any other hobby. Obviously, this kind of communication is meaningful for the student, since s\/he can talk about things s\/he is concerned with. What role then can CALL system play in this new setting? Rather than trying to play the role people are very good at (answering on the fly questions on any topic, common sense reasoning, etc.), CALL system should assist people by providing the learner with information humans are generally fairly poor at. One way to help the user is by providing him with information (databases) he is looking for. For example, all language learners are concerned with lexicons. Having fabulous browsing tools, computers have a great advantage over traditional dictionaries. Also, people are not very good in explaining the contexts in which a word may be used, or in explaining the difference between two words. Last, but not least, existing NLP technology, such as parsing or machine translation, could be incorporated into the development of 'intel-ligent dictionaries'. However, before doing so, we have to consider several basic issues : what information is useful, that is, what information should be provided to the learner, when and how? For example, rather than killing the user by an information overflow,-like these long list of translations that most electronic dictionaries provide, lists in which the user has to dig deeply in order to find the relevant word,one could parametrize the,level of detail, scope and grain size of translations for a given text or text fragment. In sum, there should be a balance between the information provided by the system and the user's competence. Following this line of reasoning we have started to work on a user friendly interface for a bilingual lexicon (English-Japanese). Two features of our prototype are worth mentioning: (a) the tool is implemented as a WWW","label":1,"title_clean":"The Internet a ``natural'' channel for language learning","abstract_clean":"The network as a motivational source for using a foreign language. Electronic networks can be useful in many ways for language learners. First of all, network facilities (e mail, news, WWW home pages) minimize not only the boundaries of time and space, but they also help to break communication bar tiers. They are a wonderful tool for USING a foreign language. E mail, for example, can be used not only for interaction between teachers and students, but also for interaction among students (collaborative learning). Students can even ask for help from friends or \"ex perts\" living elsewhere, on the other side of the globe. There have been quite a few attempts to introduce these new tools into the classroom. For example, there are several well established mailing lists between Japanese and foreign schools. This allows Japanese kids to practice, let's say English, by exchanging messages with students from \"abroad\", chatting about their favorite topics like music, sport or any other hobby. Obviously, this kind of communication is meaningful for the student, since s\/he can talk about things s\/he is concerned with. What role then can CALL system play in this new setting? Rather than trying to play the role people are very good at (answering on the fly questions on any topic, common sense reasoning, etc.), CALL system should assist people by providing the learner with information humans are generally fairly poor at. One way to help the user is by providing him with information (databases) he is looking for. For example, all language learners are concerned with lexicons. Having fabulous browsing tools, computers have a great advantage over traditional dictionaries. Also, people are not very good in explaining the contexts in which a word may be used, or in explaining the difference between two words. Last, but not least, existing NLP technology, such as parsing or machine translation, could be incorporated into the development of 'intel ligent dictionaries'. However, before doing so, we have to consider several basic issues : what information is useful, that is, what information should be provided to the learner, when and how? For example, rather than killing the user by an information overflow, like these long list of translations that most electronic dictionaries provide, lists in which the user has to dig deeply in order to find the relevant word,one could parametrize the,level of detail, scope and grain size of translations for a given text or text fragment. In sum, there should be a balance between the information provided by the system and the user's competence. Following this line of reasoning we have started to work on a user friendly interface for a bilingual lexicon (English Japanese). Two features of our prototype are worth mentioning: (a) the tool is implemented as a WWW","url":"https:\/\/aclanthology.org\/C96-2175"},{"ID":"islam-etal-2012-text","methods":["readability classifier"],"center_method":[null],"tasks":["text readability classification","natural language processing nlp application"],"center_task":[null,null],"Goal":["Quality Education"],"text":"Text Readability Classification of Textbooks of a Low-Resource Language. There are many languages considered to be low-density languages, either because the population speaking the language is not very large, or because insufficient digitized text material is available in the language even though millions of people speak the language. Bangla is one of the latter ones. Readability classification is an important Natural Language Processing (NLP) application that can be used to judge the quality of documents and assist writers to locate possible problems. This paper presents a readability classifier of Bangla textbook documents based on information-theoretic and lexical features. The features proposed in this paper result in an F-score that is 50% higher than that for traditional readability formulas.","label":1,"title_clean":"Text Readability Classification of Textbooks of a Low Resource Language","abstract_clean":"There are many languages considered to be low density languages, either because the population speaking the language is not very large, or because insufficient digitized text material is available in the language even though millions of people speak the language. Bangla is one of the latter ones. Readability classification is an important Natural Language Processing (NLP) application that can be used to judge the quality of documents and assist writers to locate possible problems. This paper presents a readability classifier of Bangla textbook documents based on information theoretic and lexical features. The features proposed in this paper result in an F score that is 50% higher than that for traditional readability formulas.","url":"https:\/\/aclanthology.org\/Y12-1059"},{"ID":"islamaj-dogan-etal-2017-biocreative","methods":["public text mining tools"],"center_method":[null],"tasks":["mining protein protein interactions","biocreative vi","annotation","manual review","biomedical literature"],"center_task":[null,null,"annotation",null,null],"Goal":["Good Health and Well-Being"],"text":"BioCreative VI Precision Medicine Track: creating a training corpus for mining protein-protein interactions affected by mutations. The Precision Medicine Track in BioCreative VI aims to bring together the BioNLP community for a novel challenge focused on mining the biomedical literature in search of mutations and protein-protein interactions (PPI). In order to support this track with an effective training dataset with limited curator time, the track organizers carefully reviewed PubMed articles from two different sources: curated public PPI databases, and the results of state-of-the-art public text mining tools. We detail here the data collection, manual review and annotation process and describe this training corpus characteristics. We also describe a corpus performance baseline. This analysis will provide useful information to developers and researchers for comparing and developing innovative text mining approaches for the BioCreative VI challenge and other Precision Medicine related applications.","label":1,"title_clean":"BioCreative VI Precision Medicine Track: creating a training corpus for mining protein protein interactions affected by mutations","abstract_clean":"The Precision Medicine Track in BioCreative VI aims to bring together the BioNLP community for a novel challenge focused on mining the biomedical literature in search of mutations and protein protein interactions (PPI). In order to support this track with an effective training dataset with limited curator time, the track organizers carefully reviewed PubMed articles from two different sources: curated public PPI databases, and the results of state of the art public text mining tools. We detail here the data collection, manual review and annotation process and describe this training corpus characteristics. We also describe a corpus performance baseline. This analysis will provide useful information to developers and researchers for comparing and developing innovative text mining approaches for the BioCreative VI challenge and other Precision Medicine related applications.","url":"https:\/\/aclanthology.org\/W17-2321"},{"ID":"iyyer-etal-2014-political","methods":["recursive neural network","automated techniques"],"center_method":[null,null],"tasks":["political ideology detection","sentiment analysis"],"center_task":[null,"sentiment analysis"],"Goal":["Peace, Justice and Strong Institutions"],"text":"Political Ideology Detection Using Recursive Neural Networks. An individual's words often reveal their political ideology. Existing automated techniques to identify ideology from text focus on bags of words or wordlists, ignoring syntax. Taking inspiration from recent work in sentiment analysis that successfully models the compositional aspect of language, we apply a recursive neural network (RNN) framework to the task of identifying the political position evinced by a sentence. To show the importance of modeling subsentential elements, we crowdsource political annotations at a phrase and sentence level. Our model outperforms existing models on our newly annotated dataset and an existing dataset.","label":1,"title_clean":"Political Ideology Detection Using Recursive Neural Networks","abstract_clean":"An individual's words often reveal their political ideology. Existing automated techniques to identify ideology from text focus on bags of words or wordlists, ignoring syntax. Taking inspiration from recent work in sentiment analysis that successfully models the compositional aspect of language, we apply a recursive neural network (RNN) framework to the task of identifying the political position evinced by a sentence. To show the importance of modeling subsentential elements, we crowdsource political annotations at a phrase and sentence level. Our model outperforms existing models on our newly annotated dataset and an existing dataset.","url":"https:\/\/aclanthology.org\/P14-1105"},{"ID":"jacobson-dalianis-2016-applying","methods":["stacked restricted boltzmann machines","stacked sparse auto encoders","deep neural network","machine learning methods"],"center_method":[null,null,"deep neural network","machine learning methods"],"tasks":["detecting healthcare associated infections","healthcare"],"center_task":[null,"healthcare"],"Goal":["Good Health and Well-Being"],"text":"Applying deep learning on electronic health records in Swedish to predict healthcare-associated infections. Detecting healthcare-associated infections pose a major challenge in healthcare. Using natural language processing and machine learning applied on electronic patient records is one approach that has been shown to work. However the results indicate that there was room for improvement and therefore we have applied deep learning methods. Specifically we implemented a network of stacked sparse auto encoders and a network of stacked restricted Boltzmann machines. Our best results were obtained using the stacked restricted Boltzmann machines with a precision of 0.79 and a recall of 0.88.","label":1,"title_clean":"Applying deep learning on electronic health records in Swedish to predict healthcare associated infections","abstract_clean":"Detecting healthcare associated infections pose a major challenge in healthcare. Using natural language processing and machine learning applied on electronic patient records is one approach that has been shown to work. However the results indicate that there was room for improvement and therefore we have applied deep learning methods. Specifically we implemented a network of stacked sparse auto encoders and a network of stacked restricted Boltzmann machines. Our best results were obtained using the stacked restricted Boltzmann machines with a precision of 0.79 and a recall of 0.88.","url":"https:\/\/aclanthology.org\/W16-2926"},{"ID":"jagannatha-yu-2016-structured","methods":["rnn potentials","skip chain crf inference","approximate version","recurrent neural networks","conditional random field based structured learning models","explicit modeling of pairwise potentials","crf lstm model","structured prediction models"],"center_method":[null,null,null,"recurrent neural networks",null,null,null,null],"tasks":["sequence labeling","exact phrase detection of clinical entities","structured prediction","extraction of relevant entities","named entity recognition","information extraction","rnn based sequence labeling"],"center_task":[null,null,null,null,"named entity recognition","information extraction",null],"Goal":["Good Health and Well-Being"],"text":"Structured prediction models for RNN based sequence labeling in clinical text. Sequence labeling is a widely used method for named entity recognition and information extraction from unstructured natural language data. In the clinical domain one major application of sequence labeling involves extraction of relevant entities such as medication, indication, and side-effects from Electronic Health Record Narratives. Sequence labeling in this domain presents its own set of challenges and objectives. In this work we experiment with Conditional Random Field based structured learning models with Recurrent Neural Networks. We extend the previously studied CRF-LSTM model with explicit modeling of pairwise potentials. We also propose an approximate version of skip-chain CRF inference with RNN potentials. We use these methods 1 for structured prediction in order to improve the exact phrase detection of clinical entities.","label":1,"title_clean":"Structured prediction models for RNN based sequence labeling in clinical text","abstract_clean":"Sequence labeling is a widely used method for named entity recognition and information extraction from unstructured natural language data. In the clinical domain one major application of sequence labeling involves extraction of relevant entities such as medication, indication, and side effects from Electronic Health Record Narratives. Sequence labeling in this domain presents its own set of challenges and objectives. In this work we experiment with Conditional Random Field based structured learning models with Recurrent Neural Networks. We extend the previously studied CRF LSTM model with explicit modeling of pairwise potentials. We also propose an approximate version of skip chain CRF inference with RNN potentials. We use these methods 1 for structured prediction in order to improve the exact phrase detection of clinical entities.","url":"https:\/\/aclanthology.org\/D16-1082.pdf"},{"ID":"jana-biemann-2021-investigation","methods":["federated framework","privacy aware models","sequence tagging models","differential privacy","privacy preserving framework","unstable models"],"center_method":[null,null,null,null,null,null],"tasks":["privacy sensitive domains","named entity recognition","differentially private sequence tagging","privacy aware sequence tagging models","privacy protection","machine learning based applications"],"center_task":[null,"named entity recognition",null,null,null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"An Investigation towards Differentially Private Sequence Tagging in a Federated Framework. To build machine learning-based applications for sensitive domains like medical, legal, etc. where the digitized text contains private information, anonymization of text is required for preserving privacy. Sequence tagging, e.g. as used for Named Entity Recognition (NER), can help to detect private information. However, to train sequence tagging models, a sufficient amount of labeled data are required but for privacy-sensitive domains, such labeled data also can not be shared directly. In this paper, we investigate the applicability of a privacy-preserving framework for sequence tagging tasks, specifically NER. Hence, we analyze a framework for the NER task, which incorporates two levels of privacy protection. Firstly, we deploy a federated learning (FL) framework where the labeled data are neither shared with the centralized server nor with the peer clients. Secondly, we apply differential privacy (DP) while the models are being trained in each client instance. While both privacy measures are suitable for privacy-aware models, their combination results in unstable models. To our knowledge, this is the first study of its kind on privacy-aware sequence tagging models.","label":1,"title_clean":"An Investigation towards Differentially Private Sequence Tagging in a Federated Framework","abstract_clean":"To build machine learning based applications for sensitive domains like medical, legal, etc. where the digitized text contains private information, anonymization of text is required for preserving privacy. Sequence tagging, e.g. as used for Named Entity Recognition (NER), can help to detect private information. However, to train sequence tagging models, a sufficient amount of labeled data are required but for privacy sensitive domains, such labeled data also can not be shared directly. In this paper, we investigate the applicability of a privacy preserving framework for sequence tagging tasks, specifically NER. Hence, we analyze a framework for the NER task, which incorporates two levels of privacy protection. Firstly, we deploy a federated learning (FL) framework where the labeled data are neither shared with the centralized server nor with the peer clients. Secondly, we apply differential privacy (DP) while the models are being trained in each client instance. While both privacy measures are suitable for privacy aware models, their combination results in unstable models. To our knowledge, this is the first study of its kind on privacy aware sequence tagging models.","url":"https:\/\/aclanthology.org\/2021.privatenlp-1.4"},{"ID":"jang-etal-2020-exploratory","methods":["topic models","sentiment analysis"],"center_method":["topic models","sentiment analysis"],"tasks":["public health questions"],"center_task":[null],"Goal":["Good Health and Well-Being"],"text":"Exploratory Analysis of COVID-19 Related Tweets in North America to Inform Public Health Institutes. Social media is a rich source where we can learn about people's reactions to social issues. As COVID-19 has significantly impacted on people's lives, it is essential to capture how people react to public health interventions and understand their concerns. In this paper, we aim to investigate people's reactions and concerns about COVID-19 in North America, especially focusing on Canada. We analyze COVID-19 related tweets using topic modeling and aspect-based sentiment analysis, and interpret the results with public health experts. We compare timeline of topics discussed with timing of implementation of public health interventions for COVID-19. We also examine people's sentiment about COVID-19 related issues. We discuss how the results can be helpful for public health agencies when designing a policy for new interventions. Our work shows how Natural Language Processing (NLP) techniques could be applied to public health questions with domain expert involvement.","label":1,"title_clean":"Exploratory Analysis of COVID 19 Related Tweets in North America to Inform Public Health Institutes","abstract_clean":"Social media is a rich source where we can learn about people's reactions to social issues. As COVID 19 has significantly impacted on people's lives, it is essential to capture how people react to public health interventions and understand their concerns. In this paper, we aim to investigate people's reactions and concerns about COVID 19 in North America, especially focusing on Canada. We analyze COVID 19 related tweets using topic modeling and aspect based sentiment analysis, and interpret the results with public health experts. We compare timeline of topics discussed with timing of implementation of public health interventions for COVID 19. We also examine people's sentiment about COVID 19 related issues. We discuss how the results can be helpful for public health agencies when designing a policy for new interventions. Our work shows how Natural Language Processing (NLP) techniques could be applied to public health questions with domain expert involvement.","url":"https:\/\/aclanthology.org\/2020.nlpcovid19-2.18"},{"ID":"jansen-etal-2018-worldtree","methods":["explanation centered tablestore","question answering models"],"center_method":[null,null],"tasks":["standardized science exams","explainable inference task","automated inference","explaining elementary science exams","question answering","medicine"],"center_task":[null,null,null,null,"question answering",null],"Goal":["Quality Education"],"text":"WorldTree: A Corpus of Explanation Graphs for Elementary Science Questions supporting Multi-hop Inference. Developing methods of automated inference that are able to provide users with compelling human-readable justifications for why the answer to a question is correct is critical for domains such as science and medicine, where user trust and detecting costly errors are limiting factors to adoption. One of the central barriers to training question answering models on explainable inference tasks is the lack of gold explanations to serve as training data. In this paper we present a corpus of explanations for standardized science exams, a recent challenge task for question answering. We manually construct a corpus of detailed explanations for nearly all publicly available standardized elementary science question (approximately 1,680 3 rd through 5 th grade questions) and represent these as \"explanation graphs\"-sets of lexically overlapping sentences that describe how to arrive at the correct answer to a question through a combination of domain and world knowledge. We also provide an explanation-centered tablestore, a collection of semi-structured tables that contain the knowledge to construct these elementary science explanations. Together, these two knowledge resources map out a substantial portion of the knowledge required for answering and explaining elementary science exams, and provide both structured and free-text training data for the explainable inference task.","label":1,"title_clean":"WorldTree: A Corpus of Explanation Graphs for Elementary Science Questions supporting Multi hop Inference","abstract_clean":"Developing methods of automated inference that are able to provide users with compelling human readable justifications for why the answer to a question is correct is critical for domains such as science and medicine, where user trust and detecting costly errors are limiting factors to adoption. One of the central barriers to training question answering models on explainable inference tasks is the lack of gold explanations to serve as training data. In this paper we present a corpus of explanations for standardized science exams, a recent challenge task for question answering. We manually construct a corpus of detailed explanations for nearly all publicly available standardized elementary science question (approximately 1,680 3 rd through 5 th grade questions) and represent these as \"explanation graphs\" sets of lexically overlapping sentences that describe how to arrive at the correct answer to a question through a combination of domain and world knowledge. We also provide an explanation centered tablestore, a collection of semi structured tables that contain the knowledge to construct these elementary science explanations. Together, these two knowledge resources map out a substantial portion of the knowledge required for answering and explaining elementary science exams, and provide both structured and free text training data for the explainable inference task.","url":"https:\/\/aclanthology.org\/L18-1433"},{"ID":"jimenez-gutierrez-etal-2020-document","methods":["language models","bert","multi label document classification models"],"center_method":["language models","bert",null],"tasks":["data ablation study","classification"],"center_task":[null,"classification"],"Goal":["Good Health and Well-Being"],"text":"Document Classification for COVID-19 Literature. The global pandemic has made it more important than ever to quickly and accurately retrieve relevant scientific literature for effective consumption by researchers in a wide range of fields. We provide an analysis of several multi-label document classification models on the LitCovid dataset, a growing collection of 23,000 research papers regarding the novel 2019 coronavirus. We find that pre-trained language models fine-tuned on this dataset outperform all other baselines and that BioBERT surpasses the others by a small margin with micro-F1 and accuracy scores of around 86% and 75% respectively on the test set. We evaluate the data efficiency and generalizability of these models as essential features of any system prepared to deal with an urgent situation like the current health crisis. We perform a data ablation study to determine how important article titles are for achieving reasonable performance on this dataset. Finally, we explore 50 errors made by the best performing models on LitCovid documents and find that they often (1) correlate certain labels too closely together and (2) fail to focus on discriminative sections of the articles; both of which are important issues to address in future work. Both data and code are available on GitHub 1 .","label":1,"title_clean":"Document Classification for COVID 19 Literature","abstract_clean":"The global pandemic has made it more important than ever to quickly and accurately retrieve relevant scientific literature for effective consumption by researchers in a wide range of fields. We provide an analysis of several multi label document classification models on the LitCovid dataset, a growing collection of 23,000 research papers regarding the novel 2019 coronavirus. We find that pre trained language models fine tuned on this dataset outperform all other baselines and that BioBERT surpasses the others by a small margin with micro F1 and accuracy scores of around 86% and 75% respectively on the test set. We evaluate the data efficiency and generalizability of these models as essential features of any system prepared to deal with an urgent situation like the current health crisis. We perform a data ablation study to determine how important article titles are for achieving reasonable performance on this dataset. Finally, we explore 50 errors made by the best performing models on LitCovid documents and find that they often (1) correlate certain labels too closely together and (2) fail to focus on discriminative sections of the articles; both of which are important issues to address in future work. Both data and code are available on GitHub 1 .","url":"https:\/\/aclanthology.org\/2020.findings-emnlp.332"},{"ID":"jin-aletras-2020-complaint","methods":["transformers","language models","feature based and task specific neural network models"],"center_method":["transformers","language models",null],"tasks":["complaint identification","complaint prediction"],"center_task":[null,null],"Goal":["Decent Work and Economic Growth","Industry, Innovation and Infrastrucure"],"text":"Complaint Identification in Social Media with Transformer Networks. Complaining is a speech act extensively used by humans to communicate a negative inconsistency between reality and expectations. Previous work on automatically identifying complaints in social media has focused on using feature-based and task-specific neural network models. Adapting state-of-the-art pre-trained neural language models and their combinations with other linguistic information from topics or sentiment for complaint prediction has yet to be explored. In this paper, we evaluate a battery of neural models underpinned by transformer networks which we subsequently combine with linguistic information. Experiments on a publicly available data set of complaints demonstrate that our models outperform previous state-of-the-art methods by a large margin achieving a macro F1 up to 87.","label":1,"title_clean":"Complaint Identification in Social Media with Transformer Networks","abstract_clean":"Complaining is a speech act extensively used by humans to communicate a negative inconsistency between reality and expectations. Previous work on automatically identifying complaints in social media has focused on using feature based and task specific neural network models. Adapting state of the art pre trained neural language models and their combinations with other linguistic information from topics or sentiment for complaint prediction has yet to be explored. In this paper, we evaluate a battery of neural models underpinned by transformer networks which we subsequently combine with linguistic information. Experiments on a publicly available data set of complaints demonstrate that our models outperform previous state of the art methods by a large margin achieving a macro F1 up to 87.","url":"https:\/\/aclanthology.org\/2020.coling-main.157"},{"ID":"jing-etal-2018-automatic","methods":["hierarchical lstm model","multi task learning","medical imaging"],"center_method":[null,"multi task learning",null],"tasks":["automatic generation of medical imaging reports","report writing","generation of paragraphs","clinical practice","diagnosis and treatment","prediction of tags"],"center_task":[null,null,null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"On the Automatic Generation of Medical Imaging Reports. Medical imaging is widely used in clinical practice for diagnosis and treatment. Report-writing can be error-prone for unexperienced physicians, and timeconsuming and tedious for experienced physicians. To address these issues, we study the automatic generation of medical imaging reports. This task presents several challenges. First, a complete report contains multiple heterogeneous forms of information, including findings and tags. Second, abnormal regions in medical images are difficult to identify. Third, the reports are typically long, containing multiple sentences. To cope with these challenges, we (1) build a multi-task learning framework which jointly performs the prediction of tags and the generation of paragraphs, (2) propose a co-attention mechanism to localize regions containing abnormalities and generate narrations for them, (3) develop a hierarchical LSTM model to generate long paragraphs. We demonstrate the effectiveness of the proposed methods on two publicly available datasets.","label":1,"title_clean":"On the Automatic Generation of Medical Imaging Reports","abstract_clean":"Medical imaging is widely used in clinical practice for diagnosis and treatment. Report writing can be error prone for unexperienced physicians, and timeconsuming and tedious for experienced physicians. To address these issues, we study the automatic generation of medical imaging reports. This task presents several challenges. First, a complete report contains multiple heterogeneous forms of information, including findings and tags. Second, abnormal regions in medical images are difficult to identify. Third, the reports are typically long, containing multiple sentences. To cope with these challenges, we (1) build a multi task learning framework which jointly performs the prediction of tags and the generation of paragraphs, (2) propose a co attention mechanism to localize regions containing abnormalities and generate narrations for them, (3) develop a hierarchical LSTM model to generate long paragraphs. We demonstrate the effectiveness of the proposed methods on two publicly available datasets.","url":"https:\/\/aclanthology.org\/P18-1240.pdf"},{"ID":"jing-etal-2019-show","methods":["cooperative multi agent system"],"center_method":[null],"tasks":["automatically writing reports","diagnostic summarization","clinical screening and diagnosis"],"center_task":[null,null,null],"Goal":["Good Health and Well-Being"],"text":"Show, Describe and Conclude: On Exploiting the Structure Information of Chest X-ray Reports. Chest X-Ray (CXR) images are commonly used for clinical screening and diagnosis. Automatically writing reports for these images can considerably lighten the workload of radiologists for summarizing descriptive findings and conclusive impressions. The complex structures between and within sections of the reports pose a great challenge to the automatic report generation. Specifically, the section Impression is a diagnostic summarization over the section Findings; and the appearance of normality dominates each section over that of abnormality. Existing studies rarely explore and consider this fundamental structure information. In this work, we propose a novel framework which exploits the structure information between and within report sections for generating CXR imaging reports. First, we propose a two-stage strategy that explicitly models the relationship between Findings and Impression. Second, we design a novel cooperative multi-agent system that implicitly captures the imbalanced distribution between abnormality and normality. Experiments on two CXR report datasets show that our method achieves state-of-the-art performance in terms of various evaluation metrics. Our results expose that the proposed approach is able to generate high-quality medical reports through integrating the structure information. Findings: The cardiac silhouette is enlarged and has a globular appearance. Mild bibasilar dependent atelectasis. No pneumothorax or large pleural effusion. No acute bone abnormality. Impression: Cardiomegaly with globular appearance of the cardiac silhouette. Considerations would include pericardial effusion or dilated cardiomyopathy.","label":1,"title_clean":"Show, Describe and Conclude: On Exploiting the Structure Information of Chest X ray Reports","abstract_clean":"Chest X Ray (CXR) images are commonly used for clinical screening and diagnosis. Automatically writing reports for these images can considerably lighten the workload of radiologists for summarizing descriptive findings and conclusive impressions. The complex structures between and within sections of the reports pose a great challenge to the automatic report generation. Specifically, the section Impression is a diagnostic summarization over the section Findings; and the appearance of normality dominates each section over that of abnormality. Existing studies rarely explore and consider this fundamental structure information. In this work, we propose a novel framework which exploits the structure information between and within report sections for generating CXR imaging reports. First, we propose a two stage strategy that explicitly models the relationship between Findings and Impression. Second, we design a novel cooperative multi agent system that implicitly captures the imbalanced distribution between abnormality and normality. Experiments on two CXR report datasets show that our method achieves state of the art performance in terms of various evaluation metrics. Our results expose that the proposed approach is able to generate high quality medical reports through integrating the structure information. Findings: The cardiac silhouette is enlarged and has a globular appearance. Mild bibasilar dependent atelectasis. No pneumothorax or large pleural effusion. No acute bone abnormality. Impression: Cardiomegaly with globular appearance of the cardiac silhouette. Considerations would include pericardial effusion or dilated cardiomyopathy.","url":"https:\/\/aclanthology.org\/P19-1657"},{"ID":"joshi-etal-2015-computational","methods":["computational approach"],"center_method":[null],"tasks":["automatic drunk texting prediction"],"center_task":[null],"Goal":["Good Health and Well-Being"],"text":"A Computational Approach to Automatic Prediction of Drunk-Texting. Alcohol abuse may lead to unsociable behavior such as crime, drunk driving, or privacy leaks. We introduce automatic drunk-texting prediction as the task of identifying whether a text was written when under the influence of alcohol. We experiment with tweets labeled using hashtags as distant supervision. Our classifiers use a set of N-gram and stylistic features to detect drunk tweets. Our observations present the first quantitative evidence that text contains signals that can be exploited to detect drunk-texting.","label":1,"title_clean":"A Computational Approach to Automatic Prediction of Drunk Texting","abstract_clean":"Alcohol abuse may lead to unsociable behavior such as crime, drunk driving, or privacy leaks. We introduce automatic drunk texting prediction as the task of identifying whether a text was written when under the influence of alcohol. We experiment with tweets labeled using hashtags as distant supervision. Our classifiers use a set of N gram and stylistic features to detect drunk tweets. Our observations present the first quantitative evidence that text contains signals that can be exploited to detect drunk texting.","url":"https:\/\/aclanthology.org\/P15-2100.pdf"},{"ID":"joshi-etal-2020-dr","methods":["pointer generator network"],"center_method":[null],"tasks":["medical conversation understanding","text summarization","follow ups","medical decision making","natural language understanding challenge","medical practice"],"center_task":[null,"text summarization",null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"Dr. Summarize: Global Summarization of Medical Dialogue by Exploiting Local Structures.. Understanding a medical conversation between a patient and a physician poses unique natural language understanding challenge since it combines elements of standard open-ended conversation with very domainspecific elements that require expertise and medical knowledge. Summarization of medical conversations is a particularly important aspect of medical conversation understanding since it addresses a very real need in medical practice: capturing the most important aspects of a medical encounter so that they can be used for medical decision making and subsequent follow ups. In this paper we present a novel approach to medical conversation summarization that leverages the unique and independent local structures created when gathering a patient's medical history. Our approach is a variation of the pointer generator network where we introduce a penalty on the generator distribution, and we explicitly model negations. The model also captures important properties of medical conversations such as medical knowledge coming from standardized medical ontologies better than when those concepts are introduced explicitly. Through evaluation by doctors, we show that our approach is preferred on twice the number of summaries to the baseline pointer generator model and captures most or all of the information in 80% of the conversations making it a realistic alternative to costly manual summarization by medical experts.","label":1,"title_clean":"Dr. Summarize: Global Summarization of Medical Dialogue by Exploiting Local Structures.","abstract_clean":"Understanding a medical conversation between a patient and a physician poses unique natural language understanding challenge since it combines elements of standard open ended conversation with very domainspecific elements that require expertise and medical knowledge. Summarization of medical conversations is a particularly important aspect of medical conversation understanding since it addresses a very real need in medical practice: capturing the most important aspects of a medical encounter so that they can be used for medical decision making and subsequent follow ups. In this paper we present a novel approach to medical conversation summarization that leverages the unique and independent local structures created when gathering a patient's medical history. Our approach is a variation of the pointer generator network where we introduce a penalty on the generator distribution, and we explicitly model negations. The model also captures important properties of medical conversations such as medical knowledge coming from standardized medical ontologies better than when those concepts are introduced explicitly. Through evaluation by doctors, we show that our approach is preferred on twice the number of summaries to the baseline pointer generator model and captures most or all of the information in 80% of the conversations making it a realistic alternative to costly manual summarization by medical experts.","url":"https:\/\/aclanthology.org\/2020.findings-emnlp.335.pdf"},{"ID":"juhasz-etal-2019-tuefact","methods":["tue fact","lstm","multi class logistic regression","bagof word ngram model","subtask a","vectorized character n grams"],"center_method":[null,"lstm",null,null,null,null],"tasks":["predictions","fact checking","community question answering forums","se meval task"],"center_task":["predictions","fact checking",null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"TueFact at SemEval 2019 Task 8: Fact checking in community question answering forums: context matters. The SemEval 2019 Task 8 on Fact-Checking in community question answering forums aimed to classify questions into categories and verify the correctness of answers given on the QatarLiving public forum. The task was divided into two subtasks: the first classifying the question, the second the answers. The Tue-Fact system described in this paper used different approaches for the two subtasks. Subtask A makes use of word vectors based on a bagof-word-ngram model using up to trigrams. Predictions are done using multi-class logistic regression. The official SemEval result lists an accuracy of 0.60. Subtask B uses vectorized character n-grams up to trigrams instead. Predictions are done using a LSTM model and achieved an accuracy of 0.53 on the final Se-mEval Task 8 evaluation set. In a comparison of contextual inputs to subtask B, it was determined that more contextual data improved results, but only up to a point.","label":1,"title_clean":"TueFact at SemEval 2019 Task 8: Fact checking in community question answering forums: context matters","abstract_clean":"The SemEval 2019 Task 8 on Fact Checking in community question answering forums aimed to classify questions into categories and verify the correctness of answers given on the QatarLiving public forum. The task was divided into two subtasks: the first classifying the question, the second the answers. The Tue Fact system described in this paper used different approaches for the two subtasks. Subtask A makes use of word vectors based on a bagof word ngram model using up to trigrams. Predictions are done using multi class logistic regression. The official SemEval result lists an accuracy of 0.60. Subtask B uses vectorized character n grams up to trigrams instead. Predictions are done using a LSTM model and achieved an accuracy of 0.53 on the final Se mEval Task 8 evaluation set. In a comparison of contextual inputs to subtask B, it was determined that more contextual data improved results, but only up to a point.","url":"https:\/\/aclanthology.org\/S19-2206"},{"ID":"kaewphan-etal-2014-utu","methods":["conditional random field","vector space representations","nersuite","utu"],"center_method":["conditional random field",null,null,null],"tasks":["normalization","named entity recognition","semeval 2014 task"],"center_task":["normalization","named entity recognition",null],"Goal":["Good Health and Well-Being"],"text":"UTU: Disease Mention Recognition and Normalization with CRFs and Vector Space Representations. In this paper we present our system participating in the SemEval-2014 Task 7 in both subtasks A and B, aiming at recognizing and normalizing disease and symptom mentions from electronic medical records respectively. In subtask A, we used an existing NER system, NERsuite, with our own feature set tailored for this task. For subtask B, we combined word vector representations and supervised machine learning to map the recognized mentions to the corresponding UMLS concepts. Our system was placed 2nd and 5th out of 21 participants on subtasks A and B respectively showing competitive performance.","label":1,"title_clean":"UTU: Disease Mention Recognition and Normalization with CRFs and Vector Space Representations","abstract_clean":"In this paper we present our system participating in the SemEval 2014 Task 7 in both subtasks A and B, aiming at recognizing and normalizing disease and symptom mentions from electronic medical records respectively. In subtask A, we used an existing NER system, NERsuite, with our own feature set tailored for this task. For subtask B, we combined word vector representations and supervised machine learning to map the recognized mentions to the corresponding UMLS concepts. Our system was placed 2nd and 5th out of 21 participants on subtasks A and B respectively showing competitive performance.","url":"https:\/\/aclanthology.org\/S14-2143"},{"ID":"kanakarajan-etal-2019-saama","methods":["attention visualisation","bert","visualization tool","bidirectional encoder representation from transformer","bertviz"],"center_method":[null,"bert",null,null,null],"tasks":["inference","writing","algorithm development","clinical domain"],"center_task":["inference",null,null,null],"Goal":["Good Health and Well-Being"],"text":"Saama Research at MEDIQA 2019: Pre-trained BioBERT with Attention Visualisation for Medical Natural Language Inference. Natural Language inference is the task of identifying relation between two sentences as entailment, contradiction or neutrality. MedNLI is a biomedical flavour of NLI for clinical domain. This paper explores the use of Bidirectional Encoder Representation from Transformer (BERT) for solving MedNLI. The proposed model, BERT pre-trained on PMC, PubMed and fine-tuned on MIMIC-III v1.4, achieves state of the art results on MedNLI (83.45%) and an accuracy of 78.5% in MEDIQA challenge. The authors present an analysis of the attention patterns that emerged as a result of training BERT on MedNLI using a visualization tool, bertviz. * *Equal Contribution: Kamal had sole access to MIMIC and MEDIQA data, focussed on the algorithm development and implementation. Suriyadeepan and Archana focussed on the attention visualisation and writing. Soham and Malaikannan focussed on reviewing","label":1,"title_clean":"Saama Research at MEDIQA 2019: Pre trained BioBERT with Attention Visualisation for Medical Natural Language Inference","abstract_clean":"Natural Language inference is the task of identifying relation between two sentences as entailment, contradiction or neutrality. MedNLI is a biomedical flavour of NLI for clinical domain. This paper explores the use of Bidirectional Encoder Representation from Transformer (BERT) for solving MedNLI. The proposed model, BERT pre trained on PMC, PubMed and fine tuned on MIMIC III v1.4, achieves state of the art results on MedNLI (83.45%) and an accuracy of 78.5% in MEDIQA challenge. The authors present an analysis of the attention patterns that emerged as a result of training BERT on MedNLI using a visualization tool, bertviz. * *Equal Contribution: Kamal had sole access to MIMIC and MEDIQA data, focussed on the algorithm development and implementation. Suriyadeepan and Archana focussed on the attention visualisation and writing. Soham and Malaikannan focussed on reviewing","url":"https:\/\/aclanthology.org\/W19-5055"},{"ID":"kang-etal-2018-dataset","methods":["nips","mean baseline","iclr","baseline models"],"center_method":[null,null,null,null],"tasks":["peer reviewing"],"center_task":[null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications. Peer reviewing is a central component in the scientific publishing process. We present the first public dataset of scientific peer reviews available for research purposes (PeerRead v1), 1 providing an opportunity to study this important artifact. The dataset consists of 14.7K paper drafts and the corresponding accept\/reject decisions in top-tier venues including ACL, NIPS and ICLR. The dataset also includes 10.7K textual peer reviews written by experts for a subset of the papers. We describe the data collection process and report interesting observed phenomena in the peer reviews. We also propose two novel NLP tasks based on this dataset and provide simple baseline models. In the first task, we show that simple models can predict whether a paper is accepted with up to 21% error reduction compared to the majority baseline. In the second task, we predict the numerical scores of review aspects and show that simple models can outperform the mean baseline for aspects with high variance such as 'originality' and 'impact'.","label":1,"title_clean":"A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications","abstract_clean":"Peer reviewing is a central component in the scientific publishing process. We present the first public dataset of scientific peer reviews available for research purposes (PeerRead v1), 1 providing an opportunity to study this important artifact. The dataset consists of 14.7K paper drafts and the corresponding accept\/reject decisions in top tier venues including ACL, NIPS and ICLR. The dataset also includes 10.7K textual peer reviews written by experts for a subset of the papers. We describe the data collection process and report interesting observed phenomena in the peer reviews. We also propose two novel NLP tasks based on this dataset and provide simple baseline models. In the first task, we show that simple models can predict whether a paper is accepted with up to 21% error reduction compared to the majority baseline. In the second task, we predict the numerical scores of review aspects and show that simple models can outperform the mean baseline for aspects with high variance such as 'originality' and 'impact'.","url":"https:\/\/aclanthology.org\/N18-1149"},{"ID":"kao-chen-2011-diagnosing","methods":["cas","misuse framework"],"center_method":[null,null],"tasks":["learner writing","correction","diagnosing discoursal organization"],"center_task":[null,null,null],"Goal":["Quality Education"],"text":"Diagnosing Discoursal Organization in Learner Writing via Conjunctive Adverbials (\u8a3a\u65b7\u5b78\u7fd2\u8005\u82f1\u8a9e\u5beb\u4f5c\u7bc7\u7ae0\u7d50\u69cb\uff1a\u4ee5\u7bc7\u7ae0\u9023\u63a5\u526f\u8a5e\u70ba\u4f8b). The present study aims to investigate genre influence on the use and misuse of conjunctive adverbials (hereafter CAs) by compiling a learner corpus annotated with discoursal information on CAs. To do so, an online interface is constructed to collect and annotate data, and an annotating system for identifying the use and misuse of CAs is developed. The results show that genre difference has no impact on the use and misuse of CAs, but that there does exist a norm distribution of textual relations performed by CAs, indicating a preference preset in human cognition. Statistic analysis also shows that the proposed misuse patterns do significantly differ from one another in terms of appropriateness and necessity, ratifying the need to differentiate these misuse patterns. The results in the present study have three possible applications. First, the annotate data can serve as training data for developing technology that automatically diagnoses learner writing on the discoursal level. Second, the founding that textual relations performed by CAs form a distribution norm can be used as a principle to evaluate discoursal organization in learner writing. Lastly, the misuse framework not only identifies the location of misuse of CAs but also indicates direction for correction.","label":1,"title_clean":"Diagnosing Discoursal Organization in Learner Writing via Conjunctive Adverbials (\u8a3a\u65b7\u5b78\u7fd2\u8005\u82f1\u8a9e\u5beb\u4f5c\u7bc7\u7ae0\u7d50\u69cb\uff1a\u4ee5\u7bc7\u7ae0\u9023\u63a5\u526f\u8a5e\u70ba\u4f8b)","abstract_clean":"The present study aims to investigate genre influence on the use and misuse of conjunctive adverbials (hereafter CAs) by compiling a learner corpus annotated with discoursal information on CAs. To do so, an online interface is constructed to collect and annotate data, and an annotating system for identifying the use and misuse of CAs is developed. The results show that genre difference has no impact on the use and misuse of CAs, but that there does exist a norm distribution of textual relations performed by CAs, indicating a preference preset in human cognition. Statistic analysis also shows that the proposed misuse patterns do significantly differ from one another in terms of appropriateness and necessity, ratifying the need to differentiate these misuse patterns. The results in the present study have three possible applications. First, the annotate data can serve as training data for developing technology that automatically diagnoses learner writing on the discoursal level. Second, the founding that textual relations performed by CAs form a distribution norm can be used as a principle to evaluate discoursal organization in learner writing. Lastly, the misuse framework not only identifies the location of misuse of CAs but also indicates direction for correction.","url":"https:\/\/aclanthology.org\/O11-2010"},{"ID":"karamanolakis-etal-2020-txtract","methods":["txtract","multi task learning","category conditional self attention","taxonomyaware knowledge extraction model"],"center_method":[null,"multi task learning",null,null],"tasks":["taxonomy aware knowledge extraction","e commerce","thousands of product categories","extracting structured knowledge"],"center_task":[null,"e commerce",null,null],"Goal":["Industry, Innovation and Infrastrucure","Decent Work and Economic Growth"],"text":"TXtract: Taxonomy-Aware Knowledge Extraction for Thousands of Product Categories. Extracting structured knowledge from product profiles is crucial for various applications in e-Commerce. State-of-the-art approaches for knowledge extraction were each designed for a single category of product, and thus do not apply to real-life e-Commerce scenarios, which often contain thousands of diverse categories. This paper proposes TXtract, a taxonomyaware knowledge extraction model that applies to thousands of product categories organized in a hierarchical taxonomy. Through category conditional self-attention and multi-task learning, our approach is both scalable, as it trains a single model for thousands of categories, and effective, as it extracts categoryspecific attribute values. Experiments on products from a taxonomy with 4,000 categories show that TXtract outperforms state-of-the-art approaches by up to 10% in F1 and 15% in coverage across all categories.","label":1,"title_clean":"TXtract: Taxonomy Aware Knowledge Extraction for Thousands of Product Categories","abstract_clean":"Extracting structured knowledge from product profiles is crucial for various applications in e Commerce. State of the art approaches for knowledge extraction were each designed for a single category of product, and thus do not apply to real life e Commerce scenarios, which often contain thousands of diverse categories. This paper proposes TXtract, a taxonomyaware knowledge extraction model that applies to thousands of product categories organized in a hierarchical taxonomy. Through category conditional self attention and multi task learning, our approach is both scalable, as it trains a single model for thousands of categories, and effective, as it extracts categoryspecific attribute values. Experiments on products from a taxonomy with 4,000 categories show that TXtract outperforms state of the art approaches by up to 10% in F1 and 15% in coverage across all categories.","url":"https:\/\/aclanthology.org\/2020.acl-main.751"},{"ID":"kardas-etal-2020-axcell","methods":["automatic machine learning pipeline","axcell"],"center_method":[null,null],"tasks":["extraction","backtranslation","machine learning methods","production","tracking"],"center_task":[null,"backtranslation","machine learning methods",null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"AxCell: Automatic Extraction of Results from Machine Learning Papers. Tracking progress in machine learning has become increasingly difficult with the recent explosion in the number of papers. In this paper, we present AXCELL, an automatic machine learning pipeline for extracting results from papers. AXCELL uses several novel components, including a table segmentation subtask, to learn relevant structural knowledge that aids extraction. When compared with existing methods, our approach significantly improves the state of the art for results extraction. We also release a structured, annotated dataset for training models for results extraction, and a dataset for evaluating the performance of models on this task. Lastly, we show the viability of our approach enables it to be used for semi-automated results extraction in production, suggesting our improvements make this task practically viable for the first time. Code is available on GitHub. 1 Back-translation. . .","label":1,"title_clean":"AxCell: Automatic Extraction of Results from Machine Learning Papers","abstract_clean":"Tracking progress in machine learning has become increasingly difficult with the recent explosion in the number of papers. In this paper, we present AXCELL, an automatic machine learning pipeline for extracting results from papers. AXCELL uses several novel components, including a table segmentation subtask, to learn relevant structural knowledge that aids extraction. When compared with existing methods, our approach significantly improves the state of the art for results extraction. We also release a structured, annotated dataset for training models for results extraction, and a dataset for evaluating the performance of models on this task. Lastly, we show the viability of our approach enables it to be used for semi automated results extraction in production, suggesting our improvements make this task practically viable for the first time. Code is available on GitHub. 1 Back translation. . .","url":"https:\/\/aclanthology.org\/2020.emnlp-main.692"},{"ID":"karimi-tang-2019-learning","methods":["hierarchical discourselevel structure","hdsf"],"center_method":[null,null],"tasks":["fake news detection","understating of fake news","structural analysis","capturing discourse level structure"],"center_task":["fake news detection",null,null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Learning Hierarchical Discourse-level Structure for Fake News Detection. On the one hand, nowadays, fake news articles are easily propagated through various online media platforms and have become a grand threat to the trustworthiness of information. On the other hand, our understanding of the language of fake news is still minimal. Incorporating hierarchical discourse-level structure of fake and real news articles is one crucial step toward a better understanding of how these articles are structured. Nevertheless, this has rarely been investigated in the fake news detection domain and faces tremendous challenges. First, existing methods for capturing discourse-level structure rely on annotated corpora which are not available for fake news datasets. Second, how to extract out useful information from such discovered structures is another challenge. To address these challenges, we propose Hierarchical Discourselevel Structure for Fake news detection. HDSF learns and constructs a discourse-level structure for fake\/real news articles in an automated and data-driven manner. Moreover, we identify insightful structure-related properties, which can explain the discovered structures and boost our understating of fake news. Conducted experiments show the effectiveness of the proposed approach. Further structural analysis suggests that real and fake news present substantial differences in the hierarchical discourse-level structures.","label":1,"title_clean":"Learning Hierarchical Discourse level Structure for Fake News Detection","abstract_clean":"On the one hand, nowadays, fake news articles are easily propagated through various online media platforms and have become a grand threat to the trustworthiness of information. On the other hand, our understanding of the language of fake news is still minimal. Incorporating hierarchical discourse level structure of fake and real news articles is one crucial step toward a better understanding of how these articles are structured. Nevertheless, this has rarely been investigated in the fake news detection domain and faces tremendous challenges. First, existing methods for capturing discourse level structure rely on annotated corpora which are not available for fake news datasets. Second, how to extract out useful information from such discovered structures is another challenge. To address these challenges, we propose Hierarchical Discourselevel Structure for Fake news detection. HDSF learns and constructs a discourse level structure for fake\/real news articles in an automated and data driven manner. Moreover, we identify insightful structure related properties, which can explain the discovered structures and boost our understating of fake news. Conducted experiments show the effectiveness of the proposed approach. Further structural analysis suggests that real and fake news present substantial differences in the hierarchical discourse level structures.","url":"https:\/\/aclanthology.org\/N19-1347"},{"ID":"kebriaei-etal-2019-emad","methods":["augmentation approach","word embeddings","deep neural network","data augmentation method","emad","machine learning and deep learning approaches","machine learning methods"],"center_method":[null,"word embeddings","deep neural network",null,null,null,"machine learning methods"],"tasks":["hate speech","automatic categorization of offense types","offenseval shared task","offense target identification"],"center_task":["hate speech",null,null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Emad at SemEval-2019 Task 6: Offensive Language Identification using Traditional Machine Learning and Deep Learning approaches. In this paper, the used methods and the results obtained by our team, entitled Emad, on the OffensEval 2019 shared task organized at Se-mEval 2019 are presented. The OffensEval shared task includes three sub-tasks namely Offensive language identification, Automatic categorization of offense types and Offense target identification. We participated in subtask A and tried various methods including traditional machine learning methods, deep learning methods and also a combination of the first two sets of methods. We also proposed a data augmentation method using word embedding to improve the performance of our methods. The results show that the augmentation approach outperforms other methods in terms of macro-f1.","label":1,"title_clean":"Emad at SemEval 2019 Task 6: Offensive Language Identification using Traditional Machine Learning and Deep Learning approaches","abstract_clean":"In this paper, the used methods and the results obtained by our team, entitled Emad, on the OffensEval 2019 shared task organized at Se mEval 2019 are presented. The OffensEval shared task includes three sub tasks namely Offensive language identification, Automatic categorization of offense types and Offense target identification. We participated in subtask A and tried various methods including traditional machine learning methods, deep learning methods and also a combination of the first two sets of methods. We also proposed a data augmentation method using word embedding to improve the performance of our methods. The results show that the augmentation approach outperforms other methods in terms of macro f1.","url":"https:\/\/aclanthology.org\/S19-2107"},{"ID":"kennedy-etal-2017-technology","methods":["machine learning methods","harassment dataset","classification"],"center_method":["machine learning methods",null,"classification"],"tasks":["online harassment"],"center_task":[null],"Goal":["Good Health and Well-Being","Peace, Justice and Strong Institutions"],"text":"Technology Solutions to Combat Online Harassment. This work is part of a new initiative to use machine learning to identify online harassment in social media and comment streams. Online harassment goes underreported due to the reliance on humans to identify and report harassment, reporting that is further slowed by requirements to fill out forms providing context. In addition, the time for moderators to respond and apply human judgment can take days, but response times in terms of minutes are needed in the online context. Though some of the major social media companies have been doing proprietary work in automating the detection of harassment, there are few tools available for use by the public. In addition, the amount of labeled online harassment data and availability of cross platform online harassment datasets is limited. We present the methodology used to create a harassment dataset and classifier and the dataset used to help the system learn what harassment looks like.","label":1,"title_clean":"Technology Solutions to Combat Online Harassment","abstract_clean":"This work is part of a new initiative to use machine learning to identify online harassment in social media and comment streams. Online harassment goes underreported due to the reliance on humans to identify and report harassment, reporting that is further slowed by requirements to fill out forms providing context. In addition, the time for moderators to respond and apply human judgment can take days, but response times in terms of minutes are needed in the online context. Though some of the major social media companies have been doing proprietary work in automating the detection of harassment, there are few tools available for use by the public. In addition, the amount of labeled online harassment data and availability of cross platform online harassment datasets is limited. We present the methodology used to create a harassment dataset and classifier and the dataset used to help the system learn what harassment looks like.","url":"https:\/\/aclanthology.org\/W17-3011"},{"ID":"khalifa-etal-2016-utahbmi","methods":["conditional random field","crf based classifiers","ensemble based approach","cleartk","machine learning methods"],"center_method":["conditional random field",null,null,null,"machine learning methods"],"tasks":["extracting temporal information","multi word spans"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"UtahBMI at SemEval-2016 Task 12: Extracting Temporal Information from Clinical Text. The 2016 Clinical TempEval continued the 2015 shared task on temporal information extraction with a new evaluation test set. Our team, UtahBMI, participated in all subtasks using machine learning approaches with ClearTK (LIBLINEAR), CRF++ and CRFsuite packages. Our experiments show that CRF-based classifiers yield, in general, higher recall for multi-word spans, while SVM-based classifiers are better at predicting correct attributes of TIMEX3. In addition, we show that an ensemble-based approach for TIMEX3 could yield improved results. Our team achieved competitive results in each subtask with an F1 75.4% for TIMEX3, F1 89.2% for EVENT, F1 84.4% for event relations with document time (DocTimeRel), and F1 51.1% for narrative container (CONTAINS) relations.","label":1,"title_clean":"UtahBMI at SemEval 2016 Task 12: Extracting Temporal Information from Clinical Text","abstract_clean":"The 2016 Clinical TempEval continued the 2015 shared task on temporal information extraction with a new evaluation test set. Our team, UtahBMI, participated in all subtasks using machine learning approaches with ClearTK (LIBLINEAR), CRF++ and CRFsuite packages. Our experiments show that CRF based classifiers yield, in general, higher recall for multi word spans, while SVM based classifiers are better at predicting correct attributes of TIMEX3. In addition, we show that an ensemble based approach for TIMEX3 could yield improved results. Our team achieved competitive results in each subtask with an F1 75.4% for TIMEX3, F1 89.2% for EVENT, F1 84.4% for event relations with document time (DocTimeRel), and F1 51.1% for narrative container (CONTAINS) relations.","url":"https:\/\/aclanthology.org\/S16-1195"},{"ID":"khanna-etal-2022-idiap","methods":["contextualized bert","attention","bert"],"center_method":[null,"attention","bert"],"tasks":["hope speech detection","equality","inclusion acl 2022"],"center_task":["hope speech detection",null,null],"Goal":["Peace, Justice and Strong Institutions","Good Health and Well-Being"],"text":"IDIAP\\_TIET@LT-EDI-ACL2022 : Hope Speech Detection in Social Media using Contextualized BERT with Attention Mechanism. With the increase of users on social media platforms, manipulating or provoking masses of people has become a piece of cake. This spread of hatred among people, which has become a loophole for freedom of speech, must be minimized. Hence, it is essential to have a system that automatically classifies the hatred content, especially on social media, to take it down. This paper presents a simple modular pipeline classifier with BERT embeddings and attention mechanism to classify hope speech content in the Hope Speech Detection shared task for Equality, Diversity, and Inclusion-ACL 2022. Our system submission ranks fourth with an F1-score of 0.84. We release our code-base here https: \/\/github.com\/Deepanshu-beep\/ hope-speech-attention.","label":1,"title_clean":"IDIAP\\_TIET@LT EDI ACL2022 : Hope Speech Detection in Social Media using Contextualized BERT with Attention Mechanism","abstract_clean":"With the increase of users on social media platforms, manipulating or provoking masses of people has become a piece of cake. This spread of hatred among people, which has become a loophole for freedom of speech, must be minimized. Hence, it is essential to have a system that automatically classifies the hatred content, especially on social media, to take it down. This paper presents a simple modular pipeline classifier with BERT embeddings and attention mechanism to classify hope speech content in the Hope Speech Detection shared task for Equality, Diversity, and Inclusion ACL 2022. Our system submission ranks fourth with an F1 score of 0.84. We release our code base here https: \/\/github.com\/Deepanshu beep\/ hope speech attention.","url":"https:\/\/aclanthology.org\/2022.ltedi-1.49.pdf"},{"ID":"kim-etal-2019-textbook","methods":["module f gcn","self supervised learning process","graph neural networks"],"center_method":[null,null,"graph neural networks"],"tasks":["multi modal context graph understanding","textbook question answering","extracting knowledge","out of domain issue","self supervised open set comprehension","tqa problems"],"center_task":[null,null,null,null,null,null],"Goal":["Quality Education"],"text":"Textbook Question Answering with Multi-modal Context Graph Understanding and Self-supervised Open-set Comprehension. In this work, we introduce a novel algorithm for solving the textbook question answering (TQA) task which describes more realistic QA problems compared to other recent tasks. We mainly focus on two related issues with analysis of the TQA dataset. First, solving the TQA problems requires to comprehend multimodal contexts in complicated input data. To tackle this issue of extracting knowledge features from long text lessons and merging them with visual features, we establish a context graph from texts and images, and propose a new module f-GCN based on graph convolutional networks (GCN). Second, scientific terms are not spread over the chapters and subjects are split in the TQA dataset. To overcome this so called 'out-of-domain' issue, before learning QA problems, we introduce a novel self-supervised open-set learning process without any annotations. The experimental results show that our model significantly outperforms prior state-of-the-art methods. Moreover, ablation studies validate that both methods of incorporating f-GCN for extracting knowledge from multi-modal contexts and our newly proposed self-supervised learning process are effective for TQA problems.","label":1,"title_clean":"Textbook Question Answering with Multi modal Context Graph Understanding and Self supervised Open set Comprehension","abstract_clean":"In this work, we introduce a novel algorithm for solving the textbook question answering (TQA) task which describes more realistic QA problems compared to other recent tasks. We mainly focus on two related issues with analysis of the TQA dataset. First, solving the TQA problems requires to comprehend multimodal contexts in complicated input data. To tackle this issue of extracting knowledge features from long text lessons and merging them with visual features, we establish a context graph from texts and images, and propose a new module f GCN based on graph convolutional networks (GCN). Second, scientific terms are not spread over the chapters and subjects are split in the TQA dataset. To overcome this so called 'out of domain' issue, before learning QA problems, we introduce a novel self supervised open set learning process without any annotations. The experimental results show that our model significantly outperforms prior state of the art methods. Moreover, ablation studies validate that both methods of incorporating f GCN for extracting knowledge from multi modal contexts and our newly proposed self supervised learning process are effective for TQA problems.","url":"https:\/\/aclanthology.org\/P19-1347.pdf"},{"ID":"kim-park-2004-bioar","methods":["bioar","bioie","anaphora resolution system","biomedical information extraction system"],"center_method":[null,null,null,null],"tasks":["relating protein names","anaphora resolution"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"BioAR: Anaphora Resolution for Relating Protein Names to Proteome Database Entries. The need for associating, or grounding, protein names in the literature with the entries of proteome databases such as Swiss-Prot is well-recognized. The protein names in the biomedical literature show a high degree of morphological and syntactic variations, and various anaphoric expressions including null anaphors. We present a biomedical anaphora resolution system, BioAR, in order to address the variations of protein names and to further associate them with Swiss-Prot entries as the actual entities in the world. The system shows the performance of 59.5%\u00a2 75.0% precision and 40.7%\u00a2 56.3% recall, depending on the specific types of anaphoric expressions. We apply BioAR to the protein names in the biological interactions as extracted by our biomedical information extraction system, or BioIE, in order to construct protein pathways automatically.","label":1,"title_clean":"BioAR: Anaphora Resolution for Relating Protein Names to Proteome Database Entries","abstract_clean":"The need for associating, or grounding, protein names in the literature with the entries of proteome databases such as Swiss Prot is well recognized. The protein names in the biomedical literature show a high degree of morphological and syntactic variations, and various anaphoric expressions including null anaphors. We present a biomedical anaphora resolution system, BioAR, in order to address the variations of protein names and to further associate them with Swiss Prot entries as the actual entities in the world. The system shows the performance of 59.5%\u00a2 75.0% precision and 40.7%\u00a2 56.3% recall, depending on the specific types of anaphoric expressions. We apply BioAR to the protein names in the biological interactions as extracted by our biomedical information extraction system, or BioIE, in order to construct protein pathways automatically.","url":"https:\/\/aclanthology.org\/W04-0711"},{"ID":"kim-park-2015-statistical","methods":["wm","l2 learners","wm based processing analysis","statistical modeling","grammatical analysis"],"center_method":[null,null,null,null,null],"tasks":["computational linguistics"],"center_task":["computational linguistics"],"Goal":["Quality Education","Peace, Justice and Strong Institutions"],"text":"A Statistical Modeling of the Correlation between Island Effects and Working-memory Capacity for L2 Learners. The cause of island effects has evoked considerable debate within syntax and other fields of linguistics. The two competing approaches stand out: the grammatical analysis; and the working-memory (WM)-based processing analysis. In this paper we report three experiments designed to test one of the premises of the WM-based processing analysis: that the strength of island effects should vary as a function of individual differences in WM capacity. The results show that island effects present even for L2 learners are more likely attributed to grammatical constraints than to limited processing resources.","label":1,"title_clean":"A Statistical Modeling of the Correlation between Island Effects and Working memory Capacity for L2 Learners","abstract_clean":"The cause of island effects has evoked considerable debate within syntax and other fields of linguistics. The two competing approaches stand out: the grammatical analysis; and the working memory (WM) based processing analysis. In this paper we report three experiments designed to test one of the premises of the WM based processing analysis: that the strength of island effects should vary as a function of individual differences in WM capacity. The results show that island effects present even for L2 learners are more likely attributed to grammatical constraints than to limited processing resources.","url":"https:\/\/aclanthology.org\/Y15-1048.pdf"},{"ID":"kim-riloff-2015-stacked","methods":["concept extraction models","stacked generalization","knowledgebased","meta classifier","machine learning methods","classification","metalearning technique"],"center_method":[null,null,null,null,"machine learning methods","classification",null],"tasks":["medical concept extraction"],"center_task":[null],"Goal":["Good Health and Well-Being"],"text":"Stacked Generalization for Medical Concept Extraction from Clinical Notes. The goal of our research is to extract medical concepts from clinical notes containing patient information. Our research explores stacked generalization as a metalearning technique to exploit a diverse set of concept extraction models. First, we create multiple models for concept extraction using a variety of information extraction techniques, including knowledgebased, rule-based, and machine learning models. Next, we train a meta-classifier using stacked generalization with a feature set generated from the outputs of the individual classifiers. The meta-classifier learns to predict concepts based on information about the predictions of the component classifiers. Our results show that the stacked generalization learner performs better than the individual models and achieves state-of-the-art performance on the 2010 i2b2 data set.","label":1,"title_clean":"Stacked Generalization for Medical Concept Extraction from Clinical Notes","abstract_clean":"The goal of our research is to extract medical concepts from clinical notes containing patient information. Our research explores stacked generalization as a metalearning technique to exploit a diverse set of concept extraction models. First, we create multiple models for concept extraction using a variety of information extraction techniques, including knowledgebased, rule based, and machine learning models. Next, we train a meta classifier using stacked generalization with a feature set generated from the outputs of the individual classifiers. The meta classifier learns to predict concepts based on information about the predictions of the component classifiers. Our results show that the stacked generalization learner performs better than the individual models and achieves state of the art performance on the 2010 i2b2 data set.","url":"https:\/\/aclanthology.org\/W15-3807"},{"ID":"kinnunen-etal-2012-swan","methods":["swan scientific writing assistant","rule based system"],"center_method":[null,"rule based system"],"tasks":["writing problems"],"center_task":[null],"Goal":["Quality Education","Industry, Innovation and Infrastrucure"],"text":"SWAN - Scientific Writing AssistaNt. A Tool for Helping Scholars to Write Reader-Friendly Manuscripts. Difficulty of reading scholarly papers is significantly reduced by reader-friendly writing principles. Writing reader-friendly text, however, is challenging due to difficulty in recognizing problems in one's own writing. To help scholars identify and correct potential writing problems, we introduce SWAN (Scientific Writing AssistaNt) tool. SWAN is a rule-based system that gives feedback based on various quality metrics based on years of experience from scientific writing classes including 960 scientists of various backgrounds: life sciences, engineering sciences and economics. According to our first experiences, users have perceived SWAN as helpful in identifying problematic sections in text and increasing overall clarity of manuscripts.","label":1,"title_clean":"SWAN  Scientific Writing AssistaNt. A Tool for Helping Scholars to Write Reader Friendly Manuscripts","abstract_clean":"Difficulty of reading scholarly papers is significantly reduced by reader friendly writing principles. Writing reader friendly text, however, is challenging due to difficulty in recognizing problems in one's own writing. To help scholars identify and correct potential writing problems, we introduce SWAN (Scientific Writing AssistaNt) tool. SWAN is a rule based system that gives feedback based on various quality metrics based on years of experience from scientific writing classes including 960 scientists of various backgrounds: life sciences, engineering sciences and economics. According to our first experiences, users have perceived SWAN as helpful in identifying problematic sections in text and increasing overall clarity of manuscripts.","url":"https:\/\/aclanthology.org\/E12-2005"},{"ID":"kirchner-2020-insights","methods":["dell emcs framework","machine translation","linguists","lsps","tool implementors"],"center_method":[null,"machine translation",null,null,null],"tasks":["machine translation","translation supply chain"],"center_task":["machine translation",null],"Goal":["Decent Work and Economic Growth","Industry, Innovation and Infrastrucure"],"text":"Insights from Gathering MT Productivity Metrics at Scale. In this paper, we describe Dell EMC's framework to automatically collect MTrelated productivity metrics from a large translation supply chain over an extended period of time, the characteristics and volume of the gathered data, and the insights from analyzing the data to guide our MT strategy. Aligning tools, processes and people required decisions, concessions and contributions from Dell management, technology providers, tool implementors, LSPs and linguists to harvest data at scale over 2+ years while Dell EMC migrated from customized SMT to generic NMT and then customized NMT systems.","label":1,"title_clean":"Insights from Gathering MT Productivity Metrics at Scale","abstract_clean":"In this paper, we describe Dell EMC's framework to automatically collect MTrelated productivity metrics from a large translation supply chain over an extended period of time, the characteristics and volume of the gathered data, and the insights from analyzing the data to guide our MT strategy. Aligning tools, processes and people required decisions, concessions and contributions from Dell management, technology providers, tool implementors, LSPs and linguists to harvest data at scale over 2+ years while Dell EMC migrated from customized SMT to generic NMT and then customized NMT systems.","url":"https:\/\/aclanthology.org\/2020.eamt-1.38"},{"ID":"kiritchenko-cherry-2011-lexically","methods":["lexically triggered hidden markov models","lt hmm","discriminative hmm","lexical match"],"center_method":[null,null,null,null],"tasks":["clinical document coding","multi label document classification","coding problem"],"center_task":[null,null,null],"Goal":["Good Health and Well-Being"],"text":"Lexically-Triggered Hidden Markov Models for Clinical Document Coding. The automatic coding of clinical documents is an important task for today's healthcare providers. Though it can be viewed as multi-label document classification, the coding problem has the interesting property that most code assignments can be supported by a single phrase found in the input document. We propose a Lexically-Triggered Hidden Markov Model (LT-HMM) that leverages these phrases to improve coding accuracy. The LT-HMM works in two stages: first, a lexical match is performed against a term dictionary to collect a set of candidate codes for a document. Next, a discriminative HMM selects the best subset of codes to assign to the document by tagging candidates as present or absent. By confirming codes proposed by a dictionary, the LT-HMM can share features across codes, enabling strong performance even on rare codes. In fact, we are able to recover codes that do not occur in the training set at all. Our approach achieves the best ever performance on the 2007 Medical NLP Challenge test set, with an F-measure of 89.84.","label":1,"title_clean":"Lexically Triggered Hidden Markov Models for Clinical Document Coding","abstract_clean":"The automatic coding of clinical documents is an important task for today's healthcare providers. Though it can be viewed as multi label document classification, the coding problem has the interesting property that most code assignments can be supported by a single phrase found in the input document. We propose a Lexically Triggered Hidden Markov Model (LT HMM) that leverages these phrases to improve coding accuracy. The LT HMM works in two stages: first, a lexical match is performed against a term dictionary to collect a set of candidate codes for a document. Next, a discriminative HMM selects the best subset of codes to assign to the document by tagging candidates as present or absent. By confirming codes proposed by a dictionary, the LT HMM can share features across codes, enabling strong performance even on rare codes. In fact, we are able to recover codes that do not occur in the training set at all. Our approach achieves the best ever performance on the 2007 Medical NLP Challenge test set, with an F measure of 89.84.","url":"https:\/\/aclanthology.org\/P11-1075.pdf"},{"ID":"kirk-etal-2021-memes","methods":["multimodal models","ocr","machine learning methods"],"center_method":[null,null,"machine learning methods"],"tasks":["detecting real world hate"],"center_task":[null],"Goal":["Reduced Inequalities","Peace, Justice and Strong Institutions"],"text":"Memes in the Wild: Assessing the Generalizability of the Hateful Memes Challenge Dataset. Hateful memes pose a unique challenge for current machine learning systems because their message is derived from both text-and visual-modalities. To this effect, Facebook released the Hateful Memes Challenge, a dataset of memes with pre-extracted text captions, but it is unclear whether these synthetic examples generalize to 'memes in the wild'. In this paper, we collect hateful and non-hateful memes from Pinterest to evaluate out-of-sample performance on models pre-trained on the Facebook dataset. We find that memes in the wild differ in two key aspects: 1) Captions must be extracted via OCR, injecting noise and diminishing performance of multimodal models, and 2) Memes are more diverse than 'traditional memes', including screenshots of conversations or text on a plain background. This paper thus serves as a reality check for the current benchmark of hateful meme detection and its applicability for detecting real world hate.","label":1,"title_clean":"Memes in the Wild: Assessing the Generalizability of the Hateful Memes Challenge Dataset","abstract_clean":"Hateful memes pose a unique challenge for current machine learning systems because their message is derived from both text and visual modalities. To this effect, Facebook released the Hateful Memes Challenge, a dataset of memes with pre extracted text captions, but it is unclear whether these synthetic examples generalize to 'memes in the wild'. In this paper, we collect hateful and non hateful memes from Pinterest to evaluate out of sample performance on models pre trained on the Facebook dataset. We find that memes in the wild differ in two key aspects: 1) Captions must be extracted via OCR, injecting noise and diminishing performance of multimodal models, and 2) Memes are more diverse than 'traditional memes', including screenshots of conversations or text on a plain background. This paper thus serves as a reality check for the current benchmark of hateful meme detection and its applicability for detecting real world hate.","url":"https:\/\/aclanthology.org\/2021.woah-1.4.pdf"},{"ID":"koay-etal-2021-sliding","methods":["neural abstractive summarizer","sliding window approach","sliding window"],"center_method":[null,null,null],"tasks":["automatic creation of meeting minutes"],"center_task":[null],"Goal":["Decent Work and Economic Growth"],"text":"A Sliding-Window Approach to Automatic Creation of Meeting Minutes. Meeting minutes record any subject matters discussed, decisions reached and actions taken at meetings. The importance of minuting cannot be overemphasized in a time when a significant number of meetings take place in the virtual space. In this paper, we present a sliding window approach to automatic generation of meeting minutes. It aims to tackle issues associated with the nature of spoken text, including lengthy transcripts and lack of document structure, which make it difficult to identify salient content to be included in the meeting minutes. Our approach combines a sliding window and a neural abstractive summarizer to navigate through the transcripts to find salient content. The approach is evaluated on transcripts of natural meeting conversations, where we compare results obtained for human transcripts and two versions of automatic transcripts and discuss how and to what extent the summarizer succeeds at capturing salient content.","label":1,"title_clean":"A Sliding Window Approach to Automatic Creation of Meeting Minutes","abstract_clean":"Meeting minutes record any subject matters discussed, decisions reached and actions taken at meetings. The importance of minuting cannot be overemphasized in a time when a significant number of meetings take place in the virtual space. In this paper, we present a sliding window approach to automatic generation of meeting minutes. It aims to tackle issues associated with the nature of spoken text, including lengthy transcripts and lack of document structure, which make it difficult to identify salient content to be included in the meeting minutes. Our approach combines a sliding window and a neural abstractive summarizer to navigate through the transcripts to find salient content. The approach is evaluated on transcripts of natural meeting conversations, where we compare results obtained for human transcripts and two versions of automatic transcripts and discuss how and to what extent the summarizer succeeds at capturing salient content.","url":"https:\/\/aclanthology.org\/2021.naacl-srw.10"},{"ID":"koeva-etal-2020-natural","methods":["natural language processing pipeline","nlp pipeline","orchestrator process","dependency parsed","nlp cube"],"center_method":[null,null,null,null,null],"tasks":["statistical reports","documents identification","annotation","end to end data processing"],"center_task":[null,null,"annotation",null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Natural Language Processing Pipeline to Annotate Bulgarian Legislative Documents. The paper presents the Bulgarian MARCELL corpus, part of a recently developed multilingual corpus representing the national legislation in seven European countries and the NLP pipeline that turns the web crawled data into structured, linguistically annotated dataset. The Bulgarian data is web crawled, extracted from the original HTML format, filtered by document type, tokenised, sentence split, tagged and lemmatised with a fine-grained version of the Bulgarian Language Processing Chain, dependency parsed with NLP-Cube, annotated with named entities (persons, locations, organisations and others), noun phrases, IATE terms and EuroVoc descriptors. An orchestrator process has been developed to control the NLP pipeline performing an end-to-end data processing and annotation starting from the documents identification and ending in the generation of statistical reports. The Bulgarian MARCELL corpus consists of 25,283 documents (at the beginning of November 2019), which are classified into eleven types.","label":1,"title_clean":"Natural Language Processing Pipeline to Annotate Bulgarian Legislative Documents","abstract_clean":"The paper presents the Bulgarian MARCELL corpus, part of a recently developed multilingual corpus representing the national legislation in seven European countries and the NLP pipeline that turns the web crawled data into structured, linguistically annotated dataset. The Bulgarian data is web crawled, extracted from the original HTML format, filtered by document type, tokenised, sentence split, tagged and lemmatised with a fine grained version of the Bulgarian Language Processing Chain, dependency parsed with NLP Cube, annotated with named entities (persons, locations, organisations and others), noun phrases, IATE terms and EuroVoc descriptors. An orchestrator process has been developed to control the NLP pipeline performing an end to end data processing and annotation starting from the documents identification and ending in the generation of statistical reports. The Bulgarian MARCELL corpus consists of 25,283 documents (at the beginning of November 2019), which are classified into eleven types.","url":"https:\/\/aclanthology.org\/2020.lrec-1.863.pdf"},{"ID":"koizumi-etal-2002-annotated","methods":["corpus tool","jsl","grammatical rules"],"center_method":[null,null,null],"tasks":["annotation","grammatical analysis","sign language translation system","data collection"],"center_task":["annotation",null,null,"data collection"],"Goal":["Reduced Inequalities"],"text":"An Annotated Japanese Sign Language Corpus. Sign language is characterized by its interactivity and multimodality, which cause difficulties in data collection and annotation. To address these difficulties, we have developed a video-based Japanese sign language (JSL) corpus and a corpus tool for annotation and linguistic analysis. As the first step of linguistic annotation, we transcribed manual signs expressing lexical information as well as non-manual signs (NMSs)-including head movements, facial actions, and posture-that are used to express grammatical information. Our purpose is to extract grammatical rules from this corpus for the sign-language translation system underdevelopment. From this viewpoint, we will discuss methods for collecting elicited data, annotation required for grammatical analysis, as well as corpus tool required for annotation and grammatical analysis. As the result of annotating 2800 utterances, we confirmed that there are at least 50 kinds of NMSs in JSL, using head (seven kinds), jaw (six kinds), mouth (18 kinds), cheeks (one kind), eyebrows (four kinds), eyes (seven kinds), eye gaze (two kinds), bydy posture (five kinds). We use this corpus for designing and testing an algorithm and grammatical rules for the sign-language translation system underdevelopment.","label":1,"title_clean":"An Annotated Japanese Sign Language Corpus","abstract_clean":"Sign language is characterized by its interactivity and multimodality, which cause difficulties in data collection and annotation. To address these difficulties, we have developed a video based Japanese sign language (JSL) corpus and a corpus tool for annotation and linguistic analysis. As the first step of linguistic annotation, we transcribed manual signs expressing lexical information as well as non manual signs (NMSs) including head movements, facial actions, and posture that are used to express grammatical information. Our purpose is to extract grammatical rules from this corpus for the sign language translation system underdevelopment. From this viewpoint, we will discuss methods for collecting elicited data, annotation required for grammatical analysis, as well as corpus tool required for annotation and grammatical analysis. As the result of annotating 2800 utterances, we confirmed that there are at least 50 kinds of NMSs in JSL, using head (seven kinds), jaw (six kinds), mouth (18 kinds), cheeks (one kind), eyebrows (four kinds), eyes (seven kinds), eye gaze (two kinds), bydy posture (five kinds). We use this corpus for designing and testing an algorithm and grammatical rules for the sign language translation system underdevelopment.","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2002\/pdf\/318.pdf"},{"ID":"kokkinakis-thurin-2007-identification","methods":["generic named entity recognition technology"],"center_method":[null],"tasks":["automatic identification of named entities","tracking disease outbreak alerts","health care statistics","diagnostic tests"],"center_task":[null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"Identification of Entity References in Hospital Discharge Letters. In the era of the Electronic Health Record the release of medical narrative textual data for research, for health care statistics, for monitoring of new diagnostic tests and for tracking disease outbreak alerts imposes tough restrictions by various public authority bodies for the protection of (patient) privacy. In this paper we present a system for automatic identification of named entities in Swedish clinical free text, in the form of discharge letters, by applying generic named entity recognition technology with minor adaptations.","label":1,"title_clean":"Identification of Entity References in Hospital Discharge Letters","abstract_clean":"In the era of the Electronic Health Record the release of medical narrative textual data for research, for health care statistics, for monitoring of new diagnostic tests and for tracking disease outbreak alerts imposes tough restrictions by various public authority bodies for the protection of (patient) privacy. In this paper we present a system for automatic identification of named entities in Swedish clinical free text, in the form of discharge letters, by applying generic named entity recognition technology with minor adaptations.","url":"https:\/\/aclanthology.org\/W07-2452"},{"ID":"kongthon-etal-2011-semantic","methods":["semantic based question answering system"],"center_method":[null],"tasks":["semantic based question answering system"],"center_task":[null],"Goal":["Decent Work and Economic Growth"],"text":"A Semantic Based Question Answering System for Thailand Tourism Information. This paper reports our ongoing research work to create a semantic based question answering system for Thailand tourism information. Our proposed system focuses on mapping expressions in Thai natural language into ontology query language (SPARQL).","label":1,"title_clean":"A Semantic Based Question Answering System for Thailand Tourism Information","abstract_clean":"This paper reports our ongoing research work to create a semantic based question answering system for Thailand tourism information. Our proposed system focuses on mapping expressions in Thai natural language into ontology query language (SPARQL).","url":"https:\/\/aclanthology.org\/W11-3106"},{"ID":"kotonya-etal-2021-graph","methods":["end system","graph attention network","multi task learning","context aware linearization","graph reasoning","objective graph model"],"center_method":[null,null,"multi task learning",null,null,null],"tasks":["fact checking","evidence extraction","veracity prediction","per cell linearization of tabular evidence","linearizing tables","interpretable fact extraction"],"center_task":["fact checking",null,null,null,null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Graph Reasoning with Context-Aware Linearization for Interpretable Fact Extraction and Verification. This paper presents an end-to-end system for fact extraction and verification using textual and tabular evidence, the performance of which we demonstrate on the FEVEROUS dataset. We experiment with both a multi-task learning paradigm to jointly train a graph attention network for both the task of evidence extraction and veracity prediction, as well as a single objective graph model for solely learning veracity prediction and separate evidence extraction. In both instances, we employ a framework for per-cell linearization of tabular evidence, thus allowing us to treat evidence from tables as sequences. The templates we employ for linearizing tables capture the context as well as the content of table data. We furthermore provide a case study to show the interpretability our approach. Our best performing system achieves a FEVEROUS score of 0.23 and 53% label accuracy on the blind test data. 1 * Work done while the author was an intern at J.P. Morgan AI Research.","label":1,"title_clean":"Graph Reasoning with Context Aware Linearization for Interpretable Fact Extraction and Verification","abstract_clean":"This paper presents an end to end system for fact extraction and verification using textual and tabular evidence, the performance of which we demonstrate on the FEVEROUS dataset. We experiment with both a multi task learning paradigm to jointly train a graph attention network for both the task of evidence extraction and veracity prediction, as well as a single objective graph model for solely learning veracity prediction and separate evidence extraction. In both instances, we employ a framework for per cell linearization of tabular evidence, thus allowing us to treat evidence from tables as sequences. The templates we employ for linearizing tables capture the context as well as the content of table data. We furthermore provide a case study to show the interpretability our approach. Our best performing system achieves a FEVEROUS score of 0.23 and 53% label accuracy on the blind test data. 1 * Work done while the author was an intern at J.P. Morgan AI Research.","url":"https:\/\/aclanthology.org\/2021.fever-1.3"},{"ID":"koufakou-scott-2020-lexicon","methods":["semantic lexicon methods","embedding based methods","deep neural network","lexicon enhancement of embedding based approaches"],"center_method":[null,null,"deep neural network",null],"tasks":["detecting personal attacks","detecting abusive language"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Lexicon-Enhancement of Embedding-based Approaches Towards the Detection of Abusive Language. Detecting abusive language is a significant research topic, which has received a lot of attention recently. Our work focuses on detecting personal attacks in online conversations. As previous research on this task has largely used deep learning based on embeddings, we explore the use of lexicons to enhance embedding-based methods in an effort to see how these methods apply in the particular task of detecting personal attacks. The methods implemented and experimented with in this paper are quite different from each other, not only in the type of lexicons they use (sentiment or semantic), but also in the way they use the knowledge from the lexicons, in order to construct or to change embeddings that are ultimately fed into the learning model. The sentiment lexicon approaches focus on integrating sentiment information (in the form of sentiment embeddings) into the learning model. The semantic lexicon approaches focus on transforming the original word embeddings so that they better represent relationships extracted from a semantic lexicon. Based on our experimental results, semantic lexicon methods are superior to the rest of the methods in this paper, with at least 4% macro-averaged F1 improvement over the baseline.","label":1,"title_clean":"Lexicon Enhancement of Embedding based Approaches Towards the Detection of Abusive Language","abstract_clean":"Detecting abusive language is a significant research topic, which has received a lot of attention recently. Our work focuses on detecting personal attacks in online conversations. As previous research on this task has largely used deep learning based on embeddings, we explore the use of lexicons to enhance embedding based methods in an effort to see how these methods apply in the particular task of detecting personal attacks. The methods implemented and experimented with in this paper are quite different from each other, not only in the type of lexicons they use (sentiment or semantic), but also in the way they use the knowledge from the lexicons, in order to construct or to change embeddings that are ultimately fed into the learning model. The sentiment lexicon approaches focus on integrating sentiment information (in the form of sentiment embeddings) into the learning model. The semantic lexicon approaches focus on transforming the original word embeddings so that they better represent relationships extracted from a semantic lexicon. Based on our experimental results, semantic lexicon methods are superior to the rest of the methods in this paper, with at least 4% macro averaged F1 improvement over the baseline.","url":"https:\/\/aclanthology.org\/2020.trac-1.24"},{"ID":"koulierakis-etal-2020-recognition","methods":["openpose software","sequential models"],"center_method":[null,null],"tasks":["recognition of static features of sign formation","sign recognition","location of sign formation"],"center_task":[null,null,null],"Goal":["Reduced Inequalities"],"text":"Recognition of Static Features in Sign Language Using Key-Points. In this paper we report on a research effort focusing on recognition of static features of sign formation in single sign videos. Three sequential models have been developed for handshape, palm orientation and location of sign formation respectively, which make use of key-points extracted via OpenPose software. The models have been applied to a Danish and a Greek Sign Language dataset, providing results around 96%. Moreover, during the reported research, a method has been developed for identifying the time-frame of real signing in the video, which allows to ignore transition frames during sign recognition processing.","label":1,"title_clean":"Recognition of Static Features in Sign Language Using Key Points","abstract_clean":"In this paper we report on a research effort focusing on recognition of static features of sign formation in single sign videos. Three sequential models have been developed for handshape, palm orientation and location of sign formation respectively, which make use of key points extracted via OpenPose software. The models have been applied to a Danish and a Greek Sign Language dataset, providing results around 96%. Moreover, during the reported research, a method has been developed for identifying the time frame of real signing in the video, which allows to ignore transition frames during sign recognition processing.","url":"https:\/\/aclanthology.org\/2020.signlang-1.20"},{"ID":"kovatchev-etal-2021-vectors","methods":["taskspecific augmentations","automatic systems","mind ca","augmentation strategies","automatic augmentations","fasttext","data augmentation strategies"],"center_method":[null,null,null,null,null,"fasttext",null],"tasks":["automated scoring of childrens mindreading ability","automatic scoring of childrens ability"],"center_task":[null,null],"Goal":["Quality Education"],"text":"Can vectors read minds better than experts? Comparing data augmentation strategies for the automated scoring of children's mindreading ability. In this paper we implement and compare 7 different data augmentation strategies for the task of automatic scoring of children's ability to understand others' thoughts, feelings, and desires (or \"mindreading\"). We recruit in-domain experts to re-annotate augmented samples and determine to what extent each strategy preserves the original rating. We also carry out multiple experiments to measure how much each augmentation strategy improves the performance of automatic scoring systems. To determine the capabilities of automatic systems to generalize to unseen data, we create UK-MIND-20-a new corpus of children's performance on tests of mindreading, consisting of 10,320 question-answer pairs. We obtain a new state-of-the-art performance on the MIND-CA corpus, improving macro-F1-score by 6 points. Results indicate that both the number of training examples and the quality of the augmentation strategies affect the performance of the systems. The taskspecific augmentations generally outperform task-agnostic augmentations. Automatic augmentations based on vectors (GloVe, FastText) perform the worst. We find that systems trained on MIND-CA generalize well to UK-MIND-20. We demonstrate that data augmentation strategies also improve the performance on unseen data.","label":1,"title_clean":"Can vectors read minds better than experts? Comparing data augmentation strategies for the automated scoring of children's mindreading ability","abstract_clean":"In this paper we implement and compare 7 different data augmentation strategies for the task of automatic scoring of children's ability to understand others' thoughts, feelings, and desires (or \"mindreading\"). We recruit in domain experts to re annotate augmented samples and determine to what extent each strategy preserves the original rating. We also carry out multiple experiments to measure how much each augmentation strategy improves the performance of automatic scoring systems. To determine the capabilities of automatic systems to generalize to unseen data, we create UK MIND 20 a new corpus of children's performance on tests of mindreading, consisting of 10,320 question answer pairs. We obtain a new state of the art performance on the MIND CA corpus, improving macro F1 score by 6 points. Results indicate that both the number of training examples and the quality of the augmentation strategies affect the performance of the systems. The taskspecific augmentations generally outperform task agnostic augmentations. Automatic augmentations based on vectors (GloVe, FastText) perform the worst. We find that systems trained on MIND CA generalize well to UK MIND 20. We demonstrate that data augmentation strategies also improve the performance on unseen data.","url":"https:\/\/aclanthology.org\/2021.acl-long.96"},{"ID":"kruengkrai-etal-2021-multi","methods":["multi level attention model","graphbased approaches"],"center_method":[null,null],"tasks":["fact checking","verification"],"center_task":["fact checking",null],"Goal":["Peace, Justice and Strong Institutions"],"text":"A Multi-Level Attention Model for Evidence-Based Fact Checking. Evidence-based fact checking aims to verify the truthfulness of a claim against evidence extracted from textual sources. Learning a representation that effectively captures relations between a claim and evidence can be challenging. Recent state-of-the-art approaches have developed increasingly sophisticated models based on graph structures. We present a simple model that can be trained on sequence structures. Our model enables inter-sentence attentions at different levels and can benefit from joint training. Results on a large-scale dataset for Fact Extraction and VERification (FEVER) show that our model outperforms the graphbased approaches and yields 1.09% and 1.42% improvements in label accuracy and FEVER score, respectively, over the best published model. 1","label":1,"title_clean":"A Multi Level Attention Model for Evidence Based Fact Checking","abstract_clean":"Evidence based fact checking aims to verify the truthfulness of a claim against evidence extracted from textual sources. Learning a representation that effectively captures relations between a claim and evidence can be challenging. Recent state of the art approaches have developed increasingly sophisticated models based on graph structures. We present a simple model that can be trained on sequence structures. Our model enables inter sentence attentions at different levels and can benefit from joint training. Results on a large scale dataset for Fact Extraction and VERification (FEVER) show that our model outperforms the graphbased approaches and yields 1.09% and 1.42% improvements in label accuracy and FEVER score, respectively, over the best published model. 1","url":"https:\/\/aclanthology.org\/2021.findings-acl.217"},{"ID":"kulkarni-boyer-2018-toward","methods":["tutorial question answering system","retrieval based and generative models","hybrid approaches","deep learning conversational models","datadriven tutorial systems"],"center_method":[null,null,null,null,null],"tasks":["data driven tutorial question answering","java programming"],"center_task":[null,null],"Goal":["Quality Education"],"text":"Toward Data-Driven Tutorial Question Answering with Deep Learning Conversational Models. There has been an increase in popularity of data-driven question answering systems given their recent success. This paper explores the possibility of building a tutorial question answering system for Java programming from data sampled from a community-based question answering forum. This paper reports on the creation of a dataset that could support building such a tutorial question answering system and discusses the methodology to create the 106,386 question strong dataset. We investigate how retrieval-based and generative models perform on the given dataset. The work also investigates the usefulness of using hybrid approaches such as combining retrieval-based and generative models. The results indicate that building datadriven tutorial systems using communitybased question answering forums holds significant promise.","label":1,"title_clean":"Toward Data Driven Tutorial Question Answering with Deep Learning Conversational Models","abstract_clean":"There has been an increase in popularity of data driven question answering systems given their recent success. This paper explores the possibility of building a tutorial question answering system for Java programming from data sampled from a community based question answering forum. This paper reports on the creation of a dataset that could support building such a tutorial question answering system and discusses the methodology to create the 106,386 question strong dataset. We investigate how retrieval based and generative models perform on the given dataset. The work also investigates the usefulness of using hybrid approaches such as combining retrieval based and generative models. The results indicate that building datadriven tutorial systems using communitybased question answering forums holds significant promise.","url":"https:\/\/aclanthology.org\/W18-0532"},{"ID":"kulkarni-etal-2018-annotated","methods":["machine learning methods"],"center_method":["machine learning methods"],"tasks":["wet lab protocols","machine reading of instructions","biological research","shallow semantic parsing of instructional texts"],"center_task":[null,null,null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"An Annotated Corpus for Machine Reading of Instructions in Wet Lab Protocols. We describe an effort to annotate a corpus of natural language instructions consisting of 622 wet lab protocols to facilitate automatic or semi-automatic conversion of protocols into a machine-readable format and benefit biological research. Experimental results demonstrate the utility of our corpus for developing machine learning approaches to shallow semantic parsing of instructional texts. We make our annotated Wet Lab Protocol Corpus available to the research community. 1 1 The dataset is available on the authors' websites.","label":1,"title_clean":"An Annotated Corpus for Machine Reading of Instructions in Wet Lab Protocols","abstract_clean":"We describe an effort to annotate a corpus of natural language instructions consisting of 622 wet lab protocols to facilitate automatic or semi automatic conversion of protocols into a machine readable format and benefit biological research. Experimental results demonstrate the utility of our corpus for developing machine learning approaches to shallow semantic parsing of instructional texts. We make our annotated Wet Lab Protocol Corpus available to the research community. 1 1 The dataset is available on the authors' websites.","url":"https:\/\/aclanthology.org\/N18-2016.pdf"},{"ID":"kumar-etal-2020-nurse","methods":["word embeddings","ran debias","gender based illicit proximity estimate","post processing methods","gender debiasing methodology"],"center_method":["word embeddings",null,null,null,null],"tasks":["resolution","bias free setting","debiasing word embeddings","semantic and syntactic representations of words"],"center_task":[null,null,null,null],"Goal":["Gender Equality"],"text":"Nurse is Closer to Woman than Surgeon? Mitigating Gender-Biased Proximities in Word Embeddings. Word embeddings are the standard model for semantic and syntactic representations of words. Unfortunately, these models have been shown to exhibit undesirable word associations resulting from gender, racial, and religious biases. Existing post-processing methods for debiasing word embeddings are unable to mitigate gender bias hidden in the spatial arrangement of word vectors. In this paper, we propose RAN-Debias, a novel gender debiasing methodology that not only eliminates the bias present in a word vector but also alters the spatial distribution of its neighboring vectors, achieving a bias-free setting while maintaining minimal semantic offset. We also propose a new bias evaluation metric, Gender-based Illicit Proximity Estimate (GIPE), which measures the extent of undue proximity in word vectors resulting from the presence of gender-based predilections. Experiments based on a suite of evaluation metrics show that RAN-Debias significantly outperforms the state-of-the-art in reducing proximity bias (GIPE) by at least 42.02%. It also reduces direct bias, adding minimal semantic disturbance, and achieves the best performance in a downstream application task (coreference resolution).","label":1,"title_clean":"Nurse is Closer to Woman than Surgeon? Mitigating Gender Biased Proximities in Word Embeddings","abstract_clean":"Word embeddings are the standard model for semantic and syntactic representations of words. Unfortunately, these models have been shown to exhibit undesirable word associations resulting from gender, racial, and religious biases. Existing post processing methods for debiasing word embeddings are unable to mitigate gender bias hidden in the spatial arrangement of word vectors. In this paper, we propose RAN Debias, a novel gender debiasing methodology that not only eliminates the bias present in a word vector but also alters the spatial distribution of its neighboring vectors, achieving a bias free setting while maintaining minimal semantic offset. We also propose a new bias evaluation metric, Gender based Illicit Proximity Estimate (GIPE), which measures the extent of undue proximity in word vectors resulting from the presence of gender based predilections. Experiments based on a suite of evaluation metrics show that RAN Debias significantly outperforms the state of the art in reducing proximity bias (GIPE) by at least 42.02%. It also reduces direct bias, adding minimal semantic disturbance, and achieves the best performance in a downstream application task (coreference resolution).","url":"https:\/\/aclanthology.org\/2020.tacl-1.32"},{"ID":"kuo-etal-2010-using","methods":["classification"],"center_method":["classification"],"tasks":["readability"],"center_task":[null],"Goal":["Quality Education"],"text":"Using Linguistic Features to Predict Readability of Short Essays for Senior High School Students in Taiwan. We investigated the problem of classifying short essays used in comprehension tests for senior high school students in Taiwan. The tests were for first and second year students, so the answers included only four categories, each for one semester of the first two years. A random-guess approach would achieve only 25% in accuracy for our problem. We analyzed three publicly available scores for readability, but did not find them directly applicable. By considering a wide array of features at the levels of word, sentence, and essay, we gradually improved the F measure achieved by our classifiers from 0.381 to 0.536.","label":1,"title_clean":"Using Linguistic Features to Predict Readability of Short Essays for Senior High School Students in Taiwan","abstract_clean":"We investigated the problem of classifying short essays used in comprehension tests for senior high school students in Taiwan. The tests were for first and second year students, so the answers included only four categories, each for one semester of the first two years. A random guess approach would achieve only 25% in accuracy for our problem. We analyzed three publicly available scores for readability, but did not find them directly applicable. By considering a wide array of features at the levels of word, sentence, and essay, we gradually improved the F measure achieved by our classifiers from 0.381 to 0.536.","url":"https:\/\/aclanthology.org\/O10-5003"},{"ID":"kuo-etal-2012-exploiting","methods":["social network analysis","learning based framework"],"center_method":[null,null],"tasks":["sna","predictions"],"center_task":[null,"predictions"],"Goal":["Peace, Justice and Strong Institutions"],"text":"Exploiting Latent Information to Predict Diffusions of Novel Topics on Social Networks. This paper brings a marriage of two seemly unrelated topics, natural language processing (NLP) and social network analysis (SNA). We propose a new task in SNA which is to predict the diffusion of a new topic, and design a learning-based framework to solve this problem. We exploit the latent semantic information among users, topics, and social connections as features for prediction. Our framework is evaluated on real data collected from public domain. The experiments show 16% AUC","label":1,"title_clean":"Exploiting Latent Information to Predict Diffusions of Novel Topics on Social Networks","abstract_clean":"This paper brings a marriage of two seemly unrelated topics, natural language processing (NLP) and social network analysis (SNA). We propose a new task in SNA which is to predict the diffusion of a new topic, and design a learning based framework to solve this problem. We exploit the latent semantic information among users, topics, and social connections as features for prediction. Our framework is evaluated on real data collected from public domain. The experiments show 16% AUC","url":"https:\/\/aclanthology.org\/P12-2067.pdf"},{"ID":"kurniawan-etal-2020-ir3218","methods":["lstm","glove pre"],"center_method":["lstm",null],"tasks":["hate speech","offenseval 2020","automatic categorization of offense types","offense target identification"],"center_task":["hate speech",null,null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"IR3218-UI at SemEval-2020 Task 12: Emoji Effects on Offensive Language IdentifiCation. In this paper, we present our approach and the results of our participation in OffensEval 2020. There are three sub-tasks in OffensEval 2020, namely offensive language identification (sub-task A), automatic categorization of offense types (sub-task B), and offense target identification (subtask C). We participated in sub-task A of English OffensEval 2020. Our approach emphasizes on how the emoji affects offensive language identification. Our model used LSTM combined with GloVe pre-trained word vectors to identify offensive language on social media. The best model obtained macro F1-score of 0.88428.","label":1,"title_clean":"IR3218 UI at SemEval 2020 Task 12: Emoji Effects on Offensive Language IdentifiCation","abstract_clean":"In this paper, we present our approach and the results of our participation in OffensEval 2020. There are three sub tasks in OffensEval 2020, namely offensive language identification (sub task A), automatic categorization of offense types (sub task B), and offense target identification (subtask C). We participated in sub task A of English OffensEval 2020. Our approach emphasizes on how the emoji affects offensive language identification. Our model used LSTM combined with GloVe pre trained word vectors to identify offensive language on social media. The best model obtained macro F1 score of 0.88428.","url":"https:\/\/aclanthology.org\/2020.semeval-1.263"},{"ID":"lai-etal-2016-better","methods":["joint representation","shared representation"],"center_method":[null,null],"tasks":["social relations prediction tasks"],"center_task":[null],"Goal":["Partnership for the Goals"],"text":"Better Together: Combining Language and Social Interactions into a Shared Representation. Despite the clear inter-dependency between analyzing the interactions in social networks, and analyzing the natural language content of these interactions, these aspects are typically studied independently. In this paper we present a first step towards finding a joint representation, by embedding the two aspects into a single vector space. We show that the new representation can help improve performance in two social relations prediction tasks.","label":1,"title_clean":"Better Together: Combining Language and Social Interactions into a Shared Representation","abstract_clean":"Despite the clear inter dependency between analyzing the interactions in social networks, and analyzing the natural language content of these interactions, these aspects are typically studied independently. In this paper we present a first step towards finding a joint representation, by embedding the two aspects into a single vector space. We show that the new representation can help improve performance in two social relations prediction tasks.","url":"https:\/\/aclanthology.org\/W16-1405"},{"ID":"lai-etal-2021-joint","methods":["keci","collective approach","knowledge graph","graph neural networks","local representations","entity linker","attention","span graph","knowledge enhanced collective inference"],"center_method":[null,null,"knowledge graph","graph neural networks",null,null,"attention",null,null],"tasks":["named entity recognition","ie","interaction detection","inference","information extraction"],"center_task":["named entity recognition",null,null,"inference","information extraction"],"Goal":["Good Health and Well-Being"],"text":"Joint Biomedical Entity and Relation Extraction with Knowledge-Enhanced Collective Inference. Compared to the general news domain, information extraction (IE) from biomedical text requires much broader domain knowledge. However, many previous IE methods do not utilize any external knowledge during inference. Due to the exponential growth of biomedical publications, models that do not go beyond their fixed set of parameters will likely fall behind. Inspired by how humans look up relevant information to comprehend a scientific text, we present a novel framework that utilizes external knowledge for joint entity and relation extraction named KECI (Knowledge-Enhanced Collective Inference). Given an input text, KECI first constructs an initial span graph representing its initial understanding of the text. It then uses an entity linker to form a knowledge graph containing relevant background knowledge for the the entity mentions in the text. To make the final predictions, KECI fuses the initial span graph and the knowledge graph into a more refined graph using an attention mechanism. KECI takes a collective approach to link mention spans to entities by integrating global relational information into local representations using graph convolutional networks. Our experimental results show that the framework is highly effective, achieving new state-of-theart results in two different benchmark datasets: BioRelEx (binding interaction detection) and ADE (adverse drug event extraction). For example, KECI achieves absolute improvements of 4.59% and 4.91% in F1 scores over the stateof-the-art on the BioRelEx entity and relation extraction tasks 1 .","label":1,"title_clean":"Joint Biomedical Entity and Relation Extraction with Knowledge Enhanced Collective Inference","abstract_clean":"Compared to the general news domain, information extraction (IE) from biomedical text requires much broader domain knowledge. However, many previous IE methods do not utilize any external knowledge during inference. Due to the exponential growth of biomedical publications, models that do not go beyond their fixed set of parameters will likely fall behind. Inspired by how humans look up relevant information to comprehend a scientific text, we present a novel framework that utilizes external knowledge for joint entity and relation extraction named KECI (Knowledge Enhanced Collective Inference). Given an input text, KECI first constructs an initial span graph representing its initial understanding of the text. It then uses an entity linker to form a knowledge graph containing relevant background knowledge for the the entity mentions in the text. To make the final predictions, KECI fuses the initial span graph and the knowledge graph into a more refined graph using an attention mechanism. KECI takes a collective approach to link mention spans to entities by integrating global relational information into local representations using graph convolutional networks. Our experimental results show that the framework is highly effective, achieving new state of theart results in two different benchmark datasets: BioRelEx (binding interaction detection) and ADE (adverse drug event extraction). For example, KECI achieves absolute improvements of 4.59% and 4.91% in F1 scores over the stateof the art on the BioRelEx entity and relation extraction tasks 1 .","url":"https:\/\/aclanthology.org\/2021.acl-long.488"},{"ID":"lakomkin-etal-2017-gradascent","methods":["emoint shared task","lexicon driven system","neural models"],"center_method":[null,null,"neural models"],"tasks":["emotion intensity estimation"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"text":"GradAscent at EmoInt-2017: Character and Word Level Recurrent Neural Network Models for Tweet Emotion Intensity Detection. The WASSA 2017 EmoInt shared task has the goal to predict emotion intensity values of tweet messages. Given the text of a tweet and its emotion category (anger, joy, fear, and sadness), the participants were asked to build a system that assigns emotion intensity values. Emotion intensity estimation is a challenging problem given the short length of the tweets, the noisy structure of the text and the lack of annotated data. To solve this problem, we developed an ensemble of two neural models, processing input on the character. and word-level with a lexicon-driven system. The correlation scores across all four emotions are averaged to determine the bottom-line competition metric, and our system ranks place forth in full intensity range and third in 0.5-1 range of intensity among 23 systems at the time of writing (June 2017).","label":1,"title_clean":"GradAscent at EmoInt 2017: Character and Word Level Recurrent Neural Network Models for Tweet Emotion Intensity Detection","abstract_clean":"The WASSA 2017 EmoInt shared task has the goal to predict emotion intensity values of tweet messages. Given the text of a tweet and its emotion category (anger, joy, fear, and sadness), the participants were asked to build a system that assigns emotion intensity values. Emotion intensity estimation is a challenging problem given the short length of the tweets, the noisy structure of the text and the lack of annotated data. To solve this problem, we developed an ensemble of two neural models, processing input on the character. and word level with a lexicon driven system. The correlation scores across all four emotions are averaged to determine the bottom line competition metric, and our system ranks place forth in full intensity range and third in 0.5 1 range of intensity among 23 systems at the time of writing (June 2017).","url":"https:\/\/aclanthology.org\/W17-5222"},{"ID":"lange-ljunglof-2018-demonstrating","methods":["muste language learning environment","grammar"],"center_method":[null,"grammar"],"tasks":["language learning exercises"],"center_task":[null],"Goal":["Quality Education"],"text":"Demonstrating the MUSTE Language Learning Environment. We present a language learning application that relies on grammars to model the learning outcome. Based on this concept we can provide a powerful framework for language learning exercises with an intuitive user interface and a high reliability. Currently the application aims to augment existing language classes and support students by improving the learner attitude and the general learning outcome. Extensions beyond that scope are promising and likely to be added in the future.","label":1,"title_clean":"Demonstrating the MUSTE Language Learning Environment","abstract_clean":"We present a language learning application that relies on grammars to model the learning outcome. Based on this concept we can provide a powerful framework for language learning exercises with an intuitive user interface and a high reliability. Currently the application aims to augment existing language classes and support students by improving the learner attitude and the general learning outcome. Extensions beyond that scope are promising and likely to be added in the future.","url":"https:\/\/aclanthology.org\/W18-7105"},{"ID":"langer-schulder-2020-collocations","methods":["word sketches","frequency based rankings","corpus based approach","semantic abstractions"],"center_method":[null,null,null,null],"tasks":["sl","collocation analysis","semantic grouping","sl lexicography","word sense discrimination","wsd","lemmatisation","entry writing","automatic syntactic parsing of corpora"],"center_task":[null,null,null,null,null,null,null,null,null],"Goal":["Reduced Inequalities"],"text":"Collocations in Sign Language Lexicography: Towards Semantic Abstractions for Word Sense Discrimination. In general monolingual lexicography a corpus-based approach to word sense discrimination (WSD) is the current standard. Automatically generated lexical profiles such as Word Sketches provide an overview on typical uses in the form of collocate lists grouped by their part of speech categories and their syntactic dependency relations to the base item. Collocates are sorted by their typicality according to frequency-based rankings. With the advancement of sign language (SL) corpora, SL lexicography can finally be based on actual language use as reflected in corpus data. In order to use such data effectively and gain new insights on sign usage, automatically generated collocation profiles need to be developed under the special conditions and circumstances of the SL data available. One of these conditions is that many of the prerequesites for the automatic syntactic parsing of corpora are not yet available for SL. In this article we describe a collocation summary generated from DGS Corpus data which is used for WSD as well as in entry-writing. The summary works based on the glosses used for lemmatisation. In addition, we explore how other resources can be utilised to add an additional layer of semantic grouping to the collocation analysis. For this experimental approach we use glosses, concepts, and wordnet supersenses.","label":1,"title_clean":"Collocations in Sign Language Lexicography: Towards Semantic Abstractions for Word Sense Discrimination","abstract_clean":"In general monolingual lexicography a corpus based approach to word sense discrimination (WSD) is the current standard. Automatically generated lexical profiles such as Word Sketches provide an overview on typical uses in the form of collocate lists grouped by their part of speech categories and their syntactic dependency relations to the base item. Collocates are sorted by their typicality according to frequency based rankings. With the advancement of sign language (SL) corpora, SL lexicography can finally be based on actual language use as reflected in corpus data. In order to use such data effectively and gain new insights on sign usage, automatically generated collocation profiles need to be developed under the special conditions and circumstances of the SL data available. One of these conditions is that many of the prerequesites for the automatic syntactic parsing of corpora are not yet available for SL. In this article we describe a collocation summary generated from DGS Corpus data which is used for WSD as well as in entry writing. The summary works based on the glosses used for lemmatisation. In addition, we explore how other resources can be utilised to add an additional layer of semantic grouping to the collocation analysis. For this experimental approach we use glosses, concepts, and wordnet supersenses.","url":"https:\/\/aclanthology.org\/2020.signlang-1.21"},{"ID":"laubli-etal-2019-post","methods":["machine translation"],"center_method":["machine translation"],"tasks":["machine translation","nmt post editing","post editing productivity","system adaptation","banking and finance domain","professional translation of financial texts"],"center_task":["machine translation",null,null,null,null,null],"Goal":["Decent Work and Economic Growth"],"text":"Post-editing Productivity with Neural Machine Translation: An Empirical Assessment of Speed and Quality in the Banking and Finance Domain. Neural machine translation (NMT) has set new quality standards in automatic translation, yet its effect on post-editing productivity is still pending thorough investigation. We empirically test how the inclusion of NMT, in addition to domain-specific translation memories and termbases, impacts speed and quality in professional translation of financial texts. We find that even with language pairs that have received little attention in research settings and small amounts of in-domain data for system adaptation, NMT post-editing allows for substantial time savings and leads to equal or slightly better quality.","label":1,"title_clean":"Post editing Productivity with Neural Machine Translation: An Empirical Assessment of Speed and Quality in the Banking and Finance Domain","abstract_clean":"Neural machine translation (NMT) has set new quality standards in automatic translation, yet its effect on post editing productivity is still pending thorough investigation. We empirically test how the inclusion of NMT, in addition to domain specific translation memories and termbases, impacts speed and quality in professional translation of financial texts. We find that even with language pairs that have received little attention in research settings and small amounts of in domain data for system adaptation, NMT post editing allows for substantial time savings and leads to equal or slightly better quality.","url":"https:\/\/aclanthology.org\/W19-6626"},{"ID":"lawson-etal-2010-annotating","methods":["sim ilar models","amazons mechanical turk service","interannotator agree ment","compet itive bonus system"],"center_method":[null,null,null,null],"tasks":["annotation","named entity recognition","nlp applications"],"center_task":["annotation","named entity recognition","nlp applications"],"Goal":["Decent Work and Economic Growth"],"text":"Annotating Large Email Datasets for Named Entity Recognition with Mechanical Turk. Amazon's Mechanical Turk service has been successfully applied to many natural language processing tasks. However, the task of named entity recognition presents unique challenges. In a large annotation task involving over 20,000 emails, we demonstrate that a compet itive bonus system and interannotator agree ment can be used to improve the quality of named entity annotations from Mechanical Turk. We also build several statistical named entity recognition models trained with these annotations, which compare favorably to sim ilar models trained on expert annotations.","label":1,"title_clean":"Annotating Large Email Datasets for Named Entity Recognition with Mechanical Turk","abstract_clean":"Amazon's Mechanical Turk service has been successfully applied to many natural language processing tasks. However, the task of named entity recognition presents unique challenges. In a large annotation task involving over 20,000 emails, we demonstrate that a compet itive bonus system and interannotator agree ment can be used to improve the quality of named entity annotations from Mechanical Turk. We also build several statistical named entity recognition models trained with these annotations, which compare favorably to sim ilar models trained on expert annotations.","url":"https:\/\/aclanthology.org\/W10-0712"},{"ID":"lee-bryant-2002-contextual","methods":["daml"],"center_method":[null],"tasks":["understanding software requirements","contextual natural language processing"],"center_task":[null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Contextual Natural Language Processing and DAML for Understanding Software Requirements Specifications. ","label":1,"title_clean":"Contextual Natural Language Processing and DAML for Understanding Software Requirements Specifications","abstract_clean":"","url":"https:\/\/aclanthology.org\/C02-1124"},{"ID":"lee-etal-2004-analysis","methods":["speech recognizer","automatic speech recognition","cognitive theory","clt","colorado literacy tutor"],"center_method":[null,"automatic speech recognition",null,null,null],"tasks":["reading miscues","oral reading","reading research","speech recognition errors","detecting miscues","technology based literacy program","interactive literacy tutors","analysis and detection of reading miscues","literacy","student achievement"],"center_task":[null,null,null,null,null,null,null,null,null,null],"Goal":["Quality Education"],"text":"Analysis and Detection of Reading Miscues for Interactive Literacy Tutors. The Colorado Literacy Tutor (CLT) is a technology-based literacy program, designed on the basis of cognitive theory and scientifically motivated reading research, which aims to improve literacy and student achievement in public schools. One of the critical components of the CLT is a speech recognition system which is used to track the child's progress during oral reading and to provide sufficient information to detect reading miscues. In this paper, we extend on prior work by examining a novel labeling of children's oral reading audio data in order to better understand the factors that contribute most significantly to speech recognition errors. While these events make up nearly 8% of the data, they are shown to account for approximately 30% of the word errors in a state-of-the-art speech recognizer. Next, we consider the problem of detecting miscues during oral reading. Using features derived from the speech recognizer, we demonstrate that 67% of reading miscues can be detected at a false alarm rate of 3%.","label":1,"title_clean":"Analysis and Detection of Reading Miscues for Interactive Literacy Tutors","abstract_clean":"The Colorado Literacy Tutor (CLT) is a technology based literacy program, designed on the basis of cognitive theory and scientifically motivated reading research, which aims to improve literacy and student achievement in public schools. One of the critical components of the CLT is a speech recognition system which is used to track the child's progress during oral reading and to provide sufficient information to detect reading miscues. In this paper, we extend on prior work by examining a novel labeling of children's oral reading audio data in order to better understand the factors that contribute most significantly to speech recognition errors. While these events make up nearly 8% of the data, they are shown to account for approximately 30% of the word errors in a state of the art speech recognizer. Next, we consider the problem of detecting miscues during oral reading. Using features derived from the speech recognizer, we demonstrate that 67% of reading miscues can be detected at a false alarm rate of 3%.","url":"https:\/\/aclanthology.org\/C04-1182"},{"ID":"lee-etal-2016-feature","methods":["neural network based de identification system","feature augmented neural networks"],"center_method":[null,null],"tasks":["patient note de identification"],"center_task":[null],"Goal":["Good Health and Well-Being","Peace, Justice and Strong Institutions"],"text":"Feature-Augmented Neural Networks for Patient Note De-identification. Patient notes contain a wealth of information of potentially great interest to medical investigators. However, to protect patients' privacy, Protected Health Information (PHI) must be removed from the patient notes before they can be legally released, a process known as patient note de-identification. The main objective for a de-identification system is to have the highest possible recall. Recently, the first neural-network-based de-identification system has been proposed, yielding state-of-the-art results. Unlike other systems, it does not rely on human-engineered features, which allows it to be quickly deployed, but does not leverage knowledge from human experts or from electronic health records (EHRs). In this work, we explore a method to incorporate human-engineered features as well as features derived from EHRs to a neural-network-based de-identification system. Our results show that the addition of features, especially the EHR-derived features, further improves the state-of-the-art in patient note de-identification, including for some of the most sensitive PHI types such as patient names. Since in a real-life setting patient notes typically come with EHRs, we recommend developers of de-identification systems to leverage the information EHRs contain.","label":1,"title_clean":"Feature Augmented Neural Networks for Patient Note De identification","abstract_clean":"Patient notes contain a wealth of information of potentially great interest to medical investigators. However, to protect patients' privacy, Protected Health Information (PHI) must be removed from the patient notes before they can be legally released, a process known as patient note de identification. The main objective for a de identification system is to have the highest possible recall. Recently, the first neural network based de identification system has been proposed, yielding state of the art results. Unlike other systems, it does not rely on human engineered features, which allows it to be quickly deployed, but does not leverage knowledge from human experts or from electronic health records (EHRs). In this work, we explore a method to incorporate human engineered features as well as features derived from EHRs to a neural network based de identification system. Our results show that the addition of features, especially the EHR derived features, further improves the state of the art in patient note de identification, including for some of the most sensitive PHI types such as patient names. Since in a real life setting patient notes typically come with EHRs, we recommend developers of de identification systems to leverage the information EHRs contain.","url":"https:\/\/aclanthology.org\/W16-4204"},{"ID":"lee-etal-2017-ntnu","methods":["conditional random field","ntnu","self defined feature templates"],"center_method":["conditional random field",null,null],"tasks":["scienceie task","extracting keyphrases and relations"],"center_task":[null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"The NTNU System at SemEval-2017 Task 10: Extracting Keyphrases and Relations from Scientific Publications Using Multiple Conditional Random Fields. This study describes the design of the NTNU system for the ScienceIE task at the SemEval 2017 workshop. We use self-defined feature templates and multiple conditional random fields with extracted features to identify keyphrases along with categorized labels and their relations from scientific publications. A total of 16 teams participated in evaluation scenario 1 (subtasks A, B, and C), with only 7 teams competing in all subtasks. Our best micro-averaging F1 across the three subtasks is 0.23, ranking in the middle among all 16 submissions.","label":1,"title_clean":"The NTNU System at SemEval 2017 Task 10: Extracting Keyphrases and Relations from Scientific Publications Using Multiple Conditional Random Fields","abstract_clean":"This study describes the design of the NTNU system for the ScienceIE task at the SemEval 2017 workshop. We use self defined feature templates and multiple conditional random fields with extracted features to identify keyphrases along with categorized labels and their relations from scientific publications. A total of 16 teams participated in evaluation scenario 1 (subtasks A, B, and C), with only 7 teams competing in all subtasks. Our best micro averaging F1 across the three subtasks is 0.23, ranking in the middle among all 16 submissions.","url":"https:\/\/aclanthology.org\/S17-2165"},{"ID":"lee-etal-2021-unifying","methods":["unifiedm2","general purpose misinformation model"],"center_method":[null,null],"tasks":["few shot learning of unseen misinformation tasksdatasets","detecting news bias","unifying misinformation detection"],"center_task":[null,null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"On Unifying Misinformation Detection. In this paper, we introduce UNIFIEDM2, a general-purpose misinformation model that jointly models multiple domains of misinformation with a single, unified setup. The model is trained to handle four tasks: detecting news bias, clickbait, fake news and verifying rumors. By grouping these tasks together, UNIFIEDM2 learns a richer representation of misinformation, which leads to stateof-the-art or comparable performance across all tasks. Furthermore, we demonstrate that UNIFIEDM2's learned representation is helpful for few-shot learning of unseen misinformation tasks\/datasets and model's generalizability to unseen events. * Work partially done while interning at Facebook AI. \u2020 Work partially done while working at Facebook AI.","label":1,"title_clean":"On Unifying Misinformation Detection","abstract_clean":"In this paper, we introduce UNIFIEDM2, a general purpose misinformation model that jointly models multiple domains of misinformation with a single, unified setup. The model is trained to handle four tasks: detecting news bias, clickbait, fake news and verifying rumors. By grouping these tasks together, UNIFIEDM2 learns a richer representation of misinformation, which leads to stateof the art or comparable performance across all tasks. Furthermore, we demonstrate that UNIFIEDM2's learned representation is helpful for few shot learning of unseen misinformation tasks\/datasets and model's generalizability to unseen events. * Work partially done while interning at Facebook AI. \u2020 Work partially done while working at Facebook AI.","url":"https:\/\/aclanthology.org\/2021.naacl-main.432.pdf"},{"ID":"lehman-etal-2019-inferring","methods":["risk of stroke"],"center_method":[null],"tasks":["evaluation methods"],"center_task":["evaluation methods"],"Goal":["Good Health and Well-Being"],"text":"Inferring Which Medical Treatments Work from Reports of Clinical Trials. How do we know if a particular medical treatment actually works? Ideally one would consult all available evidence from relevant clinical trials. Unfortunately, such results are primarily disseminated in natural language scientific articles, imposing substantial burden on those trying to make sense of them. In this paper, we present a new task and corpus for making this unstructured evidence actionable. The task entails inferring reported findings from a full-text article describing a randomized controlled trial (RCT) with respect to a given intervention, comparator, and outcome of interest, e.g., inferring if an article provides evidence supporting the use of aspirin to reduce risk of stroke, as compared to placebo. We present a new corpus for this task comprising 10,000+ prompts coupled with fulltext articles describing RCTs. Results using a suite of models-ranging from heuristic (rule-based) approaches to attentive neural architectures-demonstrate the difficulty of the task, which we believe largely owes to the lengthy, technical input texts. To facilitate further work on this important, challenging problem we make the corpus, documentation, a website and leaderboard, and code for baselines and evaluation available at http:","label":1,"title_clean":"Inferring Which Medical Treatments Work from Reports of Clinical Trials","abstract_clean":"How do we know if a particular medical treatment actually works? Ideally one would consult all available evidence from relevant clinical trials. Unfortunately, such results are primarily disseminated in natural language scientific articles, imposing substantial burden on those trying to make sense of them. In this paper, we present a new task and corpus for making this unstructured evidence actionable. The task entails inferring reported findings from a full text article describing a randomized controlled trial (RCT) with respect to a given intervention, comparator, and outcome of interest, e.g., inferring if an article provides evidence supporting the use of aspirin to reduce risk of stroke, as compared to placebo. We present a new corpus for this task comprising 10,000+ prompts coupled with fulltext articles describing RCTs. Results using a suite of models ranging from heuristic (rule based) approaches to attentive neural architectures demonstrate the difficulty of the task, which we believe largely owes to the lengthy, technical input texts. To facilitate further work on this important, challenging problem we make the corpus, documentation, a website and leaderboard, and code for baselines and evaluation available at http:","url":"https:\/\/aclanthology.org\/N19-1371"},{"ID":"lemon-gruenstein-2002-language","methods":["resource base of software agents","grammar development tools","domain general dialogue manager","application specific activity models"],"center_method":[null,null,null,null],"tasks":["multi modal dialogue systems","domain specificity of dialogue managers"],"center_task":[null,null],"Goal":["Partnership for the Goals"],"text":"Language Resources for Multi-Modal Dialogue Systems.. This paper reviews a resource base of software agents for hub-based architectures, which can be used generally for advanced dialogue systems research and deployment. The problem of domain-specificity of dialogue managers is discussed, and we describe an approach to it developed at CSLI, involving a domain-general dialogue manager with application specific \"Activity Models\". We also describe relevant grammar development tools.","label":1,"title_clean":"Language Resources for Multi Modal Dialogue Systems.","abstract_clean":"This paper reviews a resource base of software agents for hub based architectures, which can be used generally for advanced dialogue systems research and deployment. The problem of domain specificity of dialogue managers is discussed, and we describe an approach to it developed at CSLI, involving a domain general dialogue manager with application specific \"Activity Models\". We also describe relevant grammar development tools.","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2002\/pdf\/102.pdf"},{"ID":"lemon-liu-2006-dude","methods":["state update dialogue systems","dialogue and understanding development environment","business process models","gf"],"center_method":[null,null,null,null],"tasks":["shopping","automatic speech recognition","information state update dialogue systems","robust interpretation of spontaneous speech","restaurant information","cinema booking"],"center_task":[null,"automatic speech recognition",null,null,null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"DUDE: A Dialogue and Understanding Development Environment, Mapping Business Process Models to Information State Update Dialogue Systems. We demonstrate a new development environment 1 \"Information State Update\" dialogue systems which allows non-expert developers to produce complete spoken dialogue systems based only on a Business Process Model (BPM) describing their application (e.g. banking, cinema booking, shopping, restaurant information). The environment includes automatic generation of Grammatical Framework (GF) grammars for robust interpretation of spontaneous speech, and uses application databases to generate lexical entries and grammar rules. The GF grammar is compiled to an ATK or Nuance language model for speech recognition. The demonstration system allows users to create and modify spoken dialogue systems, starting with a definition of a Business Process Model and ending with a working system. This paper describes the environment, its main components, and some of the research issues involved in its development.","label":1,"title_clean":"DUDE: A Dialogue and Understanding Development Environment, Mapping Business Process Models to Information State Update Dialogue Systems","abstract_clean":"We demonstrate a new development environment 1 \"Information State Update\" dialogue systems which allows non expert developers to produce complete spoken dialogue systems based only on a Business Process Model (BPM) describing their application (e.g. banking, cinema booking, shopping, restaurant information). The environment includes automatic generation of Grammatical Framework (GF) grammars for robust interpretation of spontaneous speech, and uses application databases to generate lexical entries and grammar rules. The GF grammar is compiled to an ATK or Nuance language model for speech recognition. The demonstration system allows users to create and modify spoken dialogue systems, starting with a definition of a Business Process Model and ending with a working system. This paper describes the environment, its main components, and some of the research issues involved in its development.","url":"https:\/\/aclanthology.org\/E06-2004"},{"ID":"levi-etal-2019-identifying","methods":["automatically classifying fake news","language based baseline"],"center_method":[null,null],"tasks":["identifying nuances","automatically classifying fake news","satire"],"center_task":[null,null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Identifying Nuances in Fake News vs. Satire: Using Semantic and Linguistic Cues. The blurry line between nefarious fake news and protected-speech satire has been a notorious struggle for social media platforms. Further to the efforts of reducing exposure to misinformation on social media, purveyors of fake news have begun to masquerade as satire sites to avoid being demoted. In this work, we address the challenge of automatically classifying fake news versus satire. Previous work have studied whether fake news and satire can be distinguished based on language differences. Contrary to fake news, satire stories are usually humorous and carry some political or social message. We hypothesize that these nuances could be identified using semantic and linguistic cues. Consequently, we train a machine learning method using semantic representation, with a state-of-the-art contextual language model, and with linguistic features based on textual coherence metrics. Empirical evaluation attests to the merits of our approach compared to the language-based baseline and sheds light on the nuances between fake news and satire. As avenues for future work, we consider studying additional linguistic features related to the humor aspect, and enriching the data with current news events, to help identify a political or social message.","label":1,"title_clean":"Identifying Nuances in Fake News vs. Satire: Using Semantic and Linguistic Cues","abstract_clean":"The blurry line between nefarious fake news and protected speech satire has been a notorious struggle for social media platforms. Further to the efforts of reducing exposure to misinformation on social media, purveyors of fake news have begun to masquerade as satire sites to avoid being demoted. In this work, we address the challenge of automatically classifying fake news versus satire. Previous work have studied whether fake news and satire can be distinguished based on language differences. Contrary to fake news, satire stories are usually humorous and carry some political or social message. We hypothesize that these nuances could be identified using semantic and linguistic cues. Consequently, we train a machine learning method using semantic representation, with a state of the art contextual language model, and with linguistic features based on textual coherence metrics. Empirical evaluation attests to the merits of our approach compared to the language based baseline and sheds light on the nuances between fake news and satire. As avenues for future work, we consider studying additional linguistic features related to the humor aspect, and enriching the data with current news events, to help identify a political or social message.","url":"https:\/\/aclanthology.org\/D19-5004.pdf"},{"ID":"levy-etal-2014-ontology","methods":["machine translation based method","ontology based technical text annotation","domain aware tools","sequence labeling approach","conditional random field","semantic model"],"center_method":[null,null,null,null,"conditional random field",null],"tasks":["domain specific semantic annotation tasks","classification","event extraction","text segmentation and labeling problem","proteins and transcription factors detection"],"center_task":[null,"classification","event extraction",null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Ontology-based Technical Text Annotation. Powerful tools could help users explore and maintain domain specific documentations, provided that documents have been semantically annotated. For that, the annotations must be sufficiently specialized and rich, relying on some explicit semantic model, usually an ontology, that represents the semantics of the target domain. In this paper, we learn to annotate biomedical scientific publications with respect to a Gene Regulation Ontology. We devise a two-step approach to annotate semantic events and relations. The first step is recast as a text segmentation and labeling problem and solved using machine translation tools and a CRF, the second as multi-class classification. We evaluate the approach on the BioNLP-GRO benchmark, achieving an average 61% F-measure on the event detection by itself and 50% F-measure on biological relation annotation. This suggests that human annotators can be supported in domain specific semantic annotation tasks. Under different experimental settings, we also conclude some interesting observations: (1) For event detection and compared to classical time-consuming sequence labeling approach, the newly proposed machine translation based method performed equally well but with much less computation resource required. (2) A highly domain specific part of the task, namely proteins and transcription factors detection, is best performed by domain aware tools, which can be used separately as an initial step of the pipeline. This work is licensed under a Creative Commons Attribution 4.0 International Licence.","label":1,"title_clean":"Ontology based Technical Text Annotation","abstract_clean":"Powerful tools could help users explore and maintain domain specific documentations, provided that documents have been semantically annotated. For that, the annotations must be sufficiently specialized and rich, relying on some explicit semantic model, usually an ontology, that represents the semantics of the target domain. In this paper, we learn to annotate biomedical scientific publications with respect to a Gene Regulation Ontology. We devise a two step approach to annotate semantic events and relations. The first step is recast as a text segmentation and labeling problem and solved using machine translation tools and a CRF, the second as multi class classification. We evaluate the approach on the BioNLP GRO benchmark, achieving an average 61% F measure on the event detection by itself and 50% F measure on biological relation annotation. This suggests that human annotators can be supported in domain specific semantic annotation tasks. Under different experimental settings, we also conclude some interesting observations: (1) For event detection and compared to classical time consuming sequence labeling approach, the newly proposed machine translation based method performed equally well but with much less computation resource required. (2) A highly domain specific part of the task, namely proteins and transcription factors detection, is best performed by domain aware tools, which can be used separately as an initial step of the pipeline. This work is licensed under a Creative Commons Attribution 4.0 International Licence.","url":"https:\/\/aclanthology.org\/W14-6003.pdf"},{"ID":"lewin-2007-basenps","methods":["basenps","general purpose linguistic processors","domain specific named entity recognition"],"center_method":[null,null,null],"tasks":["basenp detection"],"center_task":[null],"Goal":["Good Health and Well-Being"],"text":"BaseNPs that contain gene names: domain specificity and genericity. The names of named entities very often occur as constituents of larger noun phrases which denote different types of entity. Understanding the structure of the embedding phrase can be an enormously beneficial first step to enhancing whatever processing is intended to follow the named entity recognition in the first place. In this paper, we examine the integration of general purpose linguistic processors together with domain specific named entity recognition in order to carry out the task of baseNP detection. We report a best F-score of 87.17% on this task. We also report an inter-annotator agreement score of 98.8 Kappa on the task of baseNP annotation of a new data set.","label":1,"title_clean":"BaseNPs that contain gene names: domain specificity and genericity","abstract_clean":"The names of named entities very often occur as constituents of larger noun phrases which denote different types of entity. Understanding the structure of the embedding phrase can be an enormously beneficial first step to enhancing whatever processing is intended to follow the named entity recognition in the first place. In this paper, we examine the integration of general purpose linguistic processors together with domain specific named entity recognition in order to carry out the task of baseNP detection. We report a best F score of 87.17% on this task. We also report an inter annotator agreement score of 98.8 Kappa on the task of baseNP annotation of a new data set.","url":"https:\/\/aclanthology.org\/W07-1022"},{"ID":"lewis-etal-2017-integrating","methods":["open data practices"],"center_method":[null],"tasks":["language technology research","research ethics"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Integrating the Management of Personal Data Protection and Open Science with Research Ethics. This paper examines the impact of the EU General Data Protection Regulation, in the context of the requirement from many research funders to provide open access research data, on current practices in Language Technology Research. We analyse the challenges that arise and the opportunities to address many of them through the use of existing open data practices for sharing language research data. We discuss the impact of this also on current practice in academic and industrial research ethics.","label":1,"title_clean":"Integrating the Management of Personal Data Protection and Open Science with Research Ethics","abstract_clean":"This paper examines the impact of the EU General Data Protection Regulation, in the context of the requirement from many research funders to provide open access research data, on current practices in Language Technology Research. We analyse the challenges that arise and the opportunities to address many of them through the use of existing open data practices for sharing language research data. We discuss the impact of this also on current practice in academic and industrial research ethics.","url":"https:\/\/aclanthology.org\/W17-1607"},{"ID":"li-2021-codewithzichao","methods":["transformers","roberta","class combination","codewithzichaodravidianlangtech eacl2021"],"center_method":["transformers","roberta",null,null],"tasks":["hate speech","dravidian languages","class imbalance problem"],"center_task":["hate speech",null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Codewithzichao@DravidianLangTech-EACL2021: Exploring Multilingual Transformers for Offensive Language Identification on Code Mixing Text. This paper describes our solution submitted to shared task on Offensive Language Identification in Dravidian Languages. We participated in all three of offensive language identification. In order to address the task, we explored multilingual models based on XLM-RoBERTa and multilingual BERT trained on mixed data of three code-mixed languages. Besides, we solved the class-imbalance problem existed in training data by class combination, class weights and focal loss. Our model achieved weighted average F1 scores of 0.75 (ranked 4th), 0.94 (ranked 4th) and 0.72 (ranked 3rd) in Tamil-English task, Malayalam-English task and Kannada-English task, respectively.","label":1,"title_clean":"Codewithzichao@DravidianLangTech EACL2021: Exploring Multilingual Transformers for Offensive Language Identification on Code Mixing Text","abstract_clean":"This paper describes our solution submitted to shared task on Offensive Language Identification in Dravidian Languages. We participated in all three of offensive language identification. In order to address the task, we explored multilingual models based on XLM RoBERTa and multilingual BERT trained on mixed data of three code mixed languages. Besides, we solved the class imbalance problem existed in training data by class combination, class weights and focal loss. Our model achieved weighted average F1 scores of 0.75 (ranked 4th), 0.94 (ranked 4th) and 0.72 (ranked 3rd) in Tamil English task, Malayalam English task and Kannada English task, respectively.","url":"https:\/\/aclanthology.org\/2021.dravidianlangtech-1.21"},{"ID":"li-etal-2011-engtube","methods":["engtube","video service","integrated subtitle environment"],"center_method":[null,null,null],"tasks":["esl","second language learners","language technology","learning process","language learning"],"center_task":[null,null,"language technology",null,null],"Goal":["Quality Education"],"text":"ENGtube: an Integrated Subtitle Environment for ESL. Movies and TV shows are probably the most attractive media of language learning, and the associated subtitle is an important resource in the learning process. Despite its significance, subtitle has never been exploited effectively as it could be. In this paper we present ENGtube, which is a video service for ESL (English as Second Language) learners. The key component of this service is an integrated environment for displaying the video clips, the source subtitle and the translated subtitle with rich information at users' disposal. The rich information of subtitle is produced by various speech and language technologies.","label":1,"title_clean":"ENGtube: an Integrated Subtitle Environment for ESL","abstract_clean":"Movies and TV shows are probably the most attractive media of language learning, and the associated subtitle is an important resource in the learning process. Despite its significance, subtitle has never been exploited effectively as it could be. In this paper we present ENGtube, which is a video service for ESL (English as Second Language) learners. The key component of this service is an integrated environment for displaying the video clips, the source subtitle and the translated subtitle with rich information at users' disposal. The rich information of subtitle is produced by various speech and language technologies.","url":"https:\/\/aclanthology.org\/2011.mtsummit-systems.2"},{"ID":"li-etal-2016-litway","methods":["machine learning methods","lit way","rule based method","hybrid method","syntactic rules"],"center_method":["machine learning methods",null,null,null,null],"tasks":["discriminative extraction","seedev task"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"LitWay, Discriminative Extraction for Different Bio-Events. Even a simple biological phenomenon may introduce a complex network of molecular interactions. Scientific literature is one of the trustful resources delivering knowledge of these networks. We propose LitWay, a system for extracting semantic relations from texts. Lit-Way utilizes a hybrid method that combines both a rule-based method and a machine learning-based method. It is tested on the SeeDev task of BioNLP-ST 2016, achieves the state-of-the-art performance with the F-score of 43.2%, ranking first of all participating teams. To further reveal the linguistic characteristics of each event, we test the system solely with syntactic rules or machine learning, and different combinations of two methods. We find that it is difficult for one method to achieve good performance for all semantic relation types due to the complication of bio-events in the literatures.","label":1,"title_clean":"LitWay, Discriminative Extraction for Different Bio Events","abstract_clean":"Even a simple biological phenomenon may introduce a complex network of molecular interactions. Scientific literature is one of the trustful resources delivering knowledge of these networks. We propose LitWay, a system for extracting semantic relations from texts. Lit Way utilizes a hybrid method that combines both a rule based method and a machine learning based method. It is tested on the SeeDev task of BioNLP ST 2016, achieves the state of the art performance with the F score of 43.2%, ranking first of all participating teams. To further reveal the linguistic characteristics of each event, we test the system solely with syntactic rules or machine learning, and different combinations of two methods. We find that it is difficult for one method to achieve good performance for all semantic relation types due to the complication of bio events in the literatures.","url":"https:\/\/aclanthology.org\/W16-3004"},{"ID":"li-etal-2019-detecting","methods":["machine translation based baselines","unilingual","machine learning methods","transfer learning"],"center_method":[null,null,"machine learning methods","transfer learning"],"tasks":["detecting dementia","detecting cognitive decline"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"Detecting dementia in Mandarin Chinese using transfer learning from a parallel corpus. Machine learning has shown promise for automatic detection of Alzheimer's disease (AD) through speech; however, efforts are hampered by a scarcity of data, especially in languages other than English. We propose a method to learn a correspondence between independently engineered lexicosyntactic features in two languages, using a large parallel corpus of outof-domain movie dialogue data. We apply it to dementia detection in Mandarin Chinese, and demonstrate that our method outperforms both unilingual and machine translation-based baselines. This appears to be the first study that transfers feature domains in detecting cognitive decline.","label":1,"title_clean":"Detecting dementia in Mandarin Chinese using transfer learning from a parallel corpus","abstract_clean":"Machine learning has shown promise for automatic detection of Alzheimer's disease (AD) through speech; however, efforts are hampered by a scarcity of data, especially in languages other than English. We propose a method to learn a correspondence between independently engineered lexicosyntactic features in two languages, using a large parallel corpus of outof domain movie dialogue data. We apply it to dementia detection in Mandarin Chinese, and demonstrate that our method outperforms both unilingual and machine translation based baselines. This appears to be the first study that transfers feature domains in detecting cognitive decline.","url":"https:\/\/aclanthology.org\/N19-1199.pdf"},{"ID":"li-etal-2020-adviser","methods":["adviser","python based implementation","cognitive scientists","multi domain dialog system toolkit"],"center_method":[null,null,null,null],"tasks":["multi modal multi domain and socially engaged conversational agents","collaborative research","linguists","machine learning researchers","engagement level prediction","sentiment analysis","vision","multi modal incorporating speech"],"center_task":[null,null,null,null,null,"sentiment analysis",null,null],"Goal":["Industry, Innovation and Infrastrucure","Good Health and Well-Being"],"text":"ADVISER: A Toolkit for Developing Multi-modal, Multi-domain and Socially-engaged Conversational Agents. We present ADVISER 1-an open-source, multi-domain dialog system toolkit that enables the development of multi-modal (incorporating speech, text and vision), sociallyengaged (e.g. emotion recognition, engagement level prediction and backchanneling) conversational agents. The final Python-based implementation of our toolkit is flexible, easy to use, and easy to extend not only for technically experienced users, such as machine learning researchers, but also for less technically experienced users, such as linguists or cognitive scientists, thereby providing a flexible platform for collaborative research.","label":1,"title_clean":"ADVISER: A Toolkit for Developing Multi modal, Multi domain and Socially engaged Conversational Agents","abstract_clean":"We present ADVISER 1 an open source, multi domain dialog system toolkit that enables the development of multi modal (incorporating speech, text and vision), sociallyengaged (e.g. emotion recognition, engagement level prediction and backchanneling) conversational agents. The final Python based implementation of our toolkit is flexible, easy to use, and easy to extend not only for technically experienced users, such as machine learning researchers, but also for less technically experienced users, such as linguists or cognitive scientists, thereby providing a flexible platform for collaborative research.","url":"https:\/\/aclanthology.org\/2020.acl-demos.31.pdf"},{"ID":"li-etal-2020-multi-task","methods":["multi task learning","multi task methods","multi task shared structure encoding approach"],"center_method":["multi task learning",null,null],"tasks":["automatic prediction","multi task peer review score prediction"],"center_task":[null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Multi-task Peer-Review Score Prediction. Automatic prediction of the peer-review aspect scores of academic papers can be a useful assistant tool for both reviewers and authors. To handle the small size of published datasets on the target aspect of scores, we propose a multi-task approach to leverage additional information from other aspects of scores for improving the performance of the target aspect. Because one of the problems of building multi-task models is how to select the proper resources of auxiliary tasks and how to select the proper shared structures, we thus propose a multi-task shared structure encoding approach that automatically selects good shared network structures as well as good auxiliary resources. The experiments based on peer-review datasets show that our approach is effective and has better performance on the target scores than the single-task method and na\u00efve multi-task methods.","label":1,"title_clean":"Multi task Peer Review Score Prediction","abstract_clean":"Automatic prediction of the peer review aspect scores of academic papers can be a useful assistant tool for both reviewers and authors. To handle the small size of published datasets on the target aspect of scores, we propose a multi task approach to leverage additional information from other aspects of scores for improving the performance of the target aspect. Because one of the problems of building multi task models is how to select the proper resources of auxiliary tasks and how to select the proper shared structures, we thus propose a multi task shared structure encoding approach that automatically selects good shared network structures as well as good auxiliary resources. The experiments based on peer review datasets show that our approach is effective and has better performance on the target scores than the single task method and na\u00efve multi task methods.","url":"https:\/\/aclanthology.org\/2020.sdp-1.14.pdf"},{"ID":"li-etal-2022-gpt","methods":["deliberate degradation of artificial neural language models","gpt d","generative neural language models","transformer dl model"],"center_method":[null,null,null,null],"tasks":["inducing dementia related linguistic anomalies","cookie theft picture description task"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"GPT-D: Inducing Dementia-related Linguistic Anomalies by Deliberate Degradation of Artificial Neural Language Models. Deep learning (DL) techniques involving finetuning large numbers of model parameters have delivered impressive performance on the task of discriminating between language produced by cognitively healthy individuals, and those with Alzheimer's disease (AD). However, questions remain about their ability to generalize beyond the small reference sets that are publicly available for research. As an alternative to fitting model parameters directly, we propose a novel method by which a Transformer DL model (GPT-2) pre-trained on general English text is paired with an artificially degraded version of itself (GPT-D), to compute the ratio between these two models' perplexities on language from cognitively healthy and impaired individuals. This technique approaches state-ofthe-art performance on text data from a widely used \"Cookie Theft\" picture description task, and unlike established alternatives also generalizes well to spontaneous conversations. Furthermore, GPT-D generates text with characteristics known to be associated with AD, demonstrating the induction of dementia-related linguistic anomalies. Our study is a step toward better understanding of the relationships between the inner workings of generative neural language models, the language that they produce, and the deleterious effects of dementia on human speech and language characteristics.","label":1,"title_clean":"GPT D: Inducing Dementia related Linguistic Anomalies by Deliberate Degradation of Artificial Neural Language Models","abstract_clean":"Deep learning (DL) techniques involving finetuning large numbers of model parameters have delivered impressive performance on the task of discriminating between language produced by cognitively healthy individuals, and those with Alzheimer's disease (AD). However, questions remain about their ability to generalize beyond the small reference sets that are publicly available for research. As an alternative to fitting model parameters directly, we propose a novel method by which a Transformer DL model (GPT 2) pre trained on general English text is paired with an artificially degraded version of itself (GPT D), to compute the ratio between these two models' perplexities on language from cognitively healthy and impaired individuals. This technique approaches state ofthe art performance on text data from a widely used \"Cookie Theft\" picture description task, and unlike established alternatives also generalizes well to spontaneous conversations. Furthermore, GPT D generates text with characteristics known to be associated with AD, demonstrating the induction of dementia related linguistic anomalies. Our study is a step toward better understanding of the relationships between the inner workings of generative neural language models, the language that they produce, and the deleterious effects of dementia on human speech and language characteristics.","url":"https:\/\/aclanthology.org\/2022.acl-long.131"},{"ID":"li-hovy-2014-sentiment","methods":["chinas foreign relations","bootstrapping approach","hierarchical bayesian model"],"center_method":[null,null,null],"tasks":["sentiment analysis","quantitative political analysis"],"center_task":["sentiment analysis",null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Sentiment Analysis on the People's Daily. We propose a semi-supervised bootstrapping algorithm for analyzing China's foreign relations from the People's Daily. Our approach addresses sentiment target clustering, subjective lexicons extraction and sentiment prediction in a unified framework. Different from existing algorithms in the literature, time information is considered in our algorithm through a hierarchical bayesian model to guide the bootstrapping approach. We are hopeful that our approach can facilitate quantitative political analysis conducted by social scientists and politicians.","label":1,"title_clean":"Sentiment Analysis on the People's Daily","abstract_clean":"We propose a semi supervised bootstrapping algorithm for analyzing China's foreign relations from the People's Daily. Our approach addresses sentiment target clustering, subjective lexicons extraction and sentiment prediction in a unified framework. Different from existing algorithms in the literature, time information is considered in our algorithm through a hierarchical bayesian model to guide the bootstrapping approach. We are hopeful that our approach can facilitate quantitative political analysis conducted by social scientists and politicians.","url":"https:\/\/aclanthology.org\/D14-1053.pdf"},{"ID":"liang-etal-2012-expert","methods":["tag based method","communication scheme"],"center_method":[null,null],"tasks":["collective intelligence","microblog misinformation identification","expert finding"],"center_task":[null,null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Expert Finding for Microblog Misinformation Identification. The growth of social media provides a convenient communication scheme for people, but at the same time it becomes a hotbed of misinformation. The wide spread of misinformation over social media is injurious to public interest. We design a framework, which integrates collective intelligence and machine intelligence, to help identify misinformation. The basic idea is: (1) automatically index the expertise of users according to their microblog contents; and (2) match the experts with given suspected misinformation. By sending the suspected misinformation to appropriate experts, we can collect the assessments of experts to judge the credibility of information, and help refute misinformation. In this paper, we focus on expert finding for misinformation identification. We propose a tag-based method to index the expertise of microblog users with social tags. Experiments on a real world dataset demonstrate the effectiveness of our method for expert finding with respect to misinformation identification in microblogs.","label":1,"title_clean":"Expert Finding for Microblog Misinformation Identification","abstract_clean":"The growth of social media provides a convenient communication scheme for people, but at the same time it becomes a hotbed of misinformation. The wide spread of misinformation over social media is injurious to public interest. We design a framework, which integrates collective intelligence and machine intelligence, to help identify misinformation. The basic idea is: (1) automatically index the expertise of users according to their microblog contents; and (2) match the experts with given suspected misinformation. By sending the suspected misinformation to appropriate experts, we can collect the assessments of experts to judge the credibility of information, and help refute misinformation. In this paper, we focus on expert finding for misinformation identification. We propose a tag based method to index the expertise of microblog users with social tags. Experiments on a real world dataset demonstrate the effectiveness of our method for expert finding with respect to misinformation identification in microblogs.","url":"https:\/\/aclanthology.org\/C12-2069.pdf"},{"ID":"liang-etal-2021-evaluation","methods":["in person counseling strategies","annotation","strategy classifier"],"center_method":[null,"annotation",null],"tasks":["physical activity chatbot","human intervention","intervention chatbots","natural language conversation strategies","personalized physical activity intervention bot","artificial intelligence chatbots"],"center_task":[null,null,null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"Evaluation of In-Person Counseling Strategies To Develop Physical Activity Chatbot for Women. Artificial intelligence chatbots are the vanguard in technology-based intervention to change people's behavior. To develop intervention chatbots, the first step is to understand natural language conversation strategies in human conversation. This work introduces an intervention conversation dataset collected from a real-world physical activity intervention program for women. We designed comprehensive annotation schemes in four dimensions (domain, strategy, social exchange, and taskfocused exchange) and annotated a subset of dialogs. We built a strategy classifier with context information to detect strategies from both trainers and participants based on the annotation. To understand how human intervention induces effective behavior changes, we analyzed the relationships between the intervention strategies and the participants' changes in the barrier and social support for physical activity. We also analyzed how participant's baseline weight correlates to the amount of occurrence of the corresponding strategy. This work lays the foundation for developing a personalized physical activity intervention bot. 1","label":1,"title_clean":"Evaluation of In Person Counseling Strategies To Develop Physical Activity Chatbot for Women","abstract_clean":"Artificial intelligence chatbots are the vanguard in technology based intervention to change people's behavior. To develop intervention chatbots, the first step is to understand natural language conversation strategies in human conversation. This work introduces an intervention conversation dataset collected from a real world physical activity intervention program for women. We designed comprehensive annotation schemes in four dimensions (domain, strategy, social exchange, and taskfocused exchange) and annotated a subset of dialogs. We built a strategy classifier with context information to detect strategies from both trainers and participants based on the annotation. To understand how human intervention induces effective behavior changes, we analyzed the relationships between the intervention strategies and the participants' changes in the barrier and social support for physical activity. We also analyzed how participant's baseline weight correlates to the amount of occurrence of the corresponding strategy. This work lays the foundation for developing a personalized physical activity intervention bot. 1","url":"https:\/\/aclanthology.org\/2021.sigdial-1.5"},{"ID":"libovicky-etal-2020-expand","methods":["transformers","classifier filter","paraphrasing component","lmu systems","translation model","crosslingual classifier","cuni","expand and filter"],"center_method":["transformers",null,null,null,null,null,null,null],"tasks":["machine translation","simultaneous translation and paraphrase for language education","wngt 2020 duolingo shared task"],"center_task":["machine translation",null,null],"Goal":["Quality Education"],"text":"Expand and Filter: CUNI and LMU Systems for the WNGT 2020 Duolingo Shared Task. We present our submission to the Simultaneous Translation And Paraphrase for Language Education (STAPLE) challenge. We used a standard Transformer model for translation, with a crosslingual classifier predicting correct translations on the output n-best list. To increase the diversity of the outputs, we used additional data to train the translation model, and we trained a paraphrasing model based on the Levenshtein Transformer architecture to generate further synonymous translations. The paraphrasing results were again filtered using our classifier. While the use of additional data and our classifier filter were able to improve results, the paraphrasing model produced too many invalid outputs to further improve the output quality. Our model without the paraphrasing component finished in the middle of the field for the shared task, improving over the best baseline by a margin of 10-22% weighted F1 absolute.","label":1,"title_clean":"Expand and Filter: CUNI and LMU Systems for the WNGT 2020 Duolingo Shared Task","abstract_clean":"We present our submission to the Simultaneous Translation And Paraphrase for Language Education (STAPLE) challenge. We used a standard Transformer model for translation, with a crosslingual classifier predicting correct translations on the output n best list. To increase the diversity of the outputs, we used additional data to train the translation model, and we trained a paraphrasing model based on the Levenshtein Transformer architecture to generate further synonymous translations. The paraphrasing results were again filtered using our classifier. While the use of additional data and our classifier filter were able to improve results, the paraphrasing model produced too many invalid outputs to further improve the output quality. Our model without the paraphrasing component finished in the middle of the field for the shared task, improving over the best baseline by a margin of 10 22% weighted F1 absolute.","url":"https:\/\/aclanthology.org\/2020.ngt-1.18"},{"ID":"lichouri-abbas-2020-speechtrans","methods":["decision tree classifier","tf idf n grams"],"center_method":[null,null],"tasks":["automatically classifying tweets"],"center_task":[null],"Goal":["Good Health and Well-Being"],"text":"SpeechTrans@SMM4H'20: Impact of Preprocessing and N-grams on Automatic Classification of Tweets That Mention Medications. This paper describes our system developed for automatically classifying tweets that mention medications. We used the Decision Tree classifier for this task. We have shown that using some elementary preprocessing steps and TF-IDF n-grams led to acceptable classifier performance. Indeed, the F1-score recorded was 74.58% in the development phase and 63.70% in the test phase.","label":1,"title_clean":"SpeechTrans@SMM4H'20: Impact of Preprocessing and N grams on Automatic Classification of Tweets That Mention Medications","abstract_clean":"This paper describes our system developed for automatically classifying tweets that mention medications. We used the Decision Tree classifier for this task. We have shown that using some elementary preprocessing steps and TF IDF n grams led to acceptable classifier performance. Indeed, the F1 score recorded was 74.58% in the development phase and 63.70% in the test phase.","url":"https:\/\/aclanthology.org\/2020.smm4h-1.19"},{"ID":"lin-etal-2012-online","methods":["ensemble framework"],"center_method":[null],"tasks":["online plagiarism","online plagiarized detection"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Online Plagiarized Detection Through Exploiting Lexical, Syntax, and Semantic Information. In this paper, we introduce a framework that identifies online plagiarism by exploiting lexical, syntactic and semantic features that includes duplication-gram, reordering and alignment of words, POS and phrase tags, and semantic similarity of sentences. We establish an ensemble framework to combine the predictions of each model. Results demonstrate that our system can not only find considerable amount of real-world online plagiarism cases but also outperforms several state-of-the-art algorithms and commercial software.","label":1,"title_clean":"Online Plagiarized Detection Through Exploiting Lexical, Syntax, and Semantic Information","abstract_clean":"In this paper, we introduce a framework that identifies online plagiarism by exploiting lexical, syntactic and semantic features that includes duplication gram, reordering and alignment of words, POS and phrase tags, and semantic similarity of sentences. We establish an ensemble framework to combine the predictions of each model. Results demonstrate that our system can not only find considerable amount of real world online plagiarism cases but also outperforms several state of the art algorithms and commercial software.","url":"https:\/\/aclanthology.org\/P12-3025"},{"ID":"lin-etal-2019-enhancing","methods":["global attention mechanism","symptom graph","global attention"],"center_method":[null,null,null],"tasks":["dialogue symptom diagnosis"],"center_task":[null],"Goal":["Good Health and Well-Being"],"text":"Enhancing Dialogue Symptom Diagnosis with Global Attention and Symptom Graph. Symptom diagnosis is a challenging yet profound problem in natural language processing. Most previous research focus on investigating the standard electronic medical records for symptom diagnosis, while the dialogues between doctors and patients that contain more rich information are not well studied. In this paper, we first construct a dialogue symptom diagnosis dataset based on an online medical forum with a large amount of dialogues between patients and doctors. Then, we provide some benchmark models on this dataset to boost the research of dialogue symptom diagnosis. In order to further enhance the performance of symptom diagnosis over dialogues, we propose a global attention mechanism to capture more symptom related information, and build a symptom graph to model the associations between symptoms rather than treating each symptom independently. Experimental results show that both the global attention and symptom graph are effective to boost dialogue symptom diagnosis. In particular, our proposed model achieves the state-of-the-art performance on the constructed dataset.","label":1,"title_clean":"Enhancing Dialogue Symptom Diagnosis with Global Attention and Symptom Graph","abstract_clean":"Symptom diagnosis is a challenging yet profound problem in natural language processing. Most previous research focus on investigating the standard electronic medical records for symptom diagnosis, while the dialogues between doctors and patients that contain more rich information are not well studied. In this paper, we first construct a dialogue symptom diagnosis dataset based on an online medical forum with a large amount of dialogues between patients and doctors. Then, we provide some benchmark models on this dataset to boost the research of dialogue symptom diagnosis. In order to further enhance the performance of symptom diagnosis over dialogues, we propose a global attention mechanism to capture more symptom related information, and build a symptom graph to model the associations between symptoms rather than treating each symptom independently. Experimental results show that both the global attention and symptom graph are effective to boost dialogue symptom diagnosis. In particular, our proposed model achieves the state of the art performance on the constructed dataset.","url":"https:\/\/aclanthology.org\/D19-1508.pdf"},{"ID":"lin-etal-2021-rumor","methods":["rumor detection method","claim guided hierarchical graph attention network","representation learning"],"center_method":[null,null,null],"tasks":["rumor detection"],"center_task":["rumor detection"],"Goal":["Peace, Justice and Strong Institutions"],"text":"Rumor Detection on Twitter with Claim-Guided Hierarchical Graph Attention Networks. Rumors are rampant in the era of social media. Conversation structures provide valuable clues to differentiate between real and fake claims. However, existing rumor detection methods are either limited to the strict relation of user responses or oversimplify the conversation structure. In this study, to substantially reinforces the interaction of user opinions while alleviating the negative impact imposed by irrelevant posts, we first represent the conversation thread as an undirected interaction graph. We then present a Claim-guided Hierarchical Graph Attention Network for rumor classification, which enhances the representation learning for responsive posts considering the entire social contexts and attends over the posts that can semantically infer the target claim. Extensive experiments on three Twitter datasets demonstrate that our rumor detection method achieves much better performance than stateof-the-art methods and exhibits a superior capacity for detecting rumors at early stages.","label":1,"title_clean":"Rumor Detection on Twitter with Claim Guided Hierarchical Graph Attention Networks","abstract_clean":"Rumors are rampant in the era of social media. Conversation structures provide valuable clues to differentiate between real and fake claims. However, existing rumor detection methods are either limited to the strict relation of user responses or oversimplify the conversation structure. In this study, to substantially reinforces the interaction of user opinions while alleviating the negative impact imposed by irrelevant posts, we first represent the conversation thread as an undirected interaction graph. We then present a Claim guided Hierarchical Graph Attention Network for rumor classification, which enhances the representation learning for responsive posts considering the entire social contexts and attends over the posts that can semantically infer the target claim. Extensive experiments on three Twitter datasets demonstrate that our rumor detection method achieves much better performance than stateof the art methods and exhibits a superior capacity for detecting rumors at early stages.","url":"https:\/\/aclanthology.org\/2021.emnlp-main.786"},{"ID":"litman-forbes-riley-2004-predicting","methods":["machine learning methods"],"center_method":["machine learning methods"],"tasks":["computer human tutoring dialogues","predicting student emotions"],"center_task":[null,null],"Goal":["Quality Education"],"text":"Predicting Student Emotions in Computer-Human Tutoring Dialogues. We examine the utility of speech and lexical features for predicting student emotions in computerhuman spoken tutoring dialogues. We first annotate student turns for negative, neutral, positive and mixed emotions. We then extract acoustic-prosodic features from the speech signal, and lexical items from the transcribed or recognized speech. We compare the results of machine learning experiments using these features alone or in combination to predict various categorizations of the annotated student emotions. Our best results yield a 19-36% relative improvement in error reduction over a baseline. Finally, we compare our results with emotion prediction in human-human tutoring dialogues.","label":1,"title_clean":"Predicting Student Emotions in Computer Human Tutoring Dialogues","abstract_clean":"We examine the utility of speech and lexical features for predicting student emotions in computerhuman spoken tutoring dialogues. We first annotate student turns for negative, neutral, positive and mixed emotions. We then extract acoustic prosodic features from the speech signal, and lexical items from the transcribed or recognized speech. We compare the results of machine learning experiments using these features alone or in combination to predict various categorizations of the annotated student emotions. Our best results yield a 19 36% relative improvement in error reduction over a baseline. Finally, we compare our results with emotion prediction in human human tutoring dialogues.","url":"https:\/\/aclanthology.org\/P04-1045"},{"ID":"litvinova-etal-2017-deception","methods":["computational linguistics","statistical analysis","word count software","classification"],"center_method":["computational linguistics",null,null,"classification"],"tasks":["text based deception detection"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Deception detection in Russian texts. Psychology studies show that people detect deception no more accurately than by chance, and it is therefore important to develop tools to enable the detection of deception. The problem of deception detection has been studied for a significant amount of time, however in the last 10-15 years we have seen methods of computational linguistics being employed with greater frequency. Texts are processed using different NLP tools and then classified as deceptive\/truthful using modern machine learning methods. While most of this research has been performed for the English language, Slavic languages have never been the focus of detection deception studies. This paper deals with deception detection in Russian narratives related to the theme \"How I Spent Yesterday\". It employs a specially designed corpus of truthful and deceptive texts on the same topic from each respondent, such that N = 113. The texts were processed using Linguistic Inquiry and Word Count software that is used in most studies of text-based deception detection. The average amount of parameters, a majority of which were related to Part-of-Speech, lexical-semantic group, and other frequencies. Using standard statistical analysis, statistically significant differences between false and truthful Russian texts was uncovered. On the basis of the chosen parameters our classifier reached an accuracy of 68.3%. The accuracy of the model was found to depend on the author's gender.","label":1,"title_clean":"Deception detection in Russian texts","abstract_clean":"Psychology studies show that people detect deception no more accurately than by chance, and it is therefore important to develop tools to enable the detection of deception. The problem of deception detection has been studied for a significant amount of time, however in the last 10 15 years we have seen methods of computational linguistics being employed with greater frequency. Texts are processed using different NLP tools and then classified as deceptive\/truthful using modern machine learning methods. While most of this research has been performed for the English language, Slavic languages have never been the focus of detection deception studies. This paper deals with deception detection in Russian narratives related to the theme \"How I Spent Yesterday\". It employs a specially designed corpus of truthful and deceptive texts on the same topic from each respondent, such that N = 113. The texts were processed using Linguistic Inquiry and Word Count software that is used in most studies of text based deception detection. The average amount of parameters, a majority of which were related to Part of Speech, lexical semantic group, and other frequencies. Using standard statistical analysis, statistically significant differences between false and truthful Russian texts was uncovered. On the basis of the chosen parameters our classifier reached an accuracy of 68.3%. The accuracy of the model was found to depend on the author's gender.","url":"https:\/\/aclanthology.org\/E17-4005"},{"ID":"liu-chan-2012-role","methods":["nmcs","generative lexicon framework","rcs"],"center_method":[null,null,null],"tasks":["child language acquisition","noun modifying constructions"],"center_task":[null,null],"Goal":["Quality Education"],"text":"The Role of Qualia Structure in Mandarin Children Acquiring Noun-modifying Constructions. This paper investigates the types and the developmental trajectory of noun modifying constructions (NMCs), in the form of [Modifier + de + (Noun)], attested in Mandarin-speaking children's speech from a semantic perspective based on the generative lexicon framework (Pustejovsky, 1995). Based on 1034 NMCs (including those traditionally defined as relative clauses (RCs)) produced by 135 children aged 3 to 6 from a cross-sectional naturalistic speech corpus \"Zhou2\" in CHILDES, we analyzed the relation between the modifier and the head noun according to the 4 major roles of qualia structure: formal, constitutive, telic and agentive. Results suggest that (i) NMCs expressing the formal facet of the head noun's meaning are most frequently produced and acquired earliest, followed by those expressing the constitutive quale, and then those expressing the telic or the agentive quale; (ii) RC-type NMCs emerge either alongside the other non-RC type NMCs at the same time, or emerge later than the other non-RC type NMCs for the constitutive quale; and (iii) the majority of NMCs expressing the agentive and telic quales are those that fall within the traditional domain of RCs (called RC-type NMCs here), while the majority of NMCs expressing the formal and the constitutive quales are non-RC type NMCs. These findings are consistent with: (i) the semantic nature and complexity of the four qualia relations: formal and constitutive aspects of an object (called natural type concepts in Pustejovsky 2001, 2006) are more basic attributes, while telic and agentive (called artificial type concepts in Pustejovsky 2001, 2006) are derived and often eventive (hence conceptually more complex); and (ii) the properties of their adult input: NMCs expressing the formal quale are also most frequently encountered in the adult input; followed by the constitutive quale, and then the agentive and telic quales. The findings are also consistent with the idea that in Asian languages such as Japanese, Korean and Chinese, RCs develop from attributive constructions specifying a semantic feature of the head noun in acquisition (Diessel 2007, c.f. also Comrie 1996, 1998, 2002). This study is probably the first of using the generative lexicon framework in the field of child language acquisition.","label":1,"title_clean":"The Role of Qualia Structure in Mandarin Children Acquiring Noun modifying Constructions","abstract_clean":"This paper investigates the types and the developmental trajectory of noun modifying constructions (NMCs), in the form of [Modifier + de + (Noun)], attested in Mandarin speaking children's speech from a semantic perspective based on the generative lexicon framework (Pustejovsky, 1995). Based on 1034 NMCs (including those traditionally defined as relative clauses (RCs)) produced by 135 children aged 3 to 6 from a cross sectional naturalistic speech corpus \"Zhou2\" in CHILDES, we analyzed the relation between the modifier and the head noun according to the 4 major roles of qualia structure: formal, constitutive, telic and agentive. Results suggest that (i) NMCs expressing the formal facet of the head noun's meaning are most frequently produced and acquired earliest, followed by those expressing the constitutive quale, and then those expressing the telic or the agentive quale; (ii) RC type NMCs emerge either alongside the other non RC type NMCs at the same time, or emerge later than the other non RC type NMCs for the constitutive quale; and (iii) the majority of NMCs expressing the agentive and telic quales are those that fall within the traditional domain of RCs (called RC type NMCs here), while the majority of NMCs expressing the formal and the constitutive quales are non RC type NMCs. These findings are consistent with: (i) the semantic nature and complexity of the four qualia relations: formal and constitutive aspects of an object (called natural type concepts in Pustejovsky 2001, 2006) are more basic attributes, while telic and agentive (called artificial type concepts in Pustejovsky 2001, 2006) are derived and often eventive (hence conceptually more complex); and (ii) the properties of their adult input: NMCs expressing the formal quale are also most frequently encountered in the adult input; followed by the constitutive quale, and then the agentive and telic quales. The findings are also consistent with the idea that in Asian languages such as Japanese, Korean and Chinese, RCs develop from attributive constructions specifying a semantic feature of the head noun in acquisition (Diessel 2007, c.f. also Comrie 1996, 1998, 2002). This study is probably the first of using the generative lexicon framework in the field of child language acquisition.","url":"https:\/\/aclanthology.org\/Y12-1069"},{"ID":"liu-etal-2020-hiring","methods":["skill aware multi attention model","skill knowledge graph","hierarchical decoder","sama"],"center_method":[null,null,null,null],"tasks":["hiring","job posting","conditional text generation problem"],"center_task":[null,null,null],"Goal":["Decent Work and Economic Growth"],"text":"Hiring Now: A Skill-Aware Multi-Attention Model for Job Posting Generation. Writing a good job posting is a critical step in the recruiting process, but the task is often more difficult than many people think. It is challenging to specify the level of education, experience, relevant skills per the company information and job description. To this end, we propose a novel task of Job Posting Generation (JPG) that is cast as a conditional text generation problem to generate job requirements according to the job descriptions. To deal with this task, we devise a data-driven global Skill-Aware Multi-Attention generation model, named SAMA. Specifically, to model the complex mapping relationships between input and output, we design a hierarchical decoder that we first label the job description with multiple skills, then we generate a complete text guided by the skill labels. At the same time, to exploit the prior knowledge about the skills, we further construct a skill knowledge graph to capture the global prior knowledge of skills and refine the generated results. The proposed approach is evaluated on real-world job posting data. Experimental results clearly demonstrate the effectiveness of the proposed method 1 .","label":1,"title_clean":"Hiring Now: A Skill Aware Multi Attention Model for Job Posting Generation","abstract_clean":"Writing a good job posting is a critical step in the recruiting process, but the task is often more difficult than many people think. It is challenging to specify the level of education, experience, relevant skills per the company information and job description. To this end, we propose a novel task of Job Posting Generation (JPG) that is cast as a conditional text generation problem to generate job requirements according to the job descriptions. To deal with this task, we devise a data driven global Skill Aware Multi Attention generation model, named SAMA. Specifically, to model the complex mapping relationships between input and output, we design a hierarchical decoder that we first label the job description with multiple skills, then we generate a complete text guided by the skill labels. At the same time, to exploit the prior knowledge about the skills, we further construct a skill knowledge graph to capture the global prior knowledge of skills and refine the generated results. The proposed approach is evaluated on real world job posting data. Experimental results clearly demonstrate the effectiveness of the proposed method 1 .","url":"https:\/\/aclanthology.org\/2020.acl-main.281.pdf"},{"ID":"liu-etal-2021-self","methods":["bert","domain specific pretrained mlms","one model for all solution","scalable metric learning framework","pretraining scheme","language models","self alignment pretraining","pipelinebased hybrid systems","selfsupervised learning","umls"],"center_method":["bert",null,null,null,null,"language models",null,null,null,"umls"],"tasks":["biomedical entity representations","named entity recognition","capturing fine grained semantic relationships","scientific domain","entity level tasks"],"center_task":[null,"named entity recognition",null,null,null],"Goal":["Good Health and Well-Being"],"text":"Self-Alignment Pretraining for Biomedical Entity Representations. Despite the widespread success of selfsupervised learning via masked language models (MLM), accurately capturing fine-grained semantic relationships in the biomedical domain remains a challenge. This is of paramount importance for entity-level tasks such as entity linking where the ability to model entity relations (especially synonymy) is pivotal. To address this challenge, we propose SAPBERT, a pretraining scheme that selfaligns the representation space of biomedical entities. We design a scalable metric learning framework that can leverage UMLS, a massive collection of biomedical ontologies with 4M+ concepts. In contrast with previous pipelinebased hybrid systems, SAPBERT offers an elegant one-model-for-all solution to the problem of medical entity linking (MEL), achieving a new state-of-the-art (SOTA) on six MEL benchmarking datasets. In the scientific domain, we achieve SOTA even without taskspecific supervision. With substantial improvement over various domain-specific pretrained MLMs such as BIOBERT, SCIBERT and PUB-MEDBERT, our pretraining scheme proves to be both effective and robust. 1","label":1,"title_clean":"Self Alignment Pretraining for Biomedical Entity Representations","abstract_clean":"Despite the widespread success of selfsupervised learning via masked language models (MLM), accurately capturing fine grained semantic relationships in the biomedical domain remains a challenge. This is of paramount importance for entity level tasks such as entity linking where the ability to model entity relations (especially synonymy) is pivotal. To address this challenge, we propose SAPBERT, a pretraining scheme that selfaligns the representation space of biomedical entities. We design a scalable metric learning framework that can leverage UMLS, a massive collection of biomedical ontologies with 4M+ concepts. In contrast with previous pipelinebased hybrid systems, SAPBERT offers an elegant one model for all solution to the problem of medical entity linking (MEL), achieving a new state of the art (SOTA) on six MEL benchmarking datasets. In the scientific domain, we achieve SOTA even without taskspecific supervision. With substantial improvement over various domain specific pretrained MLMs such as BIOBERT, SCIBERT and PUB MEDBERT, our pretraining scheme proves to be both effective and robust. 1","url":"https:\/\/aclanthology.org\/2021.naacl-main.334"},{"ID":"lochbaum-1991-algorithm","methods":["shared - plan model of collaboration"],"center_method":[null],"tasks":["plan recognition","intended recognition","collaborative discourse","discourse"],"center_task":[null,null,null,null],"Goal":["Partnership for the Goals"],"text":"An Algorithm for Plan Recognition in Collaborative Discourse. A model of plan recognition in discourse must be based on intended recognition, distinguish each agent's beliefs and intentions from the other's, and avoid assumptions about the correctness or completeness of the agents' beliefs. In this paper, we present an algorithm for plan recognition that is based on the Shared-Plan model of collaboration (Grosz and Sidner, 1990; Lochbaum et al., 1990) and that satisfies these constraints.","label":1,"title_clean":"An Algorithm for Plan Recognition in Collaborative Discourse","abstract_clean":"A model of plan recognition in discourse must be based on intended recognition, distinguish each agent's beliefs and intentions from the other's, and avoid assumptions about the correctness or completeness of the agents' beliefs. In this paper, we present an algorithm for plan recognition that is based on the Shared Plan model of collaboration (Grosz and Sidner, 1990; Lochbaum et al., 1990) and that satisfies these constraints.","url":"https:\/\/aclanthology.org\/P91-1005.pdf"},{"ID":"long-etal-2017-xjnlp","methods":["hybrid model","semi supervised learning","rule based methods","temporality"],"center_method":[null,null,null,null],"tasks":["extraction of temporal expressions","temporal processing","course of clinical events","clinical temporal information ex traction"],"center_task":[null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"XJNLP at SemEval-2017 Task 12: Clinical temporal information ex-traction with a Hybrid Model. Temporality is crucial in understanding the course of clinical events from a patient's electronic health records and temporal processing is becoming more and more important for improving access to content. SemEval 2017 Task 12 (Clinical TempEval) addressed this challenge using the THYME corpus, a corpus of clinical narratives annotated with a schema based on TimeML2 guidelines. We developed and evaluated approaches for: extraction of temporal expressions (TIMEX3) and EVENTs; EVENT attributes; document-time relations. Our approach is a hybrid model which is based on rule based methods, semi-supervised learning, and semantic features with addition of manually crafted rules.","label":1,"title_clean":"XJNLP at SemEval 2017 Task 12: Clinical temporal information ex traction with a Hybrid Model","abstract_clean":"Temporality is crucial in understanding the course of clinical events from a patient's electronic health records and temporal processing is becoming more and more important for improving access to content. SemEval 2017 Task 12 (Clinical TempEval) addressed this challenge using the THYME corpus, a corpus of clinical narratives annotated with a schema based on TimeML2 guidelines. We developed and evaluated approaches for: extraction of temporal expressions (TIMEX3) and EVENTs; EVENT attributes; document time relations. Our approach is a hybrid model which is based on rule based methods, semi supervised learning, and semantic features with addition of manually crafted rules.","url":"https:\/\/aclanthology.org\/S17-2178"},{"ID":"louis-newman-2012-summarization","methods":["concept based approach","broad clustering"],"center_method":[null,null],"tasks":["summarization of business related tweets","cluster ranking"],"center_task":[null,null],"Goal":["Decent Work and Economic Growth","Industry, Innovation and Infrastrucure"],"text":"Summarization of Business-Related Tweets: A Concept-Based Approach. We present a method for summarizing the collection of tweets related to a business. Our procedure aggregates tweets into subtopic clusters which are then ranked and summarized by a few representative tweets from each cluster. Central to our approach is the ability to group diverse tweets into clusters. The broad clustering is induced by first learning a small set of business-related concepts automatically from free text and then subdividing the tweets into these concepts. Cluster ranking is performed using an importance score which combines topic coherence and sentiment value of the tweets. We also discuss alternative methods to summarize these tweets and evaluate the approaches using a small user study. Results show that the concept-based summaries are ranked favourably by the users.","label":1,"title_clean":"Summarization of Business Related Tweets: A Concept Based Approach","abstract_clean":"We present a method for summarizing the collection of tweets related to a business. Our procedure aggregates tweets into subtopic clusters which are then ranked and summarized by a few representative tweets from each cluster. Central to our approach is the ability to group diverse tweets into clusters. The broad clustering is induced by first learning a small set of business related concepts automatically from free text and then subdividing the tweets into these concepts. Cluster ranking is performed using an importance score which combines topic coherence and sentiment value of the tweets. We also discuss alternative methods to summarize these tweets and evaluate the approaches using a small user study. Results show that the concept based summaries are ranked favourably by the users.","url":"https:\/\/aclanthology.org\/C12-2075"},{"ID":"loukas-etal-2022-finer","methods":["fin bert"],"center_method":[null],"tasks":["xbrl tagging","data and error analysis","financial filings"],"center_task":[null,null,null],"Goal":["Decent Work and Economic Growth"],"text":"FiNER: Financial Numeric Entity Recognition for XBRL Tagging. Publicly traded companies are required to submit periodic reports with eXtensive Business Reporting Language (xbrl) word-level tags. Manually tagging the reports is tedious and costly. We, therefore, introduce xbrl tagging as a new entity extraction task for the financial domain and release finer-139, a dataset of 1.1M sentences with gold xbrl tags. Unlike typical entity extraction datasets, finer-139 uses a much larger label set of 139 entity types. Most annotated tokens are numeric, with the correct tag per token depending mostly on context, rather than the token itself. We show that subword fragmentation of numeric expressions harms bert's performance, allowing word-level bilstms to perform better. To improve bert's performance, we propose two simple and effective solutions that replace numeric expressions with pseudotokens reflecting original token shapes and numeric magnitudes. We also experiment with fin-bert, an existing bert model for the financial domain, and release our own bert (sec-bert), pre-trained on financial filings, which performs best. Through data and error analysis, we finally identify possible limitations to inspire future work on xbrl tagging. Dataset Domain Entity Types conll-2003 Generic 4 ontonotes-v5 Generic 18 ace-2005 Generic 7 genia Biomedical 36 Chalkidis et al. (2019) Legal 14 Francis et al. (2019) Financial 9 finer-139 (ours) Financial 139","label":1,"title_clean":"FiNER: Financial Numeric Entity Recognition for XBRL Tagging","abstract_clean":"Publicly traded companies are required to submit periodic reports with eXtensive Business Reporting Language (xbrl) word level tags. Manually tagging the reports is tedious and costly. We, therefore, introduce xbrl tagging as a new entity extraction task for the financial domain and release finer 139, a dataset of 1.1M sentences with gold xbrl tags. Unlike typical entity extraction datasets, finer 139 uses a much larger label set of 139 entity types. Most annotated tokens are numeric, with the correct tag per token depending mostly on context, rather than the token itself. We show that subword fragmentation of numeric expressions harms bert's performance, allowing word level bilstms to perform better. To improve bert's performance, we propose two simple and effective solutions that replace numeric expressions with pseudotokens reflecting original token shapes and numeric magnitudes. We also experiment with fin bert, an existing bert model for the financial domain, and release our own bert (sec bert), pre trained on financial filings, which performs best. Through data and error analysis, we finally identify possible limitations to inspire future work on xbrl tagging. Dataset Domain Entity Types conll 2003 Generic 4 ontonotes v5 Generic 18 ace 2005 Generic 7 genia Biomedical 36 Chalkidis et al. (2019) Legal 14 Francis et al. (2019) Financial 9 finer 139 (ours) Financial 139","url":"https:\/\/aclanthology.org\/2022.acl-long.303"},{"ID":"loveys-etal-2017-small","methods":["affective micropatterns"],"center_method":[null],"tasks":["quantifying mental health"],"center_task":[null],"Goal":["Good Health and Well-Being"],"text":"Small but Mighty: Affective Micropatterns for Quantifying Mental Health from Social Media Language. Many psychological phenomena occur in small time windows, measured in minutes or hours. However, most computational linguistic techniques look at data on the order of weeks, months, or years. We explore micropatterns in sequences of messages occurring over a short time window for their prevalence and power for quantifying psychological phenomena, specifically, patterns in affect. We examine affective micropatterns in social media posts from users with anxiety, eating disorders, panic attacks, schizophrenia, suicidality, and matched controls.","label":1,"title_clean":"Small but Mighty: Affective Micropatterns for Quantifying Mental Health from Social Media Language","abstract_clean":"Many psychological phenomena occur in small time windows, measured in minutes or hours. However, most computational linguistic techniques look at data on the order of weeks, months, or years. We explore micropatterns in sequences of messages occurring over a short time window for their prevalence and power for quantifying psychological phenomena, specifically, patterns in affect. We examine affective micropatterns in social media posts from users with anxiety, eating disorders, panic attacks, schizophrenia, suicidality, and matched controls.","url":"https:\/\/aclanthology.org\/W17-3110"},{"ID":"lu-etal-2021-parameter-efficient","methods":["knowledge specific adapters","plms","small bottleneck feed forward networks","umls metathesaurus graph","attention based knowledge controller","intermediate transformer layers","language models","self supervised learning","knowledge adapters"],"center_method":[null,"plms",null,null,null,null,"language models",null,null],"tasks":["biomedical nlp tasks","knowledge sensitive areas","downstream task","domain knowledge integration"],"center_task":[null,null,"downstream task",null],"Goal":["Good Health and Well-Being"],"text":"Parameter-Efficient Domain Knowledge Integration from Multiple Sources for Biomedical Pre-trained Language Models. Domain-specific pre-trained language models (PLMs) have achieved great success over various downstream tasks in different domains. However, existing domain-specific PLMs mostly rely on self-supervised learning over large amounts of domain text, without explicitly integrating domain-specific knowledge, which can be essential in many domains. Moreover, in knowledge-sensitive areas such as the biomedical domain, knowledge is stored in multiple sources and formats, and existing biomedical PLMs either neglect them or utilize them in a limited manner. In this work, we introduce an architecture to integrate domain knowledge from diverse sources into PLMs in a parameter-efficient way. More specifically, we propose to encode domain knowledge via adapters, which are small bottleneck feed-forward networks inserted between intermediate transformer layers in PLMs. These knowledge adapters are pre-trained for individual domain knowledge sources and integrated via an attention-based knowledge controller to enrich PLMs. Taking the biomedical domain as a case study, we explore three knowledge-specific adapters for PLMs based on the UMLS Metathesaurus graph, the Wikipedia articles for diseases, and the semantic grouping information for biomedical concepts. Extensive experiments on different biomedical NLP tasks and datasets demonstrate the benefits of the proposed architecture and the knowledge-specific adapters across multiple PLMs.","label":1,"title_clean":"Parameter Efficient Domain Knowledge Integration from Multiple Sources for Biomedical Pre trained Language Models","abstract_clean":"Domain specific pre trained language models (PLMs) have achieved great success over various downstream tasks in different domains. However, existing domain specific PLMs mostly rely on self supervised learning over large amounts of domain text, without explicitly integrating domain specific knowledge, which can be essential in many domains. Moreover, in knowledge sensitive areas such as the biomedical domain, knowledge is stored in multiple sources and formats, and existing biomedical PLMs either neglect them or utilize them in a limited manner. In this work, we introduce an architecture to integrate domain knowledge from diverse sources into PLMs in a parameter efficient way. More specifically, we propose to encode domain knowledge via adapters, which are small bottleneck feed forward networks inserted between intermediate transformer layers in PLMs. These knowledge adapters are pre trained for individual domain knowledge sources and integrated via an attention based knowledge controller to enrich PLMs. Taking the biomedical domain as a case study, we explore three knowledge specific adapters for PLMs based on the UMLS Metathesaurus graph, the Wikipedia articles for diseases, and the semantic grouping information for biomedical concepts. Extensive experiments on different biomedical NLP tasks and datasets demonstrate the benefits of the proposed architecture and the knowledge specific adapters across multiple PLMs.","url":"https:\/\/aclanthology.org\/2021.findings-emnlp.325"},{"ID":"lubis-etal-2018-unsupervised","methods":["hierarchical neural network","neural dialogue system","neural dialogue system approaches"],"center_method":[null,null,null],"tasks":["positive emotion elicitation","positive emotion elicitation scenario","response generation","dialogue system interaction","unsupervised counselor dialogue clustering"],"center_task":[null,null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"Unsupervised Counselor Dialogue Clustering for Positive Emotion Elicitation in Neural Dialogue System. Positive emotion elicitation seeks to improve user's emotional state through dialogue system interaction, where a chatbased scenario is layered with an implicit goal to address user's emotional needs. Standard neural dialogue system approaches still fall short in this situation as they tend to generate only short, generic responses. Learning from expert actions is critical, as these potentially differ from standard dialogue acts. In this paper, we propose using a hierarchical neural network for response generation that is conditioned on 1) expert's action, 2) dialogue context, and 3) user emotion, encoded from user input. We construct a corpus of interactions between a counselor and 30 participants following a negative emotional exposure to learn expert actions and responses in a positive emotion elicitation scenario. Instead of relying on the expensive, labor intensive, and often ambiguous human annotations, we unsupervisedly cluster the expert's responses and use the resulting labels to train the network. Our experiments and evaluation show that the proposed approach yields lower perplexity and generates a larger variety of responses.","label":1,"title_clean":"Unsupervised Counselor Dialogue Clustering for Positive Emotion Elicitation in Neural Dialogue System","abstract_clean":"Positive emotion elicitation seeks to improve user's emotional state through dialogue system interaction, where a chatbased scenario is layered with an implicit goal to address user's emotional needs. Standard neural dialogue system approaches still fall short in this situation as they tend to generate only short, generic responses. Learning from expert actions is critical, as these potentially differ from standard dialogue acts. In this paper, we propose using a hierarchical neural network for response generation that is conditioned on 1) expert's action, 2) dialogue context, and 3) user emotion, encoded from user input. We construct a corpus of interactions between a counselor and 30 participants following a negative emotional exposure to learn expert actions and responses in a positive emotion elicitation scenario. Instead of relying on the expensive, labor intensive, and often ambiguous human annotations, we unsupervisedly cluster the expert's responses and use the resulting labels to train the network. Our experiments and evaluation show that the proposed approach yields lower perplexity and generates a larger variety of responses.","url":"https:\/\/aclanthology.org\/W18-5017"},{"ID":"lucy-bamman-2021-gender","methods":["topic models","language models","gpt"],"center_method":["topic models","language models","gpt"],"tasks":["gender and representation bias"],"center_task":[null],"Goal":["Gender Equality","Reduced Inequalities"],"text":"Gender and Representation Bias in GPT-3 Generated Stories. Using topic modeling and lexicon-based word similarity, we find that stories generated by GPT-3 exhibit many known gender stereotypes. Generated stories depict different topics and descriptions depending on GPT-3's perceived gender of the character in a prompt, with feminine characters 1 more likely to be associated with family and appearance, and described as less powerful than masculine characters, even when associated with high power verbs in a prompt. Our study raises questions on how one can avoid unintended social biases when using large language models for storytelling.","label":1,"title_clean":"Gender and Representation Bias in GPT 3 Generated Stories","abstract_clean":"Using topic modeling and lexicon based word similarity, we find that stories generated by GPT 3 exhibit many known gender stereotypes. Generated stories depict different topics and descriptions depending on GPT 3's perceived gender of the character in a prompt, with feminine characters 1 more likely to be associated with family and appearance, and described as less powerful than masculine characters, even when associated with high power verbs in a prompt. Our study raises questions on how one can avoid unintended social biases when using large language models for storytelling.","url":"https:\/\/aclanthology.org\/2021.nuse-1.5"},{"ID":"lukasik-etal-2016-hawkes","methods":["hawkes processes"],"center_method":[null],"tasks":["stance detection","classifying sequences of temporal textual data"],"center_task":["stance detection",null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Hawkes Processes for Continuous Time Sequence Classification: an Application to Rumour Stance Classification in Twitter. Classification of temporal textual data sequences is a common task in various domains such as social media and the Web. In this paper we propose to use Hawkes Processes for classifying sequences of temporal textual data, which exploit both temporal and textual information. Our experiments on rumour stance classification on four Twitter datasets show the importance of using the temporal information of tweets along with the textual content.","label":1,"title_clean":"Hawkes Processes for Continuous Time Sequence Classification: an Application to Rumour Stance Classification in Twitter","abstract_clean":"Classification of temporal textual data sequences is a common task in various domains such as social media and the Web. In this paper we propose to use Hawkes Processes for classifying sequences of temporal textual data, which exploit both temporal and textual information. Our experiments on rumour stance classification on four Twitter datasets show the importance of using the temporal information of tweets along with the textual content.","url":"https:\/\/aclanthology.org\/P16-2064"},{"ID":"ma-etal-2017-detect","methods":["rumor detection","kernel based approach","propagation tree kernel","kernel learning"],"center_method":["rumor detection",null,null,null],"tasks":["detect rumors","microblog posts diffusion"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Detect Rumors in Microblog Posts Using Propagation Structure via Kernel Learning. How fake news goes viral via social media? How does its propagation pattern differ from real stories? In this paper, we attempt to address the problem of identifying rumors, i.e., fake information, out of microblog posts based on their propagation structure. We firstly model microblog posts diffusion with propagation trees, which provide valuable clues on how an original message is transmitted and developed over time. We then propose a kernel-based method called Propagation Tree Kernel, which captures high-order patterns differentiating different types of rumors by evaluating the similarities between their propagation tree structures. Experimental results on two real-world datasets demonstrate that the proposed kernel-based approach can detect rumors more quickly and accurately than state-ofthe-art rumor detection models.","label":1,"title_clean":"Detect Rumors in Microblog Posts Using Propagation Structure via Kernel Learning","abstract_clean":"How fake news goes viral via social media? How does its propagation pattern differ from real stories? In this paper, we attempt to address the problem of identifying rumors, i.e., fake information, out of microblog posts based on their propagation structure. We firstly model microblog posts diffusion with propagation trees, which provide valuable clues on how an original message is transmitted and developed over time. We then propose a kernel based method called Propagation Tree Kernel, which captures high order patterns differentiating different types of rumors by evaluating the similarities between their propagation tree structures. Experimental results on two real world datasets demonstrate that the proposed kernel based approach can detect rumors more quickly and accurately than state ofthe art rumor detection models.","url":"https:\/\/aclanthology.org\/P17-1066.pdf"},{"ID":"maegaard-etal-2008-medar","methods":["medar","nemlar network"],"center_method":[null,null],"tasks":["language technology","machine translation","arabic","cooperation roadmap","medar project","information retrieval","dissemination"],"center_task":["language technology","machine translation",null,null,null,"information retrieval",null],"Goal":["Partnership for the Goals"],"text":"MEDAR: Collaboration between European and Mediterranean Arabic Partners to Support the Development of Language Technology for Arabic. After the successful completion of the NEMLAR project 2003-2005, a new opportunity for a project was opened by the European Commission, and a group of largely the same partners is now executing the MEDAR project. MEDAR will be updating the surveys and BLARK for Arabic already made, and will then focus on machine translation (and other tools for translation) and information retrieval with a focus on language resources, tools and evaluation for these applications. A very important part of the MEDAR project is to reinforce and extend the NEMLAR network and to create a cooperation roadmap for Human Language Technologies for Arabic. It is expected that the cooperation roadmap will attract wide attention from other parties and that it can help create a larger platform for collaborative projects. Finally, the project will focus on dissemination of knowledge about existing resources and tools, as well as actors and activities; this will happen through newsletter, website and an international conference which will follow up on the Cairo conference of 2004. Dissemination to user communities will also be important, e.g. through participation in translators' conferences. The goal of these activities is to create a stronger and lasting collaboration between EU countries and Arabic speaking countries.","label":1,"title_clean":"MEDAR: Collaboration between European and Mediterranean Arabic Partners to Support the Development of Language Technology for Arabic","abstract_clean":"After the successful completion of the NEMLAR project 2003 2005, a new opportunity for a project was opened by the European Commission, and a group of largely the same partners is now executing the MEDAR project. MEDAR will be updating the surveys and BLARK for Arabic already made, and will then focus on machine translation (and other tools for translation) and information retrieval with a focus on language resources, tools and evaluation for these applications. A very important part of the MEDAR project is to reinforce and extend the NEMLAR network and to create a cooperation roadmap for Human Language Technologies for Arabic. It is expected that the cooperation roadmap will attract wide attention from other parties and that it can help create a larger platform for collaborative projects. Finally, the project will focus on dissemination of knowledge about existing resources and tools, as well as actors and activities; this will happen through newsletter, website and an international conference which will follow up on the Cairo conference of 2004. Dissemination to user communities will also be important, e.g. through participation in translators' conferences. The goal of these activities is to create a stronger and lasting collaboration between EU countries and Arabic speaking countries.","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2008\/pdf\/917_paper.pdf"},{"ID":"maheshwari-etal-2021-scibert","methods":["scibert sentence representation","linear layer"],"center_method":[null,null],"tasks":["citation context classification","scholarly document processing workshop"],"center_task":[null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"SciBERT Sentence Representation for Citation Context Classification. This paper describes our system (IREL) for 3C-Citation Context Classification shared task of the Scholarly Document Processing Workshop at NAACL 2021 (Suchetha N Kunnath and Knoth, 2021). We participated in both subtask A and subtask B. Our best system achieved a Macro F1 score of 0.26973 on the private leaderboard for subtask A and was ranked one. For subtask B our best system achieved a Macro F1 score of 0.59071 on the private leaderboard and was ranked two. We used similar models for both the subtasks with some minor changes, as discussed in this paper. Our best performing model for both the subtask was a finetuned SciBert model followed by a linear layer. We provide a detailed description of all the approaches we tried and their results. The code can be found https:\/\/github.com\/ bhavyajeet\/3c-citation_text_ classification","label":1,"title_clean":"SciBERT Sentence Representation for Citation Context Classification","abstract_clean":"This paper describes our system (IREL) for 3C Citation Context Classification shared task of the Scholarly Document Processing Workshop at NAACL 2021 (Suchetha N Kunnath and Knoth, 2021). We participated in both subtask A and subtask B. Our best system achieved a Macro F1 score of 0.26973 on the private leaderboard for subtask A and was ranked one. For subtask B our best system achieved a Macro F1 score of 0.59071 on the private leaderboard and was ranked two. We used similar models for both the subtasks with some minor changes, as discussed in this paper. Our best performing model for both the subtask was a finetuned SciBert model followed by a linear layer. We provide a detailed description of all the approaches we tried and their results. The code can be found https:\/\/github.com\/ bhavyajeet\/3c citation_text_ classification","url":"https:\/\/aclanthology.org\/2021.sdp-1.17"},{"ID":"maldonado-harabagiu-2020-language","methods":["annotation","knowledge capture techniques","clinical electroencephalography","self attention joint learning method"],"center_method":["annotation",null,null,null],"tasks":["annotation","and relation annotations","probing neural function","language of brain signals","annotation of long distance relations between concepts"],"center_task":["annotation",null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"The Language of Brain Signals: Natural Language Processing of Electroencephalography Reports. Brain signals are captured by clinical electroencephalography (EEG) which is an excellent tool for probing neural function. When EEG tests are performed, a textual EEG report is generated by the neurologist to document the findings, thus using language that describes the brain signals and their clinical correlations. Even with the impetus provided by the BRAIN initiative (brainitititive.nih.gov), there are no annotations available in texts that use natural language describing the brain activities and their correlations with various pathologies. In this paper we describe an annotation effort carried out on a large corpus of EEG reports, providing examples of EEG-specific and clinically relevant concepts. In addition, we detail our annotation schema for brain signal attributes. We also discuss the resulting annotation of long-distance relations between concepts in EEG reports. By exemplifying a self-attention joint-learning method used to predict concept, attribute and relation annotations in the EEG report corpus, we discuss the promising results of automatic annotations, hoping that our effort will inform the design of novel knowledge capture techniques that will include the language of brain signals.","label":1,"title_clean":"The Language of Brain Signals: Natural Language Processing of Electroencephalography Reports","abstract_clean":"Brain signals are captured by clinical electroencephalography (EEG) which is an excellent tool for probing neural function. When EEG tests are performed, a textual EEG report is generated by the neurologist to document the findings, thus using language that describes the brain signals and their clinical correlations. Even with the impetus provided by the BRAIN initiative (brainitititive.nih.gov), there are no annotations available in texts that use natural language describing the brain activities and their correlations with various pathologies. In this paper we describe an annotation effort carried out on a large corpus of EEG reports, providing examples of EEG specific and clinically relevant concepts. In addition, we detail our annotation schema for brain signal attributes. We also discuss the resulting annotation of long distance relations between concepts in EEG reports. By exemplifying a self attention joint learning method used to predict concept, attribute and relation annotations in the EEG report corpus, we discuss the promising results of automatic annotations, hoping that our effort will inform the design of novel knowledge capture techniques that will include the language of brain signals.","url":"https:\/\/aclanthology.org\/2020.lrec-1.276.pdf"},{"ID":"marge-etal-2019-research","methods":["robotic platform"],"center_method":[null],"tasks":["behavior specification","multi robot dialogue","dialogue management","automatic speech recognition","spoken dialogue interaction"],"center_task":[null,null,null,"automatic speech recognition",null],"Goal":["Decent Work and Economic Growth"],"text":"A Research Platform for Multi-Robot Dialogue with Humans. This paper presents a research platform that supports spoken dialogue interaction with multiple robots. The demonstration showcases our crafted MultiBot testing scenario in which users can verbally issue search, navigate, and follow instructions to two robotic teammates: a simulated ground robot and an aerial robot. This flexible language and robotic platform takes advantage of existing tools for speech recognition and dialogue management that are compatible with new domains, and implements an inter-agent communication protocol (tactical behavior specification), where verbal instructions are encoded for tasks assigned to the appropriate robot.","label":1,"title_clean":"A Research Platform for Multi Robot Dialogue with Humans","abstract_clean":"This paper presents a research platform that supports spoken dialogue interaction with multiple robots. The demonstration showcases our crafted MultiBot testing scenario in which users can verbally issue search, navigate, and follow instructions to two robotic teammates: a simulated ground robot and an aerial robot. This flexible language and robotic platform takes advantage of existing tools for speech recognition and dialogue management that are compatible with new domains, and implements an inter agent communication protocol (tactical behavior specification), where verbal instructions are encoded for tasks assigned to the appropriate robot.","url":"https:\/\/aclanthology.org\/N19-4023.pdf"},{"ID":"mariani-etal-2016-study","methods":["nlp4nlp","speech processing"],"center_method":[null,null],"tasks":["reuse and plagiarism","speech"],"center_task":[null,null],"Goal":["Industry, Innovation and Infrastrucure","Quality Education"],"text":"A Study of Reuse and Plagiarism in Speech and Natural Language Processing papers. The aim of this experiment is to present an easy way to compare fragments of texts in order to detect (supposed) results of copy & paste operations between articles in the domain of Natural Language Processing, including Speech Processing (NLP). The search space of the comparisons is a corpus labelled as NLP4NLP, which includes 34 different sources and gathers a large part of the publications in the NLP field over the past 50 years. This study considers the similarity between the papers of each individual source and the complete set of papers in the whole corpus, according to four different types of relationship (self-reuse, self-plagiarism, reuse and plagiarism) and in both directions: a source paper borrowing a fragment of text from another paper of the collection, or in the reverse direction, fragments of text from the source paper being borrowed and inserted in another paper of the collection.","label":1,"title_clean":"A Study of Reuse and Plagiarism in Speech and Natural Language Processing papers","abstract_clean":"The aim of this experiment is to present an easy way to compare fragments of texts in order to detect (supposed) results of copy & paste operations between articles in the domain of Natural Language Processing, including Speech Processing (NLP). The search space of the comparisons is a corpus labelled as NLP4NLP, which includes 34 different sources and gathers a large part of the publications in the NLP field over the past 50 years. This study considers the similarity between the papers of each individual source and the complete set of papers in the whole corpus, according to four different types of relationship (self reuse, self plagiarism, reuse and plagiarism) and in both directions: a source paper borrowing a fragment of text from another paper of the collection, or in the reverse direction, fragments of text from the source paper being borrowed and inserted in another paper of the collection.","url":"https:\/\/aclanthology.org\/W16-1509"},{"ID":"marinelli-etal-2008-encoding","methods":["hyperonymyhyponymy relation"],"center_method":[null],"tasks":["maritime terminological database","exporting synsets","meteorology"],"center_task":[null,null,null],"Goal":["Quality Education"],"text":"Encoding Terms from a Scientific Domain in a Terminological Database: Methodology and Criteria. This paper reports on the main phases of a research which aims at enhancing a maritime terminological database by means of a set of terms belonging to meteorology. The structure of the terminological database, according to EuroWordNet\/ItalWordNet model is described; the criteria used to build corpora of specialized texts are explained as well as the use of the corpora as source for term selection and extraction. The contribution of the semantic databases is taken into account: on the one hand, the most recent version of the Princeton WordNet has been exploited as reference for comparing and evaluating synsets; on the other hand, the Italian WordNet has been employed as source for exporting synsets to be coded in the terminological resource. The set of semantic relations useful to codify new terms belonging to the discipline of meteorology is examined, revising the semantic relations provided by the IWN model, introducing new relations which are more suitably tailored to specific requirements either scientific or pragmatic. The need for a particular relation is highlighted to represent the mental association which is made when a term intuitively recalls another term, but they are neither synonyms nor connected by means of a hyperonymy\/hyponymy relation.","label":1,"title_clean":"Encoding Terms from a Scientific Domain in a Terminological Database: Methodology and Criteria","abstract_clean":"This paper reports on the main phases of a research which aims at enhancing a maritime terminological database by means of a set of terms belonging to meteorology. The structure of the terminological database, according to EuroWordNet\/ItalWordNet model is described; the criteria used to build corpora of specialized texts are explained as well as the use of the corpora as source for term selection and extraction. The contribution of the semantic databases is taken into account: on the one hand, the most recent version of the Princeton WordNet has been exploited as reference for comparing and evaluating synsets; on the other hand, the Italian WordNet has been employed as source for exporting synsets to be coded in the terminological resource. The set of semantic relations useful to codify new terms belonging to the discipline of meteorology is examined, revising the semantic relations provided by the IWN model, introducing new relations which are more suitably tailored to specific requirements either scientific or pragmatic. The need for a particular relation is highlighted to represent the mental association which is made when a term intuitively recalls another term, but they are neither synonyms nor connected by means of a hyperonymy\/hyponymy relation.","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2008\/pdf\/623_paper.pdf"},{"ID":"marty-albisser-1995-integration","methods":["machine translation"],"center_method":["machine translation"],"tasks":["business process","machine translation"],"center_task":[null,"machine translation"],"Goal":["Decent Work and Economic Growth","Partnership for the Goals"],"text":"Integration of MT into the business process. The integration of machine translation (MT) into the business process should be viewed from an overall perspective thus requiring several factors to be taken into consideration. These include selecting the right product, appointing the right people, restructuring the work flow, and measuring performance to finally achieve the projected productivity gain.","label":1,"title_clean":"Integration of MT into the business process","abstract_clean":"The integration of machine translation (MT) into the business process should be viewed from an overall perspective thus requiring several factors to be taken into consideration. These include selecting the right product, appointing the right people, restructuring the work flow, and measuring performance to finally achieve the projected productivity gain.","url":"https:\/\/aclanthology.org\/1995.mtsummit-1.30"},{"ID":"matero-etal-2019-suicide","methods":["dual rnn architecture","sbu hlab","dual context approach"],"center_method":[null,null,null],"tasks":["suicide risk assessment"],"center_task":[null],"Goal":["Good Health and Well-Being"],"text":"Suicide Risk Assessment with Multi-level Dual-Context Language and BERT. Mental health predictive systems typically model language as if from a single context (e.g. Twitter posts, status updates, or forum posts) and often limited to a single level of analysis (e.g. either the message-level or userlevel). Here, we bring these pieces together to explore the use of open-vocabulary (BERT embeddings, topics) and theoretical features (emotional expression lexica, personality) for the task of suicide risk assessment on support forums (the CLPsych-2019 Shared Task). We used dual context based approaches (modeling content from suicide forums separate from other content), built over both traditional ML models as well as a novel dual RNN architecture with user-factor adaptation. We find that while affect from the suicide context distinguishes with no-risk from those with \"anyrisk\", personality factors from the non-suicide contexts provide distinction of the levels of risk: low, medium, and high risk. Within the shared task, our dual-context approach (listed as SBU-HLAB in the official results) achieved state-of-the-art performance predicting suicide risk using a combination of suicide-context and non-suicide posts (Task B), achieving an F1 score of 0.50 over hidden test set labels.","label":1,"title_clean":"Suicide Risk Assessment with Multi level Dual Context Language and BERT","abstract_clean":"Mental health predictive systems typically model language as if from a single context (e.g. Twitter posts, status updates, or forum posts) and often limited to a single level of analysis (e.g. either the message level or userlevel). Here, we bring these pieces together to explore the use of open vocabulary (BERT embeddings, topics) and theoretical features (emotional expression lexica, personality) for the task of suicide risk assessment on support forums (the CLPsych 2019 Shared Task). We used dual context based approaches (modeling content from suicide forums separate from other content), built over both traditional ML models as well as a novel dual RNN architecture with user factor adaptation. We find that while affect from the suicide context distinguishes with no risk from those with \"anyrisk\", personality factors from the non suicide contexts provide distinction of the levels of risk: low, medium, and high risk. Within the shared task, our dual context approach (listed as SBU HLAB in the official results) achieved state of the art performance predicting suicide risk using a combination of suicide context and non suicide posts (Task B), achieving an F1 score of 0.50 over hidden test set labels.","url":"https:\/\/aclanthology.org\/W19-3005"},{"ID":"mathur-etal-2018-detecting","methods":["convolutional neural network","transfer learning"],"center_method":["convolutional neural network","transfer learning"],"tasks":["classification","detecting offensive tweets"],"center_task":["classification",null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Detecting Offensive Tweets in Hindi-English Code-Switched Language. The exponential rise of social media websites like Twitter, Facebook and Reddit in linguistically diverse geographical regions has led to hybridization of popular native languages with English in an effort to ease communication. The paper focuses on the classification of offensive tweets written in Hinglish language, which is a portmanteau of the Indic language Hindi with the Roman script. The paper introduces a novel tweet dataset, titled Hindi-English Offensive Tweet (HEOT) dataset, consisting of tweets in Hindi-English code switched language split into three classes: nonoffensive, abusive and hate-speech. Further, we approach the problem of classification of the tweets in HEOT dataset using transfer learning wherein the proposed model employing Convolutional Neural Networks is pre-trained on tweets in English followed by retraining on Hinglish tweets.","label":1,"title_clean":"Detecting Offensive Tweets in Hindi English Code Switched Language","abstract_clean":"The exponential rise of social media websites like Twitter, Facebook and Reddit in linguistically diverse geographical regions has led to hybridization of popular native languages with English in an effort to ease communication. The paper focuses on the classification of offensive tweets written in Hinglish language, which is a portmanteau of the Indic language Hindi with the Roman script. The paper introduces a novel tweet dataset, titled Hindi English Offensive Tweet (HEOT) dataset, consisting of tweets in Hindi English code switched language split into three classes: nonoffensive, abusive and hate speech. Further, we approach the problem of classification of the tweets in HEOT dataset using transfer learning wherein the proposed model employing Convolutional Neural Networks is pre trained on tweets in English followed by retraining on Hinglish tweets.","url":"https:\/\/aclanthology.org\/W18-3504"},{"ID":"mathur-etal-2018-offend","methods":["multi input multi channel transfer learning based model","lstm","baseline supervised classification models","mimct","multi channel cnn lstm architecture","transfer learning"],"center_method":[null,"lstm",null,null,null,"transfer learning"],"tasks":["classification of offensive tweets"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Did you offend me? Classification of Offensive Tweets in Hinglish Language. The use of code-switched languages (e.g., Hinglish, which is derived by the blending of Hindi with the English language) is getting much popular on Twitter due to their ease of communication in native languages. However, spelling variations and absence of grammar rules introduce ambiguity and make it difficult to understand the text automatically. This paper presents the Multi-Input Multi-Channel Transfer Learning based model (MIMCT) to detect offensive (hate speech or abusive) Hinglish tweets from the proposed Hinglish Offensive Tweet (HOT) dataset using transfer learning coupled with multiple feature inputs. Specifically, it takes multiple primary word embedding along with secondary extracted features as inputs to train a multi-channel CNN-LSTM architecture that has been pre-trained on English tweets through transfer learning. The proposed MIMCT model outperforms the baseline supervised classification models, transfer learning based CNN and LSTM models to establish itself as the state of the art in the unexplored domain of Hinglish offensive text classification.","label":1,"title_clean":"Did you offend me? Classification of Offensive Tweets in Hinglish Language","abstract_clean":"The use of code switched languages (e.g., Hinglish, which is derived by the blending of Hindi with the English language) is getting much popular on Twitter due to their ease of communication in native languages. However, spelling variations and absence of grammar rules introduce ambiguity and make it difficult to understand the text automatically. This paper presents the Multi Input Multi Channel Transfer Learning based model (MIMCT) to detect offensive (hate speech or abusive) Hinglish tweets from the proposed Hinglish Offensive Tweet (HOT) dataset using transfer learning coupled with multiple feature inputs. Specifically, it takes multiple primary word embedding along with secondary extracted features as inputs to train a multi channel CNN LSTM architecture that has been pre trained on English tweets through transfer learning. The proposed MIMCT model outperforms the baseline supervised classification models, transfer learning based CNN and LSTM models to establish itself as the state of the art in the unexplored domain of Hinglish offensive text classification.","url":"https:\/\/aclanthology.org\/W18-5118"},{"ID":"mayfield-black-2019-stance","methods":["bert contextualized word embeddings","language representations"],"center_method":[null,null],"tasks":["stance detection","nlp applications","group decision making","behavioral scientists","consensus formation","nuanced process of conflict and resolution"],"center_task":["stance detection","nlp applications",null,null,null,null],"Goal":["Partnership for the Goals","Peace, Justice and Strong Institutions"],"text":"Stance Classification, Outcome Prediction, and Impact Assessment: NLP Tasks for Studying Group Decision-Making. In group decision-making, the nuanced process of conflict and resolution that leads to consensus formation is closely tied to the quality of decisions made. Behavioral scientists rarely have rich access to process variables, though, as unstructured discussion transcripts are difficult to analyze. Here, we define ways for NLP researchers to contribute to the study of groups and teams. We introduce three tasks alongside a large new corpus of over 400,000 group debates on Wikipedia. We describe the tasks and their importance, then provide baselines showing that BERT contextualized word embeddings consistently outperform other language representations.","label":1,"title_clean":"Stance Classification, Outcome Prediction, and Impact Assessment: NLP Tasks for Studying Group Decision Making","abstract_clean":"In group decision making, the nuanced process of conflict and resolution that leads to consensus formation is closely tied to the quality of decisions made. Behavioral scientists rarely have rich access to process variables, though, as unstructured discussion transcripts are difficult to analyze. Here, we define ways for NLP researchers to contribute to the study of groups and teams. We introduce three tasks alongside a large new corpus of over 400,000 group debates on Wikipedia. We describe the tasks and their importance, then provide baselines showing that BERT contextualized word embeddings consistently outperform other language representations.","url":"https:\/\/aclanthology.org\/W19-2108.pdf"},{"ID":"mcinnes-2008-unsupervised","methods":["umls","unsupervised vector approach","senseclusters"],"center_method":["umls",null,null],"tasks":["biomedical term disambiguation"],"center_task":[null],"Goal":["Good Health and Well-Being"],"text":"An Unsupervised Vector Approach to Biomedical Term Disambiguation: Integrating UMLS and Medline. This paper introduces an unsupervised vector approach to disambiguate words in biomedical text that can be applied to all-word disambiguation. We explore using contextual information from the Unified Medical Language System (UMLS) to describe the possible senses of a word. We experiment with automatically creating individualized stoplists to help reduce the noise in our dataset. We compare our results to SenseClusters and Humphrey et al. (2006) using the NLM-WSD dataset and with SenseClusters using conflated data from the 2005 Medline Baseline.","label":1,"title_clean":"An Unsupervised Vector Approach to Biomedical Term Disambiguation: Integrating UMLS and Medline","abstract_clean":"This paper introduces an unsupervised vector approach to disambiguate words in biomedical text that can be applied to all word disambiguation. We explore using contextual information from the Unified Medical Language System (UMLS) to describe the possible senses of a word. We experiment with automatically creating individualized stoplists to help reduce the noise in our dataset. We compare our results to SenseClusters and Humphrey et al. (2006) using the NLM WSD dataset and with SenseClusters using conflated data from the 2005 Medline Baseline.","url":"https:\/\/aclanthology.org\/P08-3009.pdf"},{"ID":"mclaughlin-schwall-1998-horses","methods":["universal translation solution"],"center_method":[null],"tasks":["machine translation","arduous preprocessing activities","user acceptance"],"center_task":["machine translation",null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Horses for Courses: Changing User Acceptance of Machine Translation. The key to Machine Translation becoming a commonplace technology is user acceptance. Unfortunately, the decision whether or not to use Machine Translation is often made on the basis of output quality alone. As we all know, Machine Translation output is far from perfect, and its quality depends on a wide range of factors related to individual users, the environment in which they work, and the text types they work with-factors which are difficult and arduous to evaluate. Although output quality obviously plays an important role, it is not the only factor in user acceptance-and for some potential users it may not even be the most important one. User perception of Machine Translation is a decisive issue, and MT must be seen-not as a universal translation solution, but as one of several potential tools-not in isolation, but within the context of the user's work processes. This has important implications for Machine Translation vendors. It means that Machine Translation shouldn't be offered in isolation. Depending on the product\/target group, it must be combined with other tools and\/or combined with other services (postediting\/human translation). Products must also be scaled to the user's purse and environment, the entry threshold must be low and products must be upgradeable as the user's needs change. It must be easy to access and use Machine Translation: complicated access to Machine Translation and arduous preprocessing activities will make it a non-starter for many people. What's more, Machine Translation must be available when and where the user needs it, whatever the application.","label":1,"title_clean":"Horses for Courses: Changing User Acceptance of Machine Translation","abstract_clean":"The key to Machine Translation becoming a commonplace technology is user acceptance. Unfortunately, the decision whether or not to use Machine Translation is often made on the basis of output quality alone. As we all know, Machine Translation output is far from perfect, and its quality depends on a wide range of factors related to individual users, the environment in which they work, and the text types they work with factors which are difficult and arduous to evaluate. Although output quality obviously plays an important role, it is not the only factor in user acceptance and for some potential users it may not even be the most important one. User perception of Machine Translation is a decisive issue, and MT must be seen not as a universal translation solution, but as one of several potential tools not in isolation, but within the context of the user's work processes. This has important implications for Machine Translation vendors. It means that Machine Translation shouldn't be offered in isolation. Depending on the product\/target group, it must be combined with other tools and\/or combined with other services (postediting\/human translation). Products must also be scaled to the user's purse and environment, the entry threshold must be low and products must be upgradeable as the user's needs change. It must be easy to access and use Machine Translation: complicated access to Machine Translation and arduous preprocessing activities will make it a non starter for many people. What's more, Machine Translation must be available when and where the user needs it, whatever the application.","url":"https:\/\/aclanthology.org\/1998.tc-1.10"},{"ID":"medina-maza-etal-2020-event","methods":["event related bias removal"],"center_method":[null],"tasks":["classification of information"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Event-Related Bias Removal for Real-time Disaster Events. Social media has become an important tool to share information about crisis events such as natural disasters and mass attacks. Detecting actionable posts that contain useful information requires rapid analysis of huge volume of data in real-time. This poses a complex problem due to the large amount of posts that do not contain any actionable information. Furthermore, the classification of information in real-time systems requires training on outof-domain data, as we do not have any data from a new emerging crisis. Prior work focuses on models pre-trained on similar event types. However, those models capture unnecessary event-specific biases, like the location of the event, which affect the generalizability and performance of the classifiers on new unseen data from an emerging new event. In our work, we train an adversarial neural model to remove latent event-specific biases and improve the performance on tweet importance classification.","label":1,"title_clean":"Event Related Bias Removal for Real time Disaster Events","abstract_clean":"Social media has become an important tool to share information about crisis events such as natural disasters and mass attacks. Detecting actionable posts that contain useful information requires rapid analysis of huge volume of data in real time. This poses a complex problem due to the large amount of posts that do not contain any actionable information. Furthermore, the classification of information in real time systems requires training on outof domain data, as we do not have any data from a new emerging crisis. Prior work focuses on models pre trained on similar event types. However, those models capture unnecessary event specific biases, like the location of the event, which affect the generalizability and performance of the classifiers on new unseen data from an emerging new event. In our work, we train an adversarial neural model to remove latent event specific biases and improve the performance on tweet importance classification.","url":"https:\/\/aclanthology.org\/2020.findings-emnlp.344"},{"ID":"mencarini-2018-potential","methods":["computational linguistic analysis","micro theory","computational linguistic analysis of social media"],"center_method":[null,null,null],"tasks":["population studies","demographic analysis","population processes"],"center_task":[null,null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"The Potential of the Computational Linguistic Analysis of Social Media for Population Studies. The paper provides an outline of the scope for synergy between computational linguistic analysis and population studies. It first reviews where population studies stand in terms of using social media data. Demographers are entering the realm of big data in force. But, this paper argues, population studies have much to gain from computational linguistic analysis, especially in terms of explaining the drivers behind population processes. The paper gives two examples of how the method can be applied, and concludes with a fundamental caveat. Yes, computational linguistic analysis provides a possible key for integrating micro theory into any demographic analysis of social media data. But results may be of little value in as much as knowledge about fundamental sample characteristics are unknown.","label":1,"title_clean":"The Potential of the Computational Linguistic Analysis of Social Media for Population Studies","abstract_clean":"The paper provides an outline of the scope for synergy between computational linguistic analysis and population studies. It first reviews where population studies stand in terms of using social media data. Demographers are entering the realm of big data in force. But, this paper argues, population studies have much to gain from computational linguistic analysis, especially in terms of explaining the drivers behind population processes. The paper gives two examples of how the method can be applied, and concludes with a fundamental caveat. Yes, computational linguistic analysis provides a possible key for integrating micro theory into any demographic analysis of social media data. But results may be of little value in as much as knowledge about fundamental sample characteristics are unknown.","url":"https:\/\/aclanthology.org\/W18-1109"},{"ID":"meng-etal-2018-automatic","methods":["linear chain crfs","recurrent neural network model","conditional random field","neural network","lstm"],"center_method":[null,null,"conditional random field","neural network","lstm"],"tasks":["automatic labeling of problem solving dialogues","dialogue segment annotation","microgenetic analysis of cognitive interactions between students","robotics challenge","computational microgenetic learning analytics","analysis of students computational thinking"],"center_task":[null,null,null,null,null,null],"Goal":["Quality Education"],"text":"Automatic Labeling of Problem-Solving Dialogues for Computational Microgenetic Learning Analytics. This paper presents a recurrent neural network model to automate the analysis of students' computational thinking in problem-solving dialogue. We have collected and annotated dialogue transcripts from middle school students solving a robotics challenge, and each dialogue turn is assigned a code. We use sentence embeddings and speaker identities as features, and experiment with linear chain CRFs and RNNs with a CRF layer (LSTM-CRF). Both the linear chain CRF model and the LSTM-CRF model outperform the na\u00efve baselines by a large margin, and LSTM-CRF has an edge between the two. To our knowledge, this is the first study on dialogue segment annotation using neural network models. This study is also a stepping-stone to automating the microgenetic analysis of cognitive interactions between students.","label":1,"title_clean":"Automatic Labeling of Problem Solving Dialogues for Computational Microgenetic Learning Analytics","abstract_clean":"This paper presents a recurrent neural network model to automate the analysis of students' computational thinking in problem solving dialogue. We have collected and annotated dialogue transcripts from middle school students solving a robotics challenge, and each dialogue turn is assigned a code. We use sentence embeddings and speaker identities as features, and experiment with linear chain CRFs and RNNs with a CRF layer (LSTM CRF). Both the linear chain CRF model and the LSTM CRF model outperform the na\u00efve baselines by a large margin, and LSTM CRF has an edge between the two. To our knowledge, this is the first study on dialogue segment annotation using neural network models. This study is also a stepping stone to automating the microgenetic analysis of cognitive interactions between students.","url":"https:\/\/aclanthology.org\/L18-1639.pdf"},{"ID":"meng-etal-2021-mixture","methods":["bert","mop","mixture of partitions","sota","mixture layer","sub graph adapters","infusion approach","pretrained models","lightweight adapters"],"center_method":["bert",null,null,null,null,null,null,"pretrained models",null],"tasks":["classification","inference","downstream task","question answering","knowledgeintensive tasks"],"center_task":["classification","inference","downstream task","question answering",null],"Goal":["Good Health and Well-Being"],"text":"Mixture-of-Partitions: Infusing Large Biomedical Knowledge Graphs into BERT. Infusing factual knowledge into pretrained models is fundamental for many knowledgeintensive tasks. In this paper, we propose Mixture-of-Partitions (MoP), an infusion approach that can handle a very large knowledge graph (KG) by partitioning it into smaller subgraphs and infusing their specific knowledge into various BERT models using lightweight adapters. To leverage the overall factual knowledge for a target task, these sub-graph adapters are further fine-tuned along with the underlying BERT through a mixture layer. We evaluate our MoP with three biomedical BERTs (SciBERT, BioBERT, PubmedBERT) on six downstream tasks (inc. NLI, QA, Classification), and the results show that our MoP consistently enhances the underlying BERTs in task performance, and achieves new SOTA performances on five evaluated datasets. 1","label":1,"title_clean":"Mixture of Partitions: Infusing Large Biomedical Knowledge Graphs into BERT","abstract_clean":"Infusing factual knowledge into pretrained models is fundamental for many knowledgeintensive tasks. In this paper, we propose Mixture of Partitions (MoP), an infusion approach that can handle a very large knowledge graph (KG) by partitioning it into smaller subgraphs and infusing their specific knowledge into various BERT models using lightweight adapters. To leverage the overall factual knowledge for a target task, these sub graph adapters are further fine tuned along with the underlying BERT through a mixture layer. We evaluate our MoP with three biomedical BERTs (SciBERT, BioBERT, PubmedBERT) on six downstream tasks (inc. NLI, QA, Classification), and the results show that our MoP consistently enhances the underlying BERTs in task performance, and achieves new SOTA performances on five evaluated datasets. 1","url":"https:\/\/aclanthology.org\/2021.emnlp-main.383.pdf"},{"ID":"meng-etal-2022-rewire","methods":["probing techniques","plms","contrastive probe","umls","language models","self supervised contrastive probing approach","contrastive recipe","knowledge transfer mechanism","rewire then probe"],"center_method":[null,"plms",null,"umls","language models",null,null,null,null],"tasks":["probing","probing biomedical knowledge"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"Rewire-then-Probe: A Contrastive Recipe for Probing Biomedical Knowledge of Pre-trained Language Models. Knowledge probing is crucial for understanding the knowledge transfer mechanism behind the pre-trained language models (PLMs). Despite the growing progress of probing knowledge for PLMs in the general domain, specialised areas such as biomedical domain are vastly under-explored. To facilitate this, we release a well-curated biomedical knowledge probing benchmark, MedLAMA, constructed based on the Unified Medical Language System (UMLS) Metathesaurus. We test a wide spectrum of state-of-the-art PLMs and probing approaches on our benchmark, reaching at most 3% of acc@10. While highlighting various sources of domain-specific challenges that amount to this underwhelming performance, we illustrate that the underlying PLMs have a higher potential for probing tasks. To achieve this, we propose Contrastive-Probe, a novel self-supervised contrastive probing approach, that adjusts the underlying PLMs without using any probing data. While Contrastive-Probe pushes the acc@10 to 24%, the performance gap remains notable. Our human expert evaluation suggests that the probing performance of our Contrastive-Probe is underestimated as UMLS does not comprehensively cover all existing factual knowledge. We hope MedLAMA and Contrastive-Probe facilitate further developments of more suited probing techniques for this domain. 1","label":1,"title_clean":"Rewire then Probe: A Contrastive Recipe for Probing Biomedical Knowledge of Pre trained Language Models","abstract_clean":"Knowledge probing is crucial for understanding the knowledge transfer mechanism behind the pre trained language models (PLMs). Despite the growing progress of probing knowledge for PLMs in the general domain, specialised areas such as biomedical domain are vastly under explored. To facilitate this, we release a well curated biomedical knowledge probing benchmark, MedLAMA, constructed based on the Unified Medical Language System (UMLS) Metathesaurus. We test a wide spectrum of state of the art PLMs and probing approaches on our benchmark, reaching at most 3% of acc@10. While highlighting various sources of domain specific challenges that amount to this underwhelming performance, we illustrate that the underlying PLMs have a higher potential for probing tasks. To achieve this, we propose Contrastive Probe, a novel self supervised contrastive probing approach, that adjusts the underlying PLMs without using any probing data. While Contrastive Probe pushes the acc@10 to 24%, the performance gap remains notable. Our human expert evaluation suggests that the probing performance of our Contrastive Probe is underestimated as UMLS does not comprehensively cover all existing factual knowledge. We hope MedLAMA and Contrastive Probe facilitate further developments of more suited probing techniques for this domain. 1","url":"https:\/\/aclanthology.org\/2022.acl-long.329"},{"ID":"meyer-gamback-2019-platform","methods":["hate speech detector","optimised architecture","word embeddings","long short term memory networks","detection approaches","convolutional neural network","ngrams","platform agnostic dual strand hate speech detector"],"center_method":[null,null,"word embeddings",null,null,"convolutional neural network","ngrams",null],"tasks":["classification","detection"],"center_task":["classification","detection"],"Goal":["Peace, Justice and Strong Institutions"],"text":"A Platform Agnostic Dual-Strand Hate Speech Detector. Hate speech detectors must be applicable across a multitude of services and platforms, and there is hence a need for detection approaches that do not depend on any information specific to a given platform. For instance, the information stored about the text's author may differ between services, and so using such data would reduce a system's general applicability. The paper thus focuses on using exclusively text-based input in the detection, in an optimised architecture combining Convolutional Neural Networks and Long Short-Term Memory-networks. The hate speech detector merges two strands with character ngrams and word embeddings to produce the final classification, and is shown to outperform comparable previous approaches.","label":1,"title_clean":"A Platform Agnostic Dual Strand Hate Speech Detector","abstract_clean":"Hate speech detectors must be applicable across a multitude of services and platforms, and there is hence a need for detection approaches that do not depend on any information specific to a given platform. For instance, the information stored about the text's author may differ between services, and so using such data would reduce a system's general applicability. The paper thus focuses on using exclusively text based input in the detection, in an optimised architecture combining Convolutional Neural Networks and Long Short Term Memory networks. The hate speech detector merges two strands with character ngrams and word embeddings to produce the final classification, and is shown to outperform comparable previous approaches.","url":"https:\/\/aclanthology.org\/W19-3516"},{"ID":"mihaylov-etal-2015-exposing","methods":["classification","machine learning methods"],"center_method":["classification","machine learning methods"],"tasks":["paid trolls","training data problem"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Exposing Paid Opinion Manipulation Trolls. Recently, Web forums have been invaded by opinion manipulation trolls. Some trolls try to influence the other users driven by their own convictions, while in other cases they can be organized and paid, e.g., by a political party or a PR agency that gives them specific instructions what to write. Finding paid trolls automatically using machine learning is a hard task, as there is no enough training data to train a classifier; yet some test data is possible to obtain, as these trolls are sometimes caught and widely exposed. In this paper, we solve the training data problem by assuming that a user who is called a troll by several different people is likely to be such, and one who has never been called a troll is unlikely to be such. We compare the profiles of (i) paid trolls vs. (ii) \"mentioned\" trolls vs. (iii) non-trolls, and we further show that a classifier trained to distinguish (ii) from (iii) does quite well also at telling apart (i) from (iii).","label":1,"title_clean":"Exposing Paid Opinion Manipulation Trolls","abstract_clean":"Recently, Web forums have been invaded by opinion manipulation trolls. Some trolls try to influence the other users driven by their own convictions, while in other cases they can be organized and paid, e.g., by a political party or a PR agency that gives them specific instructions what to write. Finding paid trolls automatically using machine learning is a hard task, as there is no enough training data to train a classifier; yet some test data is possible to obtain, as these trolls are sometimes caught and widely exposed. In this paper, we solve the training data problem by assuming that a user who is called a troll by several different people is likely to be such, and one who has never been called a troll is unlikely to be such. We compare the profiles of (i) paid trolls vs. (ii) \"mentioned\" trolls vs. (iii) non trolls, and we further show that a classifier trained to distinguish (ii) from (iii) does quite well also at telling apart (i) from (iii).","url":"https:\/\/aclanthology.org\/R15-1058"},{"ID":"mim-etal-2019-unsupervised","methods":["unsupervised learning of discourse aware text representation","document embedding approaches","parser"],"center_method":[null,null,"parser"],"tasks":["essay scoring","document embedding","document classification and regression tasks"],"center_task":[null,null,null],"Goal":["Quality Education"],"text":"Unsupervised Learning of Discourse-Aware Text Representation for Essay Scoring. Existing document embedding approaches mainly focus on capturing sequences of words in documents. However, some document classification and regression tasks such as essay scoring need to consider discourse structure of documents. Although some prior approaches consider this issue and utilize discourse structure of text for document classification, these approaches are dependent on computationally expensive parsers. In this paper, we propose an unsupervised approach to capture discourse structure in terms of coherence and cohesion for document embedding that does not require any expensive parser or annotation. Extrinsic evaluation results show that the document representation obtained from our approach improves the performance of essay Organization scoring and Argument Strength scoring.","label":1,"title_clean":"Unsupervised Learning of Discourse Aware Text Representation for Essay Scoring","abstract_clean":"Existing document embedding approaches mainly focus on capturing sequences of words in documents. However, some document classification and regression tasks such as essay scoring need to consider discourse structure of documents. Although some prior approaches consider this issue and utilize discourse structure of text for document classification, these approaches are dependent on computationally expensive parsers. In this paper, we propose an unsupervised approach to capture discourse structure in terms of coherence and cohesion for document embedding that does not require any expensive parser or annotation. Extrinsic evaluation results show that the document representation obtained from our approach improves the performance of essay Organization scoring and Argument Strength scoring.","url":"https:\/\/aclanthology.org\/P19-2053.pdf"},{"ID":"minematsu-etal-2002-english","methods":["hmm sets","phonemic system","call systems","statistical methods","speech technologies","speech processing techniques"],"center_method":[null,null,null,null,null,null],"tasks":["call system development","corpus based analysis","call computer assisted language learning systems","higher educational reform","speech applications"],"center_task":[null,null,null,null,null],"Goal":["Quality Education"],"text":"English Speech Database Read by Japanese Learners for CALL System Development. With the help of recent advances in speech processing techniques, we can see various kinds of practical speech applications in both laboratories and the real world. One of the major applications in Japan is CALL (Computer Assisted Language Learning) systems. It is well-known that most of the recent speech technologies are based upon statistical methods, which require a large amount of speech data. Although we can find many speech corpora available from distribution sites such as Linguistic Data Consortium, European Language Resources Association, and so on, the number of speech corpora built especially for CALL system development is very small. In this paper, we firstly introduce a Japanese national project of \"Advanced Utilization of Multimedia to Promote Higher Educational Reform,\" under which some research groups are currently developing CALL systems. One of the main objectives of the project is to construct an English speech database read by Japanese students for CALL system development. This paper describes specification of the database and strategies adopted to select speakers and record their sentence\/word utterances in addition to preliminary discussions and investigations done before the database development. Further, by using the new database and WSJ database, corpus-based analysis and comparison between Japanese English and American English is done in view of the entire phonemic system of English. Here, tree diagrams of the two kinds of English are drawn through their HMM sets. Results show many interesting characteristics of Japanese English.","label":1,"title_clean":"English Speech Database Read by Japanese Learners for CALL System Development","abstract_clean":"With the help of recent advances in speech processing techniques, we can see various kinds of practical speech applications in both laboratories and the real world. One of the major applications in Japan is CALL (Computer Assisted Language Learning) systems. It is well known that most of the recent speech technologies are based upon statistical methods, which require a large amount of speech data. Although we can find many speech corpora available from distribution sites such as Linguistic Data Consortium, European Language Resources Association, and so on, the number of speech corpora built especially for CALL system development is very small. In this paper, we firstly introduce a Japanese national project of \"Advanced Utilization of Multimedia to Promote Higher Educational Reform,\" under which some research groups are currently developing CALL systems. One of the main objectives of the project is to construct an English speech database read by Japanese students for CALL system development. This paper describes specification of the database and strategies adopted to select speakers and record their sentence\/word utterances in addition to preliminary discussions and investigations done before the database development. Further, by using the new database and WSJ database, corpus based analysis and comparison between Japanese English and American English is done in view of the entire phonemic system of English. Here, tree diagrams of the two kinds of English are drawn through their HMM sets. Results show many interesting characteristics of Japanese English.","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2002\/pdf\/155.pdf"},{"ID":"minnema-herbelot-2019-brain","methods":["distributional representations","encoder"],"center_method":[null,"encoder"],"tasks":["perilous journeys of fmri decoding","cognitive neuroscience","computational linguistics"],"center_task":[null,null,"computational linguistics"],"Goal":["Good Health and Well-Being","Industry, Innovation and Infrastrucure"],"text":"From Brain Space to Distributional Space: The Perilous Journeys of fMRI Decoding. Recent work in cognitive neuroscience has introduced models for predicting distributional word meaning representations from brain imaging data. Such models have great potential, but the quality of their predictions has not yet been thoroughly evaluated from a computational linguistics point of view. Due to the limited size of available brain imaging datasets, standard quality metrics (e.g. similarity judgments and analogies) cannot be used. Instead, we investigate the use of several alternative measures for evaluating the predicted distributional space against a corpus-derived distributional space. We show that a stateof-the-art decoder, while performing impressively on metrics that are commonly used in cognitive neuroscience, performs unexpectedly poorly on our metrics. To address this, we propose strategies for improving the model's performance. Despite returning promising results, our experiments also demonstrate that much work remains to be done before distributional representations can reliably be predicted from brain data.","label":1,"title_clean":"From Brain Space to Distributional Space: The Perilous Journeys of fMRI Decoding","abstract_clean":"Recent work in cognitive neuroscience has introduced models for predicting distributional word meaning representations from brain imaging data. Such models have great potential, but the quality of their predictions has not yet been thoroughly evaluated from a computational linguistics point of view. Due to the limited size of available brain imaging datasets, standard quality metrics (e.g. similarity judgments and analogies) cannot be used. Instead, we investigate the use of several alternative measures for evaluating the predicted distributional space against a corpus derived distributional space. We show that a stateof the art decoder, while performing impressively on metrics that are commonly used in cognitive neuroscience, performs unexpectedly poorly on our metrics. To address this, we propose strategies for improving the model's performance. Despite returning promising results, our experiments also demonstrate that much work remains to be done before distributional representations can reliably be predicted from brain data.","url":"https:\/\/aclanthology.org\/P19-2021"},{"ID":"mirzaei-etal-2016-automatic","methods":["psc","partial and synchronized caption"],"center_method":[null,null],"tasks":["second language l2 learners listening difficulties","automatic speech recognition","transcription","speech challenges"],"center_task":[null,"automatic speech recognition",null,null],"Goal":["Quality Education"],"text":"Automatic Speech Recognition Errors as a Predictor of L2 Listening Difficulties. This paper investigates the use of automatic speech recognition (ASR) errors as indicators of the second language (L2) learners' listening difficulties and in doing so strives to overcome the shortcomings of Partial and Synchronized Caption (PSC) system. PSC is a system that generates a partial caption including difficult words detected based on high speech rate, low frequency, and specificity. To improve the choice of words in this system, and explore a better method to detect speech challenges, ASR errors were investigated as a model of the L2 listener, hypothesizing that some of these errors are similar to those of language learners' when transcribing the videos. To investigate this hypothesis, ASR errors in transcription of several TED talks were analyzed and compared with PSC's selected words. Both the overlapping and mismatching cases were analyzed to investigate possible improvement for the PSC system. Those ASR errors that were not detected by PSC as cases of learners' difficulties were further analyzed and classified into four categories: homophones, minimal pairs, breached boundaries and negatives. These errors were embedded into the baseline PSC to make the enhanced version and were evaluated in an experiment with L2 learners. The results indicated that the enhanced version, which encompasses the ASR errors addresses most of the L2 learners' difficulties and better assists them in comprehending challenging video segments as compared with the baseline.","label":1,"title_clean":"Automatic Speech Recognition Errors as a Predictor of L2 Listening Difficulties","abstract_clean":"This paper investigates the use of automatic speech recognition (ASR) errors as indicators of the second language (L2) learners' listening difficulties and in doing so strives to overcome the shortcomings of Partial and Synchronized Caption (PSC) system. PSC is a system that generates a partial caption including difficult words detected based on high speech rate, low frequency, and specificity. To improve the choice of words in this system, and explore a better method to detect speech challenges, ASR errors were investigated as a model of the L2 listener, hypothesizing that some of these errors are similar to those of language learners' when transcribing the videos. To investigate this hypothesis, ASR errors in transcription of several TED talks were analyzed and compared with PSC's selected words. Both the overlapping and mismatching cases were analyzed to investigate possible improvement for the PSC system. Those ASR errors that were not detected by PSC as cases of learners' difficulties were further analyzed and classified into four categories: homophones, minimal pairs, breached boundaries and negatives. These errors were embedded into the baseline PSC to make the enhanced version and were evaluated in an experiment with L2 learners. The results indicated that the enhanced version, which encompasses the ASR errors addresses most of the L2 learners' difficulties and better assists them in comprehending challenging video segments as compared with the baseline.","url":"https:\/\/aclanthology.org\/W16-4122"},{"ID":"mitrovic-etal-2019-nlpup","methods":["deep neural language model","gru","nlpup","convolutional neural network"],"center_method":[null,"gru",null,"convolutional neural network"],"tasks":["hate speech","detecting online aggressiveness"],"center_task":["hate speech",null],"Goal":["Peace, Justice and Strong Institutions"],"text":"nlpUP at SemEval-2019 Task 6: A Deep Neural Language Model for Offensive Language Detection. This paper presents our submission for the SemEval shared task 6, sub-task A on the identification of offensive language. Our proposed model, C-BiGRU, combines a Convolutional Neural Network (CNN) with a bidirectional Recurrent Neural Network (RNN). We utilize word2vec to capture the semantic similarities between words. This composition allows us to extract long term dependencies in tweets and distinguish between offensive and non-offensive tweets. In addition, we evaluate our approach on a different dataset and show that our model is capable of detecting online aggressiveness in both English and German tweets. Our model achieved a macro F1-score of 79.40% on the SemEval dataset.","label":1,"title_clean":"nlpUP at SemEval 2019 Task 6: A Deep Neural Language Model for Offensive Language Detection","abstract_clean":"This paper presents our submission for the SemEval shared task 6, sub task A on the identification of offensive language. Our proposed model, C BiGRU, combines a Convolutional Neural Network (CNN) with a bidirectional Recurrent Neural Network (RNN). We utilize word2vec to capture the semantic similarities between words. This composition allows us to extract long term dependencies in tweets and distinguish between offensive and non offensive tweets. In addition, we evaluate our approach on a different dataset and show that our model is capable of detecting online aggressiveness in both English and German tweets. Our model achieved a macro F1 score of 79.40% on the SemEval dataset.","url":"https:\/\/aclanthology.org\/S19-2127"},{"ID":"mondal-etal-2021-classification","methods":["machine learning methods"],"center_method":["machine learning methods"],"tasks":["classification of covid19","preprocessing tweets","media mining for health applications","feature engineering"],"center_task":[null,null,null,"feature engineering"],"Goal":["Good Health and Well-Being"],"text":"Classification of COVID19 tweets using Machine Learning Approaches. The reported work is a description of our participation in the \"Classification of COVID19 tweets containing symptoms\" shared task, organized by the \"Social Media Mining for Health Applications (SMM4H)\" workshop. The literature describes two machine learning approaches that were used to build a threeclass classification system, that categorizes tweets related to COVID19, into three classes, viz., self-reports, non-personal reports, and literature\/news mentions. The steps for preprocessing tweets, feature extraction, and the development of the machine learning models, are described extensively in the documentation. Both the developed learning models, when evaluated by the organizers, garnered F1 scores of 0.93 and 0.92 respectively.","label":1,"title_clean":"Classification of COVID19 tweets using Machine Learning Approaches","abstract_clean":"The reported work is a description of our participation in the \"Classification of COVID19 tweets containing symptoms\" shared task, organized by the \"Social Media Mining for Health Applications (SMM4H)\" workshop. The literature describes two machine learning approaches that were used to build a threeclass classification system, that categorizes tweets related to COVID19, into three classes, viz., self reports, non personal reports, and literature\/news mentions. The steps for preprocessing tweets, feature extraction, and the development of the machine learning models, are described extensively in the documentation. Both the developed learning models, when evaluated by the organizers, garnered F1 scores of 0.93 and 0.92 respectively.","url":"https:\/\/aclanthology.org\/2021.smm4h-1.29"},{"ID":"moore-etal-1997-commandtalk","methods":["commandtalk","spoken language interface","graphical processing spawning agent","com mandtalk","start it","nuance speech recognition system","open agent architecture","push to talk agent","contextual interpretation modhle","modsaf battlefield simulator","gemini naturallanguage parsing and interpretation system"],"center_method":[null,null,null,null,null,null,null,null,null,null,null],"tasks":["battlefield simulations","immediate execution","mod saf system functions","exercise time control","control simulation system functions"],"center_task":[null,null,null,null,null],"Goal":["Peace, Justice and Strong Institutions","Partnership for the Goals"],"text":"CommandTalk: A Spoken-Language Interface for Battlefield Simulations. CommandTalk is a spoken-language interface to battlefield simulations that allows the use of ordinary spoken English to create forces and control measures, assign missions to forces, modify missions during execution, and control simulation system functions. CommandTalk combines a number of separate components integrated through the use of the Open Agent Architecture, including the Nuance speech recognition system, the Gemini naturallanguage parsing and interpretation system, a contextual-interpretation modhle, a \"push-to-talk\" agent, the ModSAF battlefield simulator, and \"Start-It\" (a graphical processing-spawning agent). Com-mandTalk is installed at a number of Government and contractor sites, including NRaD and the Marine Corps Air Ground Combat Center. It is currently being extended to provide exercise-time control of all simulated U.S. forces in DARPA's STOW 97 demonstration. Put Checkpoint 1 at 937 965. Create a point called Checkpoint 2 at 930 960. Objective Alpha is 92 96. Charlie 4 5, at my command, advance in a column to Checkpoint 1. Next, proceed to Checkpoint 2. Then assault Objective Alpha. Charlie 4 5, move out. With the simulation under way, the user can exercise direct control over the simulated forces by giving commands such as the following for immediate execution: Charlie 4 5, speed up. Change formation to echelon right. Get in a line. Withdraw to Checkpoint 2. Examples of voice commands for controlling Mod-SAF system functions include the following: Show contour lines. Center on M1 platoon.","label":1,"title_clean":"CommandTalk: A Spoken Language Interface for Battlefield Simulations","abstract_clean":"CommandTalk is a spoken language interface to battlefield simulations that allows the use of ordinary spoken English to create forces and control measures, assign missions to forces, modify missions during execution, and control simulation system functions. CommandTalk combines a number of separate components integrated through the use of the Open Agent Architecture, including the Nuance speech recognition system, the Gemini naturallanguage parsing and interpretation system, a contextual interpretation modhle, a \"push to talk\" agent, the ModSAF battlefield simulator, and \"Start It\" (a graphical processing spawning agent). Com mandTalk is installed at a number of Government and contractor sites, including NRaD and the Marine Corps Air Ground Combat Center. It is currently being extended to provide exercise time control of all simulated U.S. forces in DARPA's STOW 97 demonstration. Put Checkpoint 1 at 937 965. Create a point called Checkpoint 2 at 930 960. Objective Alpha is 92 96. Charlie 4 5, at my command, advance in a column to Checkpoint 1. Next, proceed to Checkpoint 2. Then assault Objective Alpha. Charlie 4 5, move out. With the simulation under way, the user can exercise direct control over the simulated forces by giving commands such as the following for immediate execution: Charlie 4 5, speed up. Change formation to echelon right. Get in a line. Withdraw to Checkpoint 2. Examples of voice commands for controlling Mod SAF system functions include the following: Show contour lines. Center on M1 platoon.","url":"https:\/\/aclanthology.org\/A97-1001.pdf"},{"ID":"moore-rayson-2017-lancaster","methods":["finance specific word embedding model","svr","lstm","support vector regression"],"center_method":[null,null,"lstm",null],"tasks":["predicting sentiment","semeval"],"center_task":[null,"semeval"],"Goal":["Decent Work and Economic Growth"],"text":"Lancaster A at SemEval-2017 Task 5: Evaluation metrics matter: predicting sentiment from financial news headlines. This paper describes our participation in Task 5 track 2 of SemEval 2017 to predict the sentiment of financial news headlines for a specific company on a continuous scale between-1 and 1. We tackled the problem using a number of approaches, utilising a Support Vector Regression (SVR) and a Bidirectional Long Short-Term Memory (BLSTM). We found an improvement of 4-6% using the LSTM model over the SVR and came fourth in the track. We report a number of different evaluations using a finance specific word embedding model and reflect on the effects of using different evaluation metrics.","label":1,"title_clean":"Lancaster A at SemEval 2017 Task 5: Evaluation metrics matter: predicting sentiment from financial news headlines","abstract_clean":"This paper describes our participation in Task 5 track 2 of SemEval 2017 to predict the sentiment of financial news headlines for a specific company on a continuous scale between 1 and 1. We tackled the problem using a number of approaches, utilising a Support Vector Regression (SVR) and a Bidirectional Long Short Term Memory (BLSTM). We found an improvement of 4 6% using the LSTM model over the SVR and came fourth in the track. We report a number of different evaluations using a finance specific word embedding model and reflect on the effects of using different evaluation metrics.","url":"https:\/\/aclanthology.org\/S17-2095"},{"ID":"moradi-etal-2014-graph","methods":["semantic based method","analysis methods","network analysis methods","graph based analysis","graph based analysis of medical queries","web portals"],"center_method":[null,null,null,null,null,null],"tasks":["swedish health care portal","search data","information search","internet search","health information seeking","search behaviors"],"center_task":[null,null,null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"A Graph-Based Analysis of Medical Queries of a Swedish Health Care Portal. Today web portals play an increasingly important role in health care allowing information seekers to learn about diseases and treatments, and to administrate their care. Therefore, it is important that the portals are able to support this process as well as possible. In this paper, we study the search logs of a public Swedish health portal to address the questions if health information seeking differs from other types of Internet search and if there is a potential for utilizing network analysis methods in combination with semantic annotation to gain insights into search behaviors. Using a semantic-based method and a graph-based analysis of word cooccurrences in queries, we show there is an overlap among the results indicating a potential role of these types of methods to gain insights and facilitate improved information search. In addition we show that samples, windows of a month, of search logs may be sufficient to obtain similar results as using larger windows. We also show that medical queries share the same structural properties found for other types of information searches, thereby indicating an ability to re-use existing analysis methods for this type of search data.","label":1,"title_clean":"A Graph Based Analysis of Medical Queries of a Swedish Health Care Portal","abstract_clean":"Today web portals play an increasingly important role in health care allowing information seekers to learn about diseases and treatments, and to administrate their care. Therefore, it is important that the portals are able to support this process as well as possible. In this paper, we study the search logs of a public Swedish health portal to address the questions if health information seeking differs from other types of Internet search and if there is a potential for utilizing network analysis methods in combination with semantic annotation to gain insights into search behaviors. Using a semantic based method and a graph based analysis of word cooccurrences in queries, we show there is an overlap among the results indicating a potential role of these types of methods to gain insights and facilitate improved information search. In addition we show that samples, windows of a month, of search logs may be sufficient to obtain similar results as using larger windows. We also show that medical queries share the same structural properties found for other types of information searches, thereby indicating an ability to re use existing analysis methods for this type of search data.","url":"https:\/\/aclanthology.org\/W14-1102"},{"ID":"moraes-etal-2014-adapting","methods":["sight (summarizing information graphics textually) system"],"center_method":[null],"tasks":["generated summaries of line graphs","lexical choice","sentence generation","micro planning phase","natural language generation"],"center_task":[null,null,null,null,"natural language generation"],"Goal":["Quality Education"],"text":"Adapting Graph Summaries to the Users' Reading Levels. Deciding on the complexity of a generated text in NLG systems is a contentious task. Some systems propose the generation of simple text for low-skilled readers; some choose what they anticipate to be a \"good measure\" of complexity by balancing sentence length and number of sentences (using scales such as the D-level sentence complexity) for the text; while others target high-skilled readers. In this work, we discuss an approach that aims to leverage the experience of the reader when reading generated text by matching the syntactic complexity of the generated text to the reading level of the surrounding text. We propose an approach for sentence aggregation and lexical choice that allows generated summaries of line graphs in multimodal articles available online to match the reading level of the text of the article in which the graphs appear. The technique is developed in the context of the SIGHT (Summarizing Information Graphics Textually) system. This paper tackles the micro planning phase of sentence generation discussing additionally the steps of lexical choice, and pronominalization.","label":1,"title_clean":"Adapting Graph Summaries to the Users' Reading Levels","abstract_clean":"Deciding on the complexity of a generated text in NLG systems is a contentious task. Some systems propose the generation of simple text for low skilled readers; some choose what they anticipate to be a \"good measure\" of complexity by balancing sentence length and number of sentences (using scales such as the D level sentence complexity) for the text; while others target high skilled readers. In this work, we discuss an approach that aims to leverage the experience of the reader when reading generated text by matching the syntactic complexity of the generated text to the reading level of the surrounding text. We propose an approach for sentence aggregation and lexical choice that allows generated summaries of line graphs in multimodal articles available online to match the reading level of the text of the article in which the graphs appear. The technique is developed in the context of the SIGHT (Summarizing Information Graphics Textually) system. This paper tackles the micro planning phase of sentence generation discussing additionally the steps of lexical choice, and pronominalization.","url":"https:\/\/aclanthology.org\/W14-4409"},{"ID":"morales-etal-2018-linguistically","methods":["linguistically motivated fusion techniques","fusion techniques"],"center_method":[null,null],"tasks":["multimodal depression detection","multimodal design"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"A Linguistically-Informed Fusion Approach for Multimodal Depression Detection. Automated depression detection is inherently a multimodal problem. Therefore, it is critical that researchers investigate fusion techniques for multimodal design. This paper presents the first ever comprehensive study of fusion techniques for depression detection. In addition, we present novel linguistically-motivated fusion techniques, which we find outperform existing approaches.","label":1,"title_clean":"A Linguistically Informed Fusion Approach for Multimodal Depression Detection","abstract_clean":"Automated depression detection is inherently a multimodal problem. Therefore, it is critical that researchers investigate fusion techniques for multimodal design. This paper presents the first ever comprehensive study of fusion techniques for depression detection. In addition, we present novel linguistically motivated fusion techniques, which we find outperform existing approaches.","url":"https:\/\/aclanthology.org\/W18-0602"},{"ID":"morgado-da-costa-etal-2016-syntactic","methods":["semantic based machine translation","guided language drills","robust computational grammars","deep syntactic parsers"],"center_method":[null,null,null,null],"tasks":["call system","machine translation","computer assisted language learning","error based coaching","language learners errors","syntactic well formedness diagnosis"],"center_task":[null,"machine translation",null,null,null,null],"Goal":["Quality Education"],"text":"Syntactic Well-Formedness Diagnosis and Error-Based Coaching in Computer Assisted Language Learning using Machine Translation. We present a novel approach to Computer Assisted Language Learning (CALL), using deep syntactic parsers and semantic based machine translation (MT) in diagnosing and providing explicit feedback on language learners' errors. We are currently developing a proof of concept system showing how semantic-based machine translation can, in conjunction with robust computational grammars, be used to interact with students, better understand their language errors, and help students correct their grammar through a series of useful feedback messages and guided language drills. Ultimately, we aim to prove the viability of a new integrated rule-based MT approach to disambiguate students intended meaning in a CALL system. This is a necessary step to provide accurate coaching on how to correct ungrammatical input, and it will allow us to overcome a current bottleneck in the field-an exponential burst of ambiguity caused by ambiguous lexical items (Flickinger, 2010). From the users interaction with the system, we will also produce a richly annotated Learner Corpus, annotated automatically with both syntactic and semantic information.","label":1,"title_clean":"Syntactic Well Formedness Diagnosis and Error Based Coaching in Computer Assisted Language Learning using Machine Translation","abstract_clean":"We present a novel approach to Computer Assisted Language Learning (CALL), using deep syntactic parsers and semantic based machine translation (MT) in diagnosing and providing explicit feedback on language learners' errors. We are currently developing a proof of concept system showing how semantic based machine translation can, in conjunction with robust computational grammars, be used to interact with students, better understand their language errors, and help students correct their grammar through a series of useful feedback messages and guided language drills. Ultimately, we aim to prove the viability of a new integrated rule based MT approach to disambiguate students intended meaning in a CALL system. This is a necessary step to provide accurate coaching on how to correct ungrammatical input, and it will allow us to overcome a current bottleneck in the field an exponential burst of ambiguity caused by ambiguous lexical items (Flickinger, 2010). From the users interaction with the system, we will also produce a richly annotated Learner Corpus, annotated automatically with both syntactic and semantic information.","url":"https:\/\/aclanthology.org\/W16-4914"},{"ID":"mosallanezhad-etal-2019-deep","methods":["rlta","optimal strategy","deep reinforcement learning based text anonymization","latent representation"],"center_method":[null,null,null,null],"tasks":["text representations"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Deep Reinforcement Learning-based Text Anonymization against Private-Attribute Inference. User-generated textual data is rich in content and has been used in many user behavioral modeling tasks. However, it could also leak user private-attribute information that they may not want to disclose such as age and location. User's privacy concerns mandate data publishers to protect privacy. One effective way is to anonymize the textual data. In this paper, we study the problem of textual data anonymization and propose a novel Reinforcement Learning-based Text Anonymizor, RLTA, which addresses the problem of private-attribute leakage while preserving the utility of textual data. Our approach first extracts a latent representation of the original text w.r.t. a given task, then leverages deep reinforcement learning to automatically learn an optimal strategy for manipulating text representations w.r.t. the received privacy and utility feedback. Experiments show the effectiveness of this approach in terms of preserving both privacy and utility.","label":1,"title_clean":"Deep Reinforcement Learning based Text Anonymization against Private Attribute Inference","abstract_clean":"User generated textual data is rich in content and has been used in many user behavioral modeling tasks. However, it could also leak user private attribute information that they may not want to disclose such as age and location. User's privacy concerns mandate data publishers to protect privacy. One effective way is to anonymize the textual data. In this paper, we study the problem of textual data anonymization and propose a novel Reinforcement Learning based Text Anonymizor, RLTA, which addresses the problem of private attribute leakage while preserving the utility of textual data. Our approach first extracts a latent representation of the original text w.r.t. a given task, then leverages deep reinforcement learning to automatically learn an optimal strategy for manipulating text representations w.r.t. the received privacy and utility feedback. Experiments show the effectiveness of this approach in terms of preserving both privacy and utility.","url":"https:\/\/aclanthology.org\/D19-1240.pdf"},{"ID":"moser-moore-1995-investigating","methods":["coding scheme"],"center_method":[null],"tasks":["cue selection and placement","exhaustive analysis of discourse","cue selection","automatic text generation","tutorial discourse","hypotheses concerning cues"],"center_task":[null,null,null,null,null,null],"Goal":["Quality Education"],"text":"Investigating Cue Selection and Placement in Tutorial Discourse. Our goal is to identify the features that predict cue selection and placement in order to devise strategies for automatic text generation. Much previous work in this area has relied on ad hoc methods. Our coding scheme for the exhaustive analysis of discourse allows a systematic evaluation and refinement of hypotheses concerning cues. We report two results based on this analysis: a comparison of the distribution of Sn~CE and BECAUSE in our corpus, and the impact of embeddedness on cue selection.","label":1,"title_clean":"Investigating Cue Selection and Placement in Tutorial Discourse","abstract_clean":"Our goal is to identify the features that predict cue selection and placement in order to devise strategies for automatic text generation. Much previous work in this area has relied on ad hoc methods. Our coding scheme for the exhaustive analysis of discourse allows a systematic evaluation and refinement of hypotheses concerning cues. We report two results based on this analysis: a comparison of the distribution of Sn~CE and BECAUSE in our corpus, and the impact of embeddedness on cue selection.","url":"https:\/\/aclanthology.org\/P95-1018"},{"ID":"mostafazadeh-davani-etal-2021-improving","methods":["equalizing model predictions"],"center_method":[null],"tasks":["fair hate speech detection"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Improving Counterfactual Generation for Fair Hate Speech Detection. Bias mitigation approaches reduce models' dependence on sensitive features of data, such as social group tokens (SGTs), resulting in equal predictions across the sensitive features. In hate speech detection, however, equalizing model predictions may ignore important differences among targeted social groups, as hate speech can contain stereotypical language specific to each SGT. Here, to take the specific language about each SGT into account, we rely on counterfactual fairness and equalize predictions among counterfactuals, generated by changing the SGTs. Our method evaluates the similarity in sentence likelihoods (via pretrained language models) among counterfactuals, to treat SGTs equally only within interchangeable contexts. By applying logit pairing to equalize outcomes on the restricted set of counterfactuals for each instance, we improve fairness metrics while preserving model performance on hate speech detection.","label":1,"title_clean":"Improving Counterfactual Generation for Fair Hate Speech Detection","abstract_clean":"Bias mitigation approaches reduce models' dependence on sensitive features of data, such as social group tokens (SGTs), resulting in equal predictions across the sensitive features. In hate speech detection, however, equalizing model predictions may ignore important differences among targeted social groups, as hate speech can contain stereotypical language specific to each SGT. Here, to take the specific language about each SGT into account, we rely on counterfactual fairness and equalize predictions among counterfactuals, generated by changing the SGTs. Our method evaluates the similarity in sentence likelihoods (via pretrained language models) among counterfactuals, to treat SGTs equally only within interchangeable contexts. By applying logit pairing to equalize outcomes on the restricted set of counterfactuals for each instance, we improve fairness metrics while preserving model performance on hate speech detection.","url":"https:\/\/aclanthology.org\/2021.woah-1.10"},{"ID":"mowery-etal-2012-medical","methods":["medical diagnosis","translation map"],"center_method":[null,null],"tasks":["annotation"],"center_task":["annotation"],"Goal":["Good Health and Well-Being"],"text":"Medical diagnosis lost in translation -- Analysis of uncertainty and negation expressions in English and Swedish clinical texts. In the English clinical and biomedical text domains, negation and certainty usage are two well-studied phenomena. However, few studies have made an in-depth characterization of uncertainties expressed in a clinical setting, and compared this between different annotation efforts. This preliminary, qualitative study attempts to 1) create a clinical uncertainty and negation taxonomy, 2) develop a translation map to convert annotation labels from an English schema into a Swedish schema, and 3) characterize and compare two data sets using this taxonomy. We define a clinical uncertainty and negation taxonomy and a translation map for converting annotation labels between two schemas and report observed similarities and differences between the two data sets.","label":1,"title_clean":"Medical diagnosis lost in translation  Analysis of uncertainty and negation expressions in English and Swedish clinical texts","abstract_clean":"In the English clinical and biomedical text domains, negation and certainty usage are two well studied phenomena. However, few studies have made an in depth characterization of uncertainties expressed in a clinical setting, and compared this between different annotation efforts. This preliminary, qualitative study attempts to 1) create a clinical uncertainty and negation taxonomy, 2) develop a translation map to convert annotation labels from an English schema into a Swedish schema, and 3) characterize and compare two data sets using this taxonomy. We define a clinical uncertainty and negation taxonomy and a translation map for converting annotation labels between two schemas and report observed similarities and differences between the two data sets.","url":"https:\/\/aclanthology.org\/W12-2407"},{"ID":"mundra-etal-2021-wassa","methods":["multi task learning","ensembling","machine learning methods","deep neural network","bert"],"center_method":["multi task learning",null,"machine learning methods","deep neural network","bert"],"tasks":["empathy prediction","sentiment analysis","sub task","transformer finetuning"],"center_task":[null,"sentiment analysis",null,null],"Goal":["Good Health and Well-Being"],"text":"WASSA@IITK at WASSA 2021: Multi-task Learning and Transformer Finetuning for Emotion Classification and Empathy Prediction. This paper describes our contribution to the WASSA 2021 shared task on Empathy Prediction and Emotion Classification. The broad goal of this task was to model an empathy score, a distress score and the overall level of emotion of an essay written in response to a newspaper article associated with harm to someone. We have used the ELECTRA model abundantly and also advanced deep learning approaches like multi-task learning. Additionally, we also leveraged standard machine learning techniques like ensembling. Our system achieves a Pearson Correlation Coefficient of 0.533 on sub-task I and a macro F1 score of 0.5528 on sub-task II. We ranked 1 st in Emotion Classification sub-task and 3 rd in Empathy Prediction sub-task.","label":1,"title_clean":"WASSA@IITK at WASSA 2021: Multi task Learning and Transformer Finetuning for Emotion Classification and Empathy Prediction","abstract_clean":"This paper describes our contribution to the WASSA 2021 shared task on Empathy Prediction and Emotion Classification. The broad goal of this task was to model an empathy score, a distress score and the overall level of emotion of an essay written in response to a newspaper article associated with harm to someone. We have used the ELECTRA model abundantly and also advanced deep learning approaches like multi task learning. Additionally, we also leveraged standard machine learning techniques like ensembling. Our system achieves a Pearson Correlation Coefficient of 0.533 on sub task I and a macro F1 score of 0.5528 on sub task II. We ranked 1 st in Emotion Classification sub task and 3 rd in Empathy Prediction sub task.","url":"https:\/\/aclanthology.org\/2021.wassa-1.12"},{"ID":"murakami-etal-2009-annotating","methods":["statement annotation scheme"],"center_method":[null],"tasks":["statement map project","online information credibility evaluation"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Annotating Semantic Relations Combining Facts and Opinions. As part of the STATEMENT MAP project, we are constructing a Japanese corpus annotated with the semantic relations bridging facts and opinions that are necessary for online information credibility evaluation. In this paper, we identify the semantic relations essential to this task and discuss how to efficiently collect valid examples from Web documents by splitting complex sentences into fundamental units of meaning called \"statements\" and annotating relations at the statement level. We present a statement annotation scheme and examine its reliability by annotating around 1,500 pairs of statements. We are preparing the corpus for release this winter.","label":1,"title_clean":"Annotating Semantic Relations Combining Facts and Opinions","abstract_clean":"As part of the STATEMENT MAP project, we are constructing a Japanese corpus annotated with the semantic relations bridging facts and opinions that are necessary for online information credibility evaluation. In this paper, we identify the semantic relations essential to this task and discuss how to efficiently collect valid examples from Web documents by splitting complex sentences into fundamental units of meaning called \"statements\" and annotating relations at the statement level. We present a statement annotation scheme and examine its reliability by annotating around 1,500 pairs of statements. We are preparing the corpus for release this winter.","url":"https:\/\/aclanthology.org\/W09-3027"},{"ID":"mutal-etal-2020-copeco","methods":["copeco"],"center_method":[null],"tasks":["pedagogical","collaborative post editing corpus"],"center_task":[null,null],"Goal":["Quality Education"],"text":"COPECO: a Collaborative Post-Editing Corpus in Pedagogical Context. ","label":1,"title_clean":"COPECO: a Collaborative Post Editing Corpus in Pedagogical Context","abstract_clean":"","url":"https:\/\/aclanthology.org\/2020.amta-pemdt.5"},{"ID":"muti-barron-cedeno-2022-checkpoint","methods":["multilingual bert models","transformers","single language bert models","error analysis","transfer learning approach"],"center_method":[null,"transformers",null,"error analysis",null],"tasks":["identifying misogyny in tweets","zero shot classification","downstream task","pre training","transformers","multilingual misogyny identification"],"center_task":[null,null,"downstream task",null,"transformers",null],"Goal":["Gender Equality","Reduced Inequalities"],"text":"A Checkpoint on Multilingual Misogyny Identification. We address the problem of identifying misogyny in tweets in mono and multilingual settings in three languages: English, Italian and Spanish. We explore model variations considering single and multiple languages both in the pre-training of the transformer and in the training of the downstream task to explore the feasibility of detecting misogyny through a transfer learning approach across multiple languages. That is, we train monolingual transformers with monolingual data and multilingual transformers with both monolingual and multilingual data. Our models reach state-of-the-art performance on all three languages. The single-language BERT models perform the best, closely followed by different configurations of multilingual BERT models. The performance drops in zero-shot classification across languages. Our error analysis shows that multilingual and monolingual models tend to make the same mistakes.","label":1,"title_clean":"A Checkpoint on Multilingual Misogyny Identification","abstract_clean":"We address the problem of identifying misogyny in tweets in mono and multilingual settings in three languages: English, Italian and Spanish. We explore model variations considering single and multiple languages both in the pre training of the transformer and in the training of the downstream task to explore the feasibility of detecting misogyny through a transfer learning approach across multiple languages. That is, we train monolingual transformers with monolingual data and multilingual transformers with both monolingual and multilingual data. Our models reach state of the art performance on all three languages. The single language BERT models perform the best, closely followed by different configurations of multilingual BERT models. The performance drops in zero shot classification across languages. Our error analysis shows that multilingual and monolingual models tend to make the same mistakes.","url":"https:\/\/aclanthology.org\/2022.acl-srw.37.pdf"},{"ID":"nabizadeh-etal-2020-myfixit","methods":["myfixit","semi automatic web based annotator application","bags of n grams similarity","unsupervised algorithm","deep learning based sequence labeling model"],"center_method":[null,null,null,"unsupervised algorithm",null],"tasks":["automated repair assistant","information extraction","ie task"],"center_task":[null,"information extraction",null],"Goal":["Industry, Innovation and Infrastrucure","Decent Work and Economic Growth"],"text":"MyFixit: An Annotated Dataset, Annotation Tool, and Baseline Methods for Information Extraction from Repair Manuals. Text instructions are among the most widely used media for learning and teaching. Hence, to create assistance systems that are capable of supporting humans autonomously in new tasks, it would be immensely productive, if machines were enabled to extract task knowledge from such text instructions. In this paper, we, therefore, focus on information extraction (IE) from the instructional text in repair manuals. This brings with it the multiple challenges of information extraction from the situated and technical language in relatively long and often complex instructions. To tackle these challenges, we introduce a semi-structured dataset of repair manuals. The dataset is annotated in a large category of devices, with information that we consider most valuable for an automated repair assistant, including the required tools and the disassembled parts at each step of the repair progress. We then propose methods that can serve as baselines for this IE task: an unsupervised method based on a bags-of-n-grams similarity for extracting the needed tools in each repair step, and a deep-learning-based sequence labeling model for extracting the identity of disassembled parts. These baseline methods are integrated into a semi-automatic web-based annotator application that is also available along with the dataset.","label":1,"title_clean":"MyFixit: An Annotated Dataset, Annotation Tool, and Baseline Methods for Information Extraction from Repair Manuals","abstract_clean":"Text instructions are among the most widely used media for learning and teaching. Hence, to create assistance systems that are capable of supporting humans autonomously in new tasks, it would be immensely productive, if machines were enabled to extract task knowledge from such text instructions. In this paper, we, therefore, focus on information extraction (IE) from the instructional text in repair manuals. This brings with it the multiple challenges of information extraction from the situated and technical language in relatively long and often complex instructions. To tackle these challenges, we introduce a semi structured dataset of repair manuals. The dataset is annotated in a large category of devices, with information that we consider most valuable for an automated repair assistant, including the required tools and the disassembled parts at each step of the repair progress. We then propose methods that can serve as baselines for this IE task: an unsupervised method based on a bags of n grams similarity for extracting the needed tools in each repair step, and a deep learning based sequence labeling model for extracting the identity of disassembled parts. These baseline methods are integrated into a semi automatic web based annotator application that is also available along with the dataset.","url":"https:\/\/aclanthology.org\/2020.lrec-1.260"},{"ID":"nagata-2019-toward","methods":["neural retrievalbased method"],"center_method":[null],"tasks":["writing learning","feedback comment generation","automatically generating feedback comments"],"center_task":[null,null,null],"Goal":["Quality Education"],"text":"Toward a Task of Feedback Comment Generation for Writing Learning. In this paper, we introduce a novel task called feedback comment generation-a task of automatically generating feedback comments such as a hint or an explanatory note for writing learning for non-native learners of English. There has been almost no work on this task nor corpus annotated with feedback comments. We have taken the first step by creating learner corpora consisting of approximately 1,900 essays where all preposition errors are manually annotated with feedback comments. We have tested three baseline methods on the dataset, showing that a simple neural retrievalbased method sets a baseline performance with an F-measure of 0.34 to 0.41. Finally, we have looked into the results to explore what modifications we need to make to achieve better performance. We also have explored problems unaddressed in this work.","label":1,"title_clean":"Toward a Task of Feedback Comment Generation for Writing Learning","abstract_clean":"In this paper, we introduce a novel task called feedback comment generation a task of automatically generating feedback comments such as a hint or an explanatory note for writing learning for non native learners of English. There has been almost no work on this task nor corpus annotated with feedback comments. We have taken the first step by creating learner corpora consisting of approximately 1,900 essays where all preposition errors are manually annotated with feedback comments. We have tested three baseline methods on the dataset, showing that a simple neural retrievalbased method sets a baseline performance with an F measure of 0.34 to 0.41. Finally, we have looked into the results to explore what modifications we need to make to achieve better performance. We also have explored problems unaddressed in this work.","url":"https:\/\/aclanthology.org\/D19-1316"},{"ID":"nakashole-mitchell-2014-language","methods":["factchecker","iterative peer voting","language aware approach"],"center_method":[null,null,null],"tasks":["language aware truth assessment of fact candidates"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Language-Aware Truth Assessment of Fact Candidates. This paper introduces FactChecker, language-aware approach to truth-finding. FactChecker differs from prior approaches in that it does not rely on iterative peer voting, instead it leverages language to infer believability of fact candidates. In particular, FactChecker makes use of linguistic features to detect if a given source objectively states facts or is speculative and opinionated. To ensure that fact candidates mentioned in similar sources have similar believability, FactChecker augments objectivity with a co-mention score to compute the overall believability score of a fact candidate. Our experiments on various datasets show that FactChecker yields higher accuracy than existing approaches.","label":1,"title_clean":"Language Aware Truth Assessment of Fact Candidates","abstract_clean":"This paper introduces FactChecker, language aware approach to truth finding. FactChecker differs from prior approaches in that it does not rely on iterative peer voting, instead it leverages language to infer believability of fact candidates. In particular, FactChecker makes use of linguistic features to detect if a given source objectively states facts or is speculative and opinionated. To ensure that fact candidates mentioned in similar sources have similar believability, FactChecker augments objectivity with a co mention score to compute the overall believability score of a fact candidate. Our experiments on various datasets show that FactChecker yields higher accuracy than existing approaches.","url":"https:\/\/aclanthology.org\/P14-1095"},{"ID":"nakazawa-2015-promoting","methods":["machine"],"center_method":[null],"tasks":["promoting science and technology exchange"],"center_task":[null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Promoting science and technology exchange using machine translation. ","label":1,"title_clean":"Promoting science and technology exchange using machine translation","abstract_clean":"","url":"https:\/\/aclanthology.org\/2015.mtsummit-wpslt.5"},{"ID":"naz-etal-2021-fjwu","methods":["machine translation","information retrieval","transformers","fjwus system"],"center_method":["machine translation","information retrieval","transformers",null],"tasks":["machine translation","biomedical shared task"],"center_task":["machine translation",null],"Goal":["Good Health and Well-Being"],"text":"FJWU Participation for the WMT21 Biomedical Translation Task. In this paper we present the FJWU's system submitted to the biomedical shared task at WMT21. We prepared state-of-the-art multilingual neural machine translation systems for three languages (i.e. German, Spanish and French) with English as target language. Our NMT systems based on Transformer architecture, were trained on combination of indomain and out-domain parallel corpora developed using Information Retrieval (IR) and domain adaptation techniques.","label":1,"title_clean":"FJWU Participation for the WMT21 Biomedical Translation Task","abstract_clean":"In this paper we present the FJWU's system submitted to the biomedical shared task at WMT21. We prepared state of the art multilingual neural machine translation systems for three languages (i.e. German, Spanish and French) with English as target language. Our NMT systems based on Transformer architecture, were trained on combination of indomain and out domain parallel corpora developed using Information Retrieval (IR) and domain adaptation techniques.","url":"https:\/\/aclanthology.org\/2021.wmt-1.86"},{"ID":"newman-griffis-etal-2019-classifying","methods":["convolutional neural network"],"center_method":["convolutional neural network"],"tasks":["classifying performance assertions","clinical mobility descriptions","activity","textual entailment tasks","modeling health states of individuals and populations","reported ability"],"center_task":[null,null,null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"Classifying the reported ability in clinical mobility descriptions. Assessing how individuals perform different activities is key information for modeling health states of individuals and populations. Descriptions of activity performance in clinical free text are complex, including syntactic negation and similarities to textual entailment tasks. We explore a variety of methods for the novel task of classifying four types of assertions about activity performance: Able, Unable, Unclear, and None (no information). We find that ensembling an SVM trained with lexical features and a CNN achieves 77.9% macro F1 score on our task, and yields nearly 80% recall on the rare Unclear and Unable samples. Finally, we highlight several challenges in classifying performance assertions, including capturing information about sources of assistance, incorporating syntactic structure and negation scope, and handling new modalities at test time. Our findings establish a strong baseline for this novel task, and identify intriguing areas for further research.","label":1,"title_clean":"Classifying the reported ability in clinical mobility descriptions","abstract_clean":"Assessing how individuals perform different activities is key information for modeling health states of individuals and populations. Descriptions of activity performance in clinical free text are complex, including syntactic negation and similarities to textual entailment tasks. We explore a variety of methods for the novel task of classifying four types of assertions about activity performance: Able, Unable, Unclear, and None (no information). We find that ensembling an SVM trained with lexical features and a CNN achieves 77.9% macro F1 score on our task, and yields nearly 80% recall on the rare Unclear and Unable samples. Finally, we highlight several challenges in classifying performance assertions, including capturing information about sources of assistance, incorporating syntactic structure and negation scope, and handling new modalities at test time. Our findings establish a strong baseline for this novel task, and identify intriguing areas for further research.","url":"https:\/\/aclanthology.org\/W19-5001"},{"ID":"nguyen-2019-question","methods":["question answering","open domain techniques"],"center_method":["question answering",null],"tasks":["question answering"],"center_task":["question answering"],"Goal":["Good Health and Well-Being"],"text":"Question Answering in the Biomedical Domain. Question answering techniques have mainly been investigated in open domains. However, there are particular challenges in extending these open-domain techniques to extend into the biomedical domain. Question answering focusing on patients is less studied. We find that there are some challenges in patient question answering such as limited annotated data, lexical gap and quality of answer spans. We aim to address some of these gaps by extending and developing upon the literature to design a question answering system that can decide on the most appropriate answers for patients attempting to self-diagnose while including the ability to abstain from answering when confidence is low.","label":1,"title_clean":"Question Answering in the Biomedical Domain","abstract_clean":"Question answering techniques have mainly been investigated in open domains. However, there are particular challenges in extending these open domain techniques to extend into the biomedical domain. Question answering focusing on patients is less studied. We find that there are some challenges in patient question answering such as limited annotated data, lexical gap and quality of answer spans. We aim to address some of these gaps by extending and developing upon the literature to design a question answering system that can decide on the most appropriate answers for patients attempting to self diagnose while including the ability to abstain from answering when confidence is low.","url":"https:\/\/aclanthology.org\/P19-2008.pdf"},{"ID":"nikolova-ma-2008-assistive","methods":["mobile technologies"],"center_method":[null],"tasks":["communication","assistive systems","speech and language disabilities"],"center_task":["communication",null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Assistive Mobile Communication Support. This paper reflects on our work in providing communication support for people with speech and language disabilities. We discuss the role of mobile technologies in assistive systems and share ongoing research efforts.","label":1,"title_clean":"Assistive Mobile Communication Support","abstract_clean":"This paper reflects on our work in providing communication support for people with speech and language disabilities. We discuss the role of mobile technologies in assistive systems and share ongoing research efforts.","url":"https:\/\/aclanthology.org\/W08-0806"},{"ID":"nina-alcocer-2019-haterecognizer","methods":["data augmentation","neural network","features and neural networks"],"center_method":["data augmentation","neural network",null],"tasks":["hate recognition"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"text":"HATERecognizer at SemEval-2019 Task 5: Using Features and Neural Networks to Face Hate Recognition. This paper presents a detailed description of our participation in task 5 on SemEval-2019 1. This task consists of classifying English and Spanish tweets that contain hate towards women or immigrants. We carried out several experiments; for a finer-grained study of the task, we analyzed different features and designing architectures of neural networks. Additionally, to face the lack of hate content in tweets, we include data augmentation as a technique to increase hate content in our datasets.","label":1,"title_clean":"HATERecognizer at SemEval 2019 Task 5: Using Features and Neural Networks to Face Hate Recognition","abstract_clean":"This paper presents a detailed description of our participation in task 5 on SemEval 2019 1. This task consists of classifying English and Spanish tweets that contain hate towards women or immigrants. We carried out several experiments; for a finer grained study of the task, we analyzed different features and designing architectures of neural networks. Additionally, to face the lack of hate content in tweets, we include data augmentation as a technique to increase hate content in our datasets.","url":"https:\/\/aclanthology.org\/S19-2072"},{"ID":"noble-etal-2021-semantic","methods":["distributional methods"],"center_method":[null],"tasks":["semantic shift in social networks"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Semantic shift in social networks. Just as the meaning of words is tied to the communities in which they are used, so too is semantic change. But how does lexical semantic change manifest differently across different communities? In this work, we investigate the relationship between community structure and semantic change in 45 communities from the social media website Reddit. We use distributional methods to quantify lexical semantic change and induce a social network on communities, based on interactions between members. We explore the relationship between semantic change and the clustering coefficient of a community's social network graph, as well as community size and stability. While none of these factors are found to be significant on their own, we report a significant effect of their three-way interaction. We also report on significant wordlevel effects of frequency and change in frequency, which replicate previous findings.","label":1,"title_clean":"Semantic shift in social networks","abstract_clean":"Just as the meaning of words is tied to the communities in which they are used, so too is semantic change. But how does lexical semantic change manifest differently across different communities? In this work, we investigate the relationship between community structure and semantic change in 45 communities from the social media website Reddit. We use distributional methods to quantify lexical semantic change and induce a social network on communities, based on interactions between members. We explore the relationship between semantic change and the clustering coefficient of a community's social network graph, as well as community size and stability. While none of these factors are found to be significant on their own, we report a significant effect of their three way interaction. We also report on significant wordlevel effects of frequency and change in frequency, which replicate previous findings.","url":"https:\/\/aclanthology.org\/2021.starsem-1.3"},{"ID":"nomoto-2016-neal","methods":["composite model"],"center_method":[null],"tasks":["qa domain"],"center_task":[null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"NEAL: A Neurally Enhanced Approach to Linking Citation and Reference. As a way to tackle Task 1A in CL-SciSumm 2016, we introduce a composite model consisting of TFIDF and Neural Network (NN), the latter being a adaptation of the embedding model originally proposed for the Q\/A domain [2, 7]. We discuss an experiment using a development data, results thereof, and some remaining issues.","label":1,"title_clean":"NEAL: A Neurally Enhanced Approach to Linking Citation and Reference","abstract_clean":"As a way to tackle Task 1A in CL SciSumm 2016, we introduce a composite model consisting of TFIDF and Neural Network (NN), the latter being a adaptation of the embedding model originally proposed for the Q\/A domain [2, 7]. We discuss an experiment using a development data, results thereof, and some remaining issues.","url":"https:\/\/aclanthology.org\/W16-1519"},{"ID":"nothdurft-etal-2014-probabilistic","methods":["probabilistic trust handling architecture"],"center_method":[null],"tasks":["interactions","task oriented dialogs","technical systems","probabilistic human computer trust"],"center_task":["interactions",null,null,null],"Goal":["Decent Work and Economic Growth"],"text":"Probabilistic Human-Computer Trust Handling. Human-computer trust has shown to be a critical factor in influencing the complexity and frequency of interaction in technical systems. Particularly incomprehensible situations in human-computer interaction may lead to a reduced users trust in the system and by that influence the style of interaction. Analogous to human-human interaction, explaining these situations can help to remedy negative effects. In this paper we present our approach of augmenting task-oriented dialogs with selected explanation dialogs to foster the humancomputer trust relationship in those kinds of situations. We have conducted a webbased study testing the effects of different goals of explanations on the components of human-computer trust. Subsequently, we show how these results can be used in our probabilistic trust handling architecture to augment pre-defined task-oriented dialogs.","label":1,"title_clean":"Probabilistic Human Computer Trust Handling","abstract_clean":"Human computer trust has shown to be a critical factor in influencing the complexity and frequency of interaction in technical systems. Particularly incomprehensible situations in human computer interaction may lead to a reduced users trust in the system and by that influence the style of interaction. Analogous to human human interaction, explaining these situations can help to remedy negative effects. In this paper we present our approach of augmenting task oriented dialogs with selected explanation dialogs to foster the humancomputer trust relationship in those kinds of situations. We have conducted a webbased study testing the effects of different goals of explanations on the components of human computer trust. Subsequently, we show how these results can be used in our probabilistic trust handling architecture to augment pre defined task oriented dialogs.","url":"https:\/\/aclanthology.org\/W14-4307.pdf"},{"ID":"oard-2007-invited","methods":["malach project","automatic speech recognition techniques","machine learning methods"],"center_method":[null,null,"machine learning methods"],"tasks":["topic classification tasks","boundary detection","automated clustering","interactive search"],"center_task":[null,null,null,null],"Goal":["Quality Education"],"text":"Invited Talk: Lessons from the MALACH Project: Applying New Technologies to Improve Intellectual Access to Large Oral History Collections. In this talk I will describe the goals of the MALACH project (Multilingual Access to Large Spoken Archives) and our research results. I'll begin by describing the unique characteristics of the oral history collection that we used, in which Holocaust survivors, witnesses and rescuers were interviewed in several languages. Each interview has been digitized and extensively catalogued by subject matter experts, thus producing a remarkably rich collection for the application of machine learning techniques. Automatic speech recognition techniques originally developed for the domain of conversational telephone speech were adapted to process these materials with word error rates that are adequate to provide useful features to support interactive search and automated clustering, boundary detection, and topic classification tasks. As I describe our results, I will focus particularly on the evaluation methods that that we have used to assess the potential utility of this technology. I'll conclude with some remarks about possible future directions for research on applying new technologies to improve intellectual access to oral history and other spoken word collections.","label":1,"title_clean":"Invited Talk: Lessons from the MALACH Project: Applying New Technologies to Improve Intellectual Access to Large Oral History Collections","abstract_clean":"In this talk I will describe the goals of the MALACH project (Multilingual Access to Large Spoken Archives) and our research results. I'll begin by describing the unique characteristics of the oral history collection that we used, in which Holocaust survivors, witnesses and rescuers were interviewed in several languages. Each interview has been digitized and extensively catalogued by subject matter experts, thus producing a remarkably rich collection for the application of machine learning techniques. Automatic speech recognition techniques originally developed for the domain of conversational telephone speech were adapted to process these materials with word error rates that are adequate to provide useful features to support interactive search and automated clustering, boundary detection, and topic classification tasks. As I describe our results, I will focus particularly on the evaluation methods that that we have used to assess the potential utility of this technology. I'll conclude with some remarks about possible future directions for research on applying new technologies to improve intellectual access to oral history and other spoken word collections.","url":"https:\/\/aclanthology.org\/W07-0912.pdf"},{"ID":"oboronko-2000-wired","methods":["systran","internet based tools"],"center_method":[null,null],"tasks":["peace","synchronous and asynchronous translation of texts","machine translation","multi language communication"],"center_task":[null,null,"machine translation",null],"Goal":["Peace, Justice and Strong Institutions","Partnership for the Goals"],"text":"Wired for peace and multi-language communication. Our project Wired for Peace: Virtual Diplomacy in Northeast Asia (Http:\/\/wwwneacd.ucsd.edu\/) has as its main aim to provide policymakers and researchers of the U.S., China, Russia, Japan, and Korea with Internet based tools to allow for continuous communication on issues of the regional security and cooperation. Since the very beginning of the project, we have understood that Web-based translation between English and Asian languages would be one of the most necessary tools for successful development of the project. With this understanding, we have partnered with Systran (www.systransoft.com), one of the leaders in MT field, in order to develop Internet-based tools for both synchronous and asynchronous translation of texts and discussions. This submission is a report on a work in progress.","label":1,"title_clean":"Wired for peace and multi language communication","abstract_clean":"Our project Wired for Peace: Virtual Diplomacy in Northeast Asia (Http:\/\/wwwneacd.ucsd.edu\/) has as its main aim to provide policymakers and researchers of the U.S., China, Russia, Japan, and Korea with Internet based tools to allow for continuous communication on issues of the regional security and cooperation. Since the very beginning of the project, we have understood that Web based translation between English and Asian languages would be one of the most necessary tools for successful development of the project. With this understanding, we have partnered with Systran (www.systransoft.com), one of the leaders in MT field, in order to develop Internet based tools for both synchronous and asynchronous translation of texts and discussions. This submission is a report on a work in progress.","url":"https:\/\/aclanthology.org\/2000.amta-workshop.5.pdf"},{"ID":"oinam-etal-2018-treebank","methods":["berkeley parser","treebank"],"center_method":[null,"treebank"],"tasks":["healthcare","annotation"],"center_task":["healthcare","annotation"],"Goal":["Good Health and Well-Being"],"text":"A Treebank for the Healthcare Domain. This paper presents a treebank for the healthcare domain developed at ezDI. The treebank is created from a wide array of clinical health record documents across hospitals. The data has been de-identified and annotated for constituent syntactic structure. The treebank contains a total of 52053 sentences that have been sampled for subdomains as well as linguistic variations. The paper outlines the sampling process followed to ensure a better domain representation in the corpus, the annotation process and challenges, and corpus statistics. The Penn Treebank tagset and guidelines were largely followed, but there were many syntactic contexts that warranted adaptation of the guidelines. The treebank created was used to retrain the Berkeley parser and the Stanford parser. These parsers were also trained with the GENIA treebank for comparative quality assessment. Our treebank yielded greater accuracy on both parsers. Berkeley parser performed better on our treebank with an average F1 measure of 91 across 5-folds. This was a significant jump from the out-of-the-box F1 score of 70 on Berkeley parser's default grammar.","label":1,"title_clean":"A Treebank for the Healthcare Domain","abstract_clean":"This paper presents a treebank for the healthcare domain developed at ezDI. The treebank is created from a wide array of clinical health record documents across hospitals. The data has been de identified and annotated for constituent syntactic structure. The treebank contains a total of 52053 sentences that have been sampled for subdomains as well as linguistic variations. The paper outlines the sampling process followed to ensure a better domain representation in the corpus, the annotation process and challenges, and corpus statistics. The Penn Treebank tagset and guidelines were largely followed, but there were many syntactic contexts that warranted adaptation of the guidelines. The treebank created was used to retrain the Berkeley parser and the Stanford parser. These parsers were also trained with the GENIA treebank for comparative quality assessment. Our treebank yielded greater accuracy on both parsers. Berkeley parser performed better on our treebank with an average F1 measure of 91 across 5 folds. This was a significant jump from the out of the box F1 score of 70 on Berkeley parser's default grammar.","url":"https:\/\/aclanthology.org\/W18-4916"},{"ID":"ortega-etal-2019-adviser","methods":["adviser","dialog system framework","rules based and neural network based implementations","user friendly framework"],"center_method":[null,null,null,null],"tasks":["education research","interdisciplinary collaboration","multi domain task oriented conversations"],"center_task":[null,null,null],"Goal":["Quality Education"],"text":"ADVISER: A Dialog System Framework for Education \\& Research. In this paper, we present ADVISER 1-an open source dialog system framework for education and research purposes. This system supports multi-domain task-oriented conversations in two languages. It additionally provides a flexible architecture in which modules can be arbitrarily combined or exchanged-allowing for easy switching between rules-based and neural network based implementations. Furthermore, ADVISER offers a transparent, user-friendly framework designed for interdisciplinary collaboration: from a flexible back end, allowing easy integration of new features, to an intuitive graphical user interface supporting nontechnical users.","label":1,"title_clean":"ADVISER: A Dialog System Framework for Education \\& Research","abstract_clean":"In this paper, we present ADVISER 1 an open source dialog system framework for education and research purposes. This system supports multi domain task oriented conversations in two languages. It additionally provides a flexible architecture in which modules can be arbitrarily combined or exchanged allowing for easy switching between rules based and neural network based implementations. Furthermore, ADVISER offers a transparent, user friendly framework designed for interdisciplinary collaboration: from a flexible back end, allowing easy integration of new features, to an intuitive graphical user interface supporting nontechnical users.","url":"https:\/\/aclanthology.org\/P19-3016"},{"ID":"osborne-etal-2014-real","methods":["redites"],"center_method":[null],"tasks":["real time detection tracking and monitoring of automatically discovered events","westgate shooting incident","information analysts"],"center_task":[null,null,null],"Goal":["Sustainable Cities and Communities"],"text":"Real-Time Detection, Tracking, and Monitoring of Automatically Discovered Events in Social Media. We introduce ReDites, a system for realtime event detection, tracking, monitoring and visualisation. It is designed to assist Information Analysts in understanding and exploring complex events as they unfold in the world. Events are automatically detected from the Twitter stream. Then those that are categorised as being security-relevant are tracked, geolocated, summarised and visualised for the end-user. Furthermore, the system tracks changes in emotions over events, signalling possible flashpoints or abatement. We demonstrate the capabilities of ReDites using an extended use case from the September 2013 Westgate shooting incident. Through an evaluation of system latencies, we also show that enriched events are made available for users to explore within seconds of that event occurring.","label":1,"title_clean":"Real Time Detection, Tracking, and Monitoring of Automatically Discovered Events in Social Media","abstract_clean":"We introduce ReDites, a system for realtime event detection, tracking, monitoring and visualisation. It is designed to assist Information Analysts in understanding and exploring complex events as they unfold in the world. Events are automatically detected from the Twitter stream. Then those that are categorised as being security relevant are tracked, geolocated, summarised and visualised for the end user. Furthermore, the system tracks changes in emotions over events, signalling possible flashpoints or abatement. We demonstrate the capabilities of ReDites using an extended use case from the September 2013 Westgate shooting incident. Through an evaluation of system latencies, we also show that enriched events are made available for users to explore within seconds of that event occurring.","url":"https:\/\/aclanthology.org\/P14-5007"},{"ID":"ostendorff-etal-2020-aspect","methods":["aspect based document similarity","bert variations","bert","lstm baseline","xlnet","roberta","transformers","aspect based document similarity approach","recommender systems","document similarity measures"],"center_method":[null,null,"bert",null,"xlnet","roberta","transformers",null,null,null],"tasks":["recommendations","pairwise document classification task"],"center_task":[null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Aspect-based Document Similarity for Research Papers. Traditional document similarity measures provide a coarse-grained distinction between similar and dissimilar documents. Typically, they do not consider in what aspects two documents are similar. This limits the granularity of applications like recommender systems that rely on document similarity. In this paper, we extend similarity with aspect information by performing a pairwise document classification task. We evaluate our aspect-based document similarity approach for research papers. Paper citations indicate the aspect-based similarity, i. e., the title of a section in which a citation occurs acts as a label for the pair of citing and cited paper. We apply a series of Transformer models such as RoBERTa, ELECTRA, XLNet, and BERT variations and compare them to an LSTM baseline. We perform our experiments on two newly constructed datasets of 172,073 research paper pairs from the ACL Anthology and CORD-19 corpus. According to our results, SciBERT is the best performing system with F1-scores of up to 0.83. A qualitative analysis validates our quantitative results and indicates that aspect-based document similarity indeed leads to more fine-grained recommendations.","label":1,"title_clean":"Aspect based Document Similarity for Research Papers","abstract_clean":"Traditional document similarity measures provide a coarse grained distinction between similar and dissimilar documents. Typically, they do not consider in what aspects two documents are similar. This limits the granularity of applications like recommender systems that rely on document similarity. In this paper, we extend similarity with aspect information by performing a pairwise document classification task. We evaluate our aspect based document similarity approach for research papers. Paper citations indicate the aspect based similarity, i. e., the title of a section in which a citation occurs acts as a label for the pair of citing and cited paper. We apply a series of Transformer models such as RoBERTa, ELECTRA, XLNet, and BERT variations and compare them to an LSTM baseline. We perform our experiments on two newly constructed datasets of 172,073 research paper pairs from the ACL Anthology and CORD 19 corpus. According to our results, SciBERT is the best performing system with F1 scores of up to 0.83. A qualitative analysis validates our quantitative results and indicates that aspect based document similarity indeed leads to more fine grained recommendations.","url":"https:\/\/aclanthology.org\/2020.coling-main.545"},{"ID":"ousidhoum-etal-2021-probing","methods":["language models","ptlms","logistic regression","nlp applications"],"center_method":["language models",null,"logistic regression","nlp applications"],"tasks":["probing toxic content"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Probing Toxic Content in Large Pre-Trained Language Models. Large pre-trained language models (PTLMs) have been shown to carry biases towards different social groups which leads to the reproduction of stereotypical and toxic content by major NLP systems. We propose a method based on logistic regression classifiers to probe English, French, and Arabic PTLMs and quantify the potentially harmful content that they convey with respect to a set of templates. The templates are prompted by a name of a social group followed by a cause-effect relation. We use PTLMs to predict masked tokens at the end of a sentence in order to examine how likely they enable toxicity towards specific communities. We shed the light on how such negative content can be triggered within unrelated and benign contexts based on evidence from a large-scale study, then we explain how to take advantage of our methodology to assess and mitigate the toxicity transmitted by PTLMs.","label":1,"title_clean":"Probing Toxic Content in Large Pre Trained Language Models","abstract_clean":"Large pre trained language models (PTLMs) have been shown to carry biases towards different social groups which leads to the reproduction of stereotypical and toxic content by major NLP systems. We propose a method based on logistic regression classifiers to probe English, French, and Arabic PTLMs and quantify the potentially harmful content that they convey with respect to a set of templates. The templates are prompted by a name of a social group followed by a cause effect relation. We use PTLMs to predict masked tokens at the end of a sentence in order to examine how likely they enable toxicity towards specific communities. We shed the light on how such negative content can be triggered within unrelated and benign contexts based on evidence from a large scale study, then we explain how to take advantage of our methodology to assess and mitigate the toxicity transmitted by PTLMs.","url":"https:\/\/aclanthology.org\/2021.acl-long.329"},{"ID":"p-r-etal-2016-hitachi","methods":["stanford named entity recognizer","hybrid of rule based and machine learning based methods","hybrid approach"],"center_method":[null,null,"hybrid approach"],"tasks":["temporal information extraction","time expression attribute identification","identification of time spans","clinical tempeval challenge","temporal reasoning","time expression span detection","time expression based subtasks"],"center_task":[null,null,null,null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"Hitachi at SemEval-2016 Task 12: A Hybrid Approach for Temporal Information Extraction from Clinical Notes. This paper describes the system developed for the task of temporal information extraction from clinical narratives in the context of 2016 Clinical TempEval challenge. Clinical TempEval 2016 addressed the problem of temporal reasoning in clinical domain by providing annotated clinical notes and pathology reports similar to Clinical TempEval challenge 2015. The Clinical TempEval challenge consisted of six subtasks. Hitachi team participated in two time expression based subtasks: time expression span detection (TS) and time expression attribute identification (TA) for which we developed hybrid of rule-based and machine learning based methods using Stanford TokensRegex framework and Stanford Named Entity Recognizer and evaluated it on the THYME corpus. Our hybrid system achieved a maximum F-score of 0.73 for identification of time spans (TS) and 0.71 for identification of time attributes (TA).","label":1,"title_clean":"Hitachi at SemEval 2016 Task 12: A Hybrid Approach for Temporal Information Extraction from Clinical Notes","abstract_clean":"This paper describes the system developed for the task of temporal information extraction from clinical narratives in the context of 2016 Clinical TempEval challenge. Clinical TempEval 2016 addressed the problem of temporal reasoning in clinical domain by providing annotated clinical notes and pathology reports similar to Clinical TempEval challenge 2015. The Clinical TempEval challenge consisted of six subtasks. Hitachi team participated in two time expression based subtasks: time expression span detection (TS) and time expression attribute identification (TA) for which we developed hybrid of rule based and machine learning based methods using Stanford TokensRegex framework and Stanford Named Entity Recognizer and evaluated it on the THYME corpus. Our hybrid system achieved a maximum F score of 0.73 for identification of time spans (TS) and 0.71 for identification of time attributes (TA).","url":"https:\/\/aclanthology.org\/S16-1191"},{"ID":"panicheva-etal-2010-personal","methods":["authorship attribution technique","personalized approach"],"center_method":[null,null],"tasks":["opinion analysis","automatic authorship classification","automatic authorship attribution","subjectivity analysis","personal sense"],"center_task":[null,null,null,null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Personal Sense and Idiolect: Combining Authorship Attribution and Opinion Analysis. Subjectivity analysis and authorship attribution are very popular areas of research. However, work in these two areas has been done separately. Our conjecture is that by combining information about subjectivity in texts and authorship, the performance of both tasks can be improved. In the paper a personalized approach to opinion mining is presented, in which the notions of personal sense and idiolect are introduced; the approach is applied to the polarity classification task. It is assumed that different authors express their private states in text individually, and opinion mining results could be improved by analyzing texts by different authors separately. The hypothesis is tested on a corpus of movie reviews by ten authors. The results of applying the personalized approach to opinion mining are presented, confirming that the approach increases the performance of the opinion mining task. Automatic authorship attribution is further applied to model the personalized approach, classifying documents by their assumed authorship. Although the automatic authorship classification imposes a number of limitations on the dataset for further experiments, after overcoming these issues the authorship attribution technique modeling the personalized approach confirms the increase over the baseline with no authorship information used.","label":1,"title_clean":"Personal Sense and Idiolect: Combining Authorship Attribution and Opinion Analysis","abstract_clean":"Subjectivity analysis and authorship attribution are very popular areas of research. However, work in these two areas has been done separately. Our conjecture is that by combining information about subjectivity in texts and authorship, the performance of both tasks can be improved. In the paper a personalized approach to opinion mining is presented, in which the notions of personal sense and idiolect are introduced; the approach is applied to the polarity classification task. It is assumed that different authors express their private states in text individually, and opinion mining results could be improved by analyzing texts by different authors separately. The hypothesis is tested on a corpus of movie reviews by ten authors. The results of applying the personalized approach to opinion mining are presented, confirming that the approach increases the performance of the opinion mining task. Automatic authorship attribution is further applied to model the personalized approach, classifying documents by their assumed authorship. Although the automatic authorship classification imposes a number of limitations on the dataset for further experiments, after overcoming these issues the authorship attribution technique modeling the personalized approach confirms the increase over the baseline with no authorship information used.","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2010\/pdf\/491_Paper.pdf"},{"ID":"paris-vander-linden-1996-building","methods":["drafter","authoring support tool","knowledge base flom","knowledge bases"],"center_method":[null,null,null,null],"tasks":["software documentation","automated text generation","generation of software documentation"],"center_task":[null,null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Building Knowledge Bases for the Generation of Software Documentation. Automated text generation requires a underlying knowledge base fl'om which to generate, which is often difficult to produce. Software documentation is one domain in which parts of this knowledge base may be derived automatically. In this paper, we describe DRAFTER, an authoring support tool for generating usercentred software documentation, and in particular, we describe how parts of its required knowledge base can be obtained automatically.","label":1,"title_clean":"Building Knowledge Bases for the Generation of Software Documentation","abstract_clean":"Automated text generation requires a underlying knowledge base fl'om which to generate, which is often difficult to produce. Software documentation is one domain in which parts of this knowledge base may be derived automatically. In this paper, we describe DRAFTER, an authoring support tool for generating usercentred software documentation, and in particular, we describe how parts of its required knowledge base can be obtained automatically.","url":"https:\/\/aclanthology.org\/C96-2124.pdf"},{"ID":"park-etal-2021-blames","methods":["pretrained transformer"],"center_method":[null],"tasks":["social science research questions","directed sentiment extraction","question answering","nlp applications","sentiment extraction"],"center_task":[null,null,"question answering","nlp applications",null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Who Blames or Endorses Whom? Entity-to-Entity Directed Sentiment Extraction in News Text. Understanding who blames or supports whom in news text is a critical research question in computational social science. Traditional methods and datasets for sentiment analysis are, however, not suitable for the domain of political text as they do not consider the direction of sentiments expressed between entities. In this paper, we propose a novel NLP task of identifying directed sentiment relationship between political entities from a given news document, which we call directed sentiment extraction. From a million-scale news corpus, we construct a dataset of news sentences where sentiment relations of political entities are manually annotated. We present a simple but effective approach for utilizing a pretrained transformer, which infers the target class by predicting multiple question-answering tasks and combining the outcomes. We demonstrate the utility of our proposed method for social science research questions by analyzing positive and negative opinions between political entities in two major events: 2016 U.S. presidential election and COVID-19. The newly proposed problem, data, and method will facilitate future studies on interdisciplinary NLP methods and applications. 1 * This work was done while the first author was a postdoctoral researcher at UCLA.","label":1,"title_clean":"Who Blames or Endorses Whom? Entity to Entity Directed Sentiment Extraction in News Text","abstract_clean":"Understanding who blames or supports whom in news text is a critical research question in computational social science. Traditional methods and datasets for sentiment analysis are, however, not suitable for the domain of political text as they do not consider the direction of sentiments expressed between entities. In this paper, we propose a novel NLP task of identifying directed sentiment relationship between political entities from a given news document, which we call directed sentiment extraction. From a million scale news corpus, we construct a dataset of news sentences where sentiment relations of political entities are manually annotated. We present a simple but effective approach for utilizing a pretrained transformer, which infers the target class by predicting multiple question answering tasks and combining the outcomes. We demonstrate the utility of our proposed method for social science research questions by analyzing positive and negative opinions between political entities in two major events: 2016 U.S. presidential election and COVID 19. The newly proposed problem, data, and method will facilitate future studies on interdisciplinary NLP methods and applications. 1 * This work was done while the first author was a postdoctoral researcher at UCLA.","url":"https:\/\/aclanthology.org\/2021.findings-acl.358"},{"ID":"patankar-etal-2022-optimize","methods":["bert","muril","roberta","transformers","recurrent neural networks","ensemble models"],"center_method":["bert",null,"roberta","transformers","recurrent neural networks",null],"tasks":["abusive comment detection","acl 2022 shared task","low resource indic languages","comment detection"],"center_task":[null,null,null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Optimize\\_Prime@DravidianLangTech-ACL2022: Abusive Comment Detection in Tamil. This paper tries to address the problem of abusive comment detection in low-resource indic languages. Abusive comments are statements that are offensive to a person or a group of people. These comments are targeted toward individuals belonging to specific ethnicities, genders, caste, race, sexuality, etc. Abusive Comment Detection is a significant problem, especially with the recent rise in social media users. This paper presents the approach used by our team-Optimize_Prime, in the ACL 2022 shared task \"Abusive Comment Detection in Tamil.\" This task detects and classifies YouTube comments in Tamil and Tamil-English Codemixed format into multiple categories. We have used three methods to optimize our results: Ensemble models, Recurrent Neural Networks, and Transformers. In the Tamil data, MuRIL and XLM-RoBERTA were our best performing models with a macro-averaged f1 score of 0.43. Furthermore, for the Codemixed data, MuRIL and M-BERT provided sublime results, with a macro-averaged f1 score of 0.45.","label":1,"title_clean":"Optimize\\_Prime@DravidianLangTech ACL2022: Abusive Comment Detection in Tamil","abstract_clean":"This paper tries to address the problem of abusive comment detection in low resource indic languages. Abusive comments are statements that are offensive to a person or a group of people. These comments are targeted toward individuals belonging to specific ethnicities, genders, caste, race, sexuality, etc. Abusive Comment Detection is a significant problem, especially with the recent rise in social media users. This paper presents the approach used by our team Optimize_Prime, in the ACL 2022 shared task \"Abusive Comment Detection in Tamil.\" This task detects and classifies YouTube comments in Tamil and Tamil English Codemixed format into multiple categories. We have used three methods to optimize our results: Ensemble models, Recurrent Neural Networks, and Transformers. In the Tamil data, MuRIL and XLM RoBERTA were our best performing models with a macro averaged f1 score of 0.43. Furthermore, for the Codemixed data, MuRIL and M BERT provided sublime results, with a macro averaged f1 score of 0.45.","url":"https:\/\/aclanthology.org\/2022.dravidianlangtech-1.36"},{"ID":"patrick-li-2009-cascade","methods":["cascade approach"],"center_method":[null],"tasks":["extracting medication events","healthcare","computational linguists","information extraction"],"center_task":[null,"healthcare",null,"information extraction"],"Goal":["Good Health and Well-Being"],"text":"A Cascade Approach to Extracting Medication Events. Information Extraction, from the electronic clinical record is a comparatively new topic for computational linguists. In order to utilize the records to improve the efficiency and quality of health care, the knowledge content should be automatically encoded; however this poses a number of challenges for Natural Language Processing (NLP). In this paper, we present a cascade approach to discover the medicationrelated information (MEDICATION, DOSAGE, MODE, FREQUENCY, DURATION, REASON, and CONTEXT) from narrative patient records. The prototype of this system was used to participate the i2b2 2009 medication extraction challenge. The results show better than 90% accuracy on 5 out of 7 entities used in the study.","label":1,"title_clean":"A Cascade Approach to Extracting Medication Events","abstract_clean":"Information Extraction, from the electronic clinical record is a comparatively new topic for computational linguists. In order to utilize the records to improve the efficiency and quality of health care, the knowledge content should be automatically encoded; however this poses a number of challenges for Natural Language Processing (NLP). In this paper, we present a cascade approach to discover the medicationrelated information (MEDICATION, DOSAGE, MODE, FREQUENCY, DURATION, REASON, and CONTEXT) from narrative patient records. The prototype of this system was used to participate the i2b2 2009 medication extraction challenge. The results show better than 90% accuracy on 5 out of 7 entities used in the study.","url":"https:\/\/aclanthology.org\/U09-1014.pdf"},{"ID":"paul-etal-2009-mining","methods":["unsupervised clustering procedures","pattern discovery procedure","pronoun templates"],"center_method":[null,null,null],"tasks":["identifying reciprocal relationships","patterns encoding reciprocity"],"center_task":[null,null],"Goal":["Partnership for the Goals"],"text":"Mining the Web for Reciprocal Relationships. In this paper we address the problem of identifying reciprocal relationships in English. In particular we introduce an algorithm that semi-automatically discovers patterns encoding reciprocity based on a set of simple but effective pronoun templates. Using a set of most frequently occurring patterns, we extract pairs of reciprocal pattern instances by searching the web. Then we apply two unsupervised clustering procedures to form meaningful clusters of such reciprocal instances. The pattern discovery procedure yields an accuracy of 97%, while the clustering procedures indicate accuracies of 91% and 82%. Moreover, the resulting set of 10,882 reciprocal instances represent a broad-coverage resource.","label":1,"title_clean":"Mining the Web for Reciprocal Relationships","abstract_clean":"In this paper we address the problem of identifying reciprocal relationships in English. In particular we introduce an algorithm that semi automatically discovers patterns encoding reciprocity based on a set of simple but effective pronoun templates. Using a set of most frequently occurring patterns, we extract pairs of reciprocal pattern instances by searching the web. Then we apply two unsupervised clustering procedures to form meaningful clusters of such reciprocal instances. The pattern discovery procedure yields an accuracy of 97%, while the clustering procedures indicate accuracies of 91% and 82%. Moreover, the resulting set of 10,882 reciprocal instances represent a broad coverage resource.","url":"https:\/\/aclanthology.org\/W09-1111.pdf"},{"ID":"perez-miguel-etal-2018-biomedical","methods":["umls","metamap","web based interface","ixa pipeline","apache lucene tm","metathesaurus","ukb toolkit"],"center_method":["umls",null,null,null,null,null,null],"tasks":["biomedical term normalization","language processing"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"Biomedical term normalization of EHRs with UMLS. This paper presents a novel prototype for biomedical term normalization of electronic health record excerpts with the Unified Medical Language System (UMLS) Metathesaurus, a large, multilingual compendium of biomedical and health-related terminologies. Despite the prototype being multilingual and cross-lingual by design, we first focus on processing clinical text in Spanish because there is no existing tool for this language and for this specific purpose. The tool is based on Apache Lucene TM to index the Metathesaurus and generate mapping candidates from input text. It uses the IXA pipeline for basic language processing and resolves lexical ambiguities with the UKB toolkit. It has been evaluated by measuring its agreement with MetaMap-a mature software to discover UMLS concepts in English texts-in two English-Spanish parallel corpora. In addition, we present a web-based interface for the tool.","label":1,"title_clean":"Biomedical term normalization of EHRs with UMLS","abstract_clean":"This paper presents a novel prototype for biomedical term normalization of electronic health record excerpts with the Unified Medical Language System (UMLS) Metathesaurus, a large, multilingual compendium of biomedical and health related terminologies. Despite the prototype being multilingual and cross lingual by design, we first focus on processing clinical text in Spanish because there is no existing tool for this language and for this specific purpose. The tool is based on Apache Lucene TM to index the Metathesaurus and generate mapping candidates from input text. It uses the IXA pipeline for basic language processing and resolves lexical ambiguities with the UKB toolkit. It has been evaluated by measuring its agreement with MetaMap a mature software to discover UMLS concepts in English texts in two English Spanish parallel corpora. In addition, we present a web based interface for the tool.","url":"https:\/\/aclanthology.org\/L18-1322"},{"ID":"perez-rosas-etal-2014-multimodal","methods":["statistical analysis"],"center_method":[null],"tasks":["deception detection","data acquisition process"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"A Multimodal Dataset for Deception Detection. This paper presents the construction of a multimodal dataset for deception detection, including physiological, thermal, and visual responses of human subjects under three deceptive scenarios. We present the experimental protocol, as well as the data acquisition process. To evaluate the usefulness of the dataset for the task of deception detection, we present a statistical analysis of the physiological and thermal modalities associated with the deceptive and truthful conditions. Initial results show that physiological and thermal responses can differentiate between deceptive and truthful states.","label":1,"title_clean":"A Multimodal Dataset for Deception Detection","abstract_clean":"This paper presents the construction of a multimodal dataset for deception detection, including physiological, thermal, and visual responses of human subjects under three deceptive scenarios. We present the experimental protocol, as well as the data acquisition process. To evaluate the usefulness of the dataset for the task of deception detection, we present a statistical analysis of the physiological and thermal modalities associated with the deceptive and truthful conditions. Initial results show that physiological and thermal responses can differentiate between deceptive and truthful states.","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2014\/pdf\/869_Paper.pdf"},{"ID":"perez-rosas-mihalcea-2014-cross","methods":["deception classifiers"],"center_method":[null],"tasks":["cross cultural deception detection","classification"],"center_task":[null,"classification"],"Goal":["Peace, Justice and Strong Institutions"],"text":"Cross-cultural Deception Detection. In this paper, we address the task of cross-cultural deception detection. Using crowdsourcing, we collect three deception datasets, two in English (one originating from United States and one from India), and one in Spanish obtained from speakers from Mexico. We run comparative experiments to evaluate the accuracies of deception classifiers built for each culture, and also to analyze classification differences within and across cultures. Our results show that we can leverage cross-cultural information, either through translation or equivalent semantic categories, and build deception classifiers with a performance ranging between 60-70%.","label":1,"title_clean":"Cross cultural Deception Detection","abstract_clean":"In this paper, we address the task of cross cultural deception detection. Using crowdsourcing, we collect three deception datasets, two in English (one originating from United States and one from India), and one in Spanish obtained from speakers from Mexico. We run comparative experiments to evaluate the accuracies of deception classifiers built for each culture, and also to analyze classification differences within and across cultures. Our results show that we can leverage cross cultural information, either through translation or equivalent semantic categories, and build deception classifiers with a performance ranging between 60 70%.","url":"https:\/\/aclanthology.org\/P14-2072"},{"ID":"pergola-etal-2021-boosting","methods":["biomedical entity aware masking","neural models","masked lms","language models","transfer learning"],"center_method":[null,"neural models",null,"language models","transfer learning"],"tasks":["lm fine tuning","domain adaption","question answering"],"center_task":[null,"domain adaption","question answering"],"Goal":["Good Health and Well-Being"],"text":"Boosting Low-Resource Biomedical QA via Entity-Aware Masking Strategies. Biomedical question-answering (QA) has gained increased attention for its capability to provide users with high-quality information from a vast scientific literature. Although an increasing number of biomedical QA datasets has been recently made available, those resources are still rather limited and expensive to produce. Transfer learning via pre-trained language models (LMs) has been shown as a promising approach to leverage existing general-purpose knowledge. However, finetuning these large models can be costly and time consuming, often yielding limited benefits when adapting to specific themes of specialised domains, such as the COVID-19 literature. To bootstrap further their domain adaptation, we propose a simple yet unexplored approach, which we call biomedical entity-aware masking (BEM). We encourage masked language models to learn entity-centric knowledge based on the pivotal entities characterizing the domain at hand, and employ those entities to drive the LM fine-tuning. The resulting strategy is a downstream process applicable to a wide variety of masked LMs, not requiring additional memory or components in the neural architectures. Experimental results show performance on par with state-of-the-art models on several biomedical QA datasets.","label":1,"title_clean":"Boosting Low Resource Biomedical QA via Entity Aware Masking Strategies","abstract_clean":"Biomedical question answering (QA) has gained increased attention for its capability to provide users with high quality information from a vast scientific literature. Although an increasing number of biomedical QA datasets has been recently made available, those resources are still rather limited and expensive to produce. Transfer learning via pre trained language models (LMs) has been shown as a promising approach to leverage existing general purpose knowledge. However, finetuning these large models can be costly and time consuming, often yielding limited benefits when adapting to specific themes of specialised domains, such as the COVID 19 literature. To bootstrap further their domain adaptation, we propose a simple yet unexplored approach, which we call biomedical entity aware masking (BEM). We encourage masked language models to learn entity centric knowledge based on the pivotal entities characterizing the domain at hand, and employ those entities to drive the LM fine tuning. The resulting strategy is a downstream process applicable to a wide variety of masked LMs, not requiring additional memory or components in the neural architectures. Experimental results show performance on par with state of the art models on several biomedical QA datasets.","url":"https:\/\/aclanthology.org\/2021.eacl-main.169.pdf"},{"ID":"pinnis-etal-2018-tilde","methods":["tilde mt","statistical and neural machine translation system training"],"center_method":[null,null],"tasks":["machine translation","mt solution","data cleaning","linguistic data storage","client specific mt solutions","normalisation"],"center_task":["machine translation",null,null,null,null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Tilde MT Platform for Developing Client Specific MT Solutions. In this paper, we present Tilde MT, a custom machine translation (MT) platform that provides linguistic data storage (parallel, monolingual corpora, multilingual term collections), data cleaning and normalisation, statistical and neural machine translation system training and hosting functionality, as well as wide integration capabilities (a machine user API and popular computer-assisted translation tool plugins). We provide details for the most important features of the platform, as well as elaborate typical MT system training workflows for client-specific MT solution development.","label":1,"title_clean":"Tilde MT Platform for Developing Client Specific MT Solutions","abstract_clean":"In this paper, we present Tilde MT, a custom machine translation (MT) platform that provides linguistic data storage (parallel, monolingual corpora, multilingual term collections), data cleaning and normalisation, statistical and neural machine translation system training and hosting functionality, as well as wide integration capabilities (a machine user API and popular computer assisted translation tool plugins). We provide details for the most important features of the platform, as well as elaborate typical MT system training workflows for client specific MT solution development.","url":"https:\/\/aclanthology.org\/L18-1214"},{"ID":"pitenis-etal-2020-offensive","methods":["ogtd","computational models"],"center_method":[null,"computational models"],"tasks":["hate speech","online communities"],"center_task":["hate speech",null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Offensive Language Identification in Greek. As offensive language has become a rising issue for online communities and social media platforms, researchers have been investigating ways of coping with abusive content and developing systems to detect its different types: cyberbullying, hate speech, aggression, etc. With a few notable exceptions, most research on this topic so far has dealt with English. This is mostly due to the availability of language resources for English. To address this shortcoming, this paper presents the first Greek annotated dataset for offensive language identification: the Offensive Greek Tweet Dataset (OGTD). OGTD is a manually annotated dataset containing 4,779 posts from Twitter annotated as offensive and not offensive. Along with a detailed description of the dataset, we evaluate several computational models trained and tested on this data.","label":1,"title_clean":"Offensive Language Identification in Greek","abstract_clean":"As offensive language has become a rising issue for online communities and social media platforms, researchers have been investigating ways of coping with abusive content and developing systems to detect its different types: cyberbullying, hate speech, aggression, etc. With a few notable exceptions, most research on this topic so far has dealt with English. This is mostly due to the availability of language resources for English. To address this shortcoming, this paper presents the first Greek annotated dataset for offensive language identification: the Offensive Greek Tweet Dataset (OGTD). OGTD is a manually annotated dataset containing 4,779 posts from Twitter annotated as offensive and not offensive. Along with a detailed description of the dataset, we evaluate several computational models trained and tested on this data.","url":"https:\/\/aclanthology.org\/2020.lrec-1.629"},{"ID":"pokkunuri-etal-2011-role","methods":["classification","mouse genome informatics","bigram","ie based features","bag of words models","information extraction ie techniques"],"center_method":["classification",null,null,null,null,null],"tasks":["biocuration","mgi","document triage application","full text document classification"],"center_task":[null,null,null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"The Role of Information Extraction in the Design of a Document Triage Application for Biocuration. Traditionally, automated triage of papers is performed using lexical (unigram, bigram, and sometimes trigram) features. This paper explores the use of information extraction (IE) techniques to create richer linguistic features than traditional bag-of-words models. Our classifier includes lexico-syntactic patterns and more-complex features that represent a pattern coupled with its extracted noun, represented both as a lexical term and as a semantic category. Our experimental results show that the IE-based features can improve performance over unigram and bigram features alone. We present intrinsic evaluation results of full-text document classification experiments to determine automatically whether a paper should be considered of interest to biologists at the Mouse Genome Informatics (MGI) system at the Jackson Laboratories. We also further discuss issues relating to design and deployment of our classifiers as an application to support scientific knowledge curation at MGI.","label":1,"title_clean":"The Role of Information Extraction in the Design of a Document Triage Application for Biocuration","abstract_clean":"Traditionally, automated triage of papers is performed using lexical (unigram, bigram, and sometimes trigram) features. This paper explores the use of information extraction (IE) techniques to create richer linguistic features than traditional bag of words models. Our classifier includes lexico syntactic patterns and more complex features that represent a pattern coupled with its extracted noun, represented both as a lexical term and as a semantic category. Our experimental results show that the IE based features can improve performance over unigram and bigram features alone. We present intrinsic evaluation results of full text document classification experiments to determine automatically whether a paper should be considered of interest to biologists at the Mouse Genome Informatics (MGI) system at the Jackson Laboratories. We also further discuss issues relating to design and deployment of our classifiers as an application to support scientific knowledge curation at MGI.","url":"https:\/\/aclanthology.org\/W11-0206"},{"ID":"polisciuc-etal-2015-understanding","methods":["understanding urban land use","typographic maps","vacuum package metaphor"],"center_method":[null,null,null],"tasks":["visualization of points of interest","interactive visual exploration of semantic data distributed in space","placing text"],"center_task":[null,null,null],"Goal":["Sustainable Cities and Communities"],"text":"Understanding Urban Land Use through the Visualization of Points of Interest. Semantic data regarding points of interest in urban areas are hard to visualize. Due to the high number of points and categories they belong, as well as the associated textual information, maps become heavily cluttered and hard to read. Using traditional visualization techniques (e.g. dot distribution maps, typographic maps) partially solve this problem. Although, these techniques address different issues of the problem, their combination is hard and typically results in an efficient visualization. In our approach, we present a method to represent clusters of points of interest as shapes, which is based on vacuum package metaphor. The calculated shapes characterize sets of points and allow their use as containers for textual information. Additionally, we present a strategy for placing text onto polygons. The suggested method can be used in interactive visual exploration of semantic data distributed in space, and for creating maps with similar characteristics of dot distribution maps, but using shapes instead of points.","label":1,"title_clean":"Understanding Urban Land Use through the Visualization of Points of Interest","abstract_clean":"Semantic data regarding points of interest in urban areas are hard to visualize. Due to the high number of points and categories they belong, as well as the associated textual information, maps become heavily cluttered and hard to read. Using traditional visualization techniques (e.g. dot distribution maps, typographic maps) partially solve this problem. Although, these techniques address different issues of the problem, their combination is hard and typically results in an efficient visualization. In our approach, we present a method to represent clusters of points of interest as shapes, which is based on vacuum package metaphor. The calculated shapes characterize sets of points and allow their use as containers for textual information. Additionally, we present a strategy for placing text onto polygons. The suggested method can be used in interactive visual exploration of semantic data distributed in space, and for creating maps with similar characteristics of dot distribution maps, but using shapes instead of points.","url":"https:\/\/aclanthology.org\/W15-2810"},{"ID":"poswiata-perelkiewicz-2022-opi","methods":["opilt edi acl2022","bert","roberta pre trained language models","deproberta"],"center_method":[null,"bert",null,null],"tasks":["depression classification","fine tuning"],"center_task":["depression classification","fine tuning"],"Goal":["Good Health and Well-Being"],"text":"OPI@LT-EDI-ACL2022: Detecting Signs of Depression from Social Media Text using RoBERTa Pre-trained Language Models. This paper presents our winning solution for the Shared Task on Detecting Signs of Depression from Social Media Text at LT-EDI-ACL2022. The task was to create a system that, given social media posts in English, should detect the level of depression as 'not depressed', 'moderately depressed' or 'severely depressed'. We based our solution on transformer-based language models. We fine-tuned selected models: BERT, RoBERTa, XLNet, of which the best results were obtained for RoBERTa large. Then, using the prepared corpus, we trained our own language model called DepRoBERTa (RoBERTa for Depression Detection). Fine-tuning of this model improved the results. The third solution was to use the ensemble averaging, which turned out to be the best solution. It achieved a macro-averaged F1-score of 0.583. The source code of prepared solution is available at https:\/\/github.com\/rafalposwiata\/depressiondetection-lt-edi-2022.","label":1,"title_clean":"OPI@LT EDI ACL2022: Detecting Signs of Depression from Social Media Text using RoBERTa Pre trained Language Models","abstract_clean":"This paper presents our winning solution for the Shared Task on Detecting Signs of Depression from Social Media Text at LT EDI ACL2022. The task was to create a system that, given social media posts in English, should detect the level of depression as 'not depressed', 'moderately depressed' or 'severely depressed'. We based our solution on transformer based language models. We fine tuned selected models: BERT, RoBERTa, XLNet, of which the best results were obtained for RoBERTa large. Then, using the prepared corpus, we trained our own language model called DepRoBERTa (RoBERTa for Depression Detection). Fine tuning of this model improved the results. The third solution was to use the ensemble averaging, which turned out to be the best solution. It achieved a macro averaged F1 score of 0.583. The source code of prepared solution is available at https:\/\/github.com\/rafalposwiata\/depressiondetection lt edi 2022.","url":"https:\/\/aclanthology.org\/2022.ltedi-1.40"},{"ID":"prasad-etal-2020-opinion","methods":["processing hindi text","opinion mining system"],"center_method":[null,null],"tasks":["processing hindi text","home remedies"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"Opinion Mining System for Processing Hindi Text for Home Remedies Domain. Lexical and computational components developed for an Opinion Mining System that process Hindi text taken from weblogs are presented in the paper. Text chosen for processing are the ones demonstrating cause and effect relationship between related entities 'Food' and 'Health Issues'. The work is novel and lexical resources developed are useful in the current research and may be of importance for future research.","label":1,"title_clean":"Opinion Mining System for Processing Hindi Text for Home Remedies Domain","abstract_clean":"Lexical and computational components developed for an Opinion Mining System that process Hindi text taken from weblogs are presented in the paper. Text chosen for processing are the ones demonstrating cause and effect relationship between related entities 'Food' and 'Health Issues'. The work is novel and lexical resources developed are useful in the current research and may be of importance for future research.","url":"https:\/\/aclanthology.org\/2020.icon-demos.8"},{"ID":"pratt-pacak-1969-automated","methods":["digital computer"],"center_method":[null],"tasks":["automatic information retrieval","machine translation","automated language processing","artificial languages","automatic speech analysis and synthesis","determination and semantic interpretation of sentence structure","automated processing of medical english int ro duct ion"],"center_task":[null,"machine translation",null,null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"Automated Processing of Medical English. Int ro duct ion The present interest of the scientific community in automated language processing has been awakened by the enormous capabilities of the high speed digltal computer. It was recognized that the computer which has the capacity to handle symbols effectively can also treat words as symbols and language as a string of symbols. Automated language processing as exemplified by current research, had its origin in machine translation. The first attempt to use the computer for automatic language processing took place in 1954. It is known as the \"IBM-Georgetown Experiment\" in machine translation from Russian into English. (I ,2) The experiment revealed the following facts: a. the digital computer can be used for automated language processing 2 but b. much deeper knowledge about the structure and semantics of language will be required for the determination and semantic interpretation of sentence structure. The field of automated language processing is quite broad; it includes machine translation, automatic information retrieval (if based on language data), production of computer generated abstracts, indexes and catalogs, development of artificial languages, question answering systems, automatic speech analysis and synthesis, and others.","label":1,"title_clean":"Automated Processing of Medical English","abstract_clean":"Int ro duct ion The present interest of the scientific community in automated language processing has been awakened by the enormous capabilities of the high speed digltal computer. It was recognized that the computer which has the capacity to handle symbols effectively can also treat words as symbols and language as a string of symbols. Automated language processing as exemplified by current research, had its origin in machine translation. The first attempt to use the computer for automatic language processing took place in 1954. It is known as the \"IBM Georgetown Experiment\" in machine translation from Russian into English. (I ,2) The experiment revealed the following facts: a. the digital computer can be used for automated language processing 2 but b. much deeper knowledge about the structure and semantics of language will be required for the determination and semantic interpretation of sentence structure. The field of automated language processing is quite broad; it includes machine translation, automatic information retrieval (if based on language data), production of computer generated abstracts, indexes and catalogs, development of artificial languages, question answering systems, automatic speech analysis and synthesis, and others.","url":"https:\/\/aclanthology.org\/C69-1101"},{"ID":"prost-etal-2019-debiasing","methods":["downstream classifier","debiasing embeddings"],"center_method":[null,null],"tasks":["classification","reduced gender bias"],"center_task":["classification",null],"Goal":["Gender Equality"],"text":"Debiasing Embeddings for Reduced Gender Bias in Text Classification. Bolukbasi et al., 2016) demonstrated that pretrained word embeddings can inherit gender bias from the data they were trained on. We investigate how this bias affects downstream classification tasks, using the case study of occupation classification (De-Arteaga et al., 2019). We show that traditional techniques for debiasing embeddings can actually worsen the bias of the downstream classifier by providing a less noisy channel for communicating gender information. With a relatively minor adjustment, however, we show how these same techniques can be used to simultaneously reduce bias and maintain high classification accuracy.","label":1,"title_clean":"Debiasing Embeddings for Reduced Gender Bias in Text Classification","abstract_clean":"Bolukbasi et al., 2016) demonstrated that pretrained word embeddings can inherit gender bias from the data they were trained on. We investigate how this bias affects downstream classification tasks, using the case study of occupation classification (De Arteaga et al., 2019). We show that traditional techniques for debiasing embeddings can actually worsen the bias of the downstream classifier by providing a less noisy channel for communicating gender information. With a relatively minor adjustment, however, we show how these same techniques can be used to simultaneously reduce bias and maintain high classification accuracy.","url":"https:\/\/aclanthology.org\/W19-3810.pdf"},{"ID":"proux-etal-2009-natural","methods":["decision support systems"],"center_method":[null],"tasks":["information extraction","documents","risk patterns detection"],"center_task":["information extraction",null,null],"Goal":["Good Health and Well-Being"],"text":"Natural Language Processing to Detect Risk Patterns Related to Hospital Acquired Infections. Hospital Acquired Infections (HAI) has a major impact on public health and on related healthcare cost. HAI experts are fighting against this issue but they are struggling to access data. Information systems in hospitals are complex, highly heterogeneous, and generally not convenient to perform a real time surveillance. Developing a tool able to parse patient records in order to automatically detect signs of a possible issue would be a tremendous help for these experts and could allow them to react more rapidly and as a consequence to reduce the impact of such infections. Recent advances in Computational Intelligence Techniques such as Information Extraction, Risk Patterns Detection in documents and Decision Support Systems now allow to develop such systems.","label":1,"title_clean":"Natural Language Processing to Detect Risk Patterns Related to Hospital Acquired Infections","abstract_clean":"Hospital Acquired Infections (HAI) has a major impact on public health and on related healthcare cost. HAI experts are fighting against this issue but they are struggling to access data. Information systems in hospitals are complex, highly heterogeneous, and generally not convenient to perform a real time surveillance. Developing a tool able to parse patient records in order to automatically detect signs of a possible issue would be a tremendous help for these experts and could allow them to react more rapidly and as a consequence to reduce the impact of such infections. Recent advances in Computational Intelligence Techniques such as Information Extraction, Risk Patterns Detection in documents and Decision Support Systems now allow to develop such systems.","url":"https:\/\/aclanthology.org\/W09-4506"},{"ID":"prudhommeaux-etal-2017-vector","methods":["objective methods","distributional semantic models","automated approaches","vector space models"],"center_method":[null,null,null,null],"tasks":["autism","evaluating semantic fluency","neurological examination","subjective manual annotation"],"center_task":[null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"Vector space models for evaluating semantic fluency in autism. A common test administered during neurological examination is the semantic fluency test, in which the patient must list as many examples of a given semantic category as possible under timed conditions. Poor performance is associated with neurological conditions characterized by impairments in executive function, such as dementia, schizophrenia, and autism spectrum disorder (ASD). Methods for analyzing semantic fluency responses at the level of detail necessary to uncover these differences have typically relied on subjective manual annotation. In this paper, we explore automated approaches for scoring semantic fluency responses that leverage ontological resources and distributional semantic models to characterize the semantic fluency responses produced by young children with and without ASD. Using these methods, we find significant differences in the semantic fluency responses of children with ASD, demonstrating the utility of using objective methods for clinical language analysis.","label":1,"title_clean":"Vector space models for evaluating semantic fluency in autism","abstract_clean":"A common test administered during neurological examination is the semantic fluency test, in which the patient must list as many examples of a given semantic category as possible under timed conditions. Poor performance is associated with neurological conditions characterized by impairments in executive function, such as dementia, schizophrenia, and autism spectrum disorder (ASD). Methods for analyzing semantic fluency responses at the level of detail necessary to uncover these differences have typically relied on subjective manual annotation. In this paper, we explore automated approaches for scoring semantic fluency responses that leverage ontological resources and distributional semantic models to characterize the semantic fluency responses produced by young children with and without ASD. Using these methods, we find significant differences in the semantic fluency responses of children with ASD, demonstrating the utility of using objective methods for clinical language analysis.","url":"https:\/\/aclanthology.org\/P17-2006.pdf"},{"ID":"pyysalo-etal-2007-unification","methods":["stanford dependency scheme","parser","corpora","link grammar","unifying syntax formalism","syntactic annotation schemes"],"center_method":[null,"parser",null,null,null,null],"tasks":["unification","biomedical information extraction"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"On the unification of syntactic annotations under the Stanford dependency scheme: A case study on BioInfer and GENIA. Several incompatible syntactic annotation schemes are currently used by parsers and corpora in biomedical information extraction. The recently introduced Stanford dependency scheme has been suggested to be a suitable unifying syntax formalism. In this paper, we present a step towards such unification by creating a conversion from the Link Grammar to the Stanford scheme. Further, we create a version of the BioInfer corpus with syntactic annotation in this scheme. We present an application-oriented evaluation of the transformation and assess the suitability of the scheme and our conversion to the unification of the syntactic annotations of BioInfer and the GENIA Treebank. We find that a highly reliable conversion is both feasible to create and practical, increasing the applicability of both the parser and the corpus to information extraction.","label":1,"title_clean":"On the unification of syntactic annotations under the Stanford dependency scheme: A case study on BioInfer and GENIA","abstract_clean":"Several incompatible syntactic annotation schemes are currently used by parsers and corpora in biomedical information extraction. The recently introduced Stanford dependency scheme has been suggested to be a suitable unifying syntax formalism. In this paper, we present a step towards such unification by creating a conversion from the Link Grammar to the Stanford scheme. Further, we create a version of the BioInfer corpus with syntactic annotation in this scheme. We present an application oriented evaluation of the transformation and assess the suitability of the scheme and our conversion to the unification of the syntactic annotations of BioInfer and the GENIA Treebank. We find that a highly reliable conversion is both feasible to create and practical, increasing the applicability of both the parser and the corpus to information extraction.","url":"https:\/\/aclanthology.org\/W07-1004.pdf"},{"ID":"pyysalo-etal-2009-static","methods":["biomedical information extraction approaches"],"center_method":[null],"tasks":["event extraction","static relation extraction task","task setting","domain information extraction"],"center_task":["event extraction",null,null,null],"Goal":["Good Health and Well-Being"],"text":"Static Relations: a Piece in the Biomedical Information Extraction Puzzle. We propose a static relation extraction task to complement biomedical information extraction approaches. We argue that static relations such as part-whole are implicitly involved in many common extraction settings, define a task setting making them explicit, and discuss their integration into previously proposed tasks and extraction methods. We further identify a specific static relation extraction task motivated by the BioNLP'09 shared task on event extraction, introduce an annotated corpus for the task, and demonstrate the feasibility of the task by experiments showing that the defined relations can be reliably extracted. The task setting and corpus can serve to support several forms of domain information extraction.","label":1,"title_clean":"Static Relations: a Piece in the Biomedical Information Extraction Puzzle","abstract_clean":"We propose a static relation extraction task to complement biomedical information extraction approaches. We argue that static relations such as part whole are implicitly involved in many common extraction settings, define a task setting making them explicit, and discuss their integration into previously proposed tasks and extraction methods. We further identify a specific static relation extraction task motivated by the BioNLP'09 shared task on event extraction, introduce an annotated corpus for the task, and demonstrate the feasibility of the task by experiments showing that the defined relations can be reliably extracted. The task setting and corpus can serve to support several forms of domain information extraction.","url":"https:\/\/aclanthology.org\/W09-1301"},{"ID":"pyysalo-etal-2011-overview","methods":["event extraction methods"],"center_method":[null],"tasks":["information extraction task","infectious diseases","bionlp09 task","id task","molecular mechanisms of infectious diseases"],"center_task":[null,null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"Overview of the Infectious Diseases (ID) task of BioNLP Shared Task 2011. This paper presents the preparation, resources, results and analysis of the Infectious Diseases (ID) information extraction task, a main task of the BioNLP Shared Task 2011. The ID task represents an application and extension of the BioNLP'09 shared task event extraction approach to full papers on infectious diseases. Seven teams submitted final results to the task, with the highest-performing system achieving 56% F-score in the full task, comparable to state-of-the-art performance in the established BioNLP'09 task. The results indicate that event extraction methods generalize well to new domains and full-text publications and are applicable to the extraction of events relevant to the molecular mechanisms of infectious diseases.","label":1,"title_clean":"Overview of the Infectious Diseases (ID) task of BioNLP Shared Task 2011","abstract_clean":"This paper presents the preparation, resources, results and analysis of the Infectious Diseases (ID) information extraction task, a main task of the BioNLP Shared Task 2011. The ID task represents an application and extension of the BioNLP'09 shared task event extraction approach to full papers on infectious diseases. Seven teams submitted final results to the task, with the highest performing system achieving 56% F score in the full task, comparable to state of the art performance in the established BioNLP'09 task. The results indicate that event extraction methods generalize well to new domains and full text publications and are applicable to the extraction of events relevant to the molecular mechanisms of infectious diseases.","url":"https:\/\/aclanthology.org\/W11-1804"},{"ID":"qian-etal-2019-comparative","methods":["machine translation tools","word2vec based algorithm","strategic omission and explicitation strategies"],"center_method":[null,null,null],"tasks":["machine translation","jury instruction scenario"],"center_task":["machine translation",null],"Goal":["Peace, Justice and Strong Institutions"],"text":"A Comparative Study of English-Chinese Translations of Court Texts by Machine and Human Translators and the Word2Vec Based Similarity Measure's Ability To Gauge Human Evaluation Biases. In this comparative study, a jury instruction scenario was used to test the translating capabilities of multiple machine translation tools and a human translator with extensive court experience. Three certified translators\/interpreters subjectively evaluated the target texts generated using adequacy and fluency as the evaluation metrics. This subjective evaluation found that the machine generated results had much poorer adequacy and fluency compared with results produced by their human counterpart. Human translators can use strategic omission and explicitation strategies such as addition, paraphrasing, substitution, and repetition to remove ambiguity, and achieve a natural flow in the target language. We also investigate instances where human evaluators have major disagreements and found that human experts could have very biased views. On the other hand, a word2vec based algorithm, if given a good reference translation, can serve as a robust and reliable similarity reference to quantify human evalutors' biases beacuse it was trained on a large corpus using neural network models. Even though the machine generated versions had better fluency performance compared to their adequacy","label":1,"title_clean":"A Comparative Study of English Chinese Translations of Court Texts by Machine and Human Translators and the Word2Vec Based Similarity Measure's Ability To Gauge Human Evaluation Biases","abstract_clean":"In this comparative study, a jury instruction scenario was used to test the translating capabilities of multiple machine translation tools and a human translator with extensive court experience. Three certified translators\/interpreters subjectively evaluated the target texts generated using adequacy and fluency as the evaluation metrics. This subjective evaluation found that the machine generated results had much poorer adequacy and fluency compared with results produced by their human counterpart. Human translators can use strategic omission and explicitation strategies such as addition, paraphrasing, substitution, and repetition to remove ambiguity, and achieve a natural flow in the target language. We also investigate instances where human evaluators have major disagreements and found that human experts could have very biased views. On the other hand, a word2vec based algorithm, if given a good reference translation, can serve as a robust and reliable similarity reference to quantify human evalutors' biases beacuse it was trained on a large corpus using neural network models. Even though the machine generated versions had better fluency performance compared to their adequacy","url":"https:\/\/aclanthology.org\/W19-6714"},{"ID":"qian-etal-2021-lifelong","methods":["hate speech","variational representation learning","lb soinn load balancing self organizing incremental neural network","lifelong learning","memory module","lifelong learning techniques"],"center_method":["hate speech",null,null,null,null,null],"tasks":["catastrophic forgetting","automated hate speech classification"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Lifelong Learning of Hate Speech Classification on Social Media. Existing work on automated hate speech classification assumes that the dataset is fixed and the classes are pre-defined. However, the amount of data in social media increases every day, and the hot topics changes rapidly, requiring the classifiers to be able to continuously adapt to new data without forgetting the previously learned knowledge. This ability, referred to as lifelong learning, is crucial for the realword application of hate speech classifiers in social media. In this work, we propose lifelong learning of hate speech classification on social media. To alleviate catastrophic forgetting, we propose to use Variational Representation Learning (VRL) along with a memory module based on LB-SOINN (Load-Balancing Self-Organizing Incremental Neural Network). Experimentally, we show that combining variational representation learning and the LB-SOINN memory module achieves better performance than the commonly-used lifelong learning techniques.","label":1,"title_clean":"Lifelong Learning of Hate Speech Classification on Social Media","abstract_clean":"Existing work on automated hate speech classification assumes that the dataset is fixed and the classes are pre defined. However, the amount of data in social media increases every day, and the hot topics changes rapidly, requiring the classifiers to be able to continuously adapt to new data without forgetting the previously learned knowledge. This ability, referred to as lifelong learning, is crucial for the realword application of hate speech classifiers in social media. In this work, we propose lifelong learning of hate speech classification on social media. To alleviate catastrophic forgetting, we propose to use Variational Representation Learning (VRL) along with a memory module based on LB SOINN (Load Balancing Self Organizing Incremental Neural Network). Experimentally, we show that combining variational representation learning and the LB SOINN memory module achieves better performance than the commonly used lifelong learning techniques.","url":"https:\/\/aclanthology.org\/2021.naacl-main.183"},{"ID":"r-l-m-2020-nitk","methods":["fincausal 2020","fine tuning","bert"],"center_method":[null,"fine tuning","bert"],"tasks":["causality detection of factual data","financial analysis"],"center_task":[null,null],"Goal":["Decent Work and Economic Growth"],"text":"NITK NLP at FinCausal-2020 Task 1 Using BERT and Linear models.. FinCausal-2020 is the shared task which focuses on the causality detection of factual data for financial analysis. The financial data facts don't provide much explanation on the variability of these data. This paper aims to propose an efficient method to classify the data into one which is having any financial cause or not. Many models were used to classify the data, out of which SVM model gave an F-Score of 0.9435, BERT with specific fine-tuning achieved best results with F-Score of 0.9677.","label":1,"title_clean":"NITK NLP at FinCausal 2020 Task 1 Using BERT and Linear models.","abstract_clean":"FinCausal 2020 is the shared task which focuses on the causality detection of factual data for financial analysis. The financial data facts don't provide much explanation on the variability of these data. This paper aims to propose an efficient method to classify the data into one which is having any financial cause or not. Many models were used to classify the data, out of which SVM model gave an F Score of 0.9435, BERT with specific fine tuning achieved best results with F Score of 0.9677.","url":"https:\/\/aclanthology.org\/2020.fnp-1.9"},{"ID":"radford-etal-2018-adult","methods":["linear regression models"],"center_method":[null],"tasks":["psycholinguistics"],"center_task":["psycholinguistics"],"Goal":["Good Health and Well-Being"],"text":"Can adult mental health be predicted by childhood future-self narratives? Insights from the CLPsych 2018 Shared Task. The CLPsych 2018 Shared Task B explores how childhood essays can predict psychological distress throughout the author's life. Our main aim was to build tools to help our psychologists understand the data, propose features and interpret predictions. We submitted two linear regression models: MODELA uses simple demographic and wordcount features, while MODELB uses linguistic, entity, typographic, expert-gazetteer, and readability features. Our models perform best at younger prediction ages, with our best unofficial score at 23 of 0.426 disattenuated Pearson correlation. This task is challenging and although predictive performance is limited, we propose that tight integration of expertise across computational linguistics and clinical psychology is a productive direction.","label":1,"title_clean":"Can adult mental health be predicted by childhood future self narratives? Insights from the CLPsych 2018 Shared Task","abstract_clean":"The CLPsych 2018 Shared Task B explores how childhood essays can predict psychological distress throughout the author's life. Our main aim was to build tools to help our psychologists understand the data, propose features and interpret predictions. We submitted two linear regression models: MODELA uses simple demographic and wordcount features, while MODELB uses linguistic, entity, typographic, expert gazetteer, and readability features. Our models perform best at younger prediction ages, with our best unofficial score at 23 of 0.426 disattenuated Pearson correlation. This task is challenging and although predictive performance is limited, we propose that tight integration of expertise across computational linguistics and clinical psychology is a productive direction.","url":"https:\/\/aclanthology.org\/W18-0614"},{"ID":"raiyani-etal-2018-fully","methods":["english developed system","classification","deep neural network","fully connected neural network","advance preprocessor"],"center_method":[null,"classification","deep neural network",null,null],"tasks":["aggression identification","aggression","cyberharassment and cyberbullying","automatic aggression identification","hate speech"],"center_task":[null,null,null,null,"hate speech"],"Goal":["Peace, Justice and Strong Institutions"],"text":"Fully Connected Neural Network with Advance Preprocessor to Identify Aggression over Facebook and Twitter. Aggression Identification and Hate Speech detection had become an essential part of cyberharassment and cyberbullying and an automatic aggression identification can lead to the interception of such trolling. Following the same idealization, vista.ue team participated in the workshop which included a shared task on 'Aggression Identification'. A dataset of 15,000 aggression-annotated Facebook Posts and Comments written in Hindi (in both Roman and Devanagari script) and English languages were made available and different classification models were designed. This paper presents a model that outperforms Facebook FastText (Joulin et al., 2016a) and deep learning models over this dataset. Especially, the English developed system, when used to classify Twitter text, outperforms all the shared task submitted systems.","label":1,"title_clean":"Fully Connected Neural Network with Advance Preprocessor to Identify Aggression over Facebook and Twitter","abstract_clean":"Aggression Identification and Hate Speech detection had become an essential part of cyberharassment and cyberbullying and an automatic aggression identification can lead to the interception of such trolling. Following the same idealization, vista.ue team participated in the workshop which included a shared task on 'Aggression Identification'. A dataset of 15,000 aggression annotated Facebook Posts and Comments written in Hindi (in both Roman and Devanagari script) and English languages were made available and different classification models were designed. This paper presents a model that outperforms Facebook FastText (Joulin et al., 2016a) and deep learning models over this dataset. Especially, the English developed system, when used to classify Twitter text, outperforms all the shared task submitted systems.","url":"https:\/\/aclanthology.org\/W18-4404"},{"ID":"rajagopal-etal-2019-domain","methods":["domain adaption","sequence labeling neural network architecture","conditional random field","lstm"],"center_method":["domain adaption",null,"conditional random field","lstm"],"tasks":["srl","biological process","biological domain"],"center_task":[null,null,null],"Goal":["Good Health and Well-Being"],"text":"Domain Adaptation of SRL Systems for Biological Processes. Domain adaptation remains one of the most challenging aspects in the widespread use of Semantic Role Labeling (SRL) systems. Current state-of-the-art methods are typically trained on large-scale datasets, but their performances do not directly transfer to lowresource domain-specific settings. In this paper, we propose two approaches for domain adaptation in biological domain that involve pre-training LSTM-CRF based on existing large-scale datasets and adapting it for a low-resource corpus of biological processes. Our first approach defines a mapping between the source labels and the target labels, and the other approach modifies the final CRF layer in sequence-labeling neural network architecture. We perform our experiments on Pro-cessBank (Berant et al., 2014) dataset which contains less than 200 paragraphs on biological processes. We improve over the previous state-of-the-art system on this dataset by 21 F1 points. We also show that, by incorporating event-event relationship in ProcessBank, we are able to achieve an additional 2.6 F1 gain, giving us possible insights into how to improve SRL systems for biological process using richer annotations.","label":1,"title_clean":"Domain Adaptation of SRL Systems for Biological Processes","abstract_clean":"Domain adaptation remains one of the most challenging aspects in the widespread use of Semantic Role Labeling (SRL) systems. Current state of the art methods are typically trained on large scale datasets, but their performances do not directly transfer to lowresource domain specific settings. In this paper, we propose two approaches for domain adaptation in biological domain that involve pre training LSTM CRF based on existing large scale datasets and adapting it for a low resource corpus of biological processes. Our first approach defines a mapping between the source labels and the target labels, and the other approach modifies the final CRF layer in sequence labeling neural network architecture. We perform our experiments on Pro cessBank (Berant et al., 2014) dataset which contains less than 200 paragraphs on biological processes. We improve over the previous state of the art system on this dataset by 21 F1 points. We also show that, by incorporating event event relationship in ProcessBank, we are able to achieve an additional 2.6 F1 gain, giving us possible insights into how to improve SRL systems for biological process using richer annotations.","url":"https:\/\/aclanthology.org\/W19-5009"},{"ID":"rajalakshmi-etal-2022-dlrg","methods":["transformer based modeling","bert","deep neural network","machine learning methods","word embeddings","lstm","fine tuning","random forest","in dicbert","dlrgdravidianlangtech acl2022"],"center_method":[null,"bert","deep neural network","machine learning methods","word embeddings","lstm","fine tuning",null,null,null],"tasks":["abusive comment detection"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"text":"DLRG@DravidianLangTech-ACL2022: Abusive Comment Detection in Tamil using Multilingual Transformer Models. Online Social Network has let people connect and interact with each other. It does, however, also provide a platform for online abusers to propagate abusive content. The majority of these abusive remarks are written in a multilingual style, which allows them to easily slip past internet inspection. This paper presents a system developed for the Shared Task on Abusive Comment Detection (Misogyny, Misandry, Homophobia, Transphobic, Xenophobia, Coun-terSpeech, Hope Speech) in Tamil Dravidi-anLangTech@ACL 2022 to detect the abusive category of each comment. We approach the task with three methodologies-Machine Learning, Deep Learning and Transformerbased modeling, for two sets of data-Tamil and Tamil+English language dataset. The dataset used in our system can be accessed from the competition on CodaLab. For Machine Learning, eight algorithms were implemented, among which Random Forest gave the best result with Tamil+English dataset, with a weighted average F1-score of 0.78. For Deep Learning, Bi-Directional LSTM gave best result with pre-trained word embeddings. In Transformer-based modeling, we used In-dicBERT and mBERT with fine-tuning, among which mBERT gave the best result for Tamil dataset with a weighted average F1-score of 0.7.","label":1,"title_clean":"DLRG@DravidianLangTech ACL2022: Abusive Comment Detection in Tamil using Multilingual Transformer Models","abstract_clean":"Online Social Network has let people connect and interact with each other. It does, however, also provide a platform for online abusers to propagate abusive content. The majority of these abusive remarks are written in a multilingual style, which allows them to easily slip past internet inspection. This paper presents a system developed for the Shared Task on Abusive Comment Detection (Misogyny, Misandry, Homophobia, Transphobic, Xenophobia, Coun terSpeech, Hope Speech) in Tamil Dravidi anLangTech@ACL 2022 to detect the abusive category of each comment. We approach the task with three methodologies Machine Learning, Deep Learning and Transformerbased modeling, for two sets of data Tamil and Tamil+English language dataset. The dataset used in our system can be accessed from the competition on CodaLab. For Machine Learning, eight algorithms were implemented, among which Random Forest gave the best result with Tamil+English dataset, with a weighted average F1 score of 0.78. For Deep Learning, Bi Directional LSTM gave best result with pre trained word embeddings. In Transformer based modeling, we used In dicBERT and mBERT with fine tuning, among which mBERT gave the best result for Tamil dataset with a weighted average F1 score of 0.7.","url":"https:\/\/aclanthology.org\/2022.dravidianlangtech-1.32"},{"ID":"rajalakshmi-etal-2022-dlrg-tamilnlp","methods":["glove embedding","bidirectional long short term memory bilstm model","supervised approaches","dlrgtamilnlp acl2022","conditional random field"],"center_method":[null,null,null,null,"conditional random field"],"tasks":["offensive span identification","identifying offensive speech"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"DLRG@TamilNLP-ACL2022: Offensive Span Identification in Tamil usingBiLSTM-CRF approach. Identifying offensive speech is an exciting and essential area of research, with ample traction in recent times. This paper presents our system submission to the subtask 1, focusing on using supervised approaches for extracting Offensive spans from code-mixed Tamil-English comments. To identify offensive spans, we developed the Bidirectional Long Short-Term Memory (BiLSTM) model with Glove Embedding. With this method, the developed system achieved an overall F1 of 0.1728. Additionally, for comments with less than 30 characters, the developed system shows an F1 of 0.3890, competitive with other submissions.","label":1,"title_clean":"DLRG@TamilNLP ACL2022: Offensive Span Identification in Tamil usingBiLSTM CRF approach","abstract_clean":"Identifying offensive speech is an exciting and essential area of research, with ample traction in recent times. This paper presents our system submission to the subtask 1, focusing on using supervised approaches for extracting Offensive spans from code mixed Tamil English comments. To identify offensive spans, we developed the Bidirectional Long Short Term Memory (BiLSTM) model with Glove Embedding. With this method, the developed system achieved an overall F1 of 0.1728. Additionally, for comments with less than 30 characters, the developed system shows an F1 of 0.3890, competitive with other submissions.","url":"https:\/\/aclanthology.org\/2022.dravidianlangtech-1.38"},{"ID":"raleigh-2020-keynote","methods":["artificial intelligence","human coder"],"center_method":["artificial intelligence",null],"tasks":["conflict data collection","oversight","automated event data projects"],"center_task":[null,null,null],"Goal":["Decent Work and Economic Growth"],"text":"Keynote Abstract: Too soon? The limitations of AI for event data. Not all conflict datasets offer equal levels of coverage, depth, use-ability, and content. A review of the inclusion criteria, methodology, and sourcing of leading publicly available conflict datasets demonstrates that there are significant discrepancies in the output produced by ostensibly similar projects. This keynote will question the presumption of substantial overlap between datasets, and identify a number of important gaps left by deficiencies across core criteria for effective conflict data collection and analysis, including: Data Collection and Oversight : A rigorous, human coder is the best way to ensure reliable, consistent, and accurate events that are not false positives. Automated event data projects are still being refined and are not yet at the point where they can be used as accurate representations of reality. It is not appropriate to use these event datasets to present trends, maps, or distributions of violence in a state.","label":1,"title_clean":"Keynote Abstract: Too soon? The limitations of AI for event data","abstract_clean":"Not all conflict datasets offer equal levels of coverage, depth, use ability, and content. A review of the inclusion criteria, methodology, and sourcing of leading publicly available conflict datasets demonstrates that there are significant discrepancies in the output produced by ostensibly similar projects. This keynote will question the presumption of substantial overlap between datasets, and identify a number of important gaps left by deficiencies across core criteria for effective conflict data collection and analysis, including: Data Collection and Oversight : A rigorous, human coder is the best way to ensure reliable, consistent, and accurate events that are not false positives. Automated event data projects are still being refined and are not yet at the point where they can be used as accurate representations of reality. It is not appropriate to use these event datasets to present trends, maps, or distributions of violence in a state.","url":"https:\/\/aclanthology.org\/2020.aespen-1.2"},{"ID":"rambow-etal-2004-summarizing","methods":["sentence extraction techniques"],"center_method":[null],"tasks":["summarizing email threads","email threads"],"center_task":[null,null],"Goal":["Decent Work and Economic Growth"],"text":"Summarizing Email Threads. Summarizing threads of email is different from summarizing other types of written communication as it has an inherent dialog structure. We present initial research which shows that sentence extraction techniques can work for email threads as well, but profit from email-specific features. In addition, the presentation of the summary should take into account the dialogic structure of email communication.","label":1,"title_clean":"Summarizing Email Threads","abstract_clean":"Summarizing threads of email is different from summarizing other types of written communication as it has an inherent dialog structure. We present initial research which shows that sentence extraction techniques can work for email threads as well, but profit from email specific features. In addition, the presentation of the summary should take into account the dialogic structure of email communication.","url":"https:\/\/aclanthology.org\/N04-4027.pdf"},{"ID":"rangarajan-sridhar-etal-2014-framework","methods":["finite state transducers","hybrid translation approach","neural network","distributed representation of words","unsupervised normalization approach"],"center_method":[null,null,"neural network",null,null],"tasks":["machine translation","translating sms messages","cross lingual communication","normalization","communication","monolingual communication","short messaging service"],"center_task":["machine translation",null,null,"normalization","communication",null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"A Framework for Translating SMS Messages. Short Messaging Service (SMS) has become a popular form of communication. While it is predominantly used for monolingual communication, it can be extremely useful for facilitating cross-lingual communication through statistical machine translation. In this work we present an application of statistical machine translation to SMS messages. We decouple the SMS translation task into normalization followed by translation so that one can exploit existing bitext resources and present a novel unsupervised normalization approach using distributed representation of words learned through neural networks. We describe several surrogate data that are good approximations to real SMS data feeds and use a hybrid translation approach using finite-state transducers. Both objective and subjective evaluation indicate that our approach is highly suitable for translating SMS messages.","label":1,"title_clean":"A Framework for Translating SMS Messages","abstract_clean":"Short Messaging Service (SMS) has become a popular form of communication. While it is predominantly used for monolingual communication, it can be extremely useful for facilitating cross lingual communication through statistical machine translation. In this work we present an application of statistical machine translation to SMS messages. We decouple the SMS translation task into normalization followed by translation so that one can exploit existing bitext resources and present a novel unsupervised normalization approach using distributed representation of words learned through neural networks. We describe several surrogate data that are good approximations to real SMS data feeds and use a hybrid translation approach using finite state transducers. Both objective and subjective evaluation indicate that our approach is highly suitable for translating SMS messages.","url":"https:\/\/aclanthology.org\/C14-1092"},{"ID":"rao-etal-2017-scalable","methods":["robust anaphora and coreference resolution module","machine learning methods","bio molecular event extraction system"],"center_method":[null,"machine learning methods",null],"tasks":["knowledge acquisition","event extraction","understanding of physiological and pathogenesis mechanisms","knowledge base creation","automatic extraction of bio molecular events"],"center_task":[null,"event extraction",null,null,null],"Goal":["Good Health and Well-Being"],"text":"Scalable Bio-Molecular Event Extraction System towards Knowledge Acquisition. This paper presents a robust system for the automatic extraction of bio-molecular events from scientific texts. Event extraction provides information in the understanding of physiological and pathogenesis mechanisms. Event extraction from biomedical literature has a broad range of applications, such as knowledge base creation, knowledge discovery. Automatic event extraction is a challenging task due to ambiguity and diversity of natural language and linguistic phenomena, such as negations, anaphora and coreferencing leading to incorrect interpretation. In this work a machine learning based approach has been used for the event extraction. The methodology framework proposed in this work is derived from the perspective of natural language processing. The system includes a robust anaphora and coreference resolution module, developed as part of this work. An overall F-score of 54.25% is obtained, which is an improvement of 4% in comparison with the state of the art systems.","label":1,"title_clean":"Scalable Bio Molecular Event Extraction System towards Knowledge Acquisition","abstract_clean":"This paper presents a robust system for the automatic extraction of bio molecular events from scientific texts. Event extraction provides information in the understanding of physiological and pathogenesis mechanisms. Event extraction from biomedical literature has a broad range of applications, such as knowledge base creation, knowledge discovery. Automatic event extraction is a challenging task due to ambiguity and diversity of natural language and linguistic phenomena, such as negations, anaphora and coreferencing leading to incorrect interpretation. In this work a machine learning based approach has been used for the event extraction. The methodology framework proposed in this work is derived from the perspective of natural language processing. The system includes a robust anaphora and coreference resolution module, developed as part of this work. An overall F score of 54.25% is obtained, which is an improvement of 4% in comparison with the state of the art systems.","url":"https:\/\/aclanthology.org\/W17-7547"},{"ID":"rao-etal-2021-stanker","methods":["lgam bert","level grained attention masked bert","stanker","bert","encoders","transformers","stacking network","language models","ensemble methods"],"center_method":[null,null,null,"bert",null,"transformers",null,"language models","ensemble methods"],"tasks":["rumor detection"],"center_task":["rumor detection"],"Goal":["Peace, Justice and Strong Institutions"],"text":"STANKER: Stacking Network based on Level-grained Attention-masked BERT for Rumor Detection on Social Media. Rumor detection on social media puts pretrained language models (LMs), such as BERT, and auxiliary features, such as comments, into use. However, on the one hand, rumor detection datasets in Chinese companies with comments are rare; on the other hand, intensive interaction of attention on Transformer-based models like BERT may hinder performance improvement. To alleviate these problems, we build a new Chinese microblog dataset named Weibo20 1 by collecting posts and associated comments from Sina Weibo and propose a new ensemble named STANKER (Stacking neTwork bAsed-on atteNtion-masKed BERT). STANKER adopts two level-grained attentionmasked BERT (LGAM-BERT) models as base encoders. Unlike the original BERT, our new LGAM-BERT model takes comments as important auxiliary features and masks coattention between posts and comments on lower-layers. Experiments on Weibo20 and three existing social media datasets showed that STANKER outperformed all compared models, especially beating the old state-of-theart on Weibo dataset.","label":1,"title_clean":"STANKER: Stacking Network based on Level grained Attention masked BERT for Rumor Detection on Social Media","abstract_clean":"Rumor detection on social media puts pretrained language models (LMs), such as BERT, and auxiliary features, such as comments, into use. However, on the one hand, rumor detection datasets in Chinese companies with comments are rare; on the other hand, intensive interaction of attention on Transformer based models like BERT may hinder performance improvement. To alleviate these problems, we build a new Chinese microblog dataset named Weibo20 1 by collecting posts and associated comments from Sina Weibo and propose a new ensemble named STANKER (Stacking neTwork bAsed on atteNtion masKed BERT). STANKER adopts two level grained attentionmasked BERT (LGAM BERT) models as base encoders. Unlike the original BERT, our new LGAM BERT model takes comments as important auxiliary features and masks coattention between posts and comments on lower layers. Experiments on Weibo20 and three existing social media datasets showed that STANKER outperformed all compared models, especially beating the old state of theart on Weibo dataset.","url":"https:\/\/aclanthology.org\/2021.emnlp-main.269"},{"ID":"ravenscroft-etal-2018-harrigt","methods":["newspaperscientific work citation linking","harrigt"],"center_method":[null,null],"tasks":["linking news to science","information retrieval endeavour","newspaper\/scientific work citation linking"],"center_task":[null,null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"HarriGT: A Tool for Linking News to Science. Being able to reliably link scientific works to the newspaper articles that discuss them could provide a breakthrough in the way we rationalise and measure the impact of science on our society. Linking these articles is challenging because the language used in the two domains is very different, and the gathering of online resources to align the two is a substantial information retrieval endeavour. We present HarriGT, a semi-automated tool for building corpora of news articles linked to the scientific papers that they discuss. Our aim is to facilitate future development of information-retrieval tools for newspaper\/scientific work citation linking. Har-riGT retrieves newspaper articles from an archive containing 17 years of UK web content. It also integrates with 3 large external citation networks, leveraging named entity extraction, and document classification to surface relevant examples of scientific literature to the user. We also provide a tuned candidate ranking algorithm to highlight potential links between scientific papers and newspaper articles to the user, in order of likelihood. HarriGT is provided as an open source tool (http: \/\/harrigt.xyz).","label":1,"title_clean":"HarriGT: A Tool for Linking News to Science","abstract_clean":"Being able to reliably link scientific works to the newspaper articles that discuss them could provide a breakthrough in the way we rationalise and measure the impact of science on our society. Linking these articles is challenging because the language used in the two domains is very different, and the gathering of online resources to align the two is a substantial information retrieval endeavour. We present HarriGT, a semi automated tool for building corpora of news articles linked to the scientific papers that they discuss. Our aim is to facilitate future development of information retrieval tools for newspaper\/scientific work citation linking. Har riGT retrieves newspaper articles from an archive containing 17 years of UK web content. It also integrates with 3 large external citation networks, leveraging named entity extraction, and document classification to surface relevant examples of scientific literature to the user. We also provide a tuned candidate ranking algorithm to highlight potential links between scientific papers and newspaper articles to the user, in order of likelihood. HarriGT is provided as an open source tool (http: \/\/harrigt.xyz).","url":"https:\/\/aclanthology.org\/P18-4004"},{"ID":"regalado-etal-2015-salinlahi","methods":["intelligent tutoring system","salinlahi iii"],"center_method":[null,null],"tasks":["filipino grammar","sentence construction","collaborative learning","heritage language learners"],"center_task":[null,null,null,null],"Goal":["Quality Education","Reduced Inequalities"],"text":"Salinlahi III: An Intelligent Tutoring System for Filipino Heritage Language Learners. Heritage language learners are learners of the primary language of their parents which they might have been exposed to but have not learned it as a language they can fluently use to communicate with other people. Salinlahi, an Interactive Learning Environment, was developed to teach these young Filipino heritage learners about basic Filipino vocabulary while Salinlahi II included a support for collaborative learning. With the aim of teaching learners with basic knowledge in Filipino we developed Salinlahi III to teach higher level lessons focusing on Filipino grammar and sentence construction. An internal evaluation of the system has shown that the user interface and feedback of the tutor was appropriate. Moreover, in an external evaluation of the system, experimental and controlled field tests were done and results showed that there is a positive learning gain after using the system.","label":1,"title_clean":"Salinlahi III: An Intelligent Tutoring System for Filipino Heritage Language Learners","abstract_clean":"Heritage language learners are learners of the primary language of their parents which they might have been exposed to but have not learned it as a language they can fluently use to communicate with other people. Salinlahi, an Interactive Learning Environment, was developed to teach these young Filipino heritage learners about basic Filipino vocabulary while Salinlahi II included a support for collaborative learning. With the aim of teaching learners with basic knowledge in Filipino we developed Salinlahi III to teach higher level lessons focusing on Filipino grammar and sentence construction. An internal evaluation of the system has shown that the user interface and feedback of the tutor was appropriate. Moreover, in an external evaluation of the system, experimental and controlled field tests were done and results showed that there is a positive learning gain after using the system.","url":"https:\/\/aclanthology.org\/W15-4413"},{"ID":"rehm-etal-2019-developing","methods":["microservices architecture","processing services","content and document curation workflow manager"],"center_method":[null,null,null],"tasks":["document curation services","natural legal language processing"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions","Quality Education"],"text":"Developing and Orchestrating a Portfolio of Natural Legal Language Processing and Document Curation Services. We present a portfolio of natural legal language processing and document curation services currently under development in a collaborative European project. First, we give an overview of the project and the different use cases, while, in the main part of the article, we focus upon the 13 different processing services that are being deployed in different prototype applications using a flexible and scalable microservices architecture. Their orchestration is operationalised using a content and document curation workflow manager.","label":1,"title_clean":"Developing and Orchestrating a Portfolio of Natural Legal Language Processing and Document Curation Services","abstract_clean":"We present a portfolio of natural legal language processing and document curation services currently under development in a collaborative European project. First, we give an overview of the project and the different use cases, while, in the main part of the article, we focus upon the 13 different processing services that are being deployed in different prototype applications using a flexible and scalable microservices architecture. Their orchestration is operationalised using a content and document curation workflow manager.","url":"https:\/\/aclanthology.org\/W19-2207.pdf"},{"ID":"reisert-etal-2014-corpus","methods":["corpus study","microblogging service"],"center_method":[null,null],"tasks":["identifying evidence on microblogs microblogs","large scale annotation phase","discourse relations","nlp applications"],"center_task":[null,null,null,"nlp applications"],"Goal":["Peace, Justice and Strong Institutions"],"text":"A Corpus Study for Identifying Evidence on Microblogs. Microblogs are a popular way for users to communicate and have recently caught the attention of researchers in the natural language processing (NLP) field. However, regardless of their rising popularity, little attention has been given towards determining the properties of discourse relations for the rapid, large-scale microblog data. Therefore, given their importance for various NLP tasks, we begin a study of discourse relations on microblogs by focusing on evidence relations. As no annotated corpora for evidence relations on microblogs exist, we conduct a corpus study to identify such relations on Twitter, a popular microblogging service. We create annotation guidelines, conduct a large-scale annotation phase, and develop a corpus of annotated evidence relations. Finally, we report our observations, annotation difficulties, and data statistics.","label":1,"title_clean":"A Corpus Study for Identifying Evidence on Microblogs","abstract_clean":"Microblogs are a popular way for users to communicate and have recently caught the attention of researchers in the natural language processing (NLP) field. However, regardless of their rising popularity, little attention has been given towards determining the properties of discourse relations for the rapid, large scale microblog data. Therefore, given their importance for various NLP tasks, we begin a study of discourse relations on microblogs by focusing on evidence relations. As no annotated corpora for evidence relations on microblogs exist, we conduct a corpus study to identify such relations on Twitter, a popular microblogging service. We create annotation guidelines, conduct a large scale annotation phase, and develop a corpus of annotated evidence relations. Finally, we report our observations, annotation difficulties, and data statistics.","url":"https:\/\/aclanthology.org\/W14-4910"},{"ID":"ren-etal-2014-positive","methods":["lda latent dirichlet allocation","positive unlabeled learning","classification","mpipul","mixing population","semi supervised model","individual property pu learning"],"center_method":[null,null,"classification",null,null,null,null],"tasks":["deceptive reviews detection","human labeling","supervised learning"],"center_task":[null,null,"supervised learning"],"Goal":["Peace, Justice and Strong Institutions"],"text":"Positive Unlabeled Learning for Deceptive Reviews Detection. Deceptive reviews detection has attracted significant attention from both business and research communities. However, due to the difficulty of human labeling needed for supervised learning, the problem remains to be highly challenging. This paper proposed a novel angle to the problem by modeling PU (positive unlabeled) learning. A semi-supervised model, called mixing population and individual property PU learning (MPIPUL), is proposed. Firstly, some reliable negative examples are identified from the unlabeled dataset. Secondly, some representative positive examples and negative examples are generated based on LDA (Latent Dirichlet Allocation). Thirdly, for the remaining unlabeled examples (we call them spy examples), which can not be explicitly identified as positive and negative, two similarity weights are assigned, by which the probability of a spy example belonging to the positive class and the negative class are displayed. Finally, spy examples and their similarity weights are incorporated into SVM (Support Vector Machine) to build an accurate classifier. Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines.","label":1,"title_clean":"Positive Unlabeled Learning for Deceptive Reviews Detection","abstract_clean":"Deceptive reviews detection has attracted significant attention from both business and research communities. However, due to the difficulty of human labeling needed for supervised learning, the problem remains to be highly challenging. This paper proposed a novel angle to the problem by modeling PU (positive unlabeled) learning. A semi supervised model, called mixing population and individual property PU learning (MPIPUL), is proposed. Firstly, some reliable negative examples are identified from the unlabeled dataset. Secondly, some representative positive examples and negative examples are generated based on LDA (Latent Dirichlet Allocation). Thirdly, for the remaining unlabeled examples (we call them spy examples), which can not be explicitly identified as positive and negative, two similarity weights are assigned, by which the probability of a spy example belonging to the positive class and the negative class are displayed. Finally, spy examples and their similarity weights are incorporated into SVM (Support Vector Machine) to build an accurate classifier. Experiments on gold standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state of the art baselines.","url":"https:\/\/aclanthology.org\/D14-1055"},{"ID":"resnik-etal-2013-using","methods":["topic models","latent dirichlet allocation"],"center_method":["topic models",null],"tasks":["neuroticism","prediction of clinical assessments"],"center_task":[null,null],"Goal":["Good Health and Well-Being","Quality Education"],"text":"Using Topic Modeling to Improve Prediction of Neuroticism and Depression in College Students. We investigate the value-add of topic modeling in text analysis for depression, and for neuroticism as a strongly associated personality measure. Using Pennebaker's Linguistic Inquiry and Word Count (LIWC) lexicon to provide baseline features, we show that straightforward topic modeling using Latent Dirichlet Allocation (LDA) yields interpretable, psychologically relevant \"themes\" that add value in prediction of clinical assessments.","label":1,"title_clean":"Using Topic Modeling to Improve Prediction of Neuroticism and Depression in College Students","abstract_clean":"We investigate the value add of topic modeling in text analysis for depression, and for neuroticism as a strongly associated personality measure. Using Pennebaker's Linguistic Inquiry and Word Count (LIWC) lexicon to provide baseline features, we show that straightforward topic modeling using Latent Dirichlet Allocation (LDA) yields interpretable, psychologically relevant \"themes\" that add value in prediction of clinical assessments.","url":"https:\/\/aclanthology.org\/D13-1133"},{"ID":"rich-etal-2018-modeling","methods":["psycholinguistics","boosted decision trees"],"center_method":["psycholinguistics",null],"tasks":["duolingo second language acquisition modeling","cognitive science","predictive modeling competitions","learning and memory"],"center_task":[null,null,null,null],"Goal":["Quality Education"],"text":"Modeling Second-Language Learning from a Psychological Perspective. Psychological research on learning and memory has tended to emphasize small-scale laboratory studies. However, large datasets of people using educational software provide opportunities to explore these issues from a new perspective. In this paper we describe our approach to the Duolingo Second Language Acquisition Modeling (SLAM) competition which was run in early 2018. We used a well-known class of algorithms (gradient boosted decision trees), with features partially informed by theories from the psychological literature. After detailing our modeling approach and a number of supplementary simulations, we reflect on the degree to which psychological theory aided the model, and the potential for cognitive science and predictive modeling competitions to gain from each other.","label":1,"title_clean":"Modeling Second Language Learning from a Psychological Perspective","abstract_clean":"Psychological research on learning and memory has tended to emphasize small scale laboratory studies. However, large datasets of people using educational software provide opportunities to explore these issues from a new perspective. In this paper we describe our approach to the Duolingo Second Language Acquisition Modeling (SLAM) competition which was run in early 2018. We used a well known class of algorithms (gradient boosted decision trees), with features partially informed by theories from the psychological literature. After detailing our modeling approach and a number of supplementary simulations, we reflect on the degree to which psychological theory aided the model, and the potential for cognitive science and predictive modeling competitions to gain from each other.","url":"https:\/\/aclanthology.org\/W18-0526"},{"ID":"rinaldi-etal-2008-dependency","methods":["syntax based filters","dependency parsing"],"center_method":[null,"dependency parsing"],"tasks":["interaction detection","dependency based relation mining","text mining competition"],"center_task":[null,null,null],"Goal":["Good Health and Well-Being"],"text":"Dependency-Based Relation Mining for Biomedical Literature. We describe techniques for the automatic detection of relationships among domain entities (e.g. genes, proteins, diseases) mentioned in the biomedical literature. Our approach is based on the adaptive selection of candidate interactions sentences, which are then parsed using our own dependency parser. Specific syntax-based filters are used to limit the number of possible candidate interacting pairs. The approach has been implemented as a demonstrator over a corpus of 2000 richly annotated MedLine abstracts, and later tested by participation to a text mining competition. In both cases, the results obtained have proved the adequacy of the proposed approach to the task of interaction detection.","label":1,"title_clean":"Dependency Based Relation Mining for Biomedical Literature","abstract_clean":"We describe techniques for the automatic detection of relationships among domain entities (e.g. genes, proteins, diseases) mentioned in the biomedical literature. Our approach is based on the adaptive selection of candidate interactions sentences, which are then parsed using our own dependency parser. Specific syntax based filters are used to limit the number of possible candidate interacting pairs. The approach has been implemented as a demonstrator over a corpus of 2000 richly annotated MedLine abstracts, and later tested by participation to a text mining competition. In both cases, the results obtained have proved the adequacy of the proposed approach to the task of interaction detection.","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2008\/pdf\/728_paper.pdf"},{"ID":"rinsche-2004-ltc","methods":["ltc communicator","language technology centre ltd","software product","web based e communication tool"],"center_method":[null,null,null,null],"tasks":["multilingual user support","localisation"],"center_task":[null,null],"Goal":["Decent Work and Economic Growth","Industry, Innovation and Infrastrucure"],"text":"LTC Communicator -- a web-based e-communication tool. Software vendors operating in international markets face two problems: first, products must be localised to meet the requirements of each target country; then there is the need to support diverse customers, where end-users may not speak the same language as the helpdesk. Localisation (new versions of screens, help text and documentation), while not cheap, is relatively well understood, with many companies providing expertise and tools. The problem of multilingual user support is much more complex, with few off-the-shelf solutions available. LTC-Communicator, a software product from the Language Technology Centre Ltd, offers an innovative and cost-effective response to this growing need.","label":1,"title_clean":"LTC Communicator  a web based e communication tool","abstract_clean":"Software vendors operating in international markets face two problems: first, products must be localised to meet the requirements of each target country; then there is the need to support diverse customers, where end users may not speak the same language as the helpdesk. Localisation (new versions of screens, help text and documentation), while not cheap, is relatively well understood, with many companies providing expertise and tools. The problem of multilingual user support is much more complex, with few off the shelf solutions available. LTC Communicator, a software product from the Language Technology Centre Ltd, offers an innovative and cost effective response to this growing need.","url":"https:\/\/aclanthology.org\/2004.eamt-1.18"},{"ID":"rio-2002-compiling","methods":["web resources","course methodology"],"center_method":[null,null],"tasks":["literary translation","education purposes","web site","machine translation","interpreting students","computer science"],"center_task":[null,null,null,"machine translation",null,null],"Goal":["Quality Education"],"text":"Compiling an Interactive Literary Translation Web Site for Education Purposes. The project under discussion represents an attempt to exploit the potential of web resources for higher education and, more particularly, on a domain (that of literary translation) which is traditionally considered not very much in relation to technology and computer science. Translation and Interpreting students at the Universidad de M\u00e1laga are offered the possibility to take an English-Spanish Literary Translation module, which epitomises the need for debate in the field of Humanities. Sadly enough, implementation of course methodology is rendered very difficult or impossible owing to time restrictions and overcrowded classrooms. It is our contention that the setting up of a web site may solve some of these issues. We intend to provide both students and the literary translation-aware Internet audience with an integrated, scalable, multifunctional debate forum. Project contents will include a detailed course description, relevant reference materials and interaction services (mailing list, debate forum and chat rooms). This is obviously without limitation, as the Forum is open to any other contents that users may consider necessary or convenient, with a view to a more interdisciplinary approach, further research on the field of Literary Translation and future developments within the project framework.","label":1,"title_clean":"Compiling an Interactive Literary Translation Web Site for Education Purposes","abstract_clean":"The project under discussion represents an attempt to exploit the potential of web resources for higher education and, more particularly, on a domain (that of literary translation) which is traditionally considered not very much in relation to technology and computer science. Translation and Interpreting students at the Universidad de M\u00e1laga are offered the possibility to take an English Spanish Literary Translation module, which epitomises the need for debate in the field of Humanities. Sadly enough, implementation of course methodology is rendered very difficult or impossible owing to time restrictions and overcrowded classrooms. It is our contention that the setting up of a web site may solve some of these issues. We intend to provide both students and the literary translation aware Internet audience with an integrated, scalable, multifunctional debate forum. Project contents will include a detailed course description, relevant reference materials and interaction services (mailing list, debate forum and chat rooms). This is obviously without limitation, as the Forum is open to any other contents that users may consider necessary or convenient, with a view to a more interdisciplinary approach, further research on the field of Literary Translation and future developments within the project framework.","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2002\/pdf\/195.pdf"},{"ID":"risch-etal-2021-toxic","methods":["labeling processes"],"center_method":[null],"tasks":["toxic spans detection","data integration"],"center_task":["toxic spans detection",null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Data Integration for Toxic Comment Classification: Making More Than 40 Datasets Easily Accessible in One Unified Format. With the rise of research on toxic comment classification, more and more annotated datasets have been released. The wide variety of the task (different languages, different labeling processes and schemes) has led to a large amount of heterogeneous datasets that can be used for training and testing very specific settings. Despite recent efforts to create web pages that provide an overview, most publications still use only a single dataset. They are not stored in one central database, they come in many different data formats and it is difficult to interpret their class labels and how to reuse these labels in other projects.","label":1,"title_clean":"Data Integration for Toxic Comment Classification: Making More Than 40 Datasets Easily Accessible in One Unified Format","abstract_clean":"With the rise of research on toxic comment classification, more and more annotated datasets have been released. The wide variety of the task (different languages, different labeling processes and schemes) has led to a large amount of heterogeneous datasets that can be used for training and testing very specific settings. Despite recent efforts to create web pages that provide an overview, most publications still use only a single dataset. They are not stored in one central database, they come in many different data formats and it is difficult to interpret their class labels and how to reuse these labels in other projects.","url":"https:\/\/aclanthology.org\/2021.woah-1.17.pdf"},{"ID":"roller-stevenson-2014-applying","methods":["umls"],"center_method":["umls"],"tasks":["relation extraction","distantly supervised relation detection"],"center_task":["relation extraction",null],"Goal":["Good Health and Well-Being"],"text":"Applying UMLS for Distantly Supervised Relation Detection. This paper describes first results using the Unified Medical Language System (UMLS) for distantly supervised relation extraction. UMLS is a large knowledge base which contains information about millions of medical concepts and relations between them. Our approach is evaluated using existing relation extraction data sets that contain relations that are similar to some of those in UMLS.","label":1,"title_clean":"Applying UMLS for Distantly Supervised Relation Detection","abstract_clean":"This paper describes first results using the Unified Medical Language System (UMLS) for distantly supervised relation extraction. UMLS is a large knowledge base which contains information about millions of medical concepts and relations between them. Our approach is evaluated using existing relation extraction data sets that contain relations that are similar to some of those in UMLS.","url":"https:\/\/aclanthology.org\/W14-1112"},{"ID":"rosario-hearst-2005-multi","methods":["neural net","graphical models"],"center_method":[null,null],"tasks":["identification of the interactions between proteins","multi way relation classification"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"Multi-way Relation Classification: Application to Protein-Protein Interactions. We address the problem of multi-way relation classification, applied to identification of the interactions between proteins in bioscience text. A major impediment to such work is the acquisition of appropriately labeled training data; for our experiments we have identified a database that serves as a proxy for training data. We use two graphical models and a neural net for the classification of the interactions, achieving an accuracy of 64% for a 10-way distinction between relation types. We also provide evidence that the exploitation of the sentences surrounding a citation to a paper can yield higher accuracy than other sentences.","label":1,"title_clean":"Multi way Relation Classification: Application to Protein Protein Interactions","abstract_clean":"We address the problem of multi way relation classification, applied to identification of the interactions between proteins in bioscience text. A major impediment to such work is the acquisition of appropriately labeled training data; for our experiments we have identified a database that serves as a proxy for training data. We use two graphical models and a neural net for the classification of the interactions, achieving an accuracy of 64% for a 10 way distinction between relation types. We also provide evidence that the exploitation of the sentences surrounding a citation to a paper can yield higher accuracy than other sentences.","url":"https:\/\/aclanthology.org\/H05-1092"},{"ID":"rubin-vashchilko-2012-identification","methods":["rhetorical structure theory","word embeddings","rst vsm methodology","analytic framework","discourse structure analysis"],"center_method":[null,"word embeddings",null,null,null],"tasks":["automated deception detection","identification of previously unseen deceptive texts","lexico semantic analysis","pragmatics","rst analysis","rst","identification of truth"],"center_task":[null,null,null,null,null,null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Identification of Truth and Deception in Text: Application of Vector Space Model to Rhetorical Structure Theory. The paper proposes to use Rhetorical Structure Theory (RST) analytic framework to identify systematic differences between deceptive and truthful stories in terms of their coherence and structure. A sample of 36 elicited personal stories, self-ranked as completely truthful or completely deceptive, is manually analyzed by assigning RST discourse relations among a story's constituent parts. Vector Space Model (VSM) assesses each story's position in multi-dimensional RST space with respect to its distance to truth and deceptive centers as measures of the story's level of deception and truthfulness. Ten human judges evaluate if each story is deceptive or not, and assign their confidence levels, which produce measures of the human expected deception and truthfulness levels. The paper contributes to deception detection research and RST twofold: a) demonstration of discourse structure analysis in pragmatics as a prominent way of automated deception detection and, as such, an effective complement to lexico-semantic analysis, and b) development of RST-VSM methodology to interpret RST analysis in identification of previously unseen deceptive texts.","label":1,"title_clean":"Identification of Truth and Deception in Text: Application of Vector Space Model to Rhetorical Structure Theory","abstract_clean":"The paper proposes to use Rhetorical Structure Theory (RST) analytic framework to identify systematic differences between deceptive and truthful stories in terms of their coherence and structure. A sample of 36 elicited personal stories, self ranked as completely truthful or completely deceptive, is manually analyzed by assigning RST discourse relations among a story's constituent parts. Vector Space Model (VSM) assesses each story's position in multi dimensional RST space with respect to its distance to truth and deceptive centers as measures of the story's level of deception and truthfulness. Ten human judges evaluate if each story is deceptive or not, and assign their confidence levels, which produce measures of the human expected deception and truthfulness levels. The paper contributes to deception detection research and RST twofold: a) demonstration of discourse structure analysis in pragmatics as a prominent way of automated deception detection and, as such, an effective complement to lexico semantic analysis, and b) development of RST VSM methodology to interpret RST analysis in identification of previously unseen deceptive texts.","url":"https:\/\/aclanthology.org\/W12-0415"},{"ID":"rudinger-etal-2017-social","methods":["human - elicitation protocol"],"center_method":[null],"tasks":["social bias"],"center_task":[null],"Goal":["Reduced Inequalities","Gender Equality"],"text":"Social Bias in Elicited Natural Language Inferences. We analyze the Stanford Natural Language Inference (SNLI) corpus in an investigation of bias and stereotyping in NLP data. The human-elicitation protocol employed in the construction of the SNLI makes it prone to amplifying bias and stereotypical associations, which we demonstrate statistically (using pointwise mutual information) and with qualitative examples.","label":1,"title_clean":"Social Bias in Elicited Natural Language Inferences","abstract_clean":"We analyze the Stanford Natural Language Inference (SNLI) corpus in an investigation of bias and stereotyping in NLP data. The human elicitation protocol employed in the construction of the SNLI makes it prone to amplifying bias and stereotypical associations, which we demonstrate statistically (using pointwise mutual information) and with qualitative examples.","url":"https:\/\/aclanthology.org\/W17-1609.pdf"},{"ID":"rudzewitz-etal-2018-generating","methods":["comic approach","alignment approach","online component","offline generation approach"],"center_method":[null,null,null,null],"tasks":["english foreign language exercises","analysis of reading comprehension answers","real life educational settings","generating feedback"],"center_task":[null,null,null,null],"Goal":["Quality Education"],"text":"Generating Feedback for English Foreign Language Exercises. While immediate feedback on learner language is often discussed in the Second Language Acquisition literature (e.g., Mackey 2006), few systems used in real-life educational settings provide helpful, metalinguistic feedback to learners. In this paper, we present a novel approach leveraging task information to generate the expected range of well-formed and ill-formed variability in learner answers along with the required diagnosis and feedback. We combine this offline generation approach with an online component that matches the actual student answers against the pre-computed hypotheses. The results obtained for a set of 33 thousand answers of 7th grade German high school students learning English show that the approach successfully covers frequent answer patterns. At the same time, paraphrases and meaning errors require a more flexible alignment approach, for which we are planning to complement the method with the CoMiC approach successfully used for the analysis of reading comprehension answers (Meurers et al., 2011).","label":1,"title_clean":"Generating Feedback for English Foreign Language Exercises","abstract_clean":"While immediate feedback on learner language is often discussed in the Second Language Acquisition literature (e.g., Mackey 2006), few systems used in real life educational settings provide helpful, metalinguistic feedback to learners. In this paper, we present a novel approach leveraging task information to generate the expected range of well formed and ill formed variability in learner answers along with the required diagnosis and feedback. We combine this offline generation approach with an online component that matches the actual student answers against the pre computed hypotheses. The results obtained for a set of 33 thousand answers of 7th grade German high school students learning English show that the approach successfully covers frequent answer patterns. At the same time, paraphrases and meaning errors require a more flexible alignment approach, for which we are planning to complement the method with the CoMiC approach successfully used for the analysis of reading comprehension answers (Meurers et al., 2011).","url":"https:\/\/aclanthology.org\/W18-0513"},{"ID":"rupp-etal-2008-language","methods":["language technology","chemical informatics","information processing techniques"],"center_method":["language technology",null,null],"tasks":["chemistry","text mining task"],"center_task":[null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Language Resources and Chemical Informatics. Chemistry research papers are a primary source of information about chemistry, as in any scientific field. The presentation of the data is, predominantly, unstructured information, and so not immediately susceptible to processes developed within chemical informatics for carrying out chemistry research by information processing techniques. At one level, extracting the relevant information from research papers is a text mining task, requiring both extensive language resources and specialised knowledge of the subject domain. However, the papers also encode information about the way the research is conducted and the structure of the field itself. Applying language technology to research papers in chemistry can facilitate eScience on several different levels. The SciBorg project sets out to provide an extensive, analysed corpus of published chemistry research. This relies on the cooperation of several journal publishers to provide papers in an appropriate form. The work is carried out as a collaboration involving the","label":1,"title_clean":"Language Resources and Chemical Informatics","abstract_clean":"Chemistry research papers are a primary source of information about chemistry, as in any scientific field. The presentation of the data is, predominantly, unstructured information, and so not immediately susceptible to processes developed within chemical informatics for carrying out chemistry research by information processing techniques. At one level, extracting the relevant information from research papers is a text mining task, requiring both extensive language resources and specialised knowledge of the subject domain. However, the papers also encode information about the way the research is conducted and the structure of the field itself. Applying language technology to research papers in chemistry can facilitate eScience on several different levels. The SciBorg project sets out to provide an extensive, analysed corpus of published chemistry research. This relies on the cooperation of several journal publishers to provide papers in an appropriate form. The work is carried out as a collaboration involving the","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2008\/pdf\/556_paper.pdf"},{"ID":"saetre-etal-2009-protein","methods":["akaneppi"],"center_method":[null],"tasks":["bionlp09 shared task","event extraction","protein interaction task","error analysis"],"center_task":[null,"event extraction",null,"error analysis"],"Goal":["Good Health and Well-Being"],"text":"From Protein-Protein Interaction to Molecular Event Extraction. This document describes the methods and results for our participation in the BioNLP'09 Shared Task #1 on Event Extraction. It also contains some error analysis and a brief discussion of the results. Previous shared tasks in the BioNLP community have focused on extracting gene and protein names, and on finding (direct) protein-protein interactions (PPI). This year's task was slightly different, since the protein names were already manually annotated in the text. The new challenge was to extract biological events involving these given gene and gene products. We modified a publicly available system (AkanePPI) to apply it to this new, but similar, protein interaction task. AkanePPI has previously achieved state-of-the-art performance on all existing public PPI corpora, and only small changes were needed to achieve competitive results on this event extraction task. Our official result was an F-score of 36.9%, which was ranked as number six among submissions from 24 different groups. We later balanced the recall\/precision by including more predictions than just the most confident one in ambiguous cases, and this raised the F-score on the test-set to 42.6%. The new Akane program can be used freely for academic purposes.","label":1,"title_clean":"From Protein Protein Interaction to Molecular Event Extraction","abstract_clean":"This document describes the methods and results for our participation in the BioNLP'09 Shared Task #1 on Event Extraction. It also contains some error analysis and a brief discussion of the results. Previous shared tasks in the BioNLP community have focused on extracting gene and protein names, and on finding (direct) protein protein interactions (PPI). This year's task was slightly different, since the protein names were already manually annotated in the text. The new challenge was to extract biological events involving these given gene and gene products. We modified a publicly available system (AkanePPI) to apply it to this new, but similar, protein interaction task. AkanePPI has previously achieved state of the art performance on all existing public PPI corpora, and only small changes were needed to achieve competitive results on this event extraction task. Our official result was an F score of 36.9%, which was ranked as number six among submissions from 24 different groups. We later balanced the recall\/precision by including more predictions than just the most confident one in ambiguous cases, and this raised the F score on the test set to 42.6%. The new Akane program can be used freely for academic purposes.","url":"https:\/\/aclanthology.org\/W09-1414"},{"ID":"saha-etal-2018-leveraging","methods":["web - based evidence gathering"],"center_method":[null],"tasks":["drug information identification"],"center_task":[null],"Goal":["Good Health and Well-Being"],"text":"Leveraging Web Based Evidence Gathering for Drug Information Identification from Tweets. In this paper, we have explored web-based evidence gathering and different linguistic features to automatically extract drug names from tweets and further classify such tweets into Adverse Drug Events or not. We have evaluated our proposed models with the dataset as released by the SMM4H workshop shared Task-1 and Task-3 respectively. Our evaluation results shows that the proposed model achieved good results, with Precision, Recall and F-scores of 78.5%, 88% and 82.9% respectively for Task1 and 33.2%, 54.7% and 41.3% for Task3.","label":1,"title_clean":"Leveraging Web Based Evidence Gathering for Drug Information Identification from Tweets","abstract_clean":"In this paper, we have explored web based evidence gathering and different linguistic features to automatically extract drug names from tweets and further classify such tweets into Adverse Drug Events or not. We have evaluated our proposed models with the dataset as released by the SMM4H workshop shared Task 1 and Task 3 respectively. Our evaluation results shows that the proposed model achieved good results, with Precision, Recall and F scores of 78.5%, 88% and 82.9% respectively for Task1 and 33.2%, 54.7% and 41.3% for Task3.","url":"https:\/\/aclanthology.org\/W18-5919.pdf"},{"ID":"sai-sharma-2021-towards","methods":["roberta","fine tuning","multi task transfer learning techniques","bert","zeroshot learning","ensembling multilingual transformer networks","ensembled versions","language models","pretrained"],"center_method":["roberta","fine tuning",null,"bert",null,null,null,"language models",null],"tasks":["hate speech","knowledge transfer","inter task","offensive speech classification","low resource settings","transliteration","selective translation","dravidian languages"],"center_task":["hate speech",null,null,null,null,"transliteration",null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Towards Offensive Language Identification for Dravidian Languages. Offensive speech identification in countries like India poses several challenges due to the usage of code-mixed and romanized variants of multiple languages by the users in their posts on social media. The challenge of offensive language identification on social media for Dravidian languages is harder, considering the low resources available for the same. In this paper, we explored the zeroshot learning and few-shot learning paradigms based on multilingual language models for offensive speech detection in code-mixed and romanized variants of three Dravidian languages-Malayalam, Tamil, and Kannada. We propose a novel and flexible approach of selective translation and transliteration to reap better results from fine-tuning and ensembling multilingual transformer networks like XLM-RoBERTa and mBERT. We implemented pretrained, fine-tuned, and ensembled versions of XLM-RoBERTa for offensive speech classification. Further, we experimented with interlanguage, inter-task, and multi-task transfer learning techniques to leverage the rich resources available for offensive speech identification in the English language and to enrich the models with knowledge transfer from related tasks. The proposed models yielded good results and are promising for effective offensive speech identification in low resource settings. 1","label":1,"title_clean":"Towards Offensive Language Identification for Dravidian Languages","abstract_clean":"Offensive speech identification in countries like India poses several challenges due to the usage of code mixed and romanized variants of multiple languages by the users in their posts on social media. The challenge of offensive language identification on social media for Dravidian languages is harder, considering the low resources available for the same. In this paper, we explored the zeroshot learning and few shot learning paradigms based on multilingual language models for offensive speech detection in code mixed and romanized variants of three Dravidian languages Malayalam, Tamil, and Kannada. We propose a novel and flexible approach of selective translation and transliteration to reap better results from fine tuning and ensembling multilingual transformer networks like XLM RoBERTa and mBERT. We implemented pretrained, fine tuned, and ensembled versions of XLM RoBERTa for offensive speech classification. Further, we experimented with interlanguage, inter task, and multi task transfer learning techniques to leverage the rich resources available for offensive speech identification in the English language and to enrich the models with knowledge transfer from related tasks. The proposed models yielded good results and are promising for effective offensive speech identification in low resource settings. 1","url":"https:\/\/aclanthology.org\/2021.dravidianlangtech-1.3"},{"ID":"sakaji-etal-2019-financial","methods":["banker friendly inter industry relations analysis framework","financial text data analytics framework"],"center_method":[null,null],"tasks":["inter industry relations","analysis programs","business confidence indices","analyzing inter industry relations"],"center_task":[null,null,null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Financial Text Data Analytics Framework for Business Confidence Indices and Inter-Industry Relations. In this paper, we propose a novel framework for analyzing inter-industry relations using the contact histories of local banks. Contact histories are data recorded when employees communicate with customers. By analyzing contact histories, we can determine business confidence levels in the local region and analyze inter-industry relations using industrial data that is attached to the contact history. However, it is often difficult for bankers to create analysis programs. Therefore, we propose a banker-friendly inter-industry relations analysis framework. In this study, we generated regional business confidence indices and used them to analyze inter-industry relations.","label":1,"title_clean":"Financial Text Data Analytics Framework for Business Confidence Indices and Inter Industry Relations","abstract_clean":"In this paper, we propose a novel framework for analyzing inter industry relations using the contact histories of local banks. Contact histories are data recorded when employees communicate with customers. By analyzing contact histories, we can determine business confidence levels in the local region and analyze inter industry relations using industrial data that is attached to the contact history. However, it is often difficult for bankers to create analysis programs. Therefore, we propose a banker friendly inter industry relations analysis framework. In this study, we generated regional business confidence indices and used them to analyze inter industry relations.","url":"https:\/\/aclanthology.org\/W19-5507"},{"ID":"sakakini-etal-2019-equipping","methods":["bootcat","distributed document representations","dexter"],"center_method":[null,null,null],"tasks":["educational applications","distractor generation","estimating word embeddings","language models"],"center_task":["educational applications",null,null,"language models"],"Goal":["Quality Education"],"text":"Equipping Educational Applications with Domain Knowledge. One of the challenges of building natural language processing (NLP) applications for education is finding a large domain-specific corpus for the subject of interest (e.g., history or science). To address this challenge, we propose a tool, Dexter, that extracts a subjectspecific corpus from a heterogeneous corpus, such as Wikipedia, by relying on a small seed corpus and distributed document representations. We empirically show the impact of the generated corpus on language modeling, estimating word embeddings, and consequently, distractor generation, resulting in a better performance than while using a general domain corpus, a heuristically constructed domainspecific corpus, and a corpus generated by a popular system: BootCaT.","label":1,"title_clean":"Equipping Educational Applications with Domain Knowledge","abstract_clean":"One of the challenges of building natural language processing (NLP) applications for education is finding a large domain specific corpus for the subject of interest (e.g., history or science). To address this challenge, we propose a tool, Dexter, that extracts a subjectspecific corpus from a heterogeneous corpus, such as Wikipedia, by relying on a small seed corpus and distributed document representations. We empirically show the impact of the generated corpus on language modeling, estimating word embeddings, and consequently, distractor generation, resulting in a better performance than while using a general domain corpus, a heuristically constructed domainspecific corpus, and a corpus generated by a popular system: BootCaT.","url":"https:\/\/aclanthology.org\/W19-4448"},{"ID":"salawu-etal-2021-large","methods":["transformer - based deep learning models"],"center_method":[null],"tasks":["online abuse and cyberbullying detection","cyberbullying and online abuse detection"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions","Good Health and Well-Being"],"text":"A Large-Scale English Multi-Label Twitter Dataset for Cyberbullying and Online Abuse Detection. In this paper, we introduce a new English Twitter-based dataset for online abuse and cyberbullying detection. Comprising 62,587 tweets, this dataset was sourced from Twitter using specific query terms designed to retrieve tweets with high probabilities of various forms of bullying and offensive content, including insult, profanity, sarcasm, threat, porn and exclusion. Analysis performed on the dataset confirmed common cyberbullying themes reported by other studies and revealed interesting relationships between the classes. The dataset was used to train a number of transformer-based deep learning models returning impressive results.","label":1,"title_clean":"A Large Scale English Multi Label Twitter Dataset for Cyberbullying and Online Abuse Detection","abstract_clean":"In this paper, we introduce a new English Twitter based dataset for online abuse and cyberbullying detection. Comprising 62,587 tweets, this dataset was sourced from Twitter using specific query terms designed to retrieve tweets with high probabilities of various forms of bullying and offensive content, including insult, profanity, sarcasm, threat, porn and exclusion. Analysis performed on the dataset confirmed common cyberbullying themes reported by other studies and revealed interesting relationships between the classes. The dataset was used to train a number of transformer based deep learning models returning impressive results.","url":"https:\/\/aclanthology.org\/2021.woah-1.16.pdf"},{"ID":"saravanan-etal-2008-automatic","methods":["conditional random field model","mathematical model","machine learning methods"],"center_method":[null,null,"machine learning methods"],"tasks":["rhetorical role identification","legal document summarization"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Automatic Identification of Rhetorical Roles using Conditional Random Fields for Legal Document Summarization. In this paper, we propose a machine learning approach to rhetorical role identification from legal documents. In our approach, we annotate roles in sample documents with the help of legal experts and take them as training data. Conditional random field model has been trained with the data to perform rhetorical role identification with reinforcement of rich feature sets. The understanding of structure of a legal document and the application of mathematical model can brings out an effective summary in the final stage. Other important new findings in this work include that the training of a model for one sub-domain can be extended to another sub-domains with very limited augmentation of feature sets. Moreover, we can significantly improve extraction-based summarization results by modifying the ranking of sentences with the importance of specific roles.","label":1,"title_clean":"Automatic Identification of Rhetorical Roles using Conditional Random Fields for Legal Document Summarization","abstract_clean":"In this paper, we propose a machine learning approach to rhetorical role identification from legal documents. In our approach, we annotate roles in sample documents with the help of legal experts and take them as training data. Conditional random field model has been trained with the data to perform rhetorical role identification with reinforcement of rich feature sets. The understanding of structure of a legal document and the application of mathematical model can brings out an effective summary in the final stage. Other important new findings in this work include that the training of a model for one sub domain can be extended to another sub domains with very limited augmentation of feature sets. Moreover, we can significantly improve extraction based summarization results by modifying the ranking of sentences with the importance of specific roles.","url":"https:\/\/aclanthology.org\/I08-1063"},{"ID":"sasaki-etal-2008-event","methods":["10fold cross validation"],"center_method":[null],"tasks":["event frame extraction","extraction of information","gene regulation","linguistic level annotation of gene regulation events","supervised acquisition of semantic event frames","event instance extraction"],"center_task":[null,null,null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"Event Frame Extraction Based on a Gene Regulation Corpus. This paper describes the supervised acquisition of semantic event frames based on a corpus of biomedical abstracts, in which the biological process of E. coli gene regulation has been linguistically annotated by a group of biologists in the EC research project \"BOOTStrep\". Gene regulation is one of the rapidly advancing areas for which information extraction could boost research. Event frames are an essential linguistic resource for extraction of information from biological literature. This paper presents a specification for linguistic-level annotation of gene regulation events, followed by novel methods of automatic event frame extraction from text. The event frame extraction performance has been evaluated with 10fold cross validation. The experimental results show that a precision of nearly 50% and a recall of around 20% are achieved. Since the goal of this paper is event frame extraction, rather than event instance extraction, the issue of low recall could be solved by applying the methods to a larger-scale corpus. 1 Introduction This paper describes the automatic extraction of linguistic event frames based on a corpus of MEDLINE abstracts that has been annotated with gene regulation events by a group of do","label":1,"title_clean":"Event Frame Extraction Based on a Gene Regulation Corpus","abstract_clean":"This paper describes the supervised acquisition of semantic event frames based on a corpus of biomedical abstracts, in which the biological process of E. coli gene regulation has been linguistically annotated by a group of biologists in the EC research project \"BOOTStrep\". Gene regulation is one of the rapidly advancing areas for which information extraction could boost research. Event frames are an essential linguistic resource for extraction of information from biological literature. This paper presents a specification for linguistic level annotation of gene regulation events, followed by novel methods of automatic event frame extraction from text. The event frame extraction performance has been evaluated with 10fold cross validation. The experimental results show that a precision of nearly 50% and a recall of around 20% are achieved. Since the goal of this paper is event frame extraction, rather than event instance extraction, the issue of low recall could be solved by applying the methods to a larger scale corpus. 1 Introduction This paper describes the automatic extraction of linguistic event frames based on a corpus of MEDLINE abstracts that has been annotated with gene regulation events by a group of do","url":"https:\/\/aclanthology.org\/C08-1096"},{"ID":"saumya-etal-2021-offensive","methods":["deep neural network","bert","transfer learning models","logistic regression","naive bayes","hybrid deep models","vanilla neural network","ulmfit"],"center_method":["deep neural network","bert",null,"logistic regression","naive bayes",null,null,null],"tasks":["hate speech"],"center_task":["hate speech"],"Goal":["Peace, Justice and Strong Institutions"],"text":"Offensive language identification in Dravidian code mixed social media text. Hate speech and offensive language recognition in social media platforms have been an active field of research over recent years. In non-native English spoken countries, social media texts are mostly in code mixed or script mixed\/switched form. The current study presents extensive experiments using multiple machine learning, deep learning, and transfer learning models to detect offensive content on Twitter. The data set used for this study are in Tanglish (Tamil and English), Manglish (Malayalam and English) code-mixed, and Malayalam script-mixed. The experimental results showed that 1 to 6-gram character TF-IDF features are better for the said task. The best performing models were naive bayes, logistic regression, and vanilla neural network for the dataset Tamil code-mix, Malayalam code-mixed, and Malayalam script-mixed, respectively instead of more popular transfer learning models such as BERT and ULMFiT and hybrid deep models.","label":1,"title_clean":"Offensive language identification in Dravidian code mixed social media text","abstract_clean":"Hate speech and offensive language recognition in social media platforms have been an active field of research over recent years. In non native English spoken countries, social media texts are mostly in code mixed or script mixed\/switched form. The current study presents extensive experiments using multiple machine learning, deep learning, and transfer learning models to detect offensive content on Twitter. The data set used for this study are in Tanglish (Tamil and English), Manglish (Malayalam and English) code mixed, and Malayalam script mixed. The experimental results showed that 1 to 6 gram character TF IDF features are better for the said task. The best performing models were naive bayes, logistic regression, and vanilla neural network for the dataset Tamil code mix, Malayalam code mixed, and Malayalam script mixed, respectively instead of more popular transfer learning models such as BERT and ULMFiT and hybrid deep models.","url":"https:\/\/aclanthology.org\/2021.dravidianlangtech-1.5"},{"ID":"savoldi-etal-2021-gender","methods":["mitigating strategies"],"center_method":[null],"tasks":["gender bias mitigation","machine translation","communicating information"],"center_task":["gender bias mitigation","machine translation",null],"Goal":["Gender Equality"],"text":"Gender Bias in Machine Translation. Machine translation (MT) technology has facilitated our daily tasks by providing accessible shortcuts for gathering, processing, and communicating information. However, it can suffer from biases that harm users and society at large. As a relatively new field of inquiry, studies of gender bias in MT still lack cohesion. This advocates for a unified framework to ease future research. To this end, we: i) critically review current conceptualizations of bias in light of theoretical insights from related disciplines, ii) summarize previous analyses aimed at assessing gender bias in MT, iii) discuss the mitigating strategies proposed so far, and iv) point toward potential directions for future work.","label":1,"title_clean":"Gender Bias in Machine Translation","abstract_clean":"Machine translation (MT) technology has facilitated our daily tasks by providing accessible shortcuts for gathering, processing, and communicating information. However, it can suffer from biases that harm users and society at large. As a relatively new field of inquiry, studies of gender bias in MT still lack cohesion. This advocates for a unified framework to ease future research. To this end, we: i) critically review current conceptualizations of bias in light of theoretical insights from related disciplines, ii) summarize previous analyses aimed at assessing gender bias in MT, iii) discuss the mitigating strategies proposed so far, and iv) point toward potential directions for future work.","url":"https:\/\/aclanthology.org\/2021.tacl-1.51"},{"ID":"sawhney-etal-2020-time","methods":["statenet","time aware transformer based model","linguistic models"],"center_method":[null,null,null],"tasks":["suicide ideation","suicide ideation detection","identification of at risk users","suicide risk assessment","suicide prevention","preliminary screening of suicidal risk"],"center_task":[null,null,null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"A Time-Aware Transformer Based Model for Suicide Ideation Detection on Social Media. Social media's ubiquity fosters a space for users to exhibit suicidal thoughts outside of traditional clinical settings. Understanding the build-up of such ideation is critical for the identification of at-risk users and suicide prevention. Suicide ideation is often linked to a history of mental depression. The emotional spectrum of a user's historical activity on social media can be indicative of their mental state over time. In this work, we focus on identifying suicidal intent in English tweets by augmenting linguistic models with historical context. We propose STATENet, a timeaware transformer based model for preliminary screening of suicidal risk on social media. STATENet outperforms competitive methods, demonstrating the utility of emotional and temporal contextual cues for suicide risk assessment. We discuss the empirical, qualitative, practical, and ethical aspects of STATENet for suicide ideation detection. 1","label":1,"title_clean":"A Time Aware Transformer Based Model for Suicide Ideation Detection on Social Media","abstract_clean":"Social media's ubiquity fosters a space for users to exhibit suicidal thoughts outside of traditional clinical settings. Understanding the build up of such ideation is critical for the identification of at risk users and suicide prevention. Suicide ideation is often linked to a history of mental depression. The emotional spectrum of a user's historical activity on social media can be indicative of their mental state over time. In this work, we focus on identifying suicidal intent in English tweets by augmenting linguistic models with historical context. We propose STATENet, a timeaware transformer based model for preliminary screening of suicidal risk on social media. STATENet outperforms competitive methods, demonstrating the utility of emotional and temporal contextual cues for suicide risk assessment. We discuss the empirical, qualitative, practical, and ethical aspects of STATENet for suicide ideation detection. 1","url":"https:\/\/aclanthology.org\/2020.emnlp-main.619"},{"ID":"sawhney-etal-2021-multimodal","methods":["neural baselines risk prediction","baseline architecture","financial modeling","m3anet"],"center_method":[null,null,null,null],"tasks":["ma calls","financial markets","multimodal multi speaker merger acquisition"],"center_task":[null,null,null],"Goal":["Decent Work and Economic Growth"],"text":"Multimodal Multi-Speaker Merger \\& Acquisition Financial Modeling: A New Task, Dataset, and Neural Baselines. Risk prediction is an essential task in financial markets. Merger and Acquisition (M&A) calls provide key insights into the claims made by company executives about the restructuring of the financial firms. Extracting vocal and textual cues from M&A calls can help model the risk associated with such financial activities. To aid the analysis of M&A calls, we curate a dataset of conference call transcripts and their corresponding audio recordings for the time period ranging from 2016 to 2020. We introduce M3ANet, a baseline architecture that takes advantage of the multimodal multispeaker input to forecast the financial risk associated with the M&A calls. Empirical results prove that the task is challenging, with the proposed architecture performing marginally better than strong BERT-based baselines. We release the M3A dataset and benchmark models to motivate future research on this challenging problem domain.","label":1,"title_clean":"Multimodal Multi Speaker Merger \\& Acquisition Financial Modeling: A New Task, Dataset, and Neural Baselines","abstract_clean":"Risk prediction is an essential task in financial markets. Merger and Acquisition (M&A) calls provide key insights into the claims made by company executives about the restructuring of the financial firms. Extracting vocal and textual cues from M&A calls can help model the risk associated with such financial activities. To aid the analysis of M&A calls, we curate a dataset of conference call transcripts and their corresponding audio recordings for the time period ranging from 2016 to 2020. We introduce M3ANet, a baseline architecture that takes advantage of the multimodal multispeaker input to forecast the financial risk associated with the M&A calls. Empirical results prove that the task is challenging, with the proposed architecture performing marginally better than strong BERT based baselines. We release the M3A dataset and benchmark models to motivate future research on this challenging problem domain.","url":"https:\/\/aclanthology.org\/2021.acl-long.526"},{"ID":"schiffman-mckeown-2005-context","methods":["information detection system","learning algorithm"],"center_method":[null,null],"tasks":["novelty detection"],"center_task":[null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Context and Learning in Novelty Detection. We demonstrate the value of using context in a new-information detection system that achieved the highest precision scores at the Text Retrieval Conference's Novelty Track in 2004. In order to determine whether information within a sentence has been seen in material read previously, our system integrates information about the context of the sentence with novel words and named entities within the sentence, and uses a specialized learning algorithm to tune the system parameters.","label":1,"title_clean":"Context and Learning in Novelty Detection","abstract_clean":"We demonstrate the value of using context in a new information detection system that achieved the highest precision scores at the Text Retrieval Conference's Novelty Track in 2004. In order to determine whether information within a sentence has been seen in material read previously, our system integrates information about the context of the sentence with novel words and named entities within the sentence, and uses a specialized learning algorithm to tune the system parameters.","url":"https:\/\/aclanthology.org\/H05-1090"},{"ID":"schlor-etal-2020-improving","methods":["lexicon based sentiment classifier","biosignals","language models","sentiment lexicons","machine learning methods"],"center_method":[null,null,"language models",null,"machine learning methods"],"tasks":["text based sentiment analysis","sentiment analysis","computers"],"center_task":[null,"sentiment analysis",null],"Goal":["Good Health and Well-Being"],"text":"Improving Sentiment Analysis with Biofeedback Data. Humans frequently are able to read and interpret emotions of others by directly taking verbal and non-verbal signals in human-to-human communication into account or to infer or even experience emotions from mediated stories. For computers, however, emotion recognition is a complex problem: Thoughts and feelings are the roots of many behavioural responses and they are deeply entangled with neurophysiological changes within humans. As such, emotions are very subjective, often are expressed in a subtle manner, and are highly depending on context. For example, machine learning approaches for text-based sentiment analysis often rely on incorporating sentiment lexicons or language models to capture the contextual meaning. This paper explores if and how we further can enhance sentiment analysis using biofeedback of humans which are experiencing emotions while reading texts. Specifically, we record the heart rate and brain waves of readers that are presented with short texts which have been annotated with the emotions they induce. We use these physiological signals to improve the performance of a lexicon-based sentiment classifier. We find that the combination of several biosignals can improve the ability of a text-based classifier to detect the presence of a sentiment in a text on a per-sentence level.","label":1,"title_clean":"Improving Sentiment Analysis with Biofeedback Data","abstract_clean":"Humans frequently are able to read and interpret emotions of others by directly taking verbal and non verbal signals in human to human communication into account or to infer or even experience emotions from mediated stories. For computers, however, emotion recognition is a complex problem: Thoughts and feelings are the roots of many behavioural responses and they are deeply entangled with neurophysiological changes within humans. As such, emotions are very subjective, often are expressed in a subtle manner, and are highly depending on context. For example, machine learning approaches for text based sentiment analysis often rely on incorporating sentiment lexicons or language models to capture the contextual meaning. This paper explores if and how we further can enhance sentiment analysis using biofeedback of humans which are experiencing emotions while reading texts. Specifically, we record the heart rate and brain waves of readers that are presented with short texts which have been annotated with the emotions they induce. We use these physiological signals to improve the performance of a lexicon based sentiment classifier. We find that the combination of several biosignals can improve the ability of a text based classifier to detect the presence of a sentiment in a text on a per sentence level.","url":"https:\/\/aclanthology.org\/2020.onion-1.5"},{"ID":"schoene-etal-2019-dilated","methods":["attention","dilated lstm","linguistic analysis","model tree","lstm","dilated lstm with attention"],"center_method":["attention",null,null,null,"lstm",null],"tasks":["classification of suicide notes"],"center_task":[null],"Goal":["Good Health and Well-Being"],"text":"Dilated LSTM with attention for Classification of Suicide Notes. In this paper we present a dilated LSTM with attention mechanism for document-level classification of suicide notes, last statements and depressed notes. We achieve an accuracy of 87.34% compared to competitive baselines of 80.35% (Logistic Model Tree) and 82.27% (Bi-directional LSTM with Attention). Furthermore, we provide an analysis of both the grammatical and thematic content of suicide notes, last statements and depressed notes. We find that the use of personal pronouns, cognitive processes and references to loved ones are most important. Finally, we show through visualisations of attention weights that the Dilated LSTM with attention is able to identify the same distinguishing features across documents as the linguistic analysis.","label":1,"title_clean":"Dilated LSTM with attention for Classification of Suicide Notes","abstract_clean":"In this paper we present a dilated LSTM with attention mechanism for document level classification of suicide notes, last statements and depressed notes. We achieve an accuracy of 87.34% compared to competitive baselines of 80.35% (Logistic Model Tree) and 82.27% (Bi directional LSTM with Attention). Furthermore, we provide an analysis of both the grammatical and thematic content of suicide notes, last statements and depressed notes. We find that the use of personal pronouns, cognitive processes and references to loved ones are most important. Finally, we show through visualisations of attention weights that the Dilated LSTM with attention is able to identify the same distinguishing features across documents as the linguistic analysis.","url":"https:\/\/aclanthology.org\/D19-6217"},{"ID":"schumaker-2010-analysis","methods":["discrete machine learning algorithm"],"center_method":[null],"tasks":["price movement","stock price movement"],"center_task":[null,null],"Goal":["Decent Work and Economic Growth"],"text":"An Analysis of Verbs in Financial News Articles and their Impact on Stock Price. Article terms can move stock prices. By analyzing verbs in financial news articles and coupling their usage with a discrete machine learning algorithm tied to stock price movement, we can build a model of price movement based upon the verbs used, to not only identify those terms that can move a stock price the most, but also whether they move the predicted price up or down.","label":1,"title_clean":"An Analysis of Verbs in Financial News Articles and their Impact on Stock Price","abstract_clean":"Article terms can move stock prices. By analyzing verbs in financial news articles and coupling their usage with a discrete machine learning algorithm tied to stock price movement, we can build a model of price movement based upon the verbs used, to not only identify those terms that can move a stock price the most, but also whether they move the predicted price up or down.","url":"https:\/\/aclanthology.org\/W10-0502.pdf"},{"ID":"schuurman-vandeghinste-2010-cultural","methods":["spatiotemporal disambiguation"],"center_method":[null],"tasks":["cultural aspects of spatiotemporal analysis","multilingual","translation purposes","spatiotemporal aspects","analysis of the query","automatic selection of parts of texts","multidocument summarization","natural language generation"],"center_task":[null,null,null,null,null,null,null,"natural language generation"],"Goal":["Reduced Inequalities"],"text":"Cultural Aspects of Spatiotemporal Analysis in Multilingual Applications. In this paper we want to point out some issues arising when a natural language processing task involves several languages (like multilingual, multidocument summarization and the machine translation aspects involved) which are often neglected. These issues are of a more cultural nature, and may even come into play when several documents in a single language are involved. We pay special attention to those aspects dealing with the spatiotemporal characteristics of a text. Correct automatic selection of (parts of) texts such as handling the same eventuality, presupposes spatiotemporal disambiguation at a rather specific level. The same holds for the analysis of the query. For generation and translation purposes, spatiotemporal aspects may be relevant as well. At the moment English (both the British and American variants) and Dutch (the Flemish and Dutch variant) are covered, all taking into account the perspective of a contemporary, Flemish user. In our approach the cultural aspects associated with for example the language of publication and the language used by the user play a crucial role.","label":1,"title_clean":"Cultural Aspects of Spatiotemporal Analysis in Multilingual Applications","abstract_clean":"In this paper we want to point out some issues arising when a natural language processing task involves several languages (like multilingual, multidocument summarization and the machine translation aspects involved) which are often neglected. These issues are of a more cultural nature, and may even come into play when several documents in a single language are involved. We pay special attention to those aspects dealing with the spatiotemporal characteristics of a text. Correct automatic selection of (parts of) texts such as handling the same eventuality, presupposes spatiotemporal disambiguation at a rather specific level. The same holds for the analysis of the query. For generation and translation purposes, spatiotemporal aspects may be relevant as well. At the moment English (both the British and American variants) and Dutch (the Flemish and Dutch variant) are covered, all taking into account the perspective of a contemporary, Flemish user. In our approach the cultural aspects associated with for example the language of publication and the language used by the user play a crucial role.","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2010\/pdf\/180_Paper.pdf"},{"ID":"sellami-etal-2013-exploiting","methods":["5 grams language model","moses decoder"],"center_method":[null,null],"tasks":["japanese to english patent translation","machine translation"],"center_task":[null,"machine translation"],"Goal":["Quality Education","Industry, Innovation and Infrastrucure"],"text":"Exploiting multiple resources for Japanese to English patent translation. This paper describes the development of a Japanese to English translation system using multiple resources and NTCIR-10 Patent translation collection. The MT system is based on different training data, the Wiktionary as a bilingual dictionary and Moses decoder. Due to the lack of parallel data on the patent domain, additional training data of the general domain was extracted from Wikipedia. Experiments using NTCIR-10 Patent translation data collection showed an improvement of the BLEU score when using a 5-grams language model and when adding the data extracted from Wikipedia but no improvement when adding the Wiktionary.","label":1,"title_clean":"Exploiting multiple resources for Japanese to English patent translation","abstract_clean":"This paper describes the development of a Japanese to English translation system using multiple resources and NTCIR 10 Patent translation collection. The MT system is based on different training data, the Wiktionary as a bilingual dictionary and Moses decoder. Due to the lack of parallel data on the patent domain, additional training data of the general domain was extracted from Wikipedia. Experiments using NTCIR 10 Patent translation data collection showed an improvement of the BLEU score when using a 5 grams language model and when adding the data extracted from Wikipedia but no improvement when adding the Wiktionary.","url":"https:\/\/aclanthology.org\/2013.mtsummit-wpt.5.pdf"},{"ID":"senda-etal-2004-support","methods":["revision wizard","support system"],"center_method":[null,null],"tasks":["revising titles"],"center_task":[null],"Goal":["Quality Education","Decent Work and Economic Growth"],"text":"A Support System for Revising Titles to Stimulate the Lay Reader's Interest in Technical Achievements. When we write a report or an explanation on a newly-developed technology for readers including laypersons, it is very important to compose a title that can stimulate their interest in the technology. However, it is difficult for inexperienced authors to come up with an appealing title. In this research, we developed a support system for revising titles. We call it \"title revision wizard\". The wizard provides a guidance on revising draft title to compose a title meeting three key points, and support tools for coming up with and elaborating on comprehensible or appealing phrases. In order to test the effect of our title revision wizard, we conducted a questionnaire survey on the effect of the titles with or without using the wizard on the interest of lay readers. The survey showed that the wizard is effective and helpful for the authors who cannot compose appealing titles for lay readers by themselves.","label":1,"title_clean":"A Support System for Revising Titles to Stimulate the Lay Reader's Interest in Technical Achievements","abstract_clean":"When we write a report or an explanation on a newly developed technology for readers including laypersons, it is very important to compose a title that can stimulate their interest in the technology. However, it is difficult for inexperienced authors to come up with an appealing title. In this research, we developed a support system for revising titles. We call it \"title revision wizard\". The wizard provides a guidance on revising draft title to compose a title meeting three key points, and support tools for coming up with and elaborating on comprehensible or appealing phrases. In order to test the effect of our title revision wizard, we conducted a questionnaire survey on the effect of the titles with or without using the wizard on the interest of lay readers. The survey showed that the wizard is effective and helpful for the authors who cannot compose appealing titles for lay readers by themselves.","url":"https:\/\/aclanthology.org\/C04-1023.pdf"},{"ID":"seroussi-etal-2014-authorship","methods":["topical author representations","topic models"],"center_method":[null,"topic models"],"tasks":["authorship attribution","sentiment polarity of texts"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Authorship Attribution with Topic Models. Authorship attribution deals with identifying the authors of anonymous texts. Traditionally, research in this field has focused on formal texts, such as essays and novels, but recently more attention has been given to texts generated by on-line users, such as e-mails and blogs. Authorship attribution of such on-line texts is a more challenging task than traditional authorship attribution, because such texts tend to be short, and the number of candidate authors is often larger than in traditional settings. We address this challenge by using topic models to obtain author representations. In addition to exploring novel ways of applying two popular topic models to this task, we test our new model that projects authors and documents to two disjoint topic spaces. Utilizing our model in authorship attribution yields state-of-the-art performance on several data sets, containing either formal texts written by a few authors or informal texts generated by tens to thousands of on-line users. We also present experimental results that demonstrate the applicability of topical author representations to two other problems: inferring the sentiment polarity of texts, and predicting the ratings that users would give to items such as movies.","label":1,"title_clean":"Authorship Attribution with Topic Models","abstract_clean":"Authorship attribution deals with identifying the authors of anonymous texts. Traditionally, research in this field has focused on formal texts, such as essays and novels, but recently more attention has been given to texts generated by on line users, such as e mails and blogs. Authorship attribution of such on line texts is a more challenging task than traditional authorship attribution, because such texts tend to be short, and the number of candidate authors is often larger than in traditional settings. We address this challenge by using topic models to obtain author representations. In addition to exploring novel ways of applying two popular topic models to this task, we test our new model that projects authors and documents to two disjoint topic spaces. Utilizing our model in authorship attribution yields state of the art performance on several data sets, containing either formal texts written by a few authors or informal texts generated by tens to thousands of on line users. We also present experimental results that demonstrate the applicability of topical author representations to two other problems: inferring the sentiment polarity of texts, and predicting the ratings that users would give to items such as movies.","url":"https:\/\/aclanthology.org\/J14-2003"},{"ID":"shaar-etal-2021-findings","methods":["transformers","ensemble methods"],"center_method":["transformers","ensemble methods"],"tasks":["censorship detection","nlp4if 2021 shared tasks","fact checking"],"center_task":[null,null,"fact checking"],"Goal":["Peace, Justice and Strong Institutions","Good Health and Well-Being"],"text":"Findings of the NLP4IF-2021 Shared Tasks on Fighting the COVID-19 Infodemic and Censorship Detection. We present the results and the main findings of the NLP4IF-2021 shared tasks. Task 1 focused on fighting the COVID-19 infodemic in social media, and it was offered in Arabic, Bulgarian, and English. Given a tweet, it asked to predict whether that tweet contains a verifiable claim, and if so, whether it is likely to be false, is of general interest, is likely to be harmful, and is worthy of manual fact-checking; also, whether it is harmful to society, and whether it requires the attention of policy makers. Task 2 focused on censorship detection, and was offered in Chinese. A total of ten teams submitted systems for task 1, and one team participated in task 2; nine teams also submitted a system description paper. Here, we present the tasks, analyze the results, and discuss the system submissions and the methods they used. Most submissions achieved sizable improvements over several baselines, and the best systems used pre-trained Transformers and ensembles. The data, the scorers and the leaderboards for the tasks are available at http:\/\/ gitlab.com\/NLP4IF\/nlp4if-2021.","label":1,"title_clean":"Findings of the NLP4IF 2021 Shared Tasks on Fighting the COVID 19 Infodemic and Censorship Detection","abstract_clean":"We present the results and the main findings of the NLP4IF 2021 shared tasks. Task 1 focused on fighting the COVID 19 infodemic in social media, and it was offered in Arabic, Bulgarian, and English. Given a tweet, it asked to predict whether that tweet contains a verifiable claim, and if so, whether it is likely to be false, is of general interest, is likely to be harmful, and is worthy of manual fact checking; also, whether it is harmful to society, and whether it requires the attention of policy makers. Task 2 focused on censorship detection, and was offered in Chinese. A total of ten teams submitted systems for task 1, and one team participated in task 2; nine teams also submitted a system description paper. Here, we present the tasks, analyze the results, and discuss the system submissions and the methods they used. Most submissions achieved sizable improvements over several baselines, and the best systems used pre trained Transformers and ensembles. The data, the scorers and the leaderboards for the tasks are available at http:\/\/ gitlab.com\/NLP4IF\/nlp4if 2021.","url":"https:\/\/aclanthology.org\/2021.nlp4if-1.12"},{"ID":"shahid-etal-2020-detecting","methods":["framing theory","classification"],"center_method":[null,"classification"],"tasks":["moral biases of news sources","annotation and frame detection challenges"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Detecting and understanding moral biases in news. We describe work in progress on detecting and understanding the moral biases of news sources by combining framing theory with natural language processing. First we draw connections between issue-specific frames and moral frames that apply to all issues. Then we analyze the connection between moral frame presence and news source political leaning. We develop and test a simple classification model for detecting the presence of a moral frame, highlighting the need for more sophisticated models. We also discuss some of the annotation and frame detection challenges that can inform future research in this area.","label":1,"title_clean":"Detecting and understanding moral biases in news","abstract_clean":"We describe work in progress on detecting and understanding the moral biases of news sources by combining framing theory with natural language processing. First we draw connections between issue specific frames and moral frames that apply to all issues. Then we analyze the connection between moral frame presence and news source political leaning. We develop and test a simple classification model for detecting the presence of a moral frame, highlighting the need for more sophisticated models. We also discuss some of the annotation and frame detection challenges that can inform future research in this area.","url":"https:\/\/aclanthology.org\/2020.nuse-1.15"},{"ID":"shamanna-girishekar-etal-2021-training","methods":["language models","curriculum learning","data augmentation strategies","bert","automated solutions","distant supervision","classification"],"center_method":["language models",null,null,"bert",null,"distant supervision","classification"],"tasks":["advertising","classification","e commerce","adversarial advertisement detection"],"center_task":[null,"classification","e commerce",null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Training Language Models under Resource Constraints for Adversarial Advertisement Detection. Advertising on e-commerce and social media sites deliver ad impressions at web scale on a daily basis driving value to both shoppers and advertisers. This scale necessitates programmatic ways of detecting unsuitable content in ads to safeguard customer experience and trust. This paper focusses on techniques for training text classification models under resource constraints, built as part of automated solutions for advertising content moderation. We show how weak supervision, curriculum learning and multilingual training can be applied effectively to fine-tune BERT and its variants for text classification tasks in conjunction with different data augmentation strategies. Our extensive experiments on multiple languages show that these techniques detect adversarial ad categories with a substantial gain in precision at high recall threshold over the baseline.","label":1,"title_clean":"Training Language Models under Resource Constraints for Adversarial Advertisement Detection","abstract_clean":"Advertising on e commerce and social media sites deliver ad impressions at web scale on a daily basis driving value to both shoppers and advertisers. This scale necessitates programmatic ways of detecting unsuitable content in ads to safeguard customer experience and trust. This paper focusses on techniques for training text classification models under resource constraints, built as part of automated solutions for advertising content moderation. We show how weak supervision, curriculum learning and multilingual training can be applied effectively to fine tune BERT and its variants for text classification tasks in conjunction with different data augmentation strategies. Our extensive experiments on multiple languages show that these techniques detect adversarial ad categories with a substantial gain in precision at high recall threshold over the baseline.","url":"https:\/\/aclanthology.org\/2021.naacl-industry.35"},{"ID":"shaprin-etal-2019-team","methods":["fine tuning","bert"],"center_method":["fine tuning","bert"],"tasks":["detecting hyperpartisan news"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Team Jack Ryder at SemEval-2019 Task 4: Using BERT Representations for Detecting Hyperpartisan News. We describe the system submitted by the Jack Ryder team to SemEval-2019 Task 4 on Hyperpartisan News Detection. The task asked participants to predict whether a given article is hyperpartisan, i.e., extreme-left or extremeright. We propose an approach based on BERT with fine-tuning, which was ranked 7th out 28 teams on the distantly supervised dataset, where all articles from a hyperpartisan\/nonhyperpartisan news outlet are considered to be hyperpartisan\/non-hyperpartisan. On a manually annotated test dataset, where human annotators double-checked the labels, we were ranked 29th out of 42 teams.","label":1,"title_clean":"Team Jack Ryder at SemEval 2019 Task 4: Using BERT Representations for Detecting Hyperpartisan News","abstract_clean":"We describe the system submitted by the Jack Ryder team to SemEval 2019 Task 4 on Hyperpartisan News Detection. The task asked participants to predict whether a given article is hyperpartisan, i.e., extreme left or extremeright. We propose an approach based on BERT with fine tuning, which was ranked 7th out 28 teams on the distantly supervised dataset, where all articles from a hyperpartisan\/nonhyperpartisan news outlet are considered to be hyperpartisan\/non hyperpartisan. On a manually annotated test dataset, where human annotators double checked the labels, we were ranked 29th out of 42 teams.","url":"https:\/\/aclanthology.org\/S19-2176"},{"ID":"shen-etal-2003-effective","methods":["named entity recognition","rule based method","hidden markov models"],"center_method":["named entity recognition",null,"hidden markov models"],"tasks":["abbreviation problem"],"center_task":[null],"Goal":["Good Health and Well-Being"],"text":"Effective Adaptation of Hidden Markov Model-based Named Entity Recognizer for Biomedical Domain. In this paper, we explore how to adapt a general Hidden Markov Model-based named entity recognizer effectively to biomedical domain. We integrate various features, including simple deterministic features, morphological features, POS features and semantic trigger features, to capture various evidences especially for biomedical named entity and evaluate their contributions. We also present a simple algorithm to solve the abbreviation problem and a rule-based method to deal with the cascaded phenomena in biomedical domain. Our experiments on GENIA V3.0 and GENIA V1.1 achieve the 66.1 and 62.5 F-measure respectively, which outperform the previous best published results by 8.1 F-measure when using the same training and testing data.","label":1,"title_clean":"Effective Adaptation of Hidden Markov Model based Named Entity Recognizer for Biomedical Domain","abstract_clean":"In this paper, we explore how to adapt a general Hidden Markov Model based named entity recognizer effectively to biomedical domain. We integrate various features, including simple deterministic features, morphological features, POS features and semantic trigger features, to capture various evidences especially for biomedical named entity and evaluate their contributions. We also present a simple algorithm to solve the abbreviation problem and a rule based method to deal with the cascaded phenomena in biomedical domain. Our experiments on GENIA V3.0 and GENIA V1.1 achieve the 66.1 and 62.5 F measure respectively, which outperform the previous best published results by 8.1 F measure when using the same training and testing data.","url":"https:\/\/aclanthology.org\/W03-1307"},{"ID":"shen-etal-2013-participant","methods":["participant based approach","mixture model"],"center_method":[null,null],"tasks":["text summarization"],"center_task":["text summarization"],"Goal":["Peace, Justice and Strong Institutions"],"text":"A Participant-based Approach for Event Summarization Using Twitter Streams. Twitter offers an unprecedented advantage on live reporting of the events happening around the world. However, summarizing the Twitter event has been a challenging task that was not fully explored in the past. In this paper, we propose a participant-based event summarization approach that \"zooms-in\" the Twitter event streams to the participant level, detects the important sub-events associated with each participant using a novel mixture model that combines the \"burstiness\" and \"cohesiveness\" properties of the event tweets, and generates the event summaries progressively. We evaluate the proposed approach on different event types. Results show that the participantbased approach can effectively capture the sub-events that have otherwise been shadowed by the long-tail of other dominant sub-events, yielding summaries with considerably better coverage than the state-of-the-art.","label":1,"title_clean":"A Participant based Approach for Event Summarization Using Twitter Streams","abstract_clean":"Twitter offers an unprecedented advantage on live reporting of the events happening around the world. However, summarizing the Twitter event has been a challenging task that was not fully explored in the past. In this paper, we propose a participant based event summarization approach that \"zooms in\" the Twitter event streams to the participant level, detects the important sub events associated with each participant using a novel mixture model that combines the \"burstiness\" and \"cohesiveness\" properties of the event tweets, and generates the event summaries progressively. We evaluate the proposed approach on different event types. Results show that the participantbased approach can effectively capture the sub events that have otherwise been shadowed by the long tail of other dominant sub events, yielding summaries with considerably better coverage than the state of the art.","url":"https:\/\/aclanthology.org\/N13-1135.pdf"},{"ID":"shen-etal-2021-sciconceptminer","methods":["microsoft academic 1 production system","sciconceptminer","web search api","bert based sequence labeling model","self supervised end toend system"],"center_method":[null,null,null,null,null],"tasks":["large scale scientific concept discovery","concept identification","synonym detection","automatic capture of emerging scientific concepts","semantic search capability"],"center_task":[null,null,null,null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"SciConceptMiner: A system for large-scale scientific concept discovery. Scientific knowledge is evolving at an unprecedented rate of speed, with new concepts constantly being introduced from millions of academic articles published every month. In this paper, we introduce a self-supervised end-toend system, SciConceptMiner, for the automatic capture of emerging scientific concepts from both independent knowledge sources (semi-structured data) and academic publications (unstructured documents). First, we adopt a BERT-based sequence labeling model to predict candidate concept phrases with selfsupervision data. Then, we incorporate rich Web content for synonym detection and concept selection via a web search API. This two-stage approach achieves highly accurate (94.7%) concept identification with more than 740K scientific concepts. These concepts are deployed in the Microsoft Academic 1 production system and are the backbone for its semantic search capability.","label":1,"title_clean":"SciConceptMiner: A system for large scale scientific concept discovery","abstract_clean":"Scientific knowledge is evolving at an unprecedented rate of speed, with new concepts constantly being introduced from millions of academic articles published every month. In this paper, we introduce a self supervised end toend system, SciConceptMiner, for the automatic capture of emerging scientific concepts from both independent knowledge sources (semi structured data) and academic publications (unstructured documents). First, we adopt a BERT based sequence labeling model to predict candidate concept phrases with selfsupervision data. Then, we incorporate rich Web content for synonym detection and concept selection via a web search API. This two stage approach achieves highly accurate (94.7%) concept identification with more than 740K scientific concepts. These concepts are deployed in the Microsoft Academic 1 production system and are the backbone for its semantic search capability.","url":"https:\/\/aclanthology.org\/2021.acl-demo.6"},{"ID":"sheng-etal-2021-nice","methods":["top k sampling","constrained decoding technique","classification","dialogue systems"],"center_method":[null,null,"classification","dialogue systems"],"tasks":["dialogue responses","ad hominems"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"``Nice Try, Kiddo'': Investigating Ad Hominems in Dialogue Responses. Ad hominem attacks are those that target some feature of a person's character instead of the position the person is maintaining. These attacks are harmful because they propagate implicit biases and diminish a person's credibility. Since dialogue systems respond directly to user input, it is important to study ad hominems in dialogue responses. To this end, we propose categories of ad hominems, compose an annotated dataset, and build a classifier to analyze human and dialogue system responses to English Twitter posts. We specifically compare responses to Twitter topics about marginalized communities (#Black-LivesMatter, #MeToo) versus other topics (#Vegan, #WFH), because the abusive language of ad hominems could further amplify the skew of power away from marginalized populations. Furthermore, we propose a constrained decoding technique that uses salient n-gram similarity as a soft constraint for top-k sampling to reduce the amount of ad hominems generated. Our results indicate that 1) responses from both humans and DialoGPT contain more ad hominems for discussions around marginalized communities, 2) different quantities of ad hominems in the training data can influence the likelihood of generating ad hominems, and 3) we can use constrained decoding techniques to reduce ad hominems in generated dialogue responses. Post: Many are trying to co-opt and mischaracterize the #blacklivesmatter movement. We won't allow it! Resp: I hate how much of a victim complex you guys have.","label":1,"title_clean":"``Nice Try, Kiddo'': Investigating Ad Hominems in Dialogue Responses","abstract_clean":"Ad hominem attacks are those that target some feature of a person's character instead of the position the person is maintaining. These attacks are harmful because they propagate implicit biases and diminish a person's credibility. Since dialogue systems respond directly to user input, it is important to study ad hominems in dialogue responses. To this end, we propose categories of ad hominems, compose an annotated dataset, and build a classifier to analyze human and dialogue system responses to English Twitter posts. We specifically compare responses to Twitter topics about marginalized communities (#Black LivesMatter, #MeToo) versus other topics (#Vegan, #WFH), because the abusive language of ad hominems could further amplify the skew of power away from marginalized populations. Furthermore, we propose a constrained decoding technique that uses salient n gram similarity as a soft constraint for top k sampling to reduce the amount of ad hominems generated. Our results indicate that 1) responses from both humans and DialoGPT contain more ad hominems for discussions around marginalized communities, 2) different quantities of ad hominems in the training data can influence the likelihood of generating ad hominems, and 3) we can use constrained decoding techniques to reduce ad hominems in generated dialogue responses. Post: Many are trying to co opt and mischaracterize the #blacklivesmatter movement. We won't allow it! Resp: I hate how much of a victim complex you guys have.","url":"https:\/\/aclanthology.org\/2021.naacl-main.60"},{"ID":"sheremetyeva-2002-mt","methods":["mt learning environment"],"center_method":[null],"tasks":["machine translation","computational linguistics students","mt training"],"center_task":["machine translation",null,null],"Goal":["Quality Education"],"text":"An MT learning environment for computational linguistics students. This paper discusses the issue of suitability of software used for the teaching of Machine Translation. It considers requirements to such software, and describes a set of tools that have initially been created as developer environment of an APTrans MT system but can easily be included in the learning environment for MT training. The tools are user-friendly and feature modularity and reusability.","label":1,"title_clean":"An MT learning environment for computational linguistics students","abstract_clean":"This paper discusses the issue of suitability of software used for the teaching of Machine Translation. It considers requirements to such software, and describes a set of tools that have initially been created as developer environment of an APTrans MT system but can easily be included in the learning environment for MT training. The tools are user friendly and feature modularity and reusability.","url":"https:\/\/aclanthology.org\/2002.eamt-1.9"},{"ID":"shim-etal-2021-synthetic","methods":["multi task learning","multi task temporal information extraction model","synthetic data generation algorithm","language models"],"center_method":["multi task learning",null,null,"language models"],"tasks":["temporal information extraction","normalised time prediction","synthetic data generation"],"center_task":[null,null,null],"Goal":["Good Health and Well-Being"],"text":"Synthetic Data Generation and Multi-Task Learning for Extracting Temporal Information from Health-Related Narrative Text. Extracting temporal information is critical to process health-related text. Temporal information extraction is a challenging task for language models because it requires processing both texts and numbers. Moreover, the fundamental challenge is how to obtain a largescale training dataset. To address this, we propose a synthetic data generation algorithm. Also, we propose a novel multi-task temporal information extraction model and investigate whether multi-task learning can contribute to performance improvement by exploiting additional training signals with the existing training data. For experiments, we collected a custom dataset containing unstructured texts with temporal information of sleep-related activities. Experimental results show that utilising synthetic data can improve the performance when the augmentation factor is 3. The results also show that when multi-task learning is used with an appropriate amount of synthetic data, the performance can significantly improve from 82. to 88.6 and from 83.9 to 91.9 regarding micro-and macro-average exact match scores of normalised time prediction, respectively.","label":1,"title_clean":"Synthetic Data Generation and Multi Task Learning for Extracting Temporal Information from Health Related Narrative Text","abstract_clean":"Extracting temporal information is critical to process health related text. Temporal information extraction is a challenging task for language models because it requires processing both texts and numbers. Moreover, the fundamental challenge is how to obtain a largescale training dataset. To address this, we propose a synthetic data generation algorithm. Also, we propose a novel multi task temporal information extraction model and investigate whether multi task learning can contribute to performance improvement by exploiting additional training signals with the existing training data. For experiments, we collected a custom dataset containing unstructured texts with temporal information of sleep related activities. Experimental results show that utilising synthetic data can improve the performance when the augmentation factor is 3. The results also show that when multi task learning is used with an appropriate amount of synthetic data, the performance can significantly improve from 82. to 88.6 and from 83.9 to 91.9 regarding micro and macro average exact match scores of normalised time prediction, respectively.","url":"https:\/\/aclanthology.org\/2021.wnut-1.29"},{"ID":"shreevastava-foltz-2021-detecting","methods":["support vector machine"],"center_method":["support vector machine"],"tasks":["cognitive behavioral therapy"],"center_task":[null],"Goal":["Good Health and Well-Being"],"text":"Detecting Cognitive Distortions from Patient-Therapist Interactions. An important part of Cognitive Behavioral Therapy (CBT) is to recognize and restructure certain negative thinking patterns that are also known as cognitive distortions. This project aims to detect these distortions using natural language processing. We compare and contrast different types of linguistic features as well as different classification algorithms and explore the limitations of applying these techniques on a small dataset. We find that pretrained Sentence-BERT embeddings to train an SVM classifier yields the best results with an F1-score of 0.79. Lastly, we discuss how this work provides insights into the types of linguistic features that are inherent in cognitive distortions.","label":1,"title_clean":"Detecting Cognitive Distortions from Patient Therapist Interactions","abstract_clean":"An important part of Cognitive Behavioral Therapy (CBT) is to recognize and restructure certain negative thinking patterns that are also known as cognitive distortions. This project aims to detect these distortions using natural language processing. We compare and contrast different types of linguistic features as well as different classification algorithms and explore the limitations of applying these techniques on a small dataset. We find that pretrained Sentence BERT embeddings to train an SVM classifier yields the best results with an F1 score of 0.79. Lastly, we discuss how this work provides insights into the types of linguistic features that are inherent in cognitive distortions.","url":"https:\/\/aclanthology.org\/2021.clpsych-1.17.pdf"},{"ID":"si-etal-2020-new","methods":["tf - idf model"],"center_method":[null],"tasks":["claim check worthiness prediction","background training","claim detection"],"center_task":[null,null,"claim detection"],"Goal":["Peace, Justice and Strong Institutions"],"text":"A New Approach to Claim Check-Worthiness Prediction and Claim Verification. The more we are advancing towards a modern world, the more it opens the path to falsification in every aspect of life. Even in case of knowing the surrounding, common people can not judge the actual scenario as the promises, comments and opinions of the influential people at power keep changing every day. Therefore computationally determining the truthfulness of such claims and comments has a very important societal impact. This paper describes a unique method to extract check-worthy claims from the 2016 US presidential debates and verify the truthfulness of the check-worthy claims. We classify the claims for check-worthiness with our modified Tf-Idf model which is used in background training on fact-checking news articles (NBC News and Washington Post). We check the truthfulness of the claims by using POS, sentiment score and cosine similarity features.","label":1,"title_clean":"A New Approach to Claim Check Worthiness Prediction and Claim Verification","abstract_clean":"The more we are advancing towards a modern world, the more it opens the path to falsification in every aspect of life. Even in case of knowing the surrounding, common people can not judge the actual scenario as the promises, comments and opinions of the influential people at power keep changing every day. Therefore computationally determining the truthfulness of such claims and comments has a very important societal impact. This paper describes a unique method to extract check worthy claims from the 2016 US presidential debates and verify the truthfulness of the check worthy claims. We classify the claims for check worthiness with our modified Tf Idf model which is used in background training on fact checking news articles (NBC News and Washington Post). We check the truthfulness of the claims by using POS, sentiment score and cosine similarity features.","url":"https:\/\/aclanthology.org\/2020.icon-main.20"},{"ID":"sigurbergsson-derczynski-2020-offensive","methods":["automatic methods","automatic classification systems"],"center_method":[null,null],"tasks":["offensive language and hate speech detection","detection"],"center_task":[null,"detection"],"Goal":["Peace, Justice and Strong Institutions"],"text":"Offensive Language and Hate Speech Detection for Danish. The presence of offensive language on social media platforms and the implications this poses is becoming a major concern in modern society. Given the enormous amount of content created every day, automatic methods are required to detect and deal with this type of content. Until now, most of the research has focused on solving the problem for the English language, while the problem is multilingual. We construct a Danish dataset DKHATE containing user-generated comments from various social media platforms, and to our knowledge, the first of its kind, annotated for various types and target of offensive language. We develop four automatic classification systems, each designed to work for both the English and the Danish language. In the detection of offensive language in English, the best performing system achieves a macro averaged F1-score of 0.74, and the best performing system for Danish achieves a macro averaged F1-score of 0.70. In the detection of whether or not an offensive post is targeted, the best performing system for English achieves a macro averaged F1-score of 0.62, while the best performing system for Danish achieves a macro averaged F1-score of 0.73. Finally, in the detection of the target type in a targeted offensive post, the best performing system for English achieves a macro averaged F1-score of 0.56, and the best performing system for Danish achieves a macro averaged F1-score of 0.63. Our work for both the English and the Danish language captures the type and targets of offensive language, and present automatic methods for detecting different kinds of offensive language such as hate speech and cyberbullying.","label":1,"title_clean":"Offensive Language and Hate Speech Detection for Danish","abstract_clean":"The presence of offensive language on social media platforms and the implications this poses is becoming a major concern in modern society. Given the enormous amount of content created every day, automatic methods are required to detect and deal with this type of content. Until now, most of the research has focused on solving the problem for the English language, while the problem is multilingual. We construct a Danish dataset DKHATE containing user generated comments from various social media platforms, and to our knowledge, the first of its kind, annotated for various types and target of offensive language. We develop four automatic classification systems, each designed to work for both the English and the Danish language. In the detection of offensive language in English, the best performing system achieves a macro averaged F1 score of 0.74, and the best performing system for Danish achieves a macro averaged F1 score of 0.70. In the detection of whether or not an offensive post is targeted, the best performing system for English achieves a macro averaged F1 score of 0.62, while the best performing system for Danish achieves a macro averaged F1 score of 0.73. Finally, in the detection of the target type in a targeted offensive post, the best performing system for English achieves a macro averaged F1 score of 0.56, and the best performing system for Danish achieves a macro averaged F1 score of 0.63. Our work for both the English and the Danish language captures the type and targets of offensive language, and present automatic methods for detecting different kinds of offensive language such as hate speech and cyberbullying.","url":"https:\/\/aclanthology.org\/2020.lrec-1.430"},{"ID":"sikdar-etal-2018-flytxt","methods":["conditional random field"],"center_method":["conditional random field"],"tasks":["semeval"],"center_task":["semeval"],"Goal":["Peace, Justice and Strong Institutions"],"text":"Flytxt\\_NTNU at SemEval-2018 Task 8: Identifying and Classifying Malware Text Using Conditional Random Fields and Na\\\"\\ive Bayes Classifiers. Cybersecurity risks such as malware threaten the personal safety of users, but to identify malware text is a major challenge. The paper proposes a supervised learning approach to identifying malware sentences given a document (subTask1 of SemEval 2018, Task 8), as well as to classifying malware tokens in the sentences (subTask2). The approach achieved good results, ranking second of twelve participants for both subtasks, with F-scores of 57% for subTask1 and 28% for subTask2.","label":1,"title_clean":"Flytxt\\_NTNU at SemEval 2018 Task 8: Identifying and Classifying Malware Text Using Conditional Random Fields and Na\\\"\\ive Bayes Classifiers","abstract_clean":"Cybersecurity risks such as malware threaten the personal safety of users, but to identify malware text is a major challenge. The paper proposes a supervised learning approach to identifying malware sentences given a document (subTask1 of SemEval 2018, Task 8), as well as to classifying malware tokens in the sentences (subTask2). The approach achieved good results, ranking second of twelve participants for both subtasks, with F scores of 57% for subTask1 and 28% for subTask2.","url":"https:\/\/aclanthology.org\/S18-1144.pdf"},{"ID":"sinclair-etal-2018-ability","methods":["tutors"],"center_method":[null],"tasks":["second language tutorial dialogue","alignment","conversational and task based dialogues","lexical priming"],"center_task":[null,null,null,null],"Goal":["Quality Education"],"text":"Does Ability Affect Alignment in Second Language Tutorial Dialogue?. The role of alignment between interlocutors in second language learning is different to that in fluent conversational dialogue. Learners gain linguistic skill through increased alignment, yet the extent to which they can align will be constrained by their ability. Tutors may use alignment to teach and encourage the student, yet still must push the student and correct their errors, decreasing alignment. To understand how learner ability interacts with alignment, we measure the influence of ability on lexical priming, an indicator of alignment. We find that lexical priming in learner-tutor dialogues differs from that in conversational and task-based dialogues, and we find evidence that alignment increases with ability and with word complexity.","label":1,"title_clean":"Does Ability Affect Alignment in Second Language Tutorial Dialogue?","abstract_clean":"The role of alignment between interlocutors in second language learning is different to that in fluent conversational dialogue. Learners gain linguistic skill through increased alignment, yet the extent to which they can align will be constrained by their ability. Tutors may use alignment to teach and encourage the student, yet still must push the student and correct their errors, decreasing alignment. To understand how learner ability interacts with alignment, we measure the influence of ability on lexical priming, an indicator of alignment. We find that lexical priming in learner tutor dialogues differs from that in conversational and task based dialogues, and we find evidence that alignment increases with ability and with word complexity.","url":"https:\/\/aclanthology.org\/W18-5005"},{"ID":"singh-etal-2020-newssweeper","methods":["roberta","bert language model","tagging techniques","context aware rich feature representations"],"center_method":["roberta",null,null,null],"tasks":["propaganda technique classification subtask","detection of propaganda techniques","named entity recognition","span identification","semeval"],"center_task":[null,null,"named entity recognition",null,"semeval"],"Goal":["Peace, Justice and Strong Institutions"],"text":"newsSweeper at SemEval-2020 Task 11: Context-Aware Rich Feature Representations for Propaganda Classification. This paper describes our submissions to SemEval 2020 Task 11: Detection of Propaganda Techniques in News Articles for each of the two subtasks of Span Identification and Technique Classification. We make use of pre-trained BERT language model enhanced with tagging techniques developed for the task of Named Entity Recognition (NER), to develop a system for identifying propaganda spans in the text. For the second subtask, we incorporate contextual features in a pre-trained RoBERTa model for the classification of propaganda techniques. We were ranked 5 th in the propaganda technique classification subtask.","label":1,"title_clean":"newsSweeper at SemEval 2020 Task 11: Context Aware Rich Feature Representations for Propaganda Classification","abstract_clean":"This paper describes our submissions to SemEval 2020 Task 11: Detection of Propaganda Techniques in News Articles for each of the two subtasks of Span Identification and Technique Classification. We make use of pre trained BERT language model enhanced with tagging techniques developed for the task of Named Entity Recognition (NER), to develop a system for identifying propaganda spans in the text. For the second subtask, we incorporate contextual features in a pre trained RoBERTa model for the classification of propaganda techniques. We were ranked 5 th in the propaganda technique classification subtask.","url":"https:\/\/aclanthology.org\/2020.semeval-1.231"},{"ID":"singh-li-2021-exploiting","methods":["transformers","bert","domain adaption","training procedures"],"center_method":["transformers","bert","domain adaption",null],"tasks":["hate speech","old","datalabel scarcity","model training"],"center_task":["hate speech",null,null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Exploiting Auxiliary Data for Offensive Language Detection with Bidirectional Transformers. Offensive language detection (OLD) has received increasing attention due to its societal impact. Recent work shows that bidirectional transformer based methods obtain impressive performance on OLD. However, such methods usually rely on large-scale well-labeled OLD datasets for model training. To address the issue of data\/label scarcity in OLD, in this paper, we propose a simple yet effective domain adaptation approach to train bidirectional transformers. Our approach introduces domain adaptation (DA) training procedures to ALBERT, such that it can effectively exploit auxiliary data from source domains to improve the OLD performance in a target domain. Experimental results on benchmark datasets show that our approach, AL-BERT (DA), obtains the state-of-the-art performance in most cases. Particularly, our approach significantly benefits underrepresented and under-performing classes, with a significant improvement over ALBERT.","label":1,"title_clean":"Exploiting Auxiliary Data for Offensive Language Detection with Bidirectional Transformers","abstract_clean":"Offensive language detection (OLD) has received increasing attention due to its societal impact. Recent work shows that bidirectional transformer based methods obtain impressive performance on OLD. However, such methods usually rely on large scale well labeled OLD datasets for model training. To address the issue of data\/label scarcity in OLD, in this paper, we propose a simple yet effective domain adaptation approach to train bidirectional transformers. Our approach introduces domain adaptation (DA) training procedures to ALBERT, such that it can effectively exploit auxiliary data from source domains to improve the OLD performance in a target domain. Experimental results on benchmark datasets show that our approach, AL BERT (DA), obtains the state of the art performance in most cases. Particularly, our approach significantly benefits underrepresented and under performing classes, with a significant improvement over ALBERT.","url":"https:\/\/aclanthology.org\/2021.woah-1.1"},{"ID":"sinha-etal-2014-capturing","methods":["baseline ngram based approach"],"center_method":[null],"tasks":["predicting student attrition"],"center_task":[null],"Goal":["Quality Education"],"text":"Capturing ``attrition intensifying'' structural traits from didactic interaction sequences of MOOC learners. This work is an attempt to discover hidden structural configurations in learning activity sequences of students in Massive Open Online Courses (MOOCs). Leveraging combined representations of video clickstream interactions and forum activities, we seek to fundamentally understand traits that are predictive of decreasing engagement over time. Grounded in the interdisciplinary field of network science, we follow a graph based approach to successfully extract indicators of active and passive MOOC participation that reflect persistence and regularity in the overall interaction footprint. Using these rich educational semantics, we focus on the problem of predicting student attrition, one of the major highlights of MOOC literature in the recent years. Our results indicate an improvement over a baseline ngram based approach in capturing \"attrition intensifying\" features from the learning activities that MOOC learners engage in. Implications for some compelling future research are discussed.","label":1,"title_clean":"Capturing ``attrition intensifying'' structural traits from didactic interaction sequences of MOOC learners","abstract_clean":"This work is an attempt to discover hidden structural configurations in learning activity sequences of students in Massive Open Online Courses (MOOCs). Leveraging combined representations of video clickstream interactions and forum activities, we seek to fundamentally understand traits that are predictive of decreasing engagement over time. Grounded in the interdisciplinary field of network science, we follow a graph based approach to successfully extract indicators of active and passive MOOC participation that reflect persistence and regularity in the overall interaction footprint. Using these rich educational semantics, we focus on the problem of predicting student attrition, one of the major highlights of MOOC literature in the recent years. Our results indicate an improvement over a baseline ngram based approach in capturing \"attrition intensifying\" features from the learning activities that MOOC learners engage in. Implications for some compelling future research are discussed.","url":"https:\/\/aclanthology.org\/W14-4108"},{"ID":"sizov-ozturk-2013-automatic","methods":["graph based text representation","syntactic and discourse parsers","reasoning chains"],"center_method":[null,null,null],"tasks":["automatic extraction of reasoning chains","aviation investigation reports"],"center_task":[null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Automatic Extraction of Reasoning Chains from Textual Reports. Many organizations possess large collections of textual reports that document how a problem is solved or analysed, e.g. medical patient records, industrial accident reports, lawsuit records and investigation reports. Effective use of expert knowledge contained in these reports may greatly increase productivity of the organization. In this article, we propose a method for automatic extraction of reasoning chains that contain information used by the author of a report to analyse the problem at hand. For this purpose, we developed a graph-based text representation that makes the relations between textual units explicit. This representation is acquired automatically from a report using natural language processing tools including syntactic and discourse parsers. When applied to aviation investigation reports, our method generates reasoning chains that reveal the connection between initial information about the aircraft incident and its causes.","label":1,"title_clean":"Automatic Extraction of Reasoning Chains from Textual Reports","abstract_clean":"Many organizations possess large collections of textual reports that document how a problem is solved or analysed, e.g. medical patient records, industrial accident reports, lawsuit records and investigation reports. Effective use of expert knowledge contained in these reports may greatly increase productivity of the organization. In this article, we propose a method for automatic extraction of reasoning chains that contain information used by the author of a report to analyse the problem at hand. For this purpose, we developed a graph based text representation that makes the relations between textual units explicit. This representation is acquired automatically from a report using natural language processing tools including syntactic and discourse parsers. When applied to aviation investigation reports, our method generates reasoning chains that reveal the connection between initial information about the aircraft incident and its causes.","url":"https:\/\/aclanthology.org\/W13-5009.pdf"},{"ID":"soboroff-harman-2005-novelty","methods":["summarization system"],"center_method":[null],"tasks":["novelty detection"],"center_task":[null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Novelty Detection: The TREC Experience. A challenge for search systems is to detect not only when an item is relevant to the user's information need, but also when it contains something new which the user has not seen before. In the TREC novelty track, the task was to highlight sentences containing relevant and new information in a short, topical document stream. This is analogous to highlighting key parts of a document for another person to read, and this kind of output can be useful as input to a summarization system. Search topics involved both news events and reported opinions on hot-button subjects. When people performed this task, they tended to select small blocks of consecutive sentences, whereas current systems identified many relevant and novel passages. We also found that opinions are much harder to track than events.","label":1,"title_clean":"Novelty Detection: The TREC Experience","abstract_clean":"A challenge for search systems is to detect not only when an item is relevant to the user's information need, but also when it contains something new which the user has not seen before. In the TREC novelty track, the task was to highlight sentences containing relevant and new information in a short, topical document stream. This is analogous to highlighting key parts of a document for another person to read, and this kind of output can be useful as input to a summarization system. Search topics involved both news events and reported opinions on hot button subjects. When people performed this task, they tended to select small blocks of consecutive sentences, whereas current systems identified many relevant and novel passages. We also found that opinions are much harder to track than events.","url":"https:\/\/aclanthology.org\/H05-1014"},{"ID":"soh-etal-2019-legal","methods":["classification","word embeddings","topic model","statistical models","nlp applications","machine learning ml approaches"],"center_method":["classification","word embeddings",null,null,"nlp applications",null],"tasks":["legal area classification"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Legal Area Classification: A Comparative Study of Text Classifiers on Singapore Supreme Court Judgments. This paper conducts a comparative study on the performance of various machine learning (\"ML\") approaches for classifying judgments into legal areas. Using a novel dataset of 6,227 Singapore Supreme Court judgments, we investigate how state-of-the-art NLP methods compare against traditional statistical models when applied to a legal corpus that comprised few but lengthy documents. All approaches tested, including topic model, word embedding, and language model-based classifiers, performed well with as little as a few hundred judgments. However, more work needs to be done to optimize state-of-the-art methods for the legal domain.","label":1,"title_clean":"Legal Area Classification: A Comparative Study of Text Classifiers on Singapore Supreme Court Judgments","abstract_clean":"This paper conducts a comparative study on the performance of various machine learning (\"ML\") approaches for classifying judgments into legal areas. Using a novel dataset of 6,227 Singapore Supreme Court judgments, we investigate how state of the art NLP methods compare against traditional statistical models when applied to a legal corpus that comprised few but lengthy documents. All approaches tested, including topic model, word embedding, and language model based classifiers, performed well with as little as a few hundred judgments. However, more work needs to be done to optimize state of the art methods for the legal domain.","url":"https:\/\/aclanthology.org\/W19-2208.pdf"},{"ID":"sokolova-schramm-2011-building","methods":["patient - based ontology"],"center_method":[null],"tasks":["text mining of social networking web sites"],"center_task":[null],"Goal":["Good Health and Well-Being"],"text":"Building a Patient-based Ontology for User-written Web Messages. We introduce an ontology that is representative of health discussions and vocabulary used by the general public. The ontology structure is built upon general categories of information that patients use when describing their health in clinical encounters. The pilot study shows that the general structure makes the ontology useful in text mining of social networking web sites.","label":1,"title_clean":"Building a Patient based Ontology for User written Web Messages","abstract_clean":"We introduce an ontology that is representative of health discussions and vocabulary used by the general public. The ontology structure is built upon general categories of information that patients use when describing their health in clinical encounters. The pilot study shows that the general structure makes the ontology useful in text mining of social networking web sites.","url":"https:\/\/aclanthology.org\/R11-1111"},{"ID":"solorio-etal-2013-case","methods":["sockpuppets"],"center_method":[null],"tasks":["sockpuppet detection"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"text":"A Case Study of Sockpuppet Detection in Wikipedia. This paper presents preliminary results of using authorship attribution methods for the detection of sockpuppeteering in Wikipedia. Sockpuppets are fake accounts created by malicious users to bypass Wikipedia's regulations. Our dataset is composed of the comments made by the editors on the talk pages. To overcome the limitations of the short lengths of these comments, we use an voting scheme to combine predictions made on individual user entries. We show that this approach is promising and that it can be a viable alternative to the current human process that Wikipedia uses to resolve suspected sockpuppet cases.","label":1,"title_clean":"A Case Study of Sockpuppet Detection in Wikipedia","abstract_clean":"This paper presents preliminary results of using authorship attribution methods for the detection of sockpuppeteering in Wikipedia. Sockpuppets are fake accounts created by malicious users to bypass Wikipedia's regulations. Our dataset is composed of the comments made by the editors on the talk pages. To overcome the limitations of the short lengths of these comments, we use an voting scheme to combine predictions made on individual user entries. We show that this approach is promising and that it can be a viable alternative to the current human process that Wikipedia uses to resolve suspected sockpuppet cases.","url":"https:\/\/aclanthology.org\/W13-1107.pdf"},{"ID":"somasundaran-etal-2009-opinion","methods":["opinion graphs","interdependent framework","local polarity and discourse link classifiers"],"center_method":[null,null,null],"tasks":["polarity classification","joint interpretation of opinions and discourse relations","polarity and discourse classification"],"center_task":[null,null,null],"Goal":["Sustainable Cities and Communities"],"text":"Opinion Graphs for Polarity and Discourse Classification. This work shows how to construct discourse-level opinion graphs to perform a joint interpretation of opinions and discourse relations. Specifically, our opinion graphs enable us to factor in discourse information for polarity classification, and polarity information for discourse-link classification. This interdependent framework can be used to augment and improve the performance of local polarity and discourse-link classifiers.","label":1,"title_clean":"Opinion Graphs for Polarity and Discourse Classification","abstract_clean":"This work shows how to construct discourse level opinion graphs to perform a joint interpretation of opinions and discourse relations. Specifically, our opinion graphs enable us to factor in discourse information for polarity classification, and polarity information for discourse link classification. This interdependent framework can be used to augment and improve the performance of local polarity and discourse link classifiers.","url":"https:\/\/aclanthology.org\/W09-3210"},{"ID":"somers-etal-1997-multilingual","methods":["example based pattern matcher","multilingual internet based employment advertisement system"],"center_method":[null,null],"tasks":["multilingual generation and summarization of job adverts"],"center_task":[null],"Goal":["Decent Work and Economic Growth"],"text":"Multilingual Generation and Summarization of Job Adverts: the TREE Project. A multilingual Internet-based employment advertisement system is described. Job ads are submitted as e-mail texts, analysed by an example-based pattern matcher and stored in language-independent schemas in an object-oriented database. Users can search the database in their own language and get customized summaries of the job ads. The query engine uses symbolic case-based reasoning techniques, while the generation module integrates canned text, templates, and grammar rules to produce texts and hypertexts in a simple way.","label":1,"title_clean":"Multilingual Generation and Summarization of Job Adverts: the TREE Project","abstract_clean":"A multilingual Internet based employment advertisement system is described. Job ads are submitted as e mail texts, analysed by an example based pattern matcher and stored in language independent schemas in an object oriented database. Users can search the database in their own language and get customized summaries of the job ads. The query engine uses symbolic case based reasoning techniques, while the generation module integrates canned text, templates, and grammar rules to produce texts and hypertexts in a simple way.","url":"https:\/\/aclanthology.org\/A97-1040.pdf"},{"ID":"song-etal-2019-leveraging","methods":["dependency forest","graph neural networks","tree based methods"],"center_method":[null,"graph neural networks",null],"tasks":["relation extraction","medical domain"],"center_task":["relation extraction",null],"Goal":["Good Health and Well-Being"],"text":"Leveraging Dependency Forest for Neural Medical Relation Extraction. Medical relation extraction discovers relations between entity mentions in text, such as research articles. For this task, dependency syntax has been recognized as a crucial source of features. Yet in the medical domain, 1best parse trees suffer from relatively low accuracies, diminishing their usefulness. We investigate a method to alleviate this problem by utilizing dependency forests. Forests contain many possible decisions and therefore have higher recall but more noise compared with 1-best outputs. A graph neural network is used to represent the forests, automatically distinguishing the useful syntactic information from parsing noise. Results on two biomedical benchmarks show that our method outperforms the standard tree-based methods, giving the state-of-the-art results in the literature.","label":1,"title_clean":"Leveraging Dependency Forest for Neural Medical Relation Extraction","abstract_clean":"Medical relation extraction discovers relations between entity mentions in text, such as research articles. For this task, dependency syntax has been recognized as a crucial source of features. Yet in the medical domain, 1best parse trees suffer from relatively low accuracies, diminishing their usefulness. We investigate a method to alleviate this problem by utilizing dependency forests. Forests contain many possible decisions and therefore have higher recall but more noise compared with 1 best outputs. A graph neural network is used to represent the forests, automatically distinguishing the useful syntactic information from parsing noise. Results on two biomedical benchmarks show that our method outperforms the standard tree based methods, giving the state of the art results in the literature.","url":"https:\/\/aclanthology.org\/D19-1020"},{"ID":"song-etal-2020-multi","methods":["pre training","essay scorer","supervised crossprompt fine tuning"],"center_method":[null,null,null],"tasks":["weakly supervised pre training","automated chinese essay scoring"],"center_task":[null,null],"Goal":["Quality Education"],"text":"Multi-Stage Pre-training for Automated Chinese Essay Scoring. This paper proposes a pre-training based automated Chinese essay scoring method. The method involves three components: weakly supervised pre-training, supervised crossprompt fine-tuning and supervised targetprompt fine-tuning. An essay scorer is first pretrained on a large essay dataset covering diverse topics and with coarse ratings, i.e., good and poor, which are used as a kind of weak supervision. The pre-trained essay scorer would be further fine-tuned on previously rated essays from existing prompts, which have the same score range with the target prompt and provide extra supervision. At last, the scorer is fine-tuned on the target-prompt training data. The evaluation on four prompts shows that this method can improve a state-of-the-art neural essay scorer in terms of effectiveness and domain adaptation ability, while in-depth analysis also reveals its limitations.","label":1,"title_clean":"Multi Stage Pre training for Automated Chinese Essay Scoring","abstract_clean":"This paper proposes a pre training based automated Chinese essay scoring method. The method involves three components: weakly supervised pre training, supervised crossprompt fine tuning and supervised targetprompt fine tuning. An essay scorer is first pretrained on a large essay dataset covering diverse topics and with coarse ratings, i.e., good and poor, which are used as a kind of weak supervision. The pre trained essay scorer would be further fine tuned on previously rated essays from existing prompts, which have the same score range with the target prompt and provide extra supervision. At last, the scorer is fine tuned on the target prompt training data. The evaluation on four prompts shows that this method can improve a state of the art neural essay scorer in terms of effectiveness and domain adaptation ability, while in depth analysis also reveals its limitations.","url":"https:\/\/aclanthology.org\/2020.emnlp-main.546"},{"ID":"song-etal-2020-using","methods":["deep neural network","lightweight context encoder"],"center_method":["deep neural network",null],"tasks":["suicide risk","classifying suicidal behaviour","clinical surveillance","identifying statements"],"center_task":[null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"Using Deep Neural Networks with Intra- and Inter-Sentence Context to Classify Suicidal Behaviour. Identifying statements related to suicidal behaviour in psychiatric electronic health records (EHRs) is an important step when modeling that behaviour, and when assessing suicide risk. We apply a deep neural network based classification model with a lightweight context encoder, to classify sentence level suicidal behaviour in EHRs. We show that incorporating information from sentences to left and right of the target sentence significantly improves classification accuracy. Our approach achieved the best performance when classifying suicidal behaviour in Autism Spectrum Disorder patient records. The results could have implications for suicidality research and clinical surveillance.","label":1,"title_clean":"Using Deep Neural Networks with Intra and Inter Sentence Context to Classify Suicidal Behaviour","abstract_clean":"Identifying statements related to suicidal behaviour in psychiatric electronic health records (EHRs) is an important step when modeling that behaviour, and when assessing suicide risk. We apply a deep neural network based classification model with a lightweight context encoder, to classify sentence level suicidal behaviour in EHRs. We show that incorporating information from sentences to left and right of the target sentence significantly improves classification accuracy. Our approach achieved the best performance when classifying suicidal behaviour in Autism Spectrum Disorder patient records. The results could have implications for suicidality research and clinical surveillance.","url":"https:\/\/aclanthology.org\/2020.lrec-1.163"},{"ID":"sotnikova-etal-2021-analyzing","methods":["inference)"],"center_method":[null],"tasks":["language inference","generated inferences","analyzing stereotypes","generative text inference tasks"],"center_task":[null,null,null,null],"Goal":["Peace, Justice and Strong Institutions","Reduced Inequalities"],"text":"Analyzing Stereotypes in Generative Text Inference Tasks. Stereotypes are inferences drawn about people based on their demographic attributes, which may result in harms to users when a system is deployed. In generative language-inference tasks, given a premise, a model produces plausible hypotheses that follow either logically (natural language inference) or commonsensically (commonsense inference). Such tasks are therefore a fruitful setting in which to explore the degree to which NLP systems encode stereotypes. In our work, we study how stereotypes manifest when the potential targets of stereotypes are situated in real-life, neutral contexts. We collect human judgments on the presence of stereotypes in generated inferences, and compare how perceptions of stereotypes vary due to annotator positionality. Domain Target Categories Gender man, woman, non-binary person, trans man, trans woman, cis man, cis woman","label":1,"title_clean":"Analyzing Stereotypes in Generative Text Inference Tasks","abstract_clean":"Stereotypes are inferences drawn about people based on their demographic attributes, which may result in harms to users when a system is deployed. In generative language inference tasks, given a premise, a model produces plausible hypotheses that follow either logically (natural language inference) or commonsensically (commonsense inference). Such tasks are therefore a fruitful setting in which to explore the degree to which NLP systems encode stereotypes. In our work, we study how stereotypes manifest when the potential targets of stereotypes are situated in real life, neutral contexts. We collect human judgments on the presence of stereotypes in generated inferences, and compare how perceptions of stereotypes vary due to annotator positionality. Domain Target Categories Gender man, woman, non binary person, trans man, trans woman, cis man, cis woman","url":"https:\/\/aclanthology.org\/2021.findings-acl.355.pdf"},{"ID":"spangher-etal-2020-enabling","methods":["event extraction","roberta classifier","classification","co training"],"center_method":["event extraction",null,"classification",null],"tasks":["social science phenomena","cross corpora comparisons","classify policy announcements","nlp transfer learning task","low resource transfer learning"],"center_task":[null,null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"Enabling Low-Resource Transfer Learning across COVID-19 Corpora by Combining Event-Extraction and Co-Training. Social-science investigations can benefit from a direct comparison of heterogenous corpora: in this work, we compare U.S. state-level COVID-19 policy announcements with policy discussions on Twitter. To perform this task, we require classifiers with high transfer accuracy to both (1) classify policy announcements and (2) classify tweets. We find that cotraining using event-extraction views significantly improves the transfer accuracy of our RoBERTa classifier by 3% above a RoBERTa baseline and 11% above other baselines. The same improvements are not observed for baseline views. With a set of 576 COVID-19 policy announcements, hand-labeled into 1 of 6 categories, our classifier observes a maximum transfer accuracy of .77 f1-score on a handvalidated set of tweets. This work represents the first known application of these techniques to an NLP transfer learning task and facilitates cross-corpora comparisons necessary for studies of social science phenomena.","label":1,"title_clean":"Enabling Low Resource Transfer Learning across COVID 19 Corpora by Combining Event Extraction and Co Training","abstract_clean":"Social science investigations can benefit from a direct comparison of heterogenous corpora: in this work, we compare U.S. state level COVID 19 policy announcements with policy discussions on Twitter. To perform this task, we require classifiers with high transfer accuracy to both (1) classify policy announcements and (2) classify tweets. We find that cotraining using event extraction views significantly improves the transfer accuracy of our RoBERTa classifier by 3% above a RoBERTa baseline and 11% above other baselines. The same improvements are not observed for baseline views. With a set of 576 COVID 19 policy announcements, hand labeled into 1 of 6 categories, our classifier observes a maximum transfer accuracy of .77 f1 score on a handvalidated set of tweets. This work represents the first known application of these techniques to an NLP transfer learning task and facilitates cross corpora comparisons necessary for studies of social science phenomena.","url":"https:\/\/aclanthology.org\/2020.nlpcovid19-acl.4"},{"ID":"srivastava-etal-2018-identifying","methods":["human moderation","capsule network","focal loss"],"center_method":[null,null,null],"tasks":["data augmentation","transliteration","class imbalance"],"center_task":["data augmentation","transliteration",null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Identifying Aggression and Toxicity in Comments using Capsule Network. Aggression and related activities like trolling, hate speech etc. involve toxic comments in various forms. These are common scenarios in today's time and websites react by shutting down their comment sections. To tackle this, an algorithmic solution is preferred to human moderation which is slow and expensive. In this paper, we propose a single model capsule network with focal loss to achieve this task which is suitable for production environment. Our model achieves competitive results over other strong baseline methods, which show its effectiveness and that focal loss exhibits significant improvement in such cases where class imbalance is a regular issue. Additionally, we show that the problem of extensive data preprocessing, data augmentation can be tackled by capsule networks implicitly. We achieve an overall ROC AUC of 98.46 on Kaggletoxic comment dataset and show that it beats other architectures by a good margin. As comments tend to be written in more than one language, and transliteration is a common problem, we further show that our model handles this effectively by applying our model on TRAC shared task dataset which contains comments in code-mixed Hindi-English.","label":1,"title_clean":"Identifying Aggression and Toxicity in Comments using Capsule Network","abstract_clean":"Aggression and related activities like trolling, hate speech etc. involve toxic comments in various forms. These are common scenarios in today's time and websites react by shutting down their comment sections. To tackle this, an algorithmic solution is preferred to human moderation which is slow and expensive. In this paper, we propose a single model capsule network with focal loss to achieve this task which is suitable for production environment. Our model achieves competitive results over other strong baseline methods, which show its effectiveness and that focal loss exhibits significant improvement in such cases where class imbalance is a regular issue. Additionally, we show that the problem of extensive data preprocessing, data augmentation can be tackled by capsule networks implicitly. We achieve an overall ROC AUC of 98.46 on Kaggletoxic comment dataset and show that it beats other architectures by a good margin. As comments tend to be written in more than one language, and transliteration is a common problem, we further show that our model handles this effectively by applying our model on TRAC shared task dataset which contains comments in code mixed Hindi English.","url":"https:\/\/aclanthology.org\/W18-4412"},{"ID":"stafanovics-etal-2020-mitigating","methods":["machine translation"],"center_method":["machine translation"],"tasks":["machine translation","gender bias mitigation","translate"],"center_task":["machine translation","gender bias mitigation",null],"Goal":["Gender Equality"],"text":"Mitigating Gender Bias in Machine Translation with Target Gender Annotations. When translating \"The secretary asked for details.\" to a language with grammatical gender, it might be necessary to determine the gender of the subject \"secretary\". If the sentence does not contain the necessary information, it is not always possible to disambiguate. In such cases, machine translation systems select the most common translation option, which often corresponds to the stereotypical translations, thus potentially exacerbating prejudice and marginalisation of certain groups and people. We argue that the information necessary for an adequate translation can not always be deduced from the sentence being translated or even might depend on external knowledge. Therefore, in this work, we propose to decouple the task of acquiring the necessary information from the task of learning to translate correctly when such information is available. To that end, we present a method for training machine translation systems to use word-level annotations containing information about subject's gender. To prepare training data, we annotate regular source language words with grammatical gender information of the corresponding target language words. Using such data to train machine translation systems reduces their reliance on gender stereotypes when information about the subject's gender is available. Our experiments on five language pairs show that this allows improving accuracy on the WinoMT test set by up to 25.8 percentage points.","label":1,"title_clean":"Mitigating Gender Bias in Machine Translation with Target Gender Annotations","abstract_clean":"When translating \"The secretary asked for details.\" to a language with grammatical gender, it might be necessary to determine the gender of the subject \"secretary\". If the sentence does not contain the necessary information, it is not always possible to disambiguate. In such cases, machine translation systems select the most common translation option, which often corresponds to the stereotypical translations, thus potentially exacerbating prejudice and marginalisation of certain groups and people. We argue that the information necessary for an adequate translation can not always be deduced from the sentence being translated or even might depend on external knowledge. Therefore, in this work, we propose to decouple the task of acquiring the necessary information from the task of learning to translate correctly when such information is available. To that end, we present a method for training machine translation systems to use word level annotations containing information about subject's gender. To prepare training data, we annotate regular source language words with grammatical gender information of the corresponding target language words. Using such data to train machine translation systems reduces their reliance on gender stereotypes when information about the subject's gender is available. Our experiments on five language pairs show that this allows improving accuracy on the WinoMT test set by up to 25.8 percentage points.","url":"https:\/\/aclanthology.org\/2020.wmt-1.73"},{"ID":"stajner-etal-2017-effects","methods":["online processing techniques"],"center_method":[null],"tasks":["eye tracking studies"],"center_task":[null],"Goal":["Quality Education","Reduced Inequalities"],"text":"Effects of Lexical Properties on Viewing Time per Word in Autistic and Neurotypical Readers. Eye tracking studies from the past few decades have shaped the way we think of word complexity and cognitive load: words that are long, rare and ambiguous are more difficult to read. However, online processing techniques have been scarcely applied to investigating the reading difficulties of people with autism and what vocabulary is challenging for them. We present parallel gaze data obtained from adult readers with autism and a control group of neurotypical readers and show that the former required higher cognitive effort to comprehend the texts as evidenced by three gaze-based measures. We divide all words into four classes based on their viewing times for both groups and investigate the relationship between longer viewing times and word length, word frequency, and four cognitively-based measures (word concreteness, familiarity, age of acquisition and imagability).","label":1,"title_clean":"Effects of Lexical Properties on Viewing Time per Word in Autistic and Neurotypical Readers","abstract_clean":"Eye tracking studies from the past few decades have shaped the way we think of word complexity and cognitive load: words that are long, rare and ambiguous are more difficult to read. However, online processing techniques have been scarcely applied to investigating the reading difficulties of people with autism and what vocabulary is challenging for them. We present parallel gaze data obtained from adult readers with autism and a control group of neurotypical readers and show that the former required higher cognitive effort to comprehend the texts as evidenced by three gaze based measures. We divide all words into four classes based on their viewing times for both groups and investigate the relationship between longer viewing times and word length, word frequency, and four cognitively based measures (word concreteness, familiarity, age of acquisition and imagability).","url":"https:\/\/aclanthology.org\/W17-5030.pdf"},{"ID":"stathopoulos-teufel-2015-retrieval","methods":["lucene's vectorspace model implementation"],"center_method":[null],"tasks":["mathematical information retrieval","prior extraction of technical terms"],"center_task":[null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Retrieval of Research-level Mathematical Information Needs: A Test Collection and Technical Terminology Experiment. In this paper, we present a test collection for mathematical information retrieval composed of real-life, researchlevel mathematical information needs. Topics and relevance judgements have been procured from the on-line collaboration website MathOverflow by delegating domain-specific decisions to experts on-line. With our test collection, we construct a baseline using Lucene's vectorspace model implementation and conduct an experiment to investigate how prior extraction of technical terms from mathematical text can affect retrieval efficiency. We show that by boosting the importance of technical terms, statistically significant improvements in retrieval performance can be obtained over the baseline.","label":1,"title_clean":"Retrieval of Research level Mathematical Information Needs: A Test Collection and Technical Terminology Experiment","abstract_clean":"In this paper, we present a test collection for mathematical information retrieval composed of real life, researchlevel mathematical information needs. Topics and relevance judgements have been procured from the on line collaboration website MathOverflow by delegating domain specific decisions to experts on line. With our test collection, we construct a baseline using Lucene's vectorspace model implementation and conduct an experiment to investigate how prior extraction of technical terms from mathematical text can affect retrieval efficiency. We show that by boosting the importance of technical terms, statistically significant improvements in retrieval performance can be obtained over the baseline.","url":"https:\/\/aclanthology.org\/P15-2055"},{"ID":"sun-etal-2020-semi","methods":["supervised learning approach","semi supervised approach"],"center_method":[null,null],"tasks":["category specific product tags","e commerce"],"center_task":[null,"e commerce"],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Semi-supervised Category-specific Review Tagging on Indonesian E-Commerce Product Reviews. Product reviews are a huge source of natural language data in e-commerce applications. Several millions of customers write reviews regarding a variety of topics. We categorize these topics into two groups as either \"category-specific\" topics or as \"generic\" topics that span multiple product categories. While we can use a supervised learning approach to tag review text for generic topics, it is impossible to use supervised approaches to tag category-specific topics due to the sheer number of possible topics for each category. In this paper, we present an approach to tag each review with several product category-specific tags on Indonesian language product reviews using a semi-supervised approach. We show that our proposed method can work at scale on real product reviews at Tokopedia 1 , a major e-commerce platform in Indonesia. Manual evaluation shows that the proposed method can efficiently generate category-specific product tags.","label":1,"title_clean":"Semi supervised Category specific Review Tagging on Indonesian E Commerce Product Reviews","abstract_clean":"Product reviews are a huge source of natural language data in e commerce applications. Several millions of customers write reviews regarding a variety of topics. We categorize these topics into two groups as either \"category specific\" topics or as \"generic\" topics that span multiple product categories. While we can use a supervised learning approach to tag review text for generic topics, it is impossible to use supervised approaches to tag category specific topics due to the sheer number of possible topics for each category. In this paper, we present an approach to tag each review with several product category specific tags on Indonesian language product reviews using a semi supervised approach. We show that our proposed method can work at scale on real product reviews at Tokopedia 1 , a major e commerce platform in Indonesia. Manual evaluation shows that the proposed method can efficiently generate category specific product tags.","url":"https:\/\/aclanthology.org\/2020.ecnlp-1.9"},{"ID":"sun-etal-2021-medai","methods":["data privacy","negation aware pre training"],"center_method":[null,null],"tasks":["source free negation detection","domain adaption","semantic processing","source free domain adaptation","semeval 2019"],"center_task":[null,"domain adaption",null,null,"semeval 2019"],"Goal":["Good Health and Well-Being"],"text":"MedAI at SemEval-2021 Task 10: Negation-aware Pre-training for Source-free Negation Detection Domain Adaptation. Due to the increasing concerns for data privacy, source-free unsupervised domain adaptation attracts more and more research attention, where only a trained source model is assumed to be available, while the labeled source data remains private. To get promising adaptation results, we need to find effective ways to transfer knowledge learned in source domain and leverage useful domain specific information from target domain at the same time. This paper describes our winning contribution to SemEval 2021 Task 10: Source-Free Domain Adaptation for Semantic Processing. Our key idea is to leverage the model trained on source domain data to generate pseudo labels for target domain samples. Besides, we propose Negationaware Pre-training (NAP) to incorporate negation knowledge into model. Our method wins the 1st place with F1-score of 0.822 on the official blind test set of Negation Detection Track.","label":1,"title_clean":"MedAI at SemEval 2021 Task 10: Negation aware Pre training for Source free Negation Detection Domain Adaptation","abstract_clean":"Due to the increasing concerns for data privacy, source free unsupervised domain adaptation attracts more and more research attention, where only a trained source model is assumed to be available, while the labeled source data remains private. To get promising adaptation results, we need to find effective ways to transfer knowledge learned in source domain and leverage useful domain specific information from target domain at the same time. This paper describes our winning contribution to SemEval 2021 Task 10: Source Free Domain Adaptation for Semantic Processing. Our key idea is to leverage the model trained on source domain data to generate pseudo labels for target domain samples. Besides, we propose Negationaware Pre training (NAP) to incorporate negation knowledge into model. Our method wins the 1st place with F1 score of 0.822 on the official blind test set of Negation Detection Track.","url":"https:\/\/aclanthology.org\/2021.semeval-1.183.pdf"},{"ID":"surana-chinagundi-2022-ginius","methods":["hope speech detection"],"center_method":["hope speech detection"],"tasks":["english task"],"center_task":[null],"Goal":["Good Health and Well-Being"],"text":"giniUs @LT-EDI-ACL2022: Aasha: Transformers based Hope-EDI. This paper describes team giniUs' submission to the Hope Speech Detection for Equality, Diversity and Inclusion Shared Task organised by LT-EDI ACL 2022. We have fine-tuned the RoBERTa-large pre-trained model and extracted the last four Decoder layers to build a binary classifier. Our best result on the leaderboard achieves a weighted F1 score of 0.86 and a Macro F1 score of 0.51 for English. We rank fourth in the English task. We have opensourced our code implementations on GitHub to facilitate easy reproducibility by the scientific community.","label":1,"title_clean":"giniUs @LT EDI ACL2022: Aasha: Transformers based Hope EDI","abstract_clean":"This paper describes team giniUs' submission to the Hope Speech Detection for Equality, Diversity and Inclusion Shared Task organised by LT EDI ACL 2022. We have fine tuned the RoBERTa large pre trained model and extracted the last four Decoder layers to build a binary classifier. Our best result on the leaderboard achieves a weighted F1 score of 0.86 and a Macro F1 score of 0.51 for English. We rank fourth in the English task. We have opensourced our code implementations on GitHub to facilitate easy reproducibility by the scientific community.","url":"https:\/\/aclanthology.org\/2022.ltedi-1.43"},{"ID":"swanson-yamangil-2012-correction","methods":["classification","linguistically motivated feature templates","error type selection","log linear classification model"],"center_method":["classification",null,null,null],"tasks":["esl educational aid","correction detection"],"center_task":[null,null],"Goal":["Quality Education"],"text":"Correction Detection and Error Type Selection as an ESL Educational Aid. We present a classifier that discriminates between types of corrections made by teachers of English in student essays. We define a set of linguistically motivated feature templates for a log-linear classification model, train this classifier on sentence pairs extracted from the Cambridge Learner Corpus, and achieve 89% accuracy improving upon a 33% baseline. Furthermore, we incorporate our classifier into a novel application that takes as input a set of corrected essays that have been sentence aligned with their originals and outputs the individual corrections classified by error type. We report the F-Score of our implementation on this task.","label":1,"title_clean":"Correction Detection and Error Type Selection as an ESL Educational Aid","abstract_clean":"We present a classifier that discriminates between types of corrections made by teachers of English in student essays. We define a set of linguistically motivated feature templates for a log linear classification model, train this classifier on sentence pairs extracted from the Cambridge Learner Corpus, and achieve 89% accuracy improving upon a 33% baseline. Furthermore, we incorporate our classifier into a novel application that takes as input a set of corrected essays that have been sentence aligned with their originals and outputs the individual corrections classified by error type. We report the F Score of our implementation on this task.","url":"https:\/\/aclanthology.org\/N12-1037"},{"ID":"tabak-purver-2020-temporal","methods":["temporal mental health dynamics system"],"center_method":[null],"tasks":["temporal mental health dynamics","strategic decision making","distantsupervision of mental health data mining"],"center_task":[null,null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Temporal Mental Health Dynamics on Social Media. We describe a set of experiments for building a temporal mental health dynamics system. We utilise a pre-existing methodology for distantsupervision of mental health data mining from social media platforms and deploy the system during the global COVID-19 pandemic as a case study. Despite the challenging nature of the task, we produce encouraging results, both explicit to the global pandemic and implicit to a global phenomenon, Christmas Depression, supported by the literature. We propose a methodology for providing insight into temporal mental health dynamics to be utilised for strategic decision-making.","label":1,"title_clean":"Temporal Mental Health Dynamics on Social Media","abstract_clean":"We describe a set of experiments for building a temporal mental health dynamics system. We utilise a pre existing methodology for distantsupervision of mental health data mining from social media platforms and deploy the system during the global COVID 19 pandemic as a case study. Despite the challenging nature of the task, we produce encouraging results, both explicit to the global pandemic and implicit to a global phenomenon, Christmas Depression, supported by the literature. We propose a methodology for providing insight into temporal mental health dynamics to be utilised for strategic decision making.","url":"https:\/\/aclanthology.org\/2020.nlpcovid19-2.7"},{"ID":"taghipour-ng-2016-neural","methods":["automated essay scoring systems","neural models","long short term memory networks","feature engineering"],"center_method":[null,"neural models",null,"feature engineering"],"tasks":["automated essay scoring"],"center_task":[null],"Goal":["Quality Education"],"text":"A Neural Approach to Automated Essay Scoring. Traditional automated essay scoring systems rely on carefully designed features to evaluate and score essays. The performance of such systems is tightly bound to the quality of the underlying features. However, it is laborious to manually design the most informative features for such a system. In this paper, we develop an approach based on recurrent neural networks to learn the relation between an essay and its assigned score, without any feature engineering. We explore several neural network models for the task of automated essay scoring and perform some analysis to get some insights of the models. The results show that our best system, which is based on long short-term memory networks, outperforms a strong baseline by 5.6% in terms of quadratic weighted Kappa, without requiring any feature engineering.","label":1,"title_clean":"A Neural Approach to Automated Essay Scoring","abstract_clean":"Traditional automated essay scoring systems rely on carefully designed features to evaluate and score essays. The performance of such systems is tightly bound to the quality of the underlying features. However, it is laborious to manually design the most informative features for such a system. In this paper, we develop an approach based on recurrent neural networks to learn the relation between an essay and its assigned score, without any feature engineering. We explore several neural network models for the task of automated essay scoring and perform some analysis to get some insights of the models. The results show that our best system, which is based on long short term memory networks, outperforms a strong baseline by 5.6% in terms of quadratic weighted Kappa, without requiring any feature engineering.","url":"https:\/\/aclanthology.org\/D16-1193"},{"ID":"tait-etal-1999-mable","methods":["multi lingual authoring tool","mable"],"center_method":[null,null],"tasks":["business"],"center_task":[null],"Goal":["Decent Work and Economic Growth","Industry, Innovation and Infrastrucure"],"text":"MABLe: A Multi-lingual Authoring Tool for Business Letters. ","label":1,"title_clean":"MABLe: A Multi lingual Authoring Tool for Business Letters","abstract_clean":"","url":"https:\/\/aclanthology.org\/1999.tc-1.12"},{"ID":"tanase-etal-2020-upb","methods":["bert","transformer based solutions","roberta","neural models"],"center_method":["bert",null,"roberta","neural models"],"tasks":["hate speech","offenseval 2020 shared task","natural language processing field"],"center_task":["hate speech",null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"UPB at SemEval-2020 Task 12: Multilingual Offensive Language Detection on Social Media by Fine-tuning a Variety of BERT-based Models. Offensive language detection is one of the most challenging problem in the natural language processing field, being imposed by the rising presence of this phenomenon in online social media. This paper describes our Transformer-based solutions for identifying offensive language on Twitter in five languages (i.e., English, Arabic, Danish, Greek, and Turkish), which was employed in Subtask A of the Offenseval 2020 shared task. Several neural architectures (i.e., BERT, mBERT, Roberta, XLM-Roberta, and ALBERT), pre-trained using both single-language and multilingual corpora, were fine-tuned and compared using multiple combinations of datasets. Finally, the highest-scoring models were used for our submissions in the competition, which ranked our","label":1,"title_clean":"UPB at SemEval 2020 Task 12: Multilingual Offensive Language Detection on Social Media by Fine tuning a Variety of BERT based Models","abstract_clean":"Offensive language detection is one of the most challenging problem in the natural language processing field, being imposed by the rising presence of this phenomenon in online social media. This paper describes our Transformer based solutions for identifying offensive language on Twitter in five languages (i.e., English, Arabic, Danish, Greek, and Turkish), which was employed in Subtask A of the Offenseval 2020 shared task. Several neural architectures (i.e., BERT, mBERT, Roberta, XLM Roberta, and ALBERT), pre trained using both single language and multilingual corpora, were fine tuned and compared using multiple combinations of datasets. Finally, the highest scoring models were used for our submissions in the competition, which ranked our","url":"https:\/\/aclanthology.org\/2020.semeval-1.296"},{"ID":"tang-shen-2020-categorizing","methods":["hierarchical attention capsule network","explainable tool","integrated gradients","capsule system"],"center_method":[null,null,null,null],"tasks":["offensive classification","categorizing offensive language in social networks"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Categorizing Offensive Language in Social Networks: A Chinese Corpus, Systems and an Explainable Tool. Recently, more and more data have been generated in the online world, filled with offensive language such as threats, swear words or straightforward insults. It is disgraceful for a progressive society, and then the question arises on how language resources and technologies can cope with this challenge. However, previous work only analyzes the problem as a whole but fails to detect particular types of offensive content in a more fine-grained way, mainly because of the lack of annotated data. In this work, we present a densely annotated data-set COLA (Categorizing Offensive LAnguage), consists of fine-grained insulting language, antisocial language and illegal language. We study different strategies for automatically identifying offensive language on COLA data. Further, we design a capsule system with hierarchical attention to aggregate and fully utilize information, which obtains a state-of-the-art result. Results from experiments prove that our hierarchical attention capsule network (HACN) performs significantly better than existing methods in offensive classification with the precision of 94.37% and recall of 95.28%. We also explain what our model has learned with an explanation tool called Integrated Gradients. Meanwhile, our system's processing speed can handle each sentence in 10msec, suggesting the potential for efficient deployment in real situations.","label":1,"title_clean":"Categorizing Offensive Language in Social Networks: A Chinese Corpus, Systems and an Explainable Tool","abstract_clean":"Recently, more and more data have been generated in the online world, filled with offensive language such as threats, swear words or straightforward insults. It is disgraceful for a progressive society, and then the question arises on how language resources and technologies can cope with this challenge. However, previous work only analyzes the problem as a whole but fails to detect particular types of offensive content in a more fine grained way, mainly because of the lack of annotated data. In this work, we present a densely annotated data set COLA (Categorizing Offensive LAnguage), consists of fine grained insulting language, antisocial language and illegal language. We study different strategies for automatically identifying offensive language on COLA data. Further, we design a capsule system with hierarchical attention to aggregate and fully utilize information, which obtains a state of the art result. Results from experiments prove that our hierarchical attention capsule network (HACN) performs significantly better than existing methods in offensive classification with the precision of 94.37% and recall of 95.28%. We also explain what our model has learned with an explanation tool called Integrated Gradients. Meanwhile, our system's processing speed can handle each sentence in 10msec, suggesting the potential for efficient deployment in real situations.","url":"https:\/\/aclanthology.org\/2020.ccl-1.97.pdf"},{"ID":"tatman-etal-2017-non","methods":["non - lexical features"],"center_method":[null],"tasks":["twitter users' political alignment"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Non-lexical Features Encode Political Affiliation on Twitter. Previous work on classifying Twitter users' political alignment has mainly focused on lexical and social network features. This study provides evidence that political affiliation is also reflected in features which have been previously overlooked: users' discourse patterns (proportion of Tweets that are retweets or replies) and their rate of use of capitalization and punctuation. We find robust differences between politically left-and right-leaning communities with respect to these discourse and sub-lexical features, although they are not enough to train a high-accuracy classifier.","label":1,"title_clean":"Non lexical Features Encode Political Affiliation on Twitter","abstract_clean":"Previous work on classifying Twitter users' political alignment has mainly focused on lexical and social network features. This study provides evidence that political affiliation is also reflected in features which have been previously overlooked: users' discourse patterns (proportion of Tweets that are retweets or replies) and their rate of use of capitalization and punctuation. We find robust differences between politically left and right leaning communities with respect to these discourse and sub lexical features, although they are not enough to train a high accuracy classifier.","url":"https:\/\/aclanthology.org\/W17-2909"},{"ID":"temnikova-cohen-2013-recognizing","methods":["sublanguage model"],"center_method":[null],"tasks":["sublanguage recognition","text mining"],"center_task":[null,"text mining"],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Recognizing Sublanguages in Scientific Journal Articles through Closure Properties. It has long been realized that sublanguages are relevant to natural language processing and text mining. However, practical methods for recognizing or characterizing them have been lacking. This paper describes a publicly available set of tools for sublanguage recognition. Closure properties are used to assess the goodness of fit of two biomedical corpora to the sublanguage model. Scientific journal articles are compared to general English text, and it is shown that the journal articles fit the sublanguage model, while the general English text does not. A number of examples of implications of the sublanguage characteristics for natural language processing are pointed out. The software is made publicly available at [edited for anonymization].","label":1,"title_clean":"Recognizing Sublanguages in Scientific Journal Articles through Closure Properties","abstract_clean":"It has long been realized that sublanguages are relevant to natural language processing and text mining. However, practical methods for recognizing or characterizing them have been lacking. This paper describes a publicly available set of tools for sublanguage recognition. Closure properties are used to assess the goodness of fit of two biomedical corpora to the sublanguage model. Scientific journal articles are compared to general English text, and it is shown that the journal articles fit the sublanguage model, while the general English text does not. A number of examples of implications of the sublanguage characteristics for natural language processing are pointed out. The software is made publicly available at [edited for anonymization].","url":"https:\/\/aclanthology.org\/W13-1909"},{"ID":"tenfjord-etal-2006-ask","methods":["automatic tagger","web search interface","corpus workbench"],"center_method":[null,null,null],"tasks":["corpus query system","manual tag correction","text annotation","tei","ask"],"center_task":[null,null,null,null,null],"Goal":["Quality Education"],"text":"The ASK Corpus - a Language Learner Corpus of Norwegian as a Second Language. In our paper we present the design and interface of ASK, a language learner corpus of Norwegian as a second language which contains essays collected from language tests on two different proficiency levels as well as personal data from the test takers. In addition, the corpus also contains texts and relevant personal data from native Norwegians as control data. The texts as well as the personal data are marked up in XML according to the TEI Guidelines. In order to be able to classify \"errors\" in the texts, we have introduced new attributes to the TEI corr and sic tags. For each error tag, a correct form is also in the text annotation. Finally, we employ an automatic tagger developed for standard Norwegian, the \"Oslo-Bergen Tagger\", together with a facility for manual tag correction. As corpus query system, we are using the Corpus Workbench developed at the University of Stuttgart together with a web search interface developed at Aksis, University of Bergen. The system allows for searching for combinations of words, error types, grammatical annotation and personal data.","label":1,"title_clean":"The ASK Corpus  a Language Learner Corpus of Norwegian as a Second Language","abstract_clean":"In our paper we present the design and interface of ASK, a language learner corpus of Norwegian as a second language which contains essays collected from language tests on two different proficiency levels as well as personal data from the test takers. In addition, the corpus also contains texts and relevant personal data from native Norwegians as control data. The texts as well as the personal data are marked up in XML according to the TEI Guidelines. In order to be able to classify \"errors\" in the texts, we have introduced new attributes to the TEI corr and sic tags. For each error tag, a correct form is also in the text annotation. Finally, we employ an automatic tagger developed for standard Norwegian, the \"Oslo Bergen Tagger\", together with a facility for manual tag correction. As corpus query system, we are using the Corpus Workbench developed at the University of Stuttgart together with a web search interface developed at Aksis, University of Bergen. The system allows for searching for combinations of words, error types, grammatical annotation and personal data.","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2006\/pdf\/573_pdf.pdf"},{"ID":"thieberger-2007-language","methods":["language technology"],"center_method":["language technology"],"tasks":["language documentation","curation of linguistic data"],"center_task":[null,null],"Goal":["Reduced Inequalities"],"text":"Does Language Technology Offer Anything to Small Languages?. The effort currently going into recording the smaller and perhaps more endangered languages of the world may result in computationally tractable documents in those languages, but to date there has not been a tradition of corpus creation for these languages. In this talk I will outline the language situation of Australia's neighbouring region and discuss methods currently used in language documentation, observing that it is quite difficult to get linguists to create reusable records of the languages they record, let alone expecting them to create marked-up corpora. I will highlight the importance of creating shared infrastructure to support our work, including the development of Pacific and Regional Archive for Digital Sources in Endangered Cultures (PARADISEC), a facility for curation of linguistic data.","label":1,"title_clean":"Does Language Technology Offer Anything to Small Languages?","abstract_clean":"The effort currently going into recording the smaller and perhaps more endangered languages of the world may result in computationally tractable documents in those languages, but to date there has not been a tradition of corpus creation for these languages. In this talk I will outline the language situation of Australia's neighbouring region and discuss methods currently used in language documentation, observing that it is quite difficult to get linguists to create reusable records of the languages they record, let alone expecting them to create marked up corpora. I will highlight the importance of creating shared infrastructure to support our work, including the development of Pacific and Regional Archive for Digital Sources in Endangered Cultures (PARADISEC), a facility for curation of linguistic data.","url":"https:\/\/aclanthology.org\/U07-1002"},{"ID":"thorne-etal-2013-automated","methods":["clinical entity recognition techniques","umls metathesaurus"],"center_method":[null,null],"tasks":["automated activity recognition"],"center_task":[null],"Goal":["Good Health and Well-Being"],"text":"Automated Activity Recognition in Clinical Documents. We describe a first experiment on the identification and extraction of computerinterpretable guideline (CIG) components (activities, actors and consumed artifacts) from clinical documents, based on clinical entity recognition techniques. We rely on MetaMap and the UMLS Metathesaurus to provide lexical information, and study the impact of clinical document syntax and semantics on activity recognition.","label":1,"title_clean":"Automated Activity Recognition in Clinical Documents","abstract_clean":"We describe a first experiment on the identification and extraction of computerinterpretable guideline (CIG) components (activities, actors and consumed artifacts) from clinical documents, based on clinical entity recognition techniques. We rely on MetaMap and the UMLS Metathesaurus to provide lexical information, and study the impact of clinical document syntax and semantics on activity recognition.","url":"https:\/\/aclanthology.org\/I13-1160"},{"ID":"tomlinson-etal-2014-mygoal","methods":["hashtags","language model based classifier"],"center_method":[null,null],"tasks":["finding motivations"],"center_task":[null],"Goal":["Good Health and Well-Being"],"text":"\\#mygoal: Finding Motivations on Twitter. Our everyday language reflects our psychological and cognitive state and effects the states of other individuals. In this contribution we look at the intersection between motivational state and language. We create a set of hashtags, which are annotated for the degree to which they are used by individuals to markup language that is indicative of a collection of factors that interact with an individual's motivational state. We look for tags that reflect a goal mention, reward, or a perception of control. Finally, we present results for a language-model based classifier which is able to predict the presence of one of these factors in a tweet with between 69% and 80% accuracy on a balanced testing set. Our approach suggests that hashtags can be used to understand, not just the language of topics, but the deeper psychological and social meaning of a tweet.","label":1,"title_clean":"\\#mygoal: Finding Motivations on Twitter","abstract_clean":"Our everyday language reflects our psychological and cognitive state and effects the states of other individuals. In this contribution we look at the intersection between motivational state and language. We create a set of hashtags, which are annotated for the degree to which they are used by individuals to markup language that is indicative of a collection of factors that interact with an individual's motivational state. We look for tags that reflect a goal mention, reward, or a perception of control. Finally, we present results for a language model based classifier which is able to predict the presence of one of these factors in a tweet with between 69% and 80% accuracy on a balanced testing set. Our approach suggests that hashtags can be used to understand, not just the language of topics, but the deeper psychological and social meaning of a tweet.","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2014\/pdf\/1120_Paper.pdf"},{"ID":"trajanovski-etal-2021-text","methods":["text prediction model","language models"],"center_method":[null,"language models"],"tasks":["chat scenarios","contextual text prediction","real time phrase completion","daily tasks"],"center_task":[null,null,null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"When does text prediction benefit from additional context? An exploration of contextual signals for chat and email messages. Email and chat communication tools are increasingly important for completing daily tasks. Accurate real-time phrase completion can save time and bolster productivity. Modern text prediction algorithms are based on large language models which typically rely on the prior words in a message to predict a completion. We examine how additional contextual signals (from previous messages, time, and subject) affect the performance of a commercial text prediction model. We compare contextual text prediction in chat and email messages from two of the largest commercial platforms Microsoft Teams and Outlook, finding that contextual signals contribute to performance differently between these scenarios. On emails, time context is most beneficial with small relative gains of 2% over baseline. Whereas, in chat scenarios, using a tailored set of previous messages as context yields relative improvements over the baseline between 9.3% and 18.6% across various critical serviceoriented text prediction metrics.","label":1,"title_clean":"When does text prediction benefit from additional context? An exploration of contextual signals for chat and email messages","abstract_clean":"Email and chat communication tools are increasingly important for completing daily tasks. Accurate real time phrase completion can save time and bolster productivity. Modern text prediction algorithms are based on large language models which typically rely on the prior words in a message to predict a completion. We examine how additional contextual signals (from previous messages, time, and subject) affect the performance of a commercial text prediction model. We compare contextual text prediction in chat and email messages from two of the largest commercial platforms Microsoft Teams and Outlook, finding that contextual signals contribute to performance differently between these scenarios. On emails, time context is most beneficial with small relative gains of 2% over baseline. Whereas, in chat scenarios, using a tailored set of previous messages as context yields relative improvements over the baseline between 9.3% and 18.6% across various critical serviceoriented text prediction metrics.","url":"https:\/\/aclanthology.org\/2021.naacl-industry.1.pdf"},{"ID":"treharne-etal-2006-towards","methods":["graphical systems","search engine interface","svdlsa family","matrix manipulation techniques","computational models"],"center_method":[null,null,null,null,"computational models"],"tasks":["cognitive optimisation"],"center_task":[null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Towards Cognitive Optimisation of a Search Engine Interface. Search engine interfaces come in a range of variations from the familiar text-based approach to the more experimental graphical systems. It is rare however that psychological or human factors research is undertaken to properly evaluate or optimize the systems, and to the extent this has been done the results have tended to contradict some of the assumptions that have driven search engine design. Our research is focussed on a model in which at least 100 hits are selected from a corpus of documents based on a set of query words and displayed graphically. Matrix manipulation techniques in the SVD\/LSA family are used to identify significant dimensions and display documents according to a subset of these dimensions. The research questions we are investigating in this context relate to the computational methods (how to rescale the data), the linguistic information (how to characterize a document), and the visual attributes (which linguistic dimensions to display using which attributes).","label":1,"title_clean":"Towards Cognitive Optimisation of a Search Engine Interface","abstract_clean":"Search engine interfaces come in a range of variations from the familiar text based approach to the more experimental graphical systems. It is rare however that psychological or human factors research is undertaken to properly evaluate or optimize the systems, and to the extent this has been done the results have tended to contradict some of the assumptions that have driven search engine design. Our research is focussed on a model in which at least 100 hits are selected from a corpus of documents based on a set of query words and displayed graphically. Matrix manipulation techniques in the SVD\/LSA family are used to identify significant dimensions and display documents according to a subset of these dimensions. The research questions we are investigating in this context relate to the computational methods (how to rescale the data), the linguistic information (how to characterize a document), and the visual attributes (which linguistic dimensions to display using which attributes).","url":"https:\/\/aclanthology.org\/U06-1025"},{"ID":"tubay-costa-jussa-2018-neural","methods":["transformers","attention based mechanisms","neural machine translation architectures"],"center_method":["transformers",null,null],"tasks":["wmt 2018 task","machine translation"],"center_task":[null,"machine translation"],"Goal":["Good Health and Well-Being"],"text":"Neural Machine Translation with the Transformer and Multi-Source Romance Languages for the Biomedical WMT 2018 task. The Transformer architecture has become the state-of-the-art in Machine Translation. This model, which relies on attention-based mechanisms, has outperformed previous neural machine translation architectures in several tasks. In this system description paper, we report details of training neural machine translation with multi-source Romance languages with the Transformer model and in the evaluation frame of the biomedical WMT 2018 task. Using multi-source languages from the same family allows improvements of over 6 BLEU points.","label":1,"title_clean":"Neural Machine Translation with the Transformer and Multi Source Romance Languages for the Biomedical WMT 2018 task","abstract_clean":"The Transformer architecture has become the state of the art in Machine Translation. This model, which relies on attention based mechanisms, has outperformed previous neural machine translation architectures in several tasks. In this system description paper, we report details of training neural machine translation with multi source Romance languages with the Transformer model and in the evaluation frame of the biomedical WMT 2018 task. Using multi source languages from the same family allows improvements of over 6 BLEU points.","url":"https:\/\/aclanthology.org\/W18-6449"},{"ID":"tufis-etal-2020-collection","methods":["elrc infrastructure"],"center_method":[null],"tasks":["annotation","machine translation"],"center_task":["annotation","machine translation"],"Goal":["Peace, Justice and Strong Institutions"],"text":"Collection and Annotation of the Romanian Legal Corpus. We present the Romanian legislative corpus which is a valuable linguistic asset for the development of machine translation systems, especially for under-resourced languages. The knowledge that can be extracted from this resource is necessary for a deeper understanding of how law terminology is used and how it can be made more consistent. At this moment, the corpus contains more than 144k documents representing the legislative body of Romania. This corpus is processed and annotated at different levels: linguistically (tokenized, lemmatized and POS-tagged), dependency parsed, chunked, named entities identified and labeled with IATE terms and EUROVOC descriptors. Each annotated document has a CONLL-U Plus format consisting of 14 columns; in addition to the standard 10-column format, four other types of annotations were added. Moreover the repository will be periodically updated as new legislative texts are published. These will be automatically collected and transmitted to the processing and annotation pipeline. The access to the corpus is provided through ELRC infrastructure.","label":1,"title_clean":"Collection and Annotation of the Romanian Legal Corpus","abstract_clean":"We present the Romanian legislative corpus which is a valuable linguistic asset for the development of machine translation systems, especially for under resourced languages. The knowledge that can be extracted from this resource is necessary for a deeper understanding of how law terminology is used and how it can be made more consistent. At this moment, the corpus contains more than 144k documents representing the legislative body of Romania. This corpus is processed and annotated at different levels: linguistically (tokenized, lemmatized and POS tagged), dependency parsed, chunked, named entities identified and labeled with IATE terms and EUROVOC descriptors. Each annotated document has a CONLL U Plus format consisting of 14 columns; in addition to the standard 10 column format, four other types of annotations were added. Moreover the repository will be periodically updated as new legislative texts are published. These will be automatically collected and transmitted to the processing and annotation pipeline. The access to the corpus is provided through ELRC infrastructure.","url":"https:\/\/aclanthology.org\/2020.lrec-1.337"},{"ID":"tymoshenko-moschitti-2021-strong","methods":["max pooling","transformers","light baseline models"],"center_method":[null,"transformers",null],"tasks":["fever claim verification task","joint inference","fact checking"],"center_task":[null,null,"fact checking"],"Goal":["Peace, Justice and Strong Institutions"],"text":"Strong and Light Baseline Models for Fact-Checking Joint Inference. How to combine several pieces of evidence to verify a claim is an interesting semantic task. Very complex methods have been proposed, combining different evidence vectors using an evidence interaction graph. In this paper, we show that in case of inference based on transformer models, two effective approaches use either (i) a simple application of max pooling over the Transformer evidence vectors; or (ii) computing a weighted sum of the evidence vectors. Our experiments on the FEVER claim verification task show that the methods above achieve the state of the art, constituting strong baseline for much more computationally complex methods.","label":1,"title_clean":"Strong and Light Baseline Models for Fact Checking Joint Inference","abstract_clean":"How to combine several pieces of evidence to verify a claim is an interesting semantic task. Very complex methods have been proposed, combining different evidence vectors using an evidence interaction graph. In this paper, we show that in case of inference based on transformer models, two effective approaches use either (i) a simple application of max pooling over the Transformer evidence vectors; or (ii) computing a weighted sum of the evidence vectors. Our experiments on the FEVER claim verification task show that the methods above achieve the state of the art, constituting strong baseline for much more computationally complex methods.","url":"https:\/\/aclanthology.org\/2021.findings-acl.426"},{"ID":"tziafas-etal-2021-fighting","methods":["holistic bert ensemble","tokofou","majority voting approach","transformer based pre trained encoders"],"center_method":[null,null,null,null],"tasks":["ranking","misinformation detection tasks"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Fighting the COVID-19 Infodemic with a Holistic BERT Ensemble. This paper describes the TOKOFOU system, an ensemble model for misinformation detection tasks based on six different transformer-based pre-trained encoders, implemented in the context of the COVID-19 Infodemic Shared Task for English. We fine tune each model on each of the task's questions and aggregate their prediction scores using a majority voting approach. TOKOFOU obtains an overall F1 score of 89.7%, ranking first.","label":1,"title_clean":"Fighting the COVID 19 Infodemic with a Holistic BERT Ensemble","abstract_clean":"This paper describes the TOKOFOU system, an ensemble model for misinformation detection tasks based on six different transformer based pre trained encoders, implemented in the context of the COVID 19 Infodemic Shared Task for English. We fine tune each model on each of the task's questions and aggregate their prediction scores using a majority voting approach. TOKOFOU obtains an overall F1 score of 89.7%, ranking first.","url":"https:\/\/aclanthology.org\/2021.nlp4if-1.18"},{"ID":"u-etal-2008-statistical","methods":["search engine","relevance feedback","noisy channel model","statistical machine translation models"],"center_method":["search engine",null,null,null],"tasks":["re ranking phase","web search personalization","query formulation","personalized search"],"center_task":[null,null,null,null],"Goal":["Quality Education","Industry, Innovation and Infrastrucure"],"text":"Statistical Machine Translation Models for Personalized Search. Web search personalization has been well studied in the recent few years. Relevance feedback has been used in various ways to improve relevance of search results. In this paper, we propose a novel usage of relevance feedback to effectively model the process of query formulation and better characterize how a user relates his query to the document that he intends to retrieve using a noisy channel model. We model a user profile as the probabilities of translation of query to document in this noisy channel using the relevance feedback obtained from the user. The user profile thus learnt is applied in a re-ranking phase to rescore the search results retrieved using an underlying search engine. We evaluate our approach by conducting experiments using relevance feedback data collected from users using a popular search engine. The results have shown improvement over baseline, proving that our approach can be applied to personalization of web search. The experiments have also resulted in some valuable observations that learning these user profiles using snippets surrounding the results for a query gives better performance than learning from entire document collection.","label":1,"title_clean":"Statistical Machine Translation Models for Personalized Search","abstract_clean":"Web search personalization has been well studied in the recent few years. Relevance feedback has been used in various ways to improve relevance of search results. In this paper, we propose a novel usage of relevance feedback to effectively model the process of query formulation and better characterize how a user relates his query to the document that he intends to retrieve using a noisy channel model. We model a user profile as the probabilities of translation of query to document in this noisy channel using the relevance feedback obtained from the user. The user profile thus learnt is applied in a re ranking phase to rescore the search results retrieved using an underlying search engine. We evaluate our approach by conducting experiments using relevance feedback data collected from users using a popular search engine. The results have shown improvement over baseline, proving that our approach can be applied to personalization of web search. The experiments have also resulted in some valuable observations that learning these user profiles using snippets surrounding the results for a query gives better performance than learning from entire document collection.","url":"https:\/\/aclanthology.org\/I08-1068.pdf"},{"ID":"uszkoreit-2012-quality","methods":["hybrid approaches"],"center_method":[null],"tasks":["machine translation","quality estimation","translation technology","syntactic and semantic processing","technology deployment","informational inbound translation","language processing","european mt research","quality translation","language technology","transfer"],"center_task":["machine translation",null,null,null,null,null,null,null,null,"language technology",null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Quality Translation for a Multilingual Continent - Priorities and Chances for European MT Research. Recent progress in translation technology has caused a real boost for research and technology deployment. At the same time, other areas of language technology also experience scientific advances and economic success stories. However, research in machine translation is still less affected by new developments in core areas of language processing than could be expected. One reason for the low level of interaction is certainly that the predominant research paradigm in MT has not started yet to systematically concentrate on high quality translation. Most of the research and nearly all of the application efforts have focused on solutions for informational inbound translation (assimilation MT). This focus has on the one hand enabled translation of information that normally is not translated at all. In this way MT has changed work and life of many people without ever infringing on the existing translation markets. In my talk I will present a new research approach dedicated to the analytical investigation of existing quality barriers. Such a systematic thrust can serve as the basis of scientifically guided combinations of technologies including hybrid approaches to transfer and the integration of advanced methods for syntactic and semantic processing into the translation process. Together with improved techniques for quality estimation, the expected results will drive translation technology into the direction badly needed by the multilingual European society.","label":1,"title_clean":"Quality Translation for a Multilingual Continent  Priorities and Chances for European MT Research","abstract_clean":"Recent progress in translation technology has caused a real boost for research and technology deployment. At the same time, other areas of language technology also experience scientific advances and economic success stories. However, research in machine translation is still less affected by new developments in core areas of language processing than could be expected. One reason for the low level of interaction is certainly that the predominant research paradigm in MT has not started yet to systematically concentrate on high quality translation. Most of the research and nearly all of the application efforts have focused on solutions for informational inbound translation (assimilation MT). This focus has on the one hand enabled translation of information that normally is not translated at all. In this way MT has changed work and life of many people without ever infringing on the existing translation markets. In my talk I will present a new research approach dedicated to the analytical investigation of existing quality barriers. Such a systematic thrust can serve as the basis of scientifically guided combinations of technologies including hybrid approaches to transfer and the integration of advanced methods for syntactic and semantic processing into the translation process. Together with improved techniques for quality estimation, the expected results will drive translation technology into the direction badly needed by the multilingual European society.","url":"https:\/\/aclanthology.org\/F12-4001.pdf"},{"ID":"uzan-hacohen-kerner-2020-jct","methods":["jct","random forest","non neural methods","preprocessing methods"],"center_method":[null,null,null,null],"tasks":["hate speech"],"center_task":["hate speech"],"Goal":["Peace, Justice and Strong Institutions"],"text":"JCT at SemEval-2020 Task 12: Offensive Language Detection in Tweets Using Preprocessing Methods, Character and Word N-grams. In this paper, we describe our submissions to SemEval-2020 contest. We tackled subtask 12-\"Multilingual Offensive Language Identification in Social Media\". We developed different models for four languages: Arabic, Danish, Greek, and Turkish. We applied three supervised machine learning methods using various combinations of character and word n-gram features. In addition, we applied various combinations of basic preprocessing methods. Our best submission was a model we built for offensive language identification in Danish using Random Forest. This model was ranked at the 6 th position out of 39 submissions. Our result is lower by only 0.0025 than the result of the team that won the 4 th place using entirely non-neural methods. Our experiments indicate that char ngram features are more helpful than word ngram features. This phenomenon probably occurs because tweets are more characterized by characters than by words, tweets are short, and contain various special sequences of characters, e.g., hashtags, shortcuts, slang words, and typos.","label":1,"title_clean":"JCT at SemEval 2020 Task 12: Offensive Language Detection in Tweets Using Preprocessing Methods, Character and Word N grams","abstract_clean":"In this paper, we describe our submissions to SemEval 2020 contest. We tackled subtask 12 \"Multilingual Offensive Language Identification in Social Media\". We developed different models for four languages: Arabic, Danish, Greek, and Turkish. We applied three supervised machine learning methods using various combinations of character and word n gram features. In addition, we applied various combinations of basic preprocessing methods. Our best submission was a model we built for offensive language identification in Danish using Random Forest. This model was ranked at the 6 th position out of 39 submissions. Our result is lower by only 0.0025 than the result of the team that won the 4 th place using entirely non neural methods. Our experiments indicate that char ngram features are more helpful than word ngram features. This phenomenon probably occurs because tweets are more characterized by characters than by words, tweets are short, and contain various special sequences of characters, e.g., hashtags, shortcuts, slang words, and typos.","url":"https:\/\/aclanthology.org\/2020.semeval-1.266"},{"ID":"v-hahn-vertan-2002-architectures","methods":["toy systems"],"center_method":[null],"tasks":["teaching machine translation","educational toy systems","academic teaching"],"center_task":[null,null,null],"Goal":["Quality Education"],"text":"Architectures of ``toy'' systems for teaching machine translation. This paper addresses the advantages of practical academic teaching of machine translation by implementations of \"toy\" systems. This is the result of experience from several semesters with different types of courses and different categories of students. In addition to describing two possible architectures for such educational toy systems, we will also discuss how to overcome misconceptions about MT and the evaluation both of the achieved systems and the learning success.","label":1,"title_clean":"Architectures of ``toy'' systems for teaching machine translation","abstract_clean":"This paper addresses the advantages of practical academic teaching of machine translation by implementations of \"toy\" systems. This is the result of experience from several semesters with different types of courses and different categories of students. In addition to describing two possible architectures for such educational toy systems, we will also discuss how to overcome misconceptions about MT and the evaluation both of the achieved systems and the learning success.","url":"https:\/\/aclanthology.org\/2002.eamt-1.8.pdf"},{"ID":"vanderwende-etal-2013-annotating","methods":["annotation"],"center_method":["annotation"],"tasks":["identification of phenotypes","clinical events"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"Annotating Change of State for Clinical Events. Understanding the event structure of sentences and whole documents is an important step in being able to extract meaningful information from the text. Our task is the identification of phenotypes, specifically, pneumonia, from clinical narratives. In this paper, we consider the importance of identifying the change of state for events, in particular, events that measure and compare multiple states across time. Change of state is important to the clinical diagnosis of pneumonia; in the example \"there are bibasilar opacities that are unchanged\", the presence of bibasilar opacities alone may suggest pneumonia, but not when they are unchanged, which suggests the need to modify events with change of state information. Our corpus is comprised of chest Xray reports, where we find many descriptions of change of state comparing the volume and density of the lungs and surrounding areas. We propose an annotation schema to capture this information as a tuple of <location, attribute, value, change-of-state, time-reference>.","label":1,"title_clean":"Annotating Change of State for Clinical Events","abstract_clean":"Understanding the event structure of sentences and whole documents is an important step in being able to extract meaningful information from the text. Our task is the identification of phenotypes, specifically, pneumonia, from clinical narratives. In this paper, we consider the importance of identifying the change of state for events, in particular, events that measure and compare multiple states across time. Change of state is important to the clinical diagnosis of pneumonia; in the example \"there are bibasilar opacities that are unchanged\", the presence of bibasilar opacities alone may suggest pneumonia, but not when they are unchanged, which suggests the need to modify events with change of state information. Our corpus is comprised of chest Xray reports, where we find many descriptions of change of state comparing the volume and density of the lungs and surrounding areas. We propose an annotation schema to capture this information as a tuple of <location, attribute, value, change of state, time reference>.","url":"https:\/\/aclanthology.org\/W13-1206"},{"ID":"vashisth-etal-2019-exploring","methods":["diachronic changes of biomedical knowledge","temporal and distributional concept representations"],"center_method":[null,null],"tasks":["diachronic changes of biomedical knowledge","information extraction"],"center_task":[null,"information extraction"],"Goal":["Good Health and Well-Being"],"text":"Exploring Diachronic Changes of Biomedical Knowledge using Distributed Concept Representations. In research best practices can change over time as new discoveries are made and novel methods are implemented. Scientific publications reporting about the latest facts and current state-of-the-art can be possibly outdated after some years or even proved to be false. A publication usually sheds light only on the knowledge of the period it has been published. Thus, the aspect of time can play an essential role in the reliability of the presented information. In Natural Language Processing many methods focus on information extraction from text, such as detecting entities and their relationship to each other. Those methods mostly focus on the facts presented in the text itself and not on the aspects of knowledge which changes over time. This work instead examines the evolution in biomedical knowledge over time using scientific literature in terms of diachronic change. Mainly the usage of temporal and distributional concept representations are explored and evaluated by a proof-of-concept.","label":1,"title_clean":"Exploring Diachronic Changes of Biomedical Knowledge using Distributed Concept Representations","abstract_clean":"In research best practices can change over time as new discoveries are made and novel methods are implemented. Scientific publications reporting about the latest facts and current state of the art can be possibly outdated after some years or even proved to be false. A publication usually sheds light only on the knowledge of the period it has been published. Thus, the aspect of time can play an essential role in the reliability of the presented information. In Natural Language Processing many methods focus on information extraction from text, such as detecting entities and their relationship to each other. Those methods mostly focus on the facts presented in the text itself and not on the aspects of knowledge which changes over time. This work instead examines the evolution in biomedical knowledge over time using scientific literature in terms of diachronic change. Mainly the usage of temporal and distributional concept representations are explored and evaluated by a proof of concept.","url":"https:\/\/aclanthology.org\/W19-5037"},{"ID":"vassileva-etal-2021-automatic-transformation","methods":["hybrid method","classification","mbg clinicalbert","similarity search","rule based system","icd 10 codes","deep learning text based encoding","nlp applications"],"center_method":[null,"classification",null,null,"rule based system",null,null,"nlp applications"],"tasks":["automatic transformation of clinical narratives"],"center_task":[null],"Goal":["Good Health and Well-Being"],"text":"Automatic Transformation of Clinical Narratives into Structured Format. Vast amounts of data in healthcare are available in unstructured text format, usually in the local language of the countries. These documents contain valuable information. Secondary use of clinical narratives and information extraction of key facts and relations from them about the patient disease history can foster preventive medicine and improve healthcare. In this paper, we propose a hybrid method for the automatic transformation of clinical text into a structured format. The documents are automatically sectioned into the following parts: diagnosis, patient history, patient status, lab results. For the \"Diagnosis\" section a deep learning text-based encoding into ICD-10 codes is applied using MBG-ClinicalBERT-a fine-tuned ClinicalBERT model for Bulgarian medical text. From the \"Patient History\" section, we identify patient symptoms using a rule-based approach enhanced with similarity search based on MBG-ClinicalBERT word embeddings. We also identify symptom relations like negation. For the \"Patient Status\" description, binary classification is used to determine the status of each anatomic organ. In this paper, we demonstrate different methods for adapting NLP tools for English and other languages to a low resource language like Bulgarian.","label":1,"title_clean":"Automatic Transformation of Clinical Narratives into Structured Format","abstract_clean":"Vast amounts of data in healthcare are available in unstructured text format, usually in the local language of the countries. These documents contain valuable information. Secondary use of clinical narratives and information extraction of key facts and relations from them about the patient disease history can foster preventive medicine and improve healthcare. In this paper, we propose a hybrid method for the automatic transformation of clinical text into a structured format. The documents are automatically sectioned into the following parts: diagnosis, patient history, patient status, lab results. For the \"Diagnosis\" section a deep learning text based encoding into ICD 10 codes is applied using MBG ClinicalBERT a fine tuned ClinicalBERT model for Bulgarian medical text. From the \"Patient History\" section, we identify patient symptoms using a rule based approach enhanced with similarity search based on MBG ClinicalBERT word embeddings. We also identify symptom relations like negation. For the \"Patient Status\" description, binary classification is used to determine the status of each anatomic organ. In this paper, we demonstrate different methods for adapting NLP tools for English and other languages to a low resource language like Bulgarian.","url":"https:\/\/aclanthology.org\/2021.ranlp-srw.30"},{"ID":"velupillai-2014-temporal","methods":["hei deltime"],"center_method":[null],"tasks":["healthcare","temporal reasoning of swedish medical text"],"center_task":["healthcare",null],"Goal":["Good Health and Well-Being"],"text":"Temporal Expressions in Swedish Medical Text -- A Pilot Study. One of the most important features of health care is to be able to follow a patient's progress over time and identify events in a temporal order. We describe initial steps in creating resources for automatic temporal reasoning of Swedish medical text. As a first step, we focus on the identification of temporal expressions by exploiting existing resources and systems available for English. We adapt the HeidelTime system and manually evaluate its performance on a small subset of Swedish intensive care unit documents. On this subset, the adapted version of Hei-delTime achieves a precision of 92% and a recall of 66%. We also extract the most frequent temporal expressions from a separate, larger subset, and note that most expressions concern parts of days or specific times. We intend to further develop resources for temporal reasoning of Swedish medical text by creating a gold standard corpus also annotated with events and temporal links, in addition to temporal expressions and their normalised values.","label":1,"title_clean":"Temporal Expressions in Swedish Medical Text  A Pilot Study","abstract_clean":"One of the most important features of health care is to be able to follow a patient's progress over time and identify events in a temporal order. We describe initial steps in creating resources for automatic temporal reasoning of Swedish medical text. As a first step, we focus on the identification of temporal expressions by exploiting existing resources and systems available for English. We adapt the HeidelTime system and manually evaluate its performance on a small subset of Swedish intensive care unit documents. On this subset, the adapted version of Hei delTime achieves a precision of 92% and a recall of 66%. We also extract the most frequent temporal expressions from a separate, larger subset, and note that most expressions concern parts of days or specific times. We intend to further develop resources for temporal reasoning of Swedish medical text by creating a gold standard corpus also annotated with events and temporal links, in addition to temporal expressions and their normalised values.","url":"https:\/\/aclanthology.org\/W14-3413"},{"ID":"venturott-mitkov-2021-fake","methods":["deep neural network","fake news detection system","standalone tools"],"center_method":["deep neural network",null,null],"tasks":["fact checking","fake news detection","dissemination of false or misleading information","detecting false news"],"center_task":["fact checking","fake news detection",null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Fake News Detection for Portuguese with Deep Learning. The exponential growth of the internet and social media in the past decade gave way to the increase in dissemination of false or misleading information. Since the 2016 US presidential election, the term \"fake news\" became increasingly popular and this phenomenon has received more attention. In the past years several fact-checking agencies were created, but due to the great number of daily posts on social media, manual checking is insufficient. Currently, there is a pressing need for automatic fake news detection tools, either to assist manual fact-checkers or to operate as standalone tools. There are several projects underway on this topic, but most of them focus on English. This research-in-progress paper discusses the employment of deep learning methods, and the development of a tool, for detecting false news in Portuguese. As a first step we shall compare well-established architectures that were tested in other languages and analyse their performance on our Portuguese data. Based on the preliminary results of these classifiers, we shall choose a deep learning model or combine several deep learning models which hold promise to enhance the performance of our fake news detection system.","label":1,"title_clean":"Fake News Detection for Portuguese with Deep Learning","abstract_clean":"The exponential growth of the internet and social media in the past decade gave way to the increase in dissemination of false or misleading information. Since the 2016 US presidential election, the term \"fake news\" became increasingly popular and this phenomenon has received more attention. In the past years several fact checking agencies were created, but due to the great number of daily posts on social media, manual checking is insufficient. Currently, there is a pressing need for automatic fake news detection tools, either to assist manual fact checkers or to operate as standalone tools. There are several projects underway on this topic, but most of them focus on English. This research in progress paper discusses the employment of deep learning methods, and the development of a tool, for detecting false news in Portuguese. As a first step we shall compare well established architectures that were tested in other languages and analyse their performance on our Portuguese data. Based on the preliminary results of these classifiers, we shall choose a deep learning model or combine several deep learning models which hold promise to enhance the performance of our fake news detection system.","url":"https:\/\/aclanthology.org\/2021.triton-1.16"},{"ID":"vlad-etal-2019-sentence","methods":["bert bilstm capsule model","coldstart model","pre training approach","capsule","bert","deep leaning modules","unified neural network","binary classifier","propaganda","transfer learning"],"center_method":[null,null,null,null,"bert",null,null,null,null,"transfer learning"],"tasks":["sentencelevel propaganda classification problem","nlp4if","sentence level propaganda detection","communication"],"center_task":[null,null,null,"communication"],"Goal":["Peace, Justice and Strong Institutions"],"text":"Sentence-Level Propaganda Detection in News Articles with Transfer Learning and BERT-BiLSTM-Capsule Model. In recent years, the need for communication increased in online social media. Propaganda is a mechanism which was used throughout history to influence public opinion and it is gaining a new dimension with the rising interest of online social media. This paper presents our submission to NLP4IF-2019 Shared Task SLC: Sentence-level Propaganda Detection in news articles. The challenge of this task is to build a robust binary classifier able to provide corresponding propaganda labels, propaganda or non-propaganda. Our model relies on a unified neural network, which consists of several deep leaning modules, namely BERT, BiLSTM and Capsule, to solve the sentencelevel propaganda classification problem. In addition, we take a pre-training approach on a somewhat similar task (i.e., emotion classification) improving results against the coldstart model. Among the 26 participant teams in the NLP4IF-2019 Task SLC, our solution ranked 12th with an F 1-score 0.5868 on the official test data. Our proposed solution indicates promising results since our system significantly exceeds the baseline approach of the task organizers by 0.1521 and is slightly lower than the winning system by 0.0454.","label":1,"title_clean":"Sentence Level Propaganda Detection in News Articles with Transfer Learning and BERT BiLSTM Capsule Model","abstract_clean":"In recent years, the need for communication increased in online social media. Propaganda is a mechanism which was used throughout history to influence public opinion and it is gaining a new dimension with the rising interest of online social media. This paper presents our submission to NLP4IF 2019 Shared Task SLC: Sentence level Propaganda Detection in news articles. The challenge of this task is to build a robust binary classifier able to provide corresponding propaganda labels, propaganda or non propaganda. Our model relies on a unified neural network, which consists of several deep leaning modules, namely BERT, BiLSTM and Capsule, to solve the sentencelevel propaganda classification problem. In addition, we take a pre training approach on a somewhat similar task (i.e., emotion classification) improving results against the coldstart model. Among the 26 participant teams in the NLP4IF 2019 Task SLC, our solution ranked 12th with an F 1 score 0.5868 on the official test data. Our proposed solution indicates promising results since our system significantly exceeds the baseline approach of the task organizers by 0.1521 and is slightly lower than the winning system by 0.0454.","url":"https:\/\/aclanthology.org\/D19-5022"},{"ID":"volk-1997-probing","methods":["machine translation","black box method"],"center_method":["machine translation",null],"tasks":["single system evaluations","machine translation"],"center_task":[null,"machine translation"],"Goal":["Decent Work and Economic Growth"],"text":"Probing the Lexicon in Evaluating Commercial MT Systems. In the past the evaluation of machine translation systems has focused on single system evaluations because there were only few systems available. But now there are several commercial systems for the same language pair. This requires new methods of comparative evaluation. In the paper we propose a black-box method for comparing the lexical coverage of MT systems. The method is based on lists of words from different frequency classes. It is shown how these word lists can be compiled and used for testing. We also present the results of using our method on 6 MT systems that translate between English and German.","label":1,"title_clean":"Probing the Lexicon in Evaluating Commercial MT Systems","abstract_clean":"In the past the evaluation of machine translation systems has focused on single system evaluations because there were only few systems available. But now there are several commercial systems for the same language pair. This requires new methods of comparative evaluation. In the paper we propose a black box method for comparing the lexical coverage of MT systems. The method is based on lists of words from different frequency classes. It is shown how these word lists can be compiled and used for testing. We also present the results of using our method on 6 MT systems that translate between English and German.","url":"https:\/\/aclanthology.org\/P97-1015"},{"ID":"von-etter-etal-2010-assessment","methods":["classification","information extraction"],"center_method":["classification","information extraction"],"tasks":["public health application","text mining","predict relevance","learning","assessment of utility","ie"],"center_task":["public health application","text mining",null,"learning",null,null],"Goal":["Good Health and Well-Being"],"text":"Assessment of Utility in Web Mining for the Domain of Public Health. This paper presents ongoing work on application of Information Extraction (IE) technology to domain of Public Health, in a real-world scenario. A central issue in IE is the quality of the results. We present two novel points. First, we distinguish the criteria for quality: the objective criteria that measure correctness of the system's analysis in traditional terms (F-measure, recall and precision), and, on the other hand, subjective criteria that measure the utility of the results to the end-user. Second, to obtain measures of utility, we build an environment that allows users to interact with the system by rating the analyzed content. We then build and compare several classifiers that learn from the user's responses to predict the relevance scores for new events. We conduct experiments with learning to predict relevance, and discuss the results and their implications for text mining in the domain of Public Health.","label":1,"title_clean":"Assessment of Utility in Web Mining for the Domain of Public Health","abstract_clean":"This paper presents ongoing work on application of Information Extraction (IE) technology to domain of Public Health, in a real world scenario. A central issue in IE is the quality of the results. We present two novel points. First, we distinguish the criteria for quality: the objective criteria that measure correctness of the system's analysis in traditional terms (F measure, recall and precision), and, on the other hand, subjective criteria that measure the utility of the results to the end user. Second, to obtain measures of utility, we build an environment that allows users to interact with the system by rating the analyzed content. We then build and compare several classifiers that learn from the user's responses to predict the relevance scores for new events. We conduct experiments with learning to predict relevance, and discuss the results and their implications for text mining in the domain of Public Health.","url":"https:\/\/aclanthology.org\/W10-1105"},{"ID":"vydiswaran-etal-2019-towards","methods":["machine learning and deep learning models","text processing pipeline"],"center_method":[null,null],"tasks":["detecting mentions of adverse drug events"],"center_task":[null],"Goal":["Good Health and Well-Being"],"text":"Towards Text Processing Pipelines to Identify Adverse Drug Events-related Tweets: University of Michigan @ SMM4H 2019 Task 1. We participated in Task 1 of the Social Media Mining for Health Applications (SMM4H) 2019 Shared Tasks on detecting mentions of adverse drug events (ADEs) in tweets. Our approach relied on a text processing pipeline for tweets, and training traditional machine learning and deep learning models. Our submitted runs performed above average for the task.","label":1,"title_clean":"Towards Text Processing Pipelines to Identify Adverse Drug Events related Tweets: University of Michigan @ SMM4H 2019 Task 1","abstract_clean":"We participated in Task 1 of the Social Media Mining for Health Applications (SMM4H) 2019 Shared Tasks on detecting mentions of adverse drug events (ADEs) in tweets. Our approach relied on a text processing pipeline for tweets, and training traditional machine learning and deep learning models. Our submitted runs performed above average for the task.","url":"https:\/\/aclanthology.org\/W19-3217"},{"ID":"walker-etal-2018-evidence","methods":["evidence assessment process"],"center_method":[null],"tasks":["adjudicatory decisions","weighing conflicting evidence","argument mining"],"center_task":[null,null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Evidence Types, Credibility Factors, and Patterns or Soft Rules for Weighing Conflicting Evidence: Argument Mining in the Context of Legal Rules Governing Evidence Assessment. This paper reports on the results of an empirical study of adjudicatory decisions about veterans' claims for disability benefits in the United States. It develops a typology of kinds of relevant evidence (argument premises) employed in cases, and it identifies factors that the tribunal considers when assessing the credibility or trustworthiness of individual items of evidence. It also reports on patterns or \"soft rules\" that the tribunal uses to comparatively weigh the probative value of conflicting evidence. These evidence types, credibility factors, and comparison patterns are developed to be inter-operable with legal rules governing the evidence assessment process in the U.S. This approach should be transferable to other legal and non-legal domains.","label":1,"title_clean":"Evidence Types, Credibility Factors, and Patterns or Soft Rules for Weighing Conflicting Evidence: Argument Mining in the Context of Legal Rules Governing Evidence Assessment","abstract_clean":"This paper reports on the results of an empirical study of adjudicatory decisions about veterans' claims for disability benefits in the United States. It develops a typology of kinds of relevant evidence (argument premises) employed in cases, and it identifies factors that the tribunal considers when assessing the credibility or trustworthiness of individual items of evidence. It also reports on patterns or \"soft rules\" that the tribunal uses to comparatively weigh the probative value of conflicting evidence. These evidence types, credibility factors, and comparison patterns are developed to be inter operable with legal rules governing the evidence assessment process in the U.S. This approach should be transferable to other legal and non legal domains.","url":"https:\/\/aclanthology.org\/W18-5209"},{"ID":"wang-etal-2017-statistical","methods":["statistical framework"],"center_method":[null],"tasks":["product description generation","attribute specified generation","accurate and fluent product description"],"center_task":[null,null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"A Statistical Framework for Product Description Generation. We present in this paper a statistical framework that generates accurate and fluent product description from product attributes. Specifically, after extracting templates and learning writing knowledge from attribute-description parallel data, we use the learned knowledge to decide what to say and how to say for product description generation. To evaluate accuracy and fluency for the generated descriptions, in addition to BLEU and Recall, we propose to measure what to say (in terms of attribute coverage) and to measure how to say (by attribute-specified generation) separately. Experimental results show that our framework is effective.","label":1,"title_clean":"A Statistical Framework for Product Description Generation","abstract_clean":"We present in this paper a statistical framework that generates accurate and fluent product description from product attributes. Specifically, after extracting templates and learning writing knowledge from attribute description parallel data, we use the learned knowledge to decide what to say and how to say for product description generation. To evaluate accuracy and fluency for the generated descriptions, in addition to BLEU and Recall, we propose to measure what to say (in terms of attribute coverage) and to measure how to say (by attribute specified generation) separately. Experimental results show that our framework is effective.","url":"https:\/\/aclanthology.org\/I17-2032"},{"ID":"wang-etal-2019-bigodm","methods":["adr classification task","support vector machine","vote based undersampling ensemble approach","vue","word embeddings"],"center_method":[null,"support vector machine",null,null,"word embeddings"],"tasks":["healthcare","social media mining"],"center_task":["healthcare","social media mining"],"Goal":["Good Health and Well-Being"],"text":"BIGODM System in the Social Media Mining for Health Applications Shared Task 2019. In this study, we describe our methods to automatically classify Twitter posts conveying events of adverse drug reaction (ADR). Based on our previous experience in tackling the ADR classification task, we empirically applied the vote-based undersampling ensemble approach along with linear support vector machine (SVM) to develop our classifiers as part of our participation in ACL 2019 Social Media Mining for Health Applications (SMM4H) shared task 1. The best-performed model on the test sets were trained on a merged corpus consisting of the datasets released by SMM4H 2017 and 2019. By using VUE, the corpus was randomly under-sampled with 2:1 ratio between the negative and positive classes to create an ensemble using the linear kernel trained with features including bag-of-word, domain knowledge, negation and word embedding. The best performing model achieved an F-measure of 0.551 which is about 5% higher than the average F-scores of 16 teams.","label":1,"title_clean":"BIGODM System in the Social Media Mining for Health Applications Shared Task 2019","abstract_clean":"In this study, we describe our methods to automatically classify Twitter posts conveying events of adverse drug reaction (ADR). Based on our previous experience in tackling the ADR classification task, we empirically applied the vote based undersampling ensemble approach along with linear support vector machine (SVM) to develop our classifiers as part of our participation in ACL 2019 Social Media Mining for Health Applications (SMM4H) shared task 1. The best performed model on the test sets were trained on a merged corpus consisting of the datasets released by SMM4H 2017 and 2019. By using VUE, the corpus was randomly under sampled with 2:1 ratio between the negative and positive classes to create an ensemble using the linear kernel trained with features including bag of word, domain knowledge, negation and word embedding. The best performing model achieved an F measure of 0.551 which is about 5% higher than the average F scores of 16 teams.","url":"https:\/\/aclanthology.org\/W19-3220.pdf"},{"ID":"wang-etal-2020-automated","methods":["contextual embeddings","scoring model","transfer learning","clinical assessment instruments","machine translation"],"center_method":["contextual embeddings",null,"transfer learning",null,"machine translation"],"tasks":["sentence formulation tasks","language impairments","automated scoring of clinical expressive language evaluation tasks"],"center_task":[null,null,null],"Goal":["Good Health and Well-Being"],"text":"Automated Scoring of Clinical Expressive Language Evaluation Tasks. Many clinical assessment instruments used to diagnose language impairments in children include a task in which the subject must formulate a sentence to describe an image using a specific target word. Because producing sentences in this way requires the speaker to integrate syntactic and semantic knowledge in a complex manner, responses are typically evaluated on several different dimensions of appropriateness yielding a single composite score for each response. In this paper, we present a dataset consisting of non-clinically elicited responses for three related sentence formulation tasks, and we propose an approach for automatically evaluating their appropriateness. Using neural machine translation, we generate correct-incorrect sentence pairs to serve as synthetic data in order to increase the amount and diversity of training data for our scoring model. Our scoring model uses transfer learning to facilitate automatic sentence appropriateness evaluation. We further compare custom word embeddings with pre-trained contextualized embeddings serving as features for our scoring model. We find that transfer learning improves scoring accuracy, particularly when using pre-trained contextualized embeddings.","label":1,"title_clean":"Automated Scoring of Clinical Expressive Language Evaluation Tasks","abstract_clean":"Many clinical assessment instruments used to diagnose language impairments in children include a task in which the subject must formulate a sentence to describe an image using a specific target word. Because producing sentences in this way requires the speaker to integrate syntactic and semantic knowledge in a complex manner, responses are typically evaluated on several different dimensions of appropriateness yielding a single composite score for each response. In this paper, we present a dataset consisting of non clinically elicited responses for three related sentence formulation tasks, and we propose an approach for automatically evaluating their appropriateness. Using neural machine translation, we generate correct incorrect sentence pairs to serve as synthetic data in order to increase the amount and diversity of training data for our scoring model. Our scoring model uses transfer learning to facilitate automatic sentence appropriateness evaluation. We further compare custom word embeddings with pre trained contextualized embeddings serving as features for our scoring model. We find that transfer learning improves scoring accuracy, particularly when using pre trained contextualized embeddings.","url":"https:\/\/aclanthology.org\/2020.bea-1.18.pdf"},{"ID":"wang-etal-2020-evaluating","methods":["model configurations","domain specific fine tuning","language models","data augmentation techniques","pooling strategies"],"center_method":[null,null,"language models",null,null],"tasks":["clinical sts","semantic textual similarity"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"Evaluating the Utility of Model Configurations and Data Augmentation on Clinical Semantic Textual Similarity. In this paper, we apply pre-trained language models to the Semantic Textual Similarity (STS) task, with a specific focus on the clinical domain. In low-resource setting of clinical STS, these large models tend to be impractical and prone to overfitting. Building on BERT, we study the impact of a number of model design choices, namely different fine-tuning and pooling strategies. We observe that the impact of domain-specific fine-tuning on clinical STS is much less than that in the general domain, likely due to the concept richness of the domain. Based on this, we propose two data augmentation techniques. Experimental results on N2C2-STS 1 demonstrate substantial improvements, validating the utility of the proposed methods.","label":1,"title_clean":"Evaluating the Utility of Model Configurations and Data Augmentation on Clinical Semantic Textual Similarity","abstract_clean":"In this paper, we apply pre trained language models to the Semantic Textual Similarity (STS) task, with a specific focus on the clinical domain. In low resource setting of clinical STS, these large models tend to be impractical and prone to overfitting. Building on BERT, we study the impact of a number of model design choices, namely different fine tuning and pooling strategies. We observe that the impact of domain specific fine tuning on clinical STS is much less than that in the general domain, likely due to the concept richness of the domain. Based on this, we propose two data augmentation techniques. Experimental results on N2C2 STS 1 demonstrate substantial improvements, validating the utility of the proposed methods.","url":"https:\/\/aclanthology.org\/2020.bionlp-1.11"},{"ID":"wang-etal-2020-rationalizing","methods":["model rationales","interpretable framework","neural baseline models","recall","machine learning methods"],"center_method":[null,null,null,null,"machine learning methods"],"tasks":["predictions","clinical decision making","recognition","medical domain","rationalizing medical relation prediction","medical relation prediction"],"center_task":["predictions",null,null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"Rationalizing Medical Relation Prediction from Corpus-level Statistics. Nowadays, the interpretability of machine learning models is becoming increasingly important, especially in the medical domain. Aiming to shed some light on how to rationalize medical relation prediction, we present a new interpretable framework inspired by existing theories on how human memory works, e.g., theories of recall and recognition. Given the corpus-level statistics, i.e., a global cooccurrence graph of a clinical text corpus, to predict the relations between two entities, we first recall rich contexts associated with the target entities, and then recognize relational interactions between these contexts to form model rationales, which will contribute to the final prediction. We conduct experiments on a real-world public clinical dataset and show that our framework can not only achieve competitive predictive performance against a comprehensive list of neural baseline models, but also present rationales to justify its prediction. We further collaborate with medical experts deeply to verify the usefulness of our model rationales for clinical decision making 1 .","label":1,"title_clean":"Rationalizing Medical Relation Prediction from Corpus level Statistics","abstract_clean":"Nowadays, the interpretability of machine learning models is becoming increasingly important, especially in the medical domain. Aiming to shed some light on how to rationalize medical relation prediction, we present a new interpretable framework inspired by existing theories on how human memory works, e.g., theories of recall and recognition. Given the corpus level statistics, i.e., a global cooccurrence graph of a clinical text corpus, to predict the relations between two entities, we first recall rich contexts associated with the target entities, and then recognize relational interactions between these contexts to form model rationales, which will contribute to the final prediction. We conduct experiments on a real world public clinical dataset and show that our framework can not only achieve competitive predictive performance against a comprehensive list of neural baseline models, but also present rationales to justify its prediction. We further collaborate with medical experts deeply to verify the usefulness of our model rationales for clinical decision making 1 .","url":"https:\/\/aclanthology.org\/2020.acl-main.719.pdf"},{"ID":"wang-etal-2021-mesure","methods":["embeddings contextualiss","fasttext","vecteur tf idf"],"center_method":[null,"fasttext",null],"tasks":["valuation automatique de copies"],"center_task":[null],"Goal":["Quality Education"],"text":"Mesure de similarit\\'e textuelle pour l'\\'evaluation automatique de copies d'\\'etudiants (Textual similarity measurement for automatic evaluation of students' answers). Cet article d\u00e9crit la participation de l'\u00e9quipe Nantalco \u00e0 la t\u00e2che 2 du D\u00e9fi Fouille de Textes 2021 (DEFT) : \u00e9valuation automatique de copies d'apr\u00e8s une r\u00e9f\u00e9rence existante. Nous avons utilis\u00e9 principalement des traits bas\u00e9s sur la similarit\u00e9 cosinus des deux vecteurs repr\u00e9sentant la similarit\u00e9 textuelle entre des r\u00e9ponses d'\u00e9tudiant et la r\u00e9f\u00e9rence. Plusieurs types de vecteurs ont \u00e9t\u00e9 utilis\u00e9s (vecteur d'occurrences de mots, vecteur tf-idf, embeddings non contextualis\u00e9s de fastText, embeddings contextualis\u00e9s de CamemBERT et enfin Sentence Embeddings Multilingues ajust\u00e9s sur des corpus multilingues). La meilleure performance du concours sur cette t\u00e2che a \u00e9t\u00e9 de 0.682 (pr\u00e9cision) et celle de notre \u00e9quipe 0.639. Cette performance a \u00e9t\u00e9 obtenue avec les Sentence Embeddings Multilingues alors que celle des embeddings non ajust\u00e9s ne s'est \u00e9lev\u00e9e qu'\u00e0 0.55, sugg\u00e9rant que de r\u00e9cents mod\u00e8les de langues pr\u00e9-entra\u00een\u00e9s doivent \u00eatre fine-tun\u00e9s afin d'avoir des embeddings ad\u00e9quats au niveau phrastique.","label":1,"title_clean":"Mesure de similarit\\'e textuelle pour l'\\'evaluation automatique de copies d'\\'etudiants (Textual similarity measurement for automatic evaluation of students' answers)","abstract_clean":"Cet article d\u00e9crit la participation de l'\u00e9quipe Nantalco \u00e0 la t\u00e2che 2 du D\u00e9fi Fouille de Textes 2021 (DEFT) : \u00e9valuation automatique de copies d'apr\u00e8s une r\u00e9f\u00e9rence existante. Nous avons utilis\u00e9 principalement des traits bas\u00e9s sur la similarit\u00e9 cosinus des deux vecteurs repr\u00e9sentant la similarit\u00e9 textuelle entre des r\u00e9ponses d'\u00e9tudiant et la r\u00e9f\u00e9rence. Plusieurs types de vecteurs ont \u00e9t\u00e9 utilis\u00e9s (vecteur d'occurrences de mots, vecteur tf idf, embeddings non contextualis\u00e9s de fastText, embeddings contextualis\u00e9s de CamemBERT et enfin Sentence Embeddings Multilingues ajust\u00e9s sur des corpus multilingues). La meilleure performance du concours sur cette t\u00e2che a \u00e9t\u00e9 de 0.682 (pr\u00e9cision) et celle de notre \u00e9quipe 0.639. Cette performance a \u00e9t\u00e9 obtenue avec les Sentence Embeddings Multilingues alors que celle des embeddings non ajust\u00e9s ne s'est \u00e9lev\u00e9e qu'\u00e0 0.55, sugg\u00e9rant que de r\u00e9cents mod\u00e8les de langues pr\u00e9 entra\u00een\u00e9s doivent \u00eatre fine tun\u00e9s afin d'avoir des embeddings ad\u00e9quats au niveau phrastique.","url":"https:\/\/aclanthology.org\/2021.jeptalnrecital-deft.7.pdf"},{"ID":"wang-etal-2021-predicting","methods":["chinese version"],"center_method":[null],"tasks":["prediction of elders cognitive flexibility","predicting elders cognitive flexibility"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"Predicting elders' cognitive flexibility from their language use. Increasing research efforts are directed towards the relationship between cognitive decline and language use. However, few of them had focused specifically on how language use is related to cognitive flexibility. This study recruited 51 elders aged 53-74 to discuss their daily activities in focus groups. The transcribed discourse was analyzed using the Chinese version of LIWC (Lin et al., 2020; Pennebaker et al., 2015) for cognitive complexity and dynamic language as well as content words related to elders' daily activities. The interruption behavior during conversation was also analyzed. The results showed that, after controlling for education, gender and age, cognitive flexibility performance was accompanied by the increasing adoption of dynamic language, insight words and family words. These findings serve as the basis for the prediction of elders' cognitive flexibility through their daily language use.","label":1,"title_clean":"Predicting elders' cognitive flexibility from their language use","abstract_clean":"Increasing research efforts are directed towards the relationship between cognitive decline and language use. However, few of them had focused specifically on how language use is related to cognitive flexibility. This study recruited 51 elders aged 53 74 to discuss their daily activities in focus groups. The transcribed discourse was analyzed using the Chinese version of LIWC (Lin et al., 2020; Pennebaker et al., 2015) for cognitive complexity and dynamic language as well as content words related to elders' daily activities. The interruption behavior during conversation was also analyzed. The results showed that, after controlling for education, gender and age, cognitive flexibility performance was accompanied by the increasing adoption of dynamic language, insight words and family words. These findings serve as the basis for the prediction of elders' cognitive flexibility through their daily language use.","url":"https:\/\/aclanthology.org\/2021.rocling-1.18"},{"ID":"wang-matthews-2008-species","methods":["automatic species taggers","rule based and machine learning based approaches"],"center_method":[null,null],"tasks":["biomedical term identification","ti","species disambiguation","information extraction","resolving species ambiguity"],"center_task":[null,null,null,"information extraction",null],"Goal":["Good Health and Well-Being"],"text":"Species Disambiguation for Biomedical Term Identification. An important task in information extraction (IE) from biomedical articles is term identification (TI), which concerns linking entity mentions (e.g., terms denoting proteins) in text to unambiguous identifiers in standard databases (e.g., RefSeq). Previous work on TI has focused on species-specific documents. However, biomedical documents, especially full-length articles, often talk about entities across a number of species, in which case resolving species ambiguity becomes an indispensable part of TI. This paper describes our rule-based and machine-learning based approaches to species disambiguation and demonstrates that performance of TI can be improved by over 20% if the correct species are known. We also show that using the species predicted by the automatic species taggers can improve TI by a large margin.","label":1,"title_clean":"Species Disambiguation for Biomedical Term Identification","abstract_clean":"An important task in information extraction (IE) from biomedical articles is term identification (TI), which concerns linking entity mentions (e.g., terms denoting proteins) in text to unambiguous identifiers in standard databases (e.g., RefSeq). Previous work on TI has focused on species specific documents. However, biomedical documents, especially full length articles, often talk about entities across a number of species, in which case resolving species ambiguity becomes an indispensable part of TI. This paper describes our rule based and machine learning based approaches to species disambiguation and demonstrates that performance of TI can be improved by over 20% if the correct species are known. We also show that using the species predicted by the automatic species taggers can improve TI by a large margin.","url":"https:\/\/aclanthology.org\/W08-0610"},{"ID":"wei-gulla-2010-sentiment","methods":["hierarchical learning","hl sot approach"],"center_method":[null,null],"tasks":["sentiment analysis"],"center_task":["sentiment analysis"],"Goal":["Decent Work and Economic Growth","Industry, Innovation and Infrastrucure"],"text":"Sentiment Learning on Product Reviews via Sentiment Ontology Tree. Existing works on sentiment analysis on product reviews suffer from the following limitations: (1) The knowledge of hierarchical relationships of products attributes is not fully utilized. (2) Reviews or sentences mentioning several attributes associated with complicated sentiments are not dealt with very well. In this paper, we propose a novel HL-SOT approach to labeling a product's attributes and their associated sentiments in product reviews by a Hierarchical Learning (HL) process with a defined Sentiment Ontology Tree (SOT). The empirical analysis against a humanlabeled data set demonstrates promising and reasonable performance of the proposed HL-SOT approach. While this paper is mainly on sentiment analysis on reviews of one product, our proposed HL-SOT approach is easily generalized to labeling a mix of reviews of more than one products.","label":1,"title_clean":"Sentiment Learning on Product Reviews via Sentiment Ontology Tree","abstract_clean":"Existing works on sentiment analysis on product reviews suffer from the following limitations: (1) The knowledge of hierarchical relationships of products attributes is not fully utilized. (2) Reviews or sentences mentioning several attributes associated with complicated sentiments are not dealt with very well. In this paper, we propose a novel HL SOT approach to labeling a product's attributes and their associated sentiments in product reviews by a Hierarchical Learning (HL) process with a defined Sentiment Ontology Tree (SOT). The empirical analysis against a humanlabeled data set demonstrates promising and reasonable performance of the proposed HL SOT approach. While this paper is mainly on sentiment analysis on reviews of one product, our proposed HL SOT approach is easily generalized to labeling a mix of reviews of more than one products.","url":"https:\/\/aclanthology.org\/P10-1042"},{"ID":"weimer-etal-2007-automatically","methods":["classification techniques"],"center_method":[null],"tasks":["automatically assessing post quality","online discussions","quality of user generated content"],"center_task":[null,null,null],"Goal":["Decent Work and Economic Growth"],"text":"Automatically Assessing the Post Quality in Online Discussions on Software. Assessing the quality of user generated content is an important problem for many web forums. While quality is currently assessed manually, we propose an algorithm to assess the quality of forum posts automatically and test it on data provided by Nabble.com. We use state-of-the-art classification techniques and experiment with five feature classes: Surface, Lexical, Syntactic, Forum specific and Similarity features. We achieve an accuracy of 89% on the task of automatically assessing post quality in the software domain using forum specific features. Without forum specific features, we achieve an accuracy of 82%.","label":1,"title_clean":"Automatically Assessing the Post Quality in Online Discussions on Software","abstract_clean":"Assessing the quality of user generated content is an important problem for many web forums. While quality is currently assessed manually, we propose an algorithm to assess the quality of forum posts automatically and test it on data provided by Nabble.com. We use state of the art classification techniques and experiment with five feature classes: Surface, Lexical, Syntactic, Forum specific and Similarity features. We achieve an accuracy of 89% on the task of automatically assessing post quality in the software domain using forum specific features. Without forum specific features, we achieve an accuracy of 82%.","url":"https:\/\/aclanthology.org\/P07-2032"},{"ID":"wibberley-etal-2014-method51","methods":["social media analysis software platform"],"center_method":[null],"tasks":["mining insight"],"center_task":[null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Method51 for Mining Insight from Social Media Datasets. We present Method51, a social media analysis software platform with a set of accompanying methodologies. We discuss a series of case studies illustrating the platform's application, and motivating our methodological proposals.","label":1,"title_clean":"Method51 for Mining Insight from Social Media Datasets","abstract_clean":"We present Method51, a social media analysis software platform with a set of accompanying methodologies. We discuss a series of case studies illustrating the platform's application, and motivating our methodological proposals.","url":"https:\/\/aclanthology.org\/C14-2025"},{"ID":"wich-etal-2020-impact","methods":["deep neural network","hate speech"],"center_method":["deep neural network","hate speech"],"tasks":["hate speech"],"center_task":["hate speech"],"Goal":["Peace, Justice and Strong Institutions"],"text":"Impact of Politically Biased Data on Hate Speech Classification. One challenge that social media platforms are facing nowadays is hate speech. Hence, automatic hate speech detection has been increasingly researched in recent years-in particular with the rise of deep learning. A problem of these models is their vulnerability to undesirable bias in training data. We investigate the impact of political bias on hate speech classification by constructing three politicallybiased data sets (left-wing, right-wing, politically neutral) and compare the performance of classifiers trained on them. We show that (1) political bias negatively impairs the performance of hate speech classifiers and (2) an explainable machine learning model can help to visualize such bias within the training data. The results show that political bias in training data has an impact on hate speech classification and can become a serious issue.","label":1,"title_clean":"Impact of Politically Biased Data on Hate Speech Classification","abstract_clean":"One challenge that social media platforms are facing nowadays is hate speech. Hence, automatic hate speech detection has been increasingly researched in recent years in particular with the rise of deep learning. A problem of these models is their vulnerability to undesirable bias in training data. We investigate the impact of political bias on hate speech classification by constructing three politicallybiased data sets (left wing, right wing, politically neutral) and compare the performance of classifiers trained on them. We show that (1) political bias negatively impairs the performance of hate speech classifiers and (2) an explainable machine learning model can help to visualize such bias within the training data. The results show that political bias in training data has an impact on hate speech classification and can become a serious issue.","url":"https:\/\/aclanthology.org\/2020.alw-1.7"},{"ID":"wilks-1993-developments","methods":["statistical approach","systran","ibm"],"center_method":[null,null,null],"tasks":["large scale gathering of data","machine translation"],"center_task":[null,"machine translation"],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Developments in machine translation research in the US. The paper argues that the IBM statistical approach to machine translation has done rather better after a few years than many sceptics believed it could. However, it is neither as novel as its proponents suggest nor is it making claims as clear and simple as they would have us believe. The performance of the purely statistical system (and we discuss what that phrase could mean) has not equalled the performance of SYSTRAN. More importantly, the system is now being shifted to a hybrid that incorporates much of the linguistic information that it was initially claimed by IBM would not be needed for MT. Hence, one might infer that its own proponent do not believe \"pure\" statistics sufficient for MT of a usable quality. In addition to real limits on the statistical method, there are also strong economic limits imposed by their methodology of data gathering. However, the paper concludes that the IBM group have done the field a great service in pushing these methods far further than before, and by reminding everyone of the virtues of empiricism in the field and the need for large scale gathering of data.","label":1,"title_clean":"Developments in machine translation research in the US","abstract_clean":"The paper argues that the IBM statistical approach to machine translation has done rather better after a few years than many sceptics believed it could. However, it is neither as novel as its proponents suggest nor is it making claims as clear and simple as they would have us believe. The performance of the purely statistical system (and we discuss what that phrase could mean) has not equalled the performance of SYSTRAN. More importantly, the system is now being shifted to a hybrid that incorporates much of the linguistic information that it was initially claimed by IBM would not be needed for MT. Hence, one might infer that its own proponent do not believe \"pure\" statistics sufficient for MT of a usable quality. In addition to real limits on the statistical method, there are also strong economic limits imposed by their methodology of data gathering. However, the paper concludes that the IBM group have done the field a great service in pushing these methods far further than before, and by reminding everyone of the virtues of empiricism in the field and the need for large scale gathering of data.","url":"https:\/\/aclanthology.org\/1993.tc-1.1"},{"ID":"williams-liden-2017-demonstration","methods":["teaching methods","recurrent neural networks","hybrid code networks"],"center_method":[null,"recurrent neural networks",null],"tasks":["end to end dialog systems","interactive teaching","restaurant information","pizza ordering","weather forecasts","end to end dialog control"],"center_task":[null,null,null,null,null,null],"Goal":["Quality Education"],"text":"Demonstration of interactive teaching for end-to-end dialog control with hybrid code networks. This is a demonstration of interactive teaching for practical end-to-end dialog systems driven by a recurrent neural network. In this approach, a developer teaches the network by interacting with the system and providing on-the-spot corrections. Once a system is deployed, a developer can also correct mistakes in logged dialogs. This demonstration shows both of these teaching methods applied to dialog systems in three domains: pizza ordering, restaurant information, and weather forecasts.","label":1,"title_clean":"Demonstration of interactive teaching for end to end dialog control with hybrid code networks","abstract_clean":"This is a demonstration of interactive teaching for practical end to end dialog systems driven by a recurrent neural network. In this approach, a developer teaches the network by interacting with the system and providing on the spot corrections. Once a system is deployed, a developer can also correct mistakes in logged dialogs. This demonstration shows both of these teaching methods applied to dialog systems in three domains: pizza ordering, restaurant information, and weather forecasts.","url":"https:\/\/aclanthology.org\/W17-5511"},{"ID":"wilson-wun-2020-automatic","methods":["student identification tool","profile based features","machine learning methods"],"center_method":[null,null,"machine learning methods"],"tasks":["automatic classification of students","twitter behaviors","educational applications","professional networking","study of youth populations","computational social science research"],"center_task":[null,null,"educational applications",null,null,null],"Goal":["Quality Education"],"text":"Automatic Classification of Students on Twitter Using Simple Profile Information. Obtaining social media demographic information using machine learning is important for efficient computational social science research. Automatic age classification has been accomplished with relative success and allows for the study of youth populations, but student classification-determining which users are currently attending an academic institution-has not been thoroughly studied. Previous work (He et al., 2016) proposes a model which utilizes 3 tweet-content features to classify users as students or non-students. This model achieves an accuracy of 84%, but is restrictive and time intensive because it requires accessing and processing many user tweets. In this study, we propose classification models which use 7 numerical features and 10 text-based features drawn from simple profile information. These profile-based features allow for faster, more accessible data collection and enable the classification of users without needing access to their tweets. Compared to previous models, our models identify students with greater accuracy; our best model obtains an accuracy of 88.1% and an F1 score of .704. This improved student identification tool has the potential to facilitate research on topics ranging from professional networking to the impact of education on Twitter behaviors.","label":1,"title_clean":"Automatic Classification of Students on Twitter Using Simple Profile Information","abstract_clean":"Obtaining social media demographic information using machine learning is important for efficient computational social science research. Automatic age classification has been accomplished with relative success and allows for the study of youth populations, but student classification determining which users are currently attending an academic institution has not been thoroughly studied. Previous work (He et al., 2016) proposes a model which utilizes 3 tweet content features to classify users as students or non students. This model achieves an accuracy of 84%, but is restrictive and time intensive because it requires accessing and processing many user tweets. In this study, we propose classification models which use 7 numerical features and 10 text based features drawn from simple profile information. These profile based features allow for faster, more accessible data collection and enable the classification of users without needing access to their tweets. Compared to previous models, our models identify students with greater accuracy; our best model obtains an accuracy of 88.1% and an F1 score of .704. This improved student identification tool has the potential to facilitate research on topics ranging from professional networking to the impact of education on Twitter behaviors.","url":"https:\/\/aclanthology.org\/2020.aacl-srw.5"},{"ID":"wood-doughty-etal-2022-model","methods":["model distillation","teacher models","1 models","machine learning methods","student model"],"center_method":[null,null,null,"machine learning methods",null],"tasks":["assigning icd codes","explainable ai","human machine decision making","clinical or other high risk settings","faithful explanations of medical code predictions"],"center_task":[null,null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"Model Distillation for Faithful Explanations of Medical Code Predictions. Machine learning models that offer excellent predictive performance often lack the interpretability necessary to support integrated human machine decision-making. In clinical or other high-risk settings, domain experts may be unwilling to trust model predictions without explanations. Work in explainable AI must balance competing objectives along two different axes: 1) Models should ideally be both accurate and simple. 2) Explanations must balance faithfulness to the model's decisionmaking with their plausibility to a domain expert. We propose to use knowledge distillation, or training a student model that mimics the behavior of a trained teacher model, as a technique to generate faithful and plausible explanations. We evaluate our approach on the task of assigning ICD codes to clinical notes to demonstrate that the student model is faithful to the teacher model's behavior and produces quality natural language explanations.","label":1,"title_clean":"Model Distillation for Faithful Explanations of Medical Code Predictions","abstract_clean":"Machine learning models that offer excellent predictive performance often lack the interpretability necessary to support integrated human machine decision making. In clinical or other high risk settings, domain experts may be unwilling to trust model predictions without explanations. Work in explainable AI must balance competing objectives along two different axes: 1) Models should ideally be both accurate and simple. 2) Explanations must balance faithfulness to the model's decisionmaking with their plausibility to a domain expert. We propose to use knowledge distillation, or training a student model that mimics the behavior of a trained teacher model, as a technique to generate faithful and plausible explanations. We evaluate our approach on the task of assigning ICD codes to clinical notes to demonstrate that the student model is faithful to the teacher model's behavior and produces quality natural language explanations.","url":"https:\/\/aclanthology.org\/2022.bionlp-1.41"},{"ID":"wu-etal-2006-computational","methods":["care","concordancer","language model of abstract moves"],"center_method":[null,null,null],"tasks":["computational analysis of move structures","webbased computer assisted academic writing","digital learning"],"center_task":[null,null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Computational Analysis of Move Structures in Academic Abstracts. This paper introduces a method for computational analysis of move structures in abstracts of research articles. In our approach, sentences in a given abstract are analyzed and labeled with a specific move in light of various rhetorical functions. The method involves automatically gathering a large number of abstracts from the Web and building a language model of abstract moves. We also present a prototype concordancer, CARE, which exploits the move-tagged abstracts for digital learning. This system provides a promising approach to Webbased computer-assisted academic writing.","label":1,"title_clean":"Computational Analysis of Move Structures in Academic Abstracts","abstract_clean":"This paper introduces a method for computational analysis of move structures in abstracts of research articles. In our approach, sentences in a given abstract are analyzed and labeled with a specific move in light of various rhetorical functions. The method involves automatically gathering a large number of abstracts from the Web and building a language model of abstract moves. We also present a prototype concordancer, CARE, which exploits the move tagged abstracts for digital learning. This system provides a promising approach to Webbased computer assisted academic writing.","url":"https:\/\/aclanthology.org\/P06-4011"},{"ID":"wu-etal-2019-wtmed","methods":["text encoder","hybrid approach","base models","conflict resolution strategies","wtmed","ensemble models","syntax encoder"],"center_method":[null,"hybrid approach",null,null,null,null,null],"tasks":["inference"],"center_task":["inference"],"Goal":["Good Health and Well-Being"],"text":"WTMED at MEDIQA 2019: A Hybrid Approach to Biomedical Natural Language Inference. Natural language inference (NLI) is challenging, especially when it is applied to technical domains such as biomedical settings. In this paper, we propose a hybrid approach to biomedical NLI where different types of information are exploited for this task. Our base model includes a pre-trained text encoder as the core component, and a syntax encoder and a feature encoder to capture syntactic and domain-specific information. Then we combine the output of different base models to form more powerful ensemble models. Finally, we design two conflict resolution strategies when the test data contain multiple (premise, hypothesis) pairs with the same premise. We train our models on the MedNLI dataset, yielding the best performance on the test set of the MEDIQA 2019 Task 1.","label":1,"title_clean":"WTMED at MEDIQA 2019: A Hybrid Approach to Biomedical Natural Language Inference","abstract_clean":"Natural language inference (NLI) is challenging, especially when it is applied to technical domains such as biomedical settings. In this paper, we propose a hybrid approach to biomedical NLI where different types of information are exploited for this task. Our base model includes a pre trained text encoder as the core component, and a syntax encoder and a feature encoder to capture syntactic and domain specific information. Then we combine the output of different base models to form more powerful ensemble models. Finally, we design two conflict resolution strategies when the test data contain multiple (premise, hypothesis) pairs with the same premise. We train our models on the MedNLI dataset, yielding the best performance on the test set of the MEDIQA 2019 Task 1.","url":"https:\/\/aclanthology.org\/W19-5044.pdf"},{"ID":"wu-etal-2021-counterfactual","methods":["counterfactual multi granularity graph supporting facts extraction","emrs","hierarchical graph network","graph network"],"center_method":[null,null,null,null],"tasks":["diagnosis","medical field","artificial intelligence","counterfactual supporting facts extraction","explainable medical record based diagnosis"],"center_task":[null,null,"artificial intelligence",null,null],"Goal":["Good Health and Well-Being"],"text":"Counterfactual Supporting Facts Extraction for Explainable Medical Record Based Diagnosis with Graph Network. Providing a reliable explanation for clinical diagnosis based on the Electronic Medical Record (EMR) is fundamental to the application of Artificial Intelligence in the medical field. Current methods mostly treat the EMR as a text sequence and provide explanations based on a precise medical knowledge base, which is disease-specific and difficult to obtain for experts in reality. Therefore, we propose a counterfactual multi-granularity graph supporting facts extraction (CMGE) method to extract supporting facts from the irregular EMR itself without external knowledge bases in this paper. Specifically, we first structure the sequence of the EMR into a hierarchical graph network and then obtain the causal relationship between multi-granularity features and diagnosis results through counterfactual intervention on the graph. Features having the strongest causal connection with the results provide interpretive support for the diagnosis. Experimental results on real Chinese EMRs of the lymphedema demonstrate that our method can diagnose four types of EMRs correctly, and can provide accurate supporting facts for the results. More importantly, the results on different diseases demonstrate the robustness of our approach, which represents the potential application in the medical field 1 .","label":1,"title_clean":"Counterfactual Supporting Facts Extraction for Explainable Medical Record Based Diagnosis with Graph Network","abstract_clean":"Providing a reliable explanation for clinical diagnosis based on the Electronic Medical Record (EMR) is fundamental to the application of Artificial Intelligence in the medical field. Current methods mostly treat the EMR as a text sequence and provide explanations based on a precise medical knowledge base, which is disease specific and difficult to obtain for experts in reality. Therefore, we propose a counterfactual multi granularity graph supporting facts extraction (CMGE) method to extract supporting facts from the irregular EMR itself without external knowledge bases in this paper. Specifically, we first structure the sequence of the EMR into a hierarchical graph network and then obtain the causal relationship between multi granularity features and diagnosis results through counterfactual intervention on the graph. Features having the strongest causal connection with the results provide interpretive support for the diagnosis. Experimental results on real Chinese EMRs of the lymphedema demonstrate that our method can diagnose four types of EMRs correctly, and can provide accurate supporting facts for the results. More importantly, the results on different diseases demonstrate the robustness of our approach, which represents the potential application in the medical field 1 .","url":"https:\/\/aclanthology.org\/2021.naacl-main.156"},{"ID":"wu-etal-2021-multimodal","methods":["multimodal co attention networks","co attention networks","multimodal fusion"],"center_method":[null,null,null],"tasks":["expert identification","fake news detection"],"center_task":[null,"fake news detection"],"Goal":["Peace, Justice and Strong Institutions"],"text":"Multimodal Fusion with Co-Attention Networks for Fake News Detection. Fake news with textual and visual contents has a better story-telling ability than text-only contents, and can be spread quickly with social media. People can be easily deceived by such fake news, and traditional expert identification is labor-intensive. Therefore, automatic detection of multimodal fake news has become a new hot-spot issue. A shortcoming of existing approaches is their inability to fuse multimodality features effectively. They simply concatenate unimodal features without considering inter-modality relations. Inspired by the way people read news with image and text, we propose a novel Multimodal Co-Attention Networks (MCAN) to better fuse textual and visual features for fake news detection. Extensive experiments conducted on two realworld datasets demonstrate that MCAN can learn inter-dependencies among multimodal features and outperforms state-of-the-art methods.","label":1,"title_clean":"Multimodal Fusion with Co Attention Networks for Fake News Detection","abstract_clean":"Fake news with textual and visual contents has a better story telling ability than text only contents, and can be spread quickly with social media. People can be easily deceived by such fake news, and traditional expert identification is labor intensive. Therefore, automatic detection of multimodal fake news has become a new hot spot issue. A shortcoming of existing approaches is their inability to fuse multimodality features effectively. They simply concatenate unimodal features without considering inter modality relations. Inspired by the way people read news with image and text, we propose a novel Multimodal Co Attention Networks (MCAN) to better fuse textual and visual features for fake news detection. Extensive experiments conducted on two realworld datasets demonstrate that MCAN can learn inter dependencies among multimodal features and outperforms state of the art methods.","url":"https:\/\/aclanthology.org\/2021.findings-acl.226"},{"ID":"xianwei-etal-2021-emotion","methods":["emotion classification model","bert","emotion category description"],"center_method":[null,"bert",null],"tasks":["sentiment analysis","question answering","modeling","content of microblogs"],"center_task":["sentiment analysis","question answering",null,null],"Goal":["Good Health and Well-Being","Peace, Justice and Strong Institutions"],"text":"Emotion Classification of COVID-19 Chinese Microblogs based on the Emotion Category Description. Emotion classification of COVID-19 Chinese microblogs helps analyze the public opinion triggered by COVID-19. Existing methods only consider the features of the microblog itself, without combining the semantics of emotion categories for modeling. Emotion classification of microblogs is a process of reading the content of microblogs and combining the semantics of emotion categories to understand whether it contains a certain emotion. Inspired by this, we propose an emotion classification model based on the emotion category description for COVID-19 Chinese microblogs. Firstly, we expand all emotion categories into formalized category descriptions. Secondly, based on the idea of question answering, we construct a question for each microblog in the form of 'What is the emotion expressed in the text X?' and regard all category descriptions as candidate answers. Finally, we construct a question-and-answer pair and use it as the input of the BERT model to complete emotion classification. By integrating rich contextual and category semantics, the model can better understand the emotion of microblogs. Experiments on the COVID-19 Chinese microblog dataset show that our approach outperforms many existing emotion classification methods, including the BERT baseline.","label":1,"title_clean":"Emotion Classification of COVID 19 Chinese Microblogs based on the Emotion Category Description","abstract_clean":"Emotion classification of COVID 19 Chinese microblogs helps analyze the public opinion triggered by COVID 19. Existing methods only consider the features of the microblog itself, without combining the semantics of emotion categories for modeling. Emotion classification of microblogs is a process of reading the content of microblogs and combining the semantics of emotion categories to understand whether it contains a certain emotion. Inspired by this, we propose an emotion classification model based on the emotion category description for COVID 19 Chinese microblogs. Firstly, we expand all emotion categories into formalized category descriptions. Secondly, based on the idea of question answering, we construct a question for each microblog in the form of 'What is the emotion expressed in the text X?' and regard all category descriptions as candidate answers. Finally, we construct a question and answer pair and use it as the input of the BERT model to complete emotion classification. By integrating rich contextual and category semantics, the model can better understand the emotion of microblogs. Experiments on the COVID 19 Chinese microblog dataset show that our approach outperforms many existing emotion classification methods, including the BERT baseline.","url":"https:\/\/aclanthology.org\/2021.ccl-1.82"},{"ID":"xiao-etal-2021-end","methods":["utterance transfer approach","utterance transfer","end to end conversational search system","dialogue system","search engine","convsearch","product schemaknowledge"],"center_method":[null,null,null,"dialogue system","search engine",null,null],"tasks":["online shopping","end to end conversational search","natural adaptive and interactive shopping experience"],"center_task":[null,null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"End-to-End Conversational Search for Online Shopping with Utterance Transfer. Successful conversational search systems can present natural, adaptive and interactive shopping experience for online shopping customers. However, building such systems from scratch faces real word challenges from both imperfect product schema\/knowledge and lack of training dialog data. In this work we first propose ConvSearch, an end-to-end conversational search system that deeply combines the dialog system with search. It leverages the text profile to retrieve products, which is more robust against imperfect product schema\/knowledge compared with using product attributes alone. We then address the lack of data challenges by proposing an utterance transfer approach that generates dialogue utterances by using existing dialog from other domains, and leveraging the search behavior data from e-commerce retailer. With utterance transfer, we introduce a new conversational search dataset for online shopping. Experiments show that our utterance transfer method can significantly improve the availability of training dialogue data without crowd-sourcing, and the conversational search system significantly outperformed the best tested baseline.","label":1,"title_clean":"End to End Conversational Search for Online Shopping with Utterance Transfer","abstract_clean":"Successful conversational search systems can present natural, adaptive and interactive shopping experience for online shopping customers. However, building such systems from scratch faces real word challenges from both imperfect product schema\/knowledge and lack of training dialog data. In this work we first propose ConvSearch, an end to end conversational search system that deeply combines the dialog system with search. It leverages the text profile to retrieve products, which is more robust against imperfect product schema\/knowledge compared with using product attributes alone. We then address the lack of data challenges by proposing an utterance transfer approach that generates dialogue utterances by using existing dialog from other domains, and leveraging the search behavior data from e commerce retailer. With utterance transfer, we introduce a new conversational search dataset for online shopping. Experiments show that our utterance transfer method can significantly improve the availability of training dialogue data without crowd sourcing, and the conversational search system significantly outperformed the best tested baseline.","url":"https:\/\/aclanthology.org\/2021.emnlp-main.280"},{"ID":"xie-etal-2021-humorhunter","methods":["deberta model","language models","disentangled attention mechanism"],"center_method":[null,"language models",null],"tasks":["humor and offense recognition","semeval"],"center_task":[null,"semeval"],"Goal":["Peace, Justice and Strong Institutions"],"text":"HumorHunter at SemEval-2021 Task 7: Humor and Offense Recognition with Disentangled Attention. In this paper, we describe our system submitted to SemEval 2021 Task 7: HaHackathon: Detecting and Rating Humor and Offense. The task aims at predicting whether the given text is humorous, the average humor rating given by the annotators, and whether the humor rating is controversial. In addition, the task also involves predicting how offensive the text is. Our approach adopts the DeBERTa architecture with disentangled attention mechanism, where the attention scores between words are calculated based on their content vectors and relative position vectors. We also took advantage of the pre-trained language models and fine-tuned the DeBERTa model on all the four subtasks. We experimented with several BERT-like structures and found that the large DeBERTa model generally performs better. During the evaluation phase, our system achieved an F-score of 0.9480 on subtask 1a, an RMSE of 0.5510 on subtask 1b, an F-score of 0.4764 on subtask 1c, and an RMSE of 0.4230 on subtask 2a (rank 3 on the leaderboard).","label":1,"title_clean":"HumorHunter at SemEval 2021 Task 7: Humor and Offense Recognition with Disentangled Attention","abstract_clean":"In this paper, we describe our system submitted to SemEval 2021 Task 7: HaHackathon: Detecting and Rating Humor and Offense. The task aims at predicting whether the given text is humorous, the average humor rating given by the annotators, and whether the humor rating is controversial. In addition, the task also involves predicting how offensive the text is. Our approach adopts the DeBERTa architecture with disentangled attention mechanism, where the attention scores between words are calculated based on their content vectors and relative position vectors. We also took advantage of the pre trained language models and fine tuned the DeBERTa model on all the four subtasks. We experimented with several BERT like structures and found that the large DeBERTa model generally performs better. During the evaluation phase, our system achieved an F score of 0.9480 on subtask 1a, an RMSE of 0.5510 on subtask 1b, an F score of 0.4764 on subtask 1c, and an RMSE of 0.4230 on subtask 2a (rank 3 on the leaderboard).","url":"https:\/\/aclanthology.org\/2021.semeval-1.33.pdf"},{"ID":"xie-pu-2021-empathetic","methods":["language models"],"center_method":["language models"],"tasks":["empathetic dialog generation","response generation"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"Empathetic Dialog Generation with Fine-Grained Intents. Empathetic dialog generation aims at generating coherent responses following previous dialog turns and, more importantly, showing a sense of caring and a desire to help. Existing models either rely on pre-defined emotion labels to guide the response generation, or use deterministic rules to decide the emotion of the response. With the advent of advanced language models, it is possible to learn subtle interactions directly from the dataset, providing that the emotion categories offer sufficient nuances and other non-emotional but emotional regulating intents are included. In this paper, we describe how to incorporate a taxonomy of 32 emotion categories and 8 additional emotion regulating intents to succeed the task of empathetic response generation. To facilitate the training, we also curated a largescale emotional dialog dataset from movie subtitles. Through a carefully designed crowdsourcing experiment, we evaluated and demonstrated how our model produces more empathetic dialogs compared with its baselines.","label":1,"title_clean":"Empathetic Dialog Generation with Fine Grained Intents","abstract_clean":"Empathetic dialog generation aims at generating coherent responses following previous dialog turns and, more importantly, showing a sense of caring and a desire to help. Existing models either rely on pre defined emotion labels to guide the response generation, or use deterministic rules to decide the emotion of the response. With the advent of advanced language models, it is possible to learn subtle interactions directly from the dataset, providing that the emotion categories offer sufficient nuances and other non emotional but emotional regulating intents are included. In this paper, we describe how to incorporate a taxonomy of 32 emotion categories and 8 additional emotion regulating intents to succeed the task of empathetic response generation. To facilitate the training, we also curated a largescale emotional dialog dataset from movie subtitles. Through a carefully designed crowdsourcing experiment, we evaluated and demonstrated how our model produces more empathetic dialogs compared with its baselines.","url":"https:\/\/aclanthology.org\/2021.conll-1.10"},{"ID":"xiong-litman-2011-understanding","methods":["feature engineering","product review analysis techniques"],"center_method":["feature engineering",null],"tasks":["experts perceived helpfulness","perceived peer review helpfulness","automatic prediction","peer review domain"],"center_task":[null,null,null,null],"Goal":["Quality Education"],"text":"Understanding Differences in Perceived Peer-Review Helpfulness using Natural Language Processing. Identifying peer-review helpfulness is an important task for improving the quality of feedback received by students, as well as for helping students write better reviews. As we tailor standard product review analysis techniques to our peer-review domain, we notice that peerreview helpfulness differs not only between students and experts but also between types of experts. In this paper, we investigate how different types of perceived helpfulness might influence the utility of features for automatic prediction. Our feature selection results show that certain low-level linguistic features are more useful for predicting student perceived helpfulness, while high-level cognitive constructs are more effective in modeling experts' perceived helpfulness.","label":1,"title_clean":"Understanding Differences in Perceived Peer Review Helpfulness using Natural Language Processing","abstract_clean":"Identifying peer review helpfulness is an important task for improving the quality of feedback received by students, as well as for helping students write better reviews. As we tailor standard product review analysis techniques to our peer review domain, we notice that peerreview helpfulness differs not only between students and experts but also between types of experts. In this paper, we investigate how different types of perceived helpfulness might influence the utility of features for automatic prediction. Our feature selection results show that certain low level linguistic features are more useful for predicting student perceived helpfulness, while high level cognitive constructs are more effective in modeling experts' perceived helpfulness.","url":"https:\/\/aclanthology.org\/W11-1402"},{"ID":"xu-etal-2013-examination","methods":["regrettable posts predictor","exploratory analysis"],"center_method":[null,null],"tasks":["regret"],"center_task":[null],"Goal":["Good Health and Well-Being","Peace, Justice and Strong Institutions"],"text":"An Examination of Regret in Bullying Tweets. Social media users who post bullying related tweets may later experience regret, potentially causing them to delete their posts. In this paper, we construct a corpus of bullying tweets and periodically check the existence of each tweet in order to infer if and when it becomes deleted. We then conduct exploratory analysis in order to isolate factors associated with deleted posts. Finally, we propose the construction of a regrettable posts predictor to warn users if a tweet might cause regret.","label":1,"title_clean":"An Examination of Regret in Bullying Tweets","abstract_clean":"Social media users who post bullying related tweets may later experience regret, potentially causing them to delete their posts. In this paper, we construct a corpus of bullying tweets and periodically check the existence of each tweet in order to infer if and when it becomes deleted. We then conduct exploratory analysis in order to isolate factors associated with deleted posts. Finally, we propose the construction of a regrettable posts predictor to warn users if a tweet might cause regret.","url":"https:\/\/aclanthology.org\/N13-1082"},{"ID":"xu-zhao-2012-using","methods":["syntactic dependency parsing tree","deep linguistic features"],"center_method":[null,null],"tasks":["opinion spam","language tasks"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Using Deep Linguistic Features for Finding Deceptive Opinion Spam. While most recent work has focused on instances of opinion spam which are manually identifiable or deceptive opinion spam which are written by paid writers separately, in this work we study both of these interesting topics and propose an effective framework which has good performance on both datasets. Based on the golden-standard opinion spam dataset, we propose a novel model which integrates some deep linguistic features derived from a syntactic dependency parsing tree to discriminate deceptive opinions from normal ones. On a background of multiple language tasks, our model is evaluated on both English (gold-standard) and Chinese (non-gold) datasets. The experimental results show that our model produces state-of-the-art results on both of the topics.","label":1,"title_clean":"Using Deep Linguistic Features for Finding Deceptive Opinion Spam","abstract_clean":"While most recent work has focused on instances of opinion spam which are manually identifiable or deceptive opinion spam which are written by paid writers separately, in this work we study both of these interesting topics and propose an effective framework which has good performance on both datasets. Based on the golden standard opinion spam dataset, we propose a novel model which integrates some deep linguistic features derived from a syntactic dependency parsing tree to discriminate deceptive opinions from normal ones. On a background of multiple language tasks, our model is evaluated on both English (gold standard) and Chinese (non gold) datasets. The experimental results show that our model produces state of the art results on both of the topics.","url":"https:\/\/aclanthology.org\/C12-2131"},{"ID":"yamakoshi-etal-2021-evaluation","methods":["translation methods","isdit"],"center_method":[null,null],"tasks":["machine translation","focal translation","differential translation"],"center_task":["machine translation",null,null],"Goal":["Decent Work and Economic Growth","Partnership for the Goals"],"text":"Evaluation Scheme of Focal Translation for Japanese Partially Amended Statutes. For updating the translations of Japanese statutes based on their amendments, we need to consider the translation \"focality;\" that is, we should only modify expressions that are relevant to the amendment and retain the others to avoid misconstruing its contents. In this paper, we introduce an evaluation metric and a corpus to improve focality evaluations. Our metric is called an Inclusive Score for DIfferential Translation: (ISDIT). ISDIT consists of two factors: (1) the n-gram recall of expressions unaffected by the amendment and (2) the n-gram precision of the output compared to the reference. This metric supersedes an existing one for focality by simultaneously calculating the translation quality of the changed expressions in addition to that of the unchanged expressions. We also newly compile a corpus for Japanese partially amendment translation that secures the focality of the post-amendment translations, while an existing evaluation corpus does not. With the metric and the corpus, we examine the performance of existing translation methods for Japanese partially amendment translations.","label":1,"title_clean":"Evaluation Scheme of Focal Translation for Japanese Partially Amended Statutes","abstract_clean":"For updating the translations of Japanese statutes based on their amendments, we need to consider the translation \"focality;\" that is, we should only modify expressions that are relevant to the amendment and retain the others to avoid misconstruing its contents. In this paper, we introduce an evaluation metric and a corpus to improve focality evaluations. Our metric is called an Inclusive Score for DIfferential Translation: (ISDIT). ISDIT consists of two factors: (1) the n gram recall of expressions unaffected by the amendment and (2) the n gram precision of the output compared to the reference. This metric supersedes an existing one for focality by simultaneously calculating the translation quality of the changed expressions in addition to that of the unchanged expressions. We also newly compile a corpus for Japanese partially amendment translation that secures the focality of the post amendment translations, while an existing evaluation corpus does not. With the metric and the corpus, we examine the performance of existing translation methods for Japanese partially amendment translations.","url":"https:\/\/aclanthology.org\/2021.wat-1.12"},{"ID":"yan-etal-2021-adatag","methods":["adatag","pretrained attribute embeddings","encoder","multiattribute model","hypernetwork","attribute specific models","sequence labeling architectures","mixture of experts moe module","adaptive decoding"],"center_method":[null,null,"encoder",null,null,null,null,null,null],"tasks":["extraction","knowledge sharing","multi attribute value extraction","e commerce"],"center_task":[null,null,null,"e commerce"],"Goal":["Decent Work and Economic Growth","Industry, Innovation and Infrastrucure"],"text":"AdaTag: Multi-Attribute Value Extraction from Product Profiles with Adaptive Decoding. Automatic extraction of product attribute values is an important enabling technology in e-Commerce platforms. This task is usually modeled using sequence labeling architectures, with several extensions to handle multi-attribute extraction. One line of previous work constructs attribute-specific models, through separate decoders or entirely separate models. However, this approach constrains knowledge sharing across different attributes. Other contributions use a single multiattribute model, with different techniques to embed attribute information. But sharing the entire network parameters across all attributes can limit the model's capacity to capture attribute-specific characteristics. In this paper we present AdaTag, which uses adaptive decoding to handle extraction. We parameterize the decoder with pretrained attribute embeddings, through a hypernetwork and a Mixture-of-Experts (MoE) module. This allows for separate, but semantically correlated, decoders to be generated on the fly for different attributes. This approach facilitates knowledge sharing, while maintaining the specificity of each attribute. Our experiments on a realworld e-Commerce dataset show marked improvements over previous methods. * Most of the work was done during an internship at Amazon.","label":1,"title_clean":"AdaTag: Multi Attribute Value Extraction from Product Profiles with Adaptive Decoding","abstract_clean":"Automatic extraction of product attribute values is an important enabling technology in e Commerce platforms. This task is usually modeled using sequence labeling architectures, with several extensions to handle multi attribute extraction. One line of previous work constructs attribute specific models, through separate decoders or entirely separate models. However, this approach constrains knowledge sharing across different attributes. Other contributions use a single multiattribute model, with different techniques to embed attribute information. But sharing the entire network parameters across all attributes can limit the model's capacity to capture attribute specific characteristics. In this paper we present AdaTag, which uses adaptive decoding to handle extraction. We parameterize the decoder with pretrained attribute embeddings, through a hypernetwork and a Mixture of Experts (MoE) module. This allows for separate, but semantically correlated, decoders to be generated on the fly for different attributes. This approach facilitates knowledge sharing, while maintaining the specificity of each attribute. Our experiments on a realworld e Commerce dataset show marked improvements over previous methods. * Most of the work was done during an internship at Amazon.","url":"https:\/\/aclanthology.org\/2021.acl-long.362"},{"ID":"yang-etal-2014-towards","methods":["predictive ensemble model","conceptual model","mooc","structural equation modeling technique"],"center_method":[null,null,null,null],"tasks":["thread resolveability","moocs"],"center_task":[null,null],"Goal":["Quality Education"],"text":"Towards Identifying the Resolvability of Threads in MOOCs. One important function of the discussion forums of Massive Open Online Courses (MOOCs) is for students to post problems they are unable to resolve and receive help from their peers and instructors. There are a large proportion of threads that are not resolved to the satisfaction of the students for various reasons. In this paper, we attack this problem by firstly constructing a conceptual model validated using a Structural Equation Modeling technique, which enables us to understand the factors that influence whether a problem thread is satisfactorily resolved. We then demonstrate the robustness of these findings using a predictive model that illustrates how accurately those factors can be used to predict whether a thread is resolved or unresolved. Experiments conducted on one MOOC show that thread resolveability connects closely to our proposed five dimensions and that the predictive ensemble model gives better performance over several baselines.","label":1,"title_clean":"Towards Identifying the Resolvability of Threads in MOOCs","abstract_clean":"One important function of the discussion forums of Massive Open Online Courses (MOOCs) is for students to post problems they are unable to resolve and receive help from their peers and instructors. There are a large proportion of threads that are not resolved to the satisfaction of the students for various reasons. In this paper, we attack this problem by firstly constructing a conceptual model validated using a Structural Equation Modeling technique, which enables us to understand the factors that influence whether a problem thread is satisfactorily resolved. We then demonstrate the robustness of these findings using a predictive model that illustrates how accurately those factors can be used to predict whether a thread is resolved or unresolved. Experiments conducted on one MOOC show that thread resolveability connects closely to our proposed five dimensions and that the predictive ensemble model gives better performance over several baselines.","url":"https:\/\/aclanthology.org\/W14-4104"},{"ID":"yang-etal-2015-sampling","methods":["sampling based alignment","alignment methods","bilingual hierarchical sub sentential alignment"],"center_method":[null,null,null],"tasks":["hierarchical sub sentential alignment","machine translation","asian translation","chinese japanese translation of patents"],"center_task":[null,"machine translation",null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Sampling-based Alignment and Hierarchical Sub-sentential Alignment in Chinese--Japanese Translation of Patents. This paper describes Chinese-Japanese translation systems based on different alignment methods using the JPO corpus and our submission (ID: WASUIPS) to the subtask of the 2015 Workshop on Asian Translation. One of the alignment methods used is bilingual hierarchical sub-sentential alignment combined with sampling-based multilingual alignment. We also accelerated this method and in this paper, we evaluate the translation results and time spent on several machine translation tasks. The training time is much faster than the standard baseline pipeline (GIZA++\/Moses) and MGIZA\/Moses.","label":1,"title_clean":"Sampling based Alignment and Hierarchical Sub sentential Alignment in Chinese Japanese Translation of Patents","abstract_clean":"This paper describes Chinese Japanese translation systems based on different alignment methods using the JPO corpus and our submission (ID: WASUIPS) to the subtask of the 2015 Workshop on Asian Translation. One of the alignment methods used is bilingual hierarchical sub sentential alignment combined with sampling based multilingual alignment. We also accelerated this method and in this paper, we evaluate the translation results and time spent on several machine translation tasks. The training time is much faster than the standard baseline pipeline (GIZA++\/Moses) and MGIZA\/Moses.","url":"https:\/\/aclanthology.org\/W15-5011"},{"ID":"yang-etal-2016-extraction","methods":["machine translation","sampling based alignment method"],"center_method":["machine translation",null],"tasks":["bilingual term extraction","machine translation","chinese japanese patent translation"],"center_task":[null,"machine translation",null],"Goal":["Peace, Justice and Strong Institutions","Industry, Innovation and Infrastrucure"],"text":"Extraction of Bilingual Technical Terms for Chinese-Japanese Patent Translation. The translation of patents or scientific papers is a key issue that should be helped by the use of statistical machine translation (SMT). In this paper, we propose a method to improve Chinese-Japanese patent SMT by premarking the training corpus with aligned bilingual multi-word terms. We automatically extract multi-word terms from monolingual corpora by combining statistical and linguistic filtering methods. We use the sampling-based alignment method to identify aligned terms and set some threshold on translation probabilities to select the most promising bilingual multi-word terms. We pre-mark a Chinese-Japanese training corpus with such selected aligned bilingual multi-word terms. We obtain the performance of over 70% precision in bilingual term extraction and a significant improvement of BLEU scores in our experiments on a Chinese-Japanese patent parallel corpus.","label":1,"title_clean":"Extraction of Bilingual Technical Terms for Chinese Japanese Patent Translation","abstract_clean":"The translation of patents or scientific papers is a key issue that should be helped by the use of statistical machine translation (SMT). In this paper, we propose a method to improve Chinese Japanese patent SMT by premarking the training corpus with aligned bilingual multi word terms. We automatically extract multi word terms from monolingual corpora by combining statistical and linguistic filtering methods. We use the sampling based alignment method to identify aligned terms and set some threshold on translation probabilities to select the most promising bilingual multi word terms. We pre mark a Chinese Japanese training corpus with such selected aligned bilingual multi word terms. We obtain the performance of over 70% precision in bilingual term extraction and a significant improvement of BLEU scores in our experiments on a Chinese Japanese patent parallel corpus.","url":"https:\/\/aclanthology.org\/N16-2012"},{"ID":"yao-etal-2010-practical","methods":["speech recognizers","acoustic and language model adaptation"],"center_method":[null,null],"tasks":["virtual human dialogue systems","speech recognition consumers","dialogue community","spoken dialogue systems"],"center_task":[null,null,null,null],"Goal":["Decent Work and Economic Growth"],"text":"Practical Evaluation of Speech Recognizers for Virtual Human Dialogue Systems. We perform a large-scale evaluation of multiple off-the-shelf speech recognizers across diverse domains for virtual human dialogue systems. Our evaluation is aimed at speech recognition consumers and potential consumers with limited experience with readily available recognizers. We focus on practical factors to determine what levels of performance can be expected from different available recognizers in various projects featuring different types of conversational utterances. Our results show that there is no single recognizer that outperforms all other recognizers in all domains. The performance of each recognizer may vary significantly depending on the domain, the size and perplexity of the corpus, the out-of-vocabulary rate, and whether acoustic and language model adaptation has been used or not. We expect that our evaluation will prove useful to other speech recognition consumers, especially in the dialogue community, and will shed some light on the key problem in spoken dialogue systems of selecting the most suitable available speech recognition system for a particular application, and what impact training will have.","label":1,"title_clean":"Practical Evaluation of Speech Recognizers for Virtual Human Dialogue Systems","abstract_clean":"We perform a large scale evaluation of multiple off the shelf speech recognizers across diverse domains for virtual human dialogue systems. Our evaluation is aimed at speech recognition consumers and potential consumers with limited experience with readily available recognizers. We focus on practical factors to determine what levels of performance can be expected from different available recognizers in various projects featuring different types of conversational utterances. Our results show that there is no single recognizer that outperforms all other recognizers in all domains. The performance of each recognizer may vary significantly depending on the domain, the size and perplexity of the corpus, the out of vocabulary rate, and whether acoustic and language model adaptation has been used or not. We expect that our evaluation will prove useful to other speech recognition consumers, especially in the dialogue community, and will shed some light on the key problem in spoken dialogue systems of selecting the most suitable available speech recognition system for a particular application, and what impact training will have.","url":"http:\/\/www.lrec-conf.org\/proceedings\/lrec2010\/pdf\/675_Paper.pdf"},{"ID":"yasaswini-etal-2021-iiitt","methods":["transfer learning"],"center_method":["transfer learning"],"tasks":["hate speech","dravidian languages"],"center_task":["hate speech",null],"Goal":["Peace, Justice and Strong Institutions"],"text":"IIITT@DravidianLangTech-EACL2021: Transfer Learning for Offensive Language Detection in Dravidian Languages. This paper demonstrates our work for the shared task on Offensive Language Identification in Dravidian Languages-EACL 2021. Offensive language detection in the various social media platforms was identified previously. However, with the increase in the diversity of users, there is a need to identify the offensive language in multilingual posts which are largely code-mixed or written in a non-native script. We approach this challenge with various transfer learning-based models to classify a given post or comment in Dravidian languages (Malayalam, Tamil and Kannada) into 6 categories. The source codes for our systems are published 1 .","label":1,"title_clean":"IIITT@DravidianLangTech EACL2021: Transfer Learning for Offensive Language Detection in Dravidian Languages","abstract_clean":"This paper demonstrates our work for the shared task on Offensive Language Identification in Dravidian Languages EACL 2021. Offensive language detection in the various social media platforms was identified previously. However, with the increase in the diversity of users, there is a need to identify the offensive language in multilingual posts which are largely code mixed or written in a non native script. We approach this challenge with various transfer learning based models to classify a given post or comment in Dravidian languages (Malayalam, Tamil and Kannada) into 6 categories. The source codes for our systems are published 1 .","url":"https:\/\/aclanthology.org\/2021.dravidianlangtech-1.25"},{"ID":"yates-etal-2016-effects","methods":["trend detection methods"],"center_method":[null],"tasks":["prevalence estimates","sampling","twitter trend detection","ili"],"center_task":[null,null,null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Effects of Sampling on Twitter Trend Detection. Much research has focused on detecting trends on Twitter, including health-related trends such as mentions of Influenza-like illnesses or their symptoms. The majority of this research has been conducted using Twitter's public feed, which includes only about 1% of all public tweets. It is unclear if, when, and how using Twitter's 1% feed has affected the evaluation of trend detection methods. In this work we use a larger feed to investigate the effects of sampling on Twitter trend detection. We focus on using health-related trends to estimate the prevalence of Influenza-like illnesses based on tweets, and use ground truth obtained from the CDC and Google Flu Trends to explore how the prevalence estimates degrade when moving from a 100% to a 1% sample. We find that using the public 1% sample is unlikely to substantially harm ILI estimates made at the national level, but can cause poor performance when estimates are made at the city level.","label":1,"title_clean":"Effects of Sampling on Twitter Trend Detection","abstract_clean":"Much research has focused on detecting trends on Twitter, including health related trends such as mentions of Influenza like illnesses or their symptoms. The majority of this research has been conducted using Twitter's public feed, which includes only about 1% of all public tweets. It is unclear if, when, and how using Twitter's 1% feed has affected the evaluation of trend detection methods. In this work we use a larger feed to investigate the effects of sampling on Twitter trend detection. We focus on using health related trends to estimate the prevalence of Influenza like illnesses based on tweets, and use ground truth obtained from the CDC and Google Flu Trends to explore how the prevalence estimates degrade when moving from a 100% to a 1% sample. We find that using the public 1% sample is unlikely to substantially harm ILI estimates made at the national level, but can cause poor performance when estimates are made at the city level.","url":"https:\/\/aclanthology.org\/L16-1479"},{"ID":"ying-etal-2021-longsumm","methods":["session and ensemble mechanism","session based automatic summarization model","bert","language models","longsumm 2021"],"center_method":[null,null,"bert","language models",null],"tasks":["summarization task","longsumm task","scientific document"],"center_task":[null,null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"LongSumm 2021: Session based automatic summarization model for scientific document. Most summarization task focuses on generating relatively short summaries. Such a length constraint might not be appropriate when summarizing scientific work. The LongSumm task needs participants generate long summary for scientific document. This task usual can be solved by language model. But an important problem is that model like BERT is limit to memory, and can not deal with a long input like a document. Also generate a long output is hard. In this paper, we propose a session based automatic summarization model (SBAS) which using a session and ensemble mechanism to generate long summary. And our model achieves the best performance in the LongSumm task.","label":1,"title_clean":"LongSumm 2021: Session based automatic summarization model for scientific document","abstract_clean":"Most summarization task focuses on generating relatively short summaries. Such a length constraint might not be appropriate when summarizing scientific work. The LongSumm task needs participants generate long summary for scientific document. This task usual can be solved by language model. But an important problem is that model like BERT is limit to memory, and can not deal with a long input like a document. Also generate a long output is hard. In this paper, we propose a session based automatic summarization model (SBAS) which using a session and ensemble mechanism to generate long summary. And our model achieves the best performance in the LongSumm task.","url":"https:\/\/aclanthology.org\/2021.sdp-1.12"},{"ID":"yousef-etal-2021-press","methods":["press freedom monitor","machine learning methods"],"center_method":[null,"machine learning methods"],"tasks":["detection of reported press and media freedom violations","country mapping","duplicates removal","production pipeline","querying apis","training phase","data gathering"],"center_task":[null,null,null,null,null,null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Press Freedom Monitor: Detection of Reported Press and Media Freedom Violations in Twitter and News Articles. Freedom of the press and media is of vital importance for democratically organised states and open societies. We introduce the Press Freedom Monitor, a tool that aims to detect reported press and media freedom violations in news articles and tweets. It is used by press and media freedom organisations to support their daily monitoring and to trigger rapid response actions. The Press Freedom Monitor enables the monitoring experts to get a swift overview of recently reported incidents and it has performed impressively in this regard. This paper presents our work on the tool, starting with the training phase, which comprises defining the topic-related keywords to be used for querying APIs for news and Twitter content and evaluating different machine learning models based on a training dataset specifically created for our use case. Then, we describe the components of the production pipeline, including data gathering, duplicates removal, country mapping, case mapping and the user interface. We also conducted a usability study to evaluate the effectiveness of the user interface, and describe improvement plans for future work.","label":1,"title_clean":"Press Freedom Monitor: Detection of Reported Press and Media Freedom Violations in Twitter and News Articles","abstract_clean":"Freedom of the press and media is of vital importance for democratically organised states and open societies. We introduce the Press Freedom Monitor, a tool that aims to detect reported press and media freedom violations in news articles and tweets. It is used by press and media freedom organisations to support their daily monitoring and to trigger rapid response actions. The Press Freedom Monitor enables the monitoring experts to get a swift overview of recently reported incidents and it has performed impressively in this regard. This paper presents our work on the tool, starting with the training phase, which comprises defining the topic related keywords to be used for querying APIs for news and Twitter content and evaluating different machine learning models based on a training dataset specifically created for our use case. Then, we describe the components of the production pipeline, including data gathering, duplicates removal, country mapping, case mapping and the user interface. We also conducted a usability study to evaluate the effectiveness of the user interface, and describe improvement plans for future work.","url":"https:\/\/aclanthology.org\/2021.emnlp-demo.18"},{"ID":"yu-etal-2020-mooccube","methods":["mooccube"],"center_method":[null],"tasks":["moocs","prerequisite discovery task","course concept extraction"],"center_task":[null,null,null],"Goal":["Quality Education"],"text":"MOOCCube: A Large-scale Data Repository for NLP Applications in MOOCs. The prosperity of Massive Open Online Courses (MOOCs) provides fodder for many NLP and AI research for education applications, e.g., course concept extraction, prerequisite relation discovery, etc. However, the publicly available datasets of MOOC are limited in size with few types of data, which hinders advanced models and novel attempts in related topics. Therefore, we present MOOCCube, a large-scale data repository of over 700 MOOC courses, 100k concepts, 8 million student behaviors with an external resource. Moreover, we conduct a prerequisite discovery task as an example application to show the potential of MOOCCube in facilitating relevant research. The data repository is now available at http: \/\/moocdata.cn\/data\/MOOCCube.","label":1,"title_clean":"MOOCCube: A Large scale Data Repository for NLP Applications in MOOCs","abstract_clean":"The prosperity of Massive Open Online Courses (MOOCs) provides fodder for many NLP and AI research for education applications, e.g., course concept extraction, prerequisite relation discovery, etc. However, the publicly available datasets of MOOC are limited in size with few types of data, which hinders advanced models and novel attempts in related topics. Therefore, we present MOOCCube, a large scale data repository of over 700 MOOC courses, 100k concepts, 8 million student behaviors with an external resource. Moreover, we conduct a prerequisite discovery task as an example application to show the potential of MOOCCube in facilitating relevant research. The data repository is now available at http: \/\/moocdata.cn\/data\/MOOCCube.","url":"https:\/\/aclanthology.org\/2020.acl-main.285"},{"ID":"yu-etal-2021-interpretable","methods":["interpretable propaganda detection","language models","deception techniques"],"center_method":[null,"language models",null],"tasks":["interpretable propaganda detection","online news and media consumption"],"center_task":[null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Interpretable Propaganda Detection in News Articles. Online users today are exposed to misleading and propagandistic news articles and media posts on a daily basis. To counter thus, a number of approaches have been designed aiming to achieve a healthier and safer online news and media consumption. Automatic systems are able to support humans in detecting such content; yet, a major impediment to their broad adoption is that besides being accurate, the decisions of such systems need also to be interpretable in order to be trusted and widely adopted by users. Since misleading and propagandistic content influences readers through the use of a number of deception techniques, we propose to detect and to show the use of such techniques as a way to offer interpretability. In particular, we define qualitatively descriptive features and we analyze their suitability for detecting deception techniques. We further show that our interpretable features can be easily combined with pre-trained language models, yielding state-of-the-art results.","label":1,"title_clean":"Interpretable Propaganda Detection in News Articles","abstract_clean":"Online users today are exposed to misleading and propagandistic news articles and media posts on a daily basis. To counter thus, a number of approaches have been designed aiming to achieve a healthier and safer online news and media consumption. Automatic systems are able to support humans in detecting such content; yet, a major impediment to their broad adoption is that besides being accurate, the decisions of such systems need also to be interpretable in order to be trusted and widely adopted by users. Since misleading and propagandistic content influences readers through the use of a number of deception techniques, we propose to detect and to show the use of such techniques as a way to offer interpretability. In particular, we define qualitatively descriptive features and we analyze their suitability for detecting deception techniques. We further show that our interpretable features can be easily combined with pre trained language models, yielding state of the art results.","url":"https:\/\/aclanthology.org\/2021.ranlp-1.179.pdf"},{"ID":"yue-zhou-2020-phicon","methods":["phicon","de identification models","data augmentation","random word insertion","augmentation method","synonym replacement","ph icon"],"center_method":[null,null,"data augmentation",null,null,null,null],"tasks":["de identification","generalization of clinical text","context augmentation"],"center_task":[null,null,null],"Goal":["Good Health and Well-Being","Peace, Justice and Strong Institutions"],"text":"PHICON: Improving Generalization of Clinical Text De-identification Models via Data Augmentation. De-identification is the task of identifying protected health information (PHI) in the clinical text. Existing neural de-identification models often fail to generalize to a new dataset. We propose a simple yet effective data augmentation method PHICON to alleviate the generalization issue. PHICON consists of PHI augmentation and Context augmentation, which creates augmented training corpora by replacing PHI entities with named-entities sampled from external sources, and by changing background context with synonym replacement or random word insertion, respectively. Experimental results on the i2b2 2006 and 2014 deidentification challenge datasets show that PH-ICON can help three selected de-identification models boost F1-score (by at most 8.6%) on cross-dataset test. We also discuss how much augmentation to use and how each augmentation method influences the performance. 1 3 https:\/\/portal.dbmi.hms.harvard.edu\/ projects\/n2c2-nlp\/","label":1,"title_clean":"PHICON: Improving Generalization of Clinical Text De identification Models via Data Augmentation","abstract_clean":"De identification is the task of identifying protected health information (PHI) in the clinical text. Existing neural de identification models often fail to generalize to a new dataset. We propose a simple yet effective data augmentation method PHICON to alleviate the generalization issue. PHICON consists of PHI augmentation and Context augmentation, which creates augmented training corpora by replacing PHI entities with named entities sampled from external sources, and by changing background context with synonym replacement or random word insertion, respectively. Experimental results on the i2b2 2006 and 2014 deidentification challenge datasets show that PH ICON can help three selected de identification models boost F1 score (by at most 8.6%) on cross dataset test. We also discuss how much augmentation to use and how each augmentation method influences the performance. 1 3 https:\/\/portal.dbmi.hms.harvard.edu\/ projects\/n2c2 nlp\/","url":"https:\/\/aclanthology.org\/2020.clinicalnlp-1.23"},{"ID":"zeng-etal-2019-faceted","methods":["classification","hypernym detection","hierarchy growth algorithm","faceted hierarchy","information extraction techniques"],"center_method":["classification",null,null,null,null],"tasks":["faceted concept hierarchies"],"center_task":[null],"Goal":["Industry, Innovation and Infrastrucure","Quality Education"],"text":"Faceted Hierarchy: A New Graph Type to Organize Scientific Concepts and a Construction Method. On a scientific concept hierarchy, a parent concept may have a few attributes, each of which has multiple values being a group of child concepts. We call these attributes facets: classification has a few facets such as application (e.g., face recognition), model (e.g., svm, knn), and metric (e.g., precision). In this work, we aim at building faceted concept hierarchies from scientific literature. Hierarchy construction methods heavily rely on hypernym detection, however, the faceted relations are parent-to-child links but the hypernym relation is a multi-hop, i.e., ancestor-todescendent link with a specific facet \"type-of\". We use information extraction techniques to find synonyms, sibling concepts, and ancestordescendent relations from a data science corpus. And we propose a hierarchy growth algorithm to infer the parent-child links from the three types of relationships. It resolves conflicts by maintaining the acyclic structure of a hierarchy.","label":1,"title_clean":"Faceted Hierarchy: A New Graph Type to Organize Scientific Concepts and a Construction Method","abstract_clean":"On a scientific concept hierarchy, a parent concept may have a few attributes, each of which has multiple values being a group of child concepts. We call these attributes facets: classification has a few facets such as application (e.g., face recognition), model (e.g., svm, knn), and metric (e.g., precision). In this work, we aim at building faceted concept hierarchies from scientific literature. Hierarchy construction methods heavily rely on hypernym detection, however, the faceted relations are parent to child links but the hypernym relation is a multi hop, i.e., ancestor todescendent link with a specific facet \"type of\". We use information extraction techniques to find synonyms, sibling concepts, and ancestordescendent relations from a data science corpus. And we propose a hierarchy growth algorithm to infer the parent child links from the three types of relationships. It resolves conflicts by maintaining the acyclic structure of a hierarchy.","url":"https:\/\/aclanthology.org\/D19-5317.pdf"},{"ID":"zhang-duh-2021-approaching","methods":["hyperparameter search","backtranslation","cascaded sign language translation system"],"center_method":[null,"backtranslation",null],"tasks":["gloss translation","low resource machine translation task","gloss translation component"],"center_task":[null,null,null],"Goal":["Reduced Inequalities"],"text":"Approaching Sign Language Gloss Translation as a Low-Resource Machine Translation Task. A cascaded Sign Language Translation system first maps sign videos to gloss annotations and then translates glosses into a spoken languages. This work focuses on the second-stage gloss translation component, which is challenging due to the scarcity of publicly available parallel data. We approach gloss translation as a low-resource machine translation task and investigate two popular methods for improving translation quality: hyperparameter search and backtranslation. We discuss the potentials and pitfalls of these methods based on experiments on the RWTH-PHOENIX-Weather 2014T dataset.","label":1,"title_clean":"Approaching Sign Language Gloss Translation as a Low Resource Machine Translation Task","abstract_clean":"A cascaded Sign Language Translation system first maps sign videos to gloss annotations and then translates glosses into a spoken languages. This work focuses on the second stage gloss translation component, which is challenging due to the scarcity of publicly available parallel data. We approach gloss translation as a low resource machine translation task and investigate two popular methods for improving translation quality: hyperparameter search and backtranslation. We discuss the potentials and pitfalls of these methods based on experiments on the RWTH PHOENIX Weather 2014T dataset.","url":"https:\/\/aclanthology.org\/2021.mtsummit-at4ssl.7"},{"ID":"zhang-ma-2020-dual","methods":["dual attention model","neural network","additive attention","citation recommender methods","embedding based neural network"],"center_method":[null,"neural network",null,null,null],"tasks":["citation recommendation","manuscript preparation"],"center_task":[null,null],"Goal":["Industry, Innovation and Infrastrucure"],"text":"Dual Attention Model for Citation Recommendation. Based on an exponentially increasing number of academic articles, discovering and citing comprehensive and appropriate resources has become a non-trivial task. Conventional citation recommender methods suffer from severe information loss. For example, they do not consider the section of the paper that the user is writing and for which they need to find a citation, the relatedness between the words in the local context (the text span that describes a citation), or the importance on each word from the local context. These shortcomings make such methods insufficient for recommending adequate citations to academic manuscripts. In this study, we propose a novel embedding-based neural network called \"dual attention model for citation recommendation (DACR)\" to recommend citations during manuscript preparation. Our method adapts embedding of three semantic information: words in the local context, structural contexts 1 , and the section on which a user is working. A neural network model is designed to maximize the similarity between the embedding of the three input (local context words, section and structural contexts) and the target citation appearing in the context. The core of the neural network model is composed of self-attention and additive attention, where the former aims to capture the relatedness between the contextual words and structural context, and the latter aims to learn the importance of them. The experiments on real-world datasets demonstrate the effectiveness of the proposed approach.","label":1,"title_clean":"Dual Attention Model for Citation Recommendation","abstract_clean":"Based on an exponentially increasing number of academic articles, discovering and citing comprehensive and appropriate resources has become a non trivial task. Conventional citation recommender methods suffer from severe information loss. For example, they do not consider the section of the paper that the user is writing and for which they need to find a citation, the relatedness between the words in the local context (the text span that describes a citation), or the importance on each word from the local context. These shortcomings make such methods insufficient for recommending adequate citations to academic manuscripts. In this study, we propose a novel embedding based neural network called \"dual attention model for citation recommendation (DACR)\" to recommend citations during manuscript preparation. Our method adapts embedding of three semantic information: words in the local context, structural contexts 1 , and the section on which a user is working. A neural network model is designed to maximize the similarity between the embedding of the three input (local context words, section and structural contexts) and the target citation appearing in the context. The core of the neural network model is composed of self attention and additive attention, where the former aims to capture the relatedness between the contextual words and structural context, and the latter aims to learn the importance of them. The experiments on real world datasets demonstrate the effectiveness of the proposed approach.","url":"https:\/\/aclanthology.org\/2020.coling-main.283"},{"ID":"zhang-patrick-2006-extracting","methods":["markup tag set"],"center_method":[null],"tasks":["description of clinical case studies"],"center_task":[null],"Goal":["Good Health and Well-Being"],"text":"Extracting Patient Clinical Profiles from Case Reports. This research aims to extract detailed clinical profiles, such as signs and symptoms, and important laboratory test results of the patient from descriptions of the diagnostic and treatment procedures in journal articles. This paper proposes a novel markup tag set to cover a wide variety of semantics in the description of clinical case studies in the clinical literature. A manually annotated corpus which consists of 75 clinical reports with 5,117 sentences has been created and a sentence classification system is reported as the preliminary attempt to exploit the fast growing online repositories of clinical case reports.","label":1,"title_clean":"Extracting Patient Clinical Profiles from Case Reports","abstract_clean":"This research aims to extract detailed clinical profiles, such as signs and symptoms, and important laboratory test results of the patient from descriptions of the diagnostic and treatment procedures in journal articles. This paper proposes a novel markup tag set to cover a wide variety of semantics in the description of clinical case studies in the clinical literature. A manually annotated corpus which consists of 75 clinical reports with 5,117 sentences has been created and a sentence classification system is reported as the preliminary attempt to exploit the fast growing online repositories of clinical case reports.","url":"https:\/\/aclanthology.org\/U06-1027.pdf"},{"ID":"zhao-caragea-2021-knowledge","methods":["student model","bert","supervised learning counterpart","tag representations","semi supervised approach","knowledge distillation","self distillation","privacy prediction systems"],"center_method":[null,"bert",null,null,null,"knowledge distillation",null,null],"tasks":["private image identification","semi supervised learning task","predicting private or sensitive content","image tag based privacy prediction","privacy decisions"],"center_task":[null,null,null,null,null],"Goal":["Peace, Justice and Strong Institutions"],"text":"Knowledge Distillation with BERT for Image Tag-Based Privacy Prediction. Text in the form of tags associated with online images is often informative for predicting private or sensitive content from images. When using privacy prediction systems running on social networking sites that decide whether each uploaded image should get posted or be protected, users may be reluctant to share real images that may reveal their identity, but may share image tags. In such cases, privacy-aware tags become good indicators of image privacy and can be utilized to generate privacy decisions. In this paper, our aim is to learn tag representations for images to improve tagbased image privacy prediction. To achieve this, we explore self-distillation with BERT, in which we utilize knowledge in the form of soft probability distributions (soft labels) from the teacher model to help with the training of the student model. Our approach effectively learns better tag representations with improved performance on private image identification and outperforms state-of-the-art models for this task. Moreover, we utilize the idea of knowledge distillation to improve tag representations in a semi-supervised learning task. Our semi-supervised approach with only 20% of annotated data achieves similar performance compared with its supervised learning counterpart. Last, we provide a comprehensive analysis to get a better understanding of our approach.","label":1,"title_clean":"Knowledge Distillation with BERT for Image Tag Based Privacy Prediction","abstract_clean":"Text in the form of tags associated with online images is often informative for predicting private or sensitive content from images. When using privacy prediction systems running on social networking sites that decide whether each uploaded image should get posted or be protected, users may be reluctant to share real images that may reveal their identity, but may share image tags. In such cases, privacy aware tags become good indicators of image privacy and can be utilized to generate privacy decisions. In this paper, our aim is to learn tag representations for images to improve tagbased image privacy prediction. To achieve this, we explore self distillation with BERT, in which we utilize knowledge in the form of soft probability distributions (soft labels) from the teacher model to help with the training of the student model. Our approach effectively learns better tag representations with improved performance on private image identification and outperforms state of the art models for this task. Moreover, we utilize the idea of knowledge distillation to improve tag representations in a semi supervised learning task. Our semi supervised approach with only 20% of annotated data achieves similar performance compared with its supervised learning counterpart. Last, we provide a comprehensive analysis to get a better understanding of our approach.","url":"https:\/\/aclanthology.org\/2021.ranlp-1.181"},{"ID":"zheng-etal-2021-duo","methods":["multimodal representation","multimodal language measurement methods"],"center_method":[null,null],"tasks":["elementary school mathematics classroom","classroom teaching","multimodal language deduction","language measurement","analysis and measurement of multimodal discourse","annotation"],"center_task":[null,null,null,null,null,"annotation"],"Goal":["Quality Education"],"text":"\u591a\u6a21\u6001\u8868\u8ff0\u89c6\u57df\u4e0b\u7684\u5c0f\u5b66\u6570\u5b66\u8bfe\u5802\u8bed\u8a00\u8ba1\u91cf\u521d\u63a2(A preliminary study of language measurement in elementary school mathematics classrooms from the perspective of multimodal representation). This paper focuses on the analysis and measurement of multimodal discourse in elementary school mathematics classroom. Based on a high-quality mathematics class, this paper explores the processing and annotation of a multimodal corpus, proposes two multimodal language measurement methods: multimodal value and multimodal representation discreteness, and analyzes the results of quantified multimodal language sampling data. The results show that teachers can better transfer abstract knowledge with the help of multimodal languages, and the measurement results can reflect the cooperative representation relationship between modes and the appropriateness of multimodal language deduction in classroom teaching.","label":1,"title_clean":"\u591a\u6a21\u6001\u8868\u8ff0\u89c6\u57df\u4e0b\u7684\u5c0f\u5b66\u6570\u5b66\u8bfe\u5802\u8bed\u8a00\u8ba1\u91cf\u521d\u63a2(A preliminary study of language measurement in elementary school mathematics classrooms from the perspective of multimodal representation)","abstract_clean":"This paper focuses on the analysis and measurement of multimodal discourse in elementary school mathematics classroom. Based on a high quality mathematics class, this paper explores the processing and annotation of a multimodal corpus, proposes two multimodal language measurement methods: multimodal value and multimodal representation discreteness, and analyzes the results of quantified multimodal language sampling data. The results show that teachers can better transfer abstract knowledge with the help of multimodal languages, and the measurement results can reflect the cooperative representation relationship between modes and the appropriateness of multimodal language deduction in classroom teaching.","url":"https:\/\/aclanthology.org\/2021.ccl-1.38"},{"ID":"zheng-yu-2015-identifying","methods":["domain adaption","key concept identification"],"center_method":["domain adaption",null],"tasks":["clinical outcome","retrieval of education materials","ehr query generation","patient centered tailored education","self management","identifying key concepts"],"center_task":[null,null,null,null,null,null],"Goal":["Good Health and Well-Being"],"text":"Identifying Key Concepts from EHR Notes Using Domain Adaptation. Linking electronic health records (EHRs) to relevant education materials can provide patient-centered tailored education which can potentially improve patients' medical knowledge, self-management and clinical outcome. It is shown that EHR query generation using key concept identification improves retrieval of education materials. In this study, we explored domain adaptation approaches to improve key concept identification. Our experiments show that a 20.7% improvement in the F1 measure can be achieved by leveraging data from Wikipedia. Queries generated from the best performing approach achieved a 20.6% and 27.8% improvement over the queries generated from the baseline approach.","label":1,"title_clean":"Identifying Key Concepts from EHR Notes Using Domain Adaptation","abstract_clean":"Linking electronic health records (EHRs) to relevant education materials can provide patient centered tailored education which can potentially improve patients' medical knowledge, self management and clinical outcome. It is shown that EHR query generation using key concept identification improves retrieval of education materials. In this study, we explored domain adaptation approaches to improve key concept identification. Our experiments show that a 20.7% improvement in the F1 measure can be achieved by leveraging data from Wikipedia. Queries generated from the best performing approach achieved a 20.6% and 27.8% improvement over the queries generated from the baseline approach.","url":"https:\/\/aclanthology.org\/W15-2615"},{"ID":"zhong-etal-2020-element","methods":["law article element aware multi representation model","multiple representations","attention","law aware fact representation","topjudge"],"center_method":[null,null,"attention",null,null],"tasks":["law article prediction","classification"],"center_task":[null,"classification"],"Goal":["Peace, Justice and Strong Institutions"],"text":"An Element-aware Multi-representation Model for Law Article Prediction. Existing works have proved that using law articles as external knowledge can improve the performance of the Legal Judgment Prediction. However, they do not fully use law article information and most of the current work is only for single label samples. In this paper, we propose a Law Article Element-aware Multi-representation Model (LEMM), which can make full use of law article information and can be used for multi-label samples. The model uses the labeled elements of law articles to extract fact description features from multiple angles. It generates multiple representations of a fact for classification. Every label has a law-aware fact representation to encode more information. To capture the dependencies between law articles, the model also introduces a self-attention mechanism between multiple representations. Compared with baseline models like TopJudge, this model improves the accuracy of 5.84%, the macro F1 of 6.42%, and the micro F1 of 4.28%.","label":1,"title_clean":"An Element aware Multi representation Model for Law Article Prediction","abstract_clean":"Existing works have proved that using law articles as external knowledge can improve the performance of the Legal Judgment Prediction. However, they do not fully use law article information and most of the current work is only for single label samples. In this paper, we propose a Law Article Element aware Multi representation Model (LEMM), which can make full use of law article information and can be used for multi label samples. The model uses the labeled elements of law articles to extract fact description features from multiple angles. It generates multiple representations of a fact for classification. Every label has a law aware fact representation to encode more information. To capture the dependencies between law articles, the model also introduces a self attention mechanism between multiple representations. Compared with baseline models like TopJudge, this model improves the accuracy of 5.84%, the macro F1 of 6.42%, and the micro F1 of 4.28%.","url":"https:\/\/aclanthology.org\/2020.emnlp-main.540"},{"ID":"zhou-etal-2010-exploiting","methods":["machine learning methods","conditional random field","hedge cue identification system"],"center_method":["machine learning methods","conditional random field",null],"tasks":["biological hedge detection task","scope finding task"],"center_task":[null,null],"Goal":["Good Health and Well-Being"],"text":"Exploiting Multi-Features to Detect Hedges and their Scope in Biomedical Texts. In this paper, we present a machine learning approach that detects hedge cues and their scope in biomedical texts. Identifying hedged information in texts is a kind of semantic filtering of texts and it is important since it could extract speculative information from factual information. In order to deal with the semantic analysis problem, various evidential features are proposed and integrated through a Conditional Random Fields (CRFs) model. Hedge cues that appear in the training dataset are regarded as keywords and employed as an important feature in hedge cue identification system. For the scope finding, we construct a CRF-based system and a syntactic pattern-based system, and compare their performances. Experiments using test data from CoNLL-2010 shared task show that our proposed method is robust. F-score of the biological hedge detection task and scope finding task achieves 86.32% and 54.18% in in-domain evaluations respectively.","label":1,"title_clean":"Exploiting Multi Features to Detect Hedges and their Scope in Biomedical Texts","abstract_clean":"In this paper, we present a machine learning approach that detects hedge cues and their scope in biomedical texts. Identifying hedged information in texts is a kind of semantic filtering of texts and it is important since it could extract speculative information from factual information. In order to deal with the semantic analysis problem, various evidential features are proposed and integrated through a Conditional Random Fields (CRFs) model. Hedge cues that appear in the training dataset are regarded as keywords and employed as an important feature in hedge cue identification system. For the scope finding, we construct a CRF based system and a syntactic pattern based system, and compare their performances. Experiments using test data from CoNLL 2010 shared task show that our proposed method is robust. F score of the biological hedge detection task and scope finding task achieves 86.32% and 54.18% in in domain evaluations respectively.","url":"https:\/\/aclanthology.org\/W10-3015"},{"ID":"zhou-huang-2019-towards","methods":["fusion mechanism","neural network","entity enforced loss"],"center_method":[null,"neural network",null],"tasks":["math word problem","evaluation methods","generating math word problems"],"center_task":[null,"evaluation methods",null],"Goal":["Quality Education"],"text":"Towards Generating Math Word Problems from Equations and Topics. A math word problem is a narrative with a specific topic that provides clues to the correct equation with numerical quantities and variables therein. In this paper, we focus on the task of generating math word problems. Previous works are mainly templatebased with pre-defined rules. We propose a novel neural network model to generate math word problems from the given equations and topics. First, we design a fusion mechanism to incorporate the information of both equations and topics. Second, an entity-enforced loss is introduced to ensure the relevance between the generated math problem and the equation. Automatic evaluation results show that the proposed model significantly outperforms the baseline models. In human evaluations, the math word problems generated by our model are rated as being more relevant (in terms of solvability of the given equations and relevance to topics) and natural (i.e., grammaticality, fluency) than the baseline models.","label":1,"title_clean":"Towards Generating Math Word Problems from Equations and Topics","abstract_clean":"A math word problem is a narrative with a specific topic that provides clues to the correct equation with numerical quantities and variables therein. In this paper, we focus on the task of generating math word problems. Previous works are mainly templatebased with pre defined rules. We propose a novel neural network model to generate math word problems from the given equations and topics. First, we design a fusion mechanism to incorporate the information of both equations and topics. Second, an entity enforced loss is introduced to ensure the relevance between the generated math problem and the equation. Automatic evaluation results show that the proposed model significantly outperforms the baseline models. In human evaluations, the math word problems generated by our model are rated as being more relevant (in terms of solvability of the given equations and relevance to topics) and natural (i.e., grammaticality, fluency) than the baseline models.","url":"https:\/\/aclanthology.org\/W19-8661.pdf"},{"ID":"zhu-etal-2020-crosswoz","methods":["user simulator"],"center_method":[null],"tasks":["multi domain cross domain dialogue modeling","policy learning","dialogue state tracking","user simulation","pipelined taskoriented dialogue systems"],"center_task":[null,null,null,null,null],"Goal":["Decent Work and Economic Growth"],"text":"CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset. To advance multi-domain (cross-domain) dialogue modeling as well as alleviate the shortage of Chinese task-oriented datasets, we propose CrossWOZ, the first large-scale Chinese Cross-Domain Wizard-of-Oz task-oriented dataset. It contains 6K dialogue sessions and 102K utterances for 5 domains, including hotel, restaurant, attraction, metro, and taxi. Moreover, the corpus contains rich annotation of dialogue states and dialogue acts on both user and system sides. About 60% of the dialogues have cross-domain user goals that favor inter-domain dependency and encourage natural transition across domains in conversation. We also provide a user simulator and several benchmark models for pipelined taskoriented dialogue systems, which will facilitate researchers to compare and evaluate their models on this corpus. The large size and rich annotation of CrossWOZ make it suitable to investigate a variety of tasks in cross-domain dialogue modeling, such as dialogue state tracking, policy learning, user simulation, etc.","label":1,"title_clean":"CrossWOZ: A Large Scale Chinese Cross Domain Task Oriented Dialogue Dataset","abstract_clean":"To advance multi domain (cross domain) dialogue modeling as well as alleviate the shortage of Chinese task oriented datasets, we propose CrossWOZ, the first large scale Chinese Cross Domain Wizard of Oz task oriented dataset. It contains 6K dialogue sessions and 102K utterances for 5 domains, including hotel, restaurant, attraction, metro, and taxi. Moreover, the corpus contains rich annotation of dialogue states and dialogue acts on both user and system sides. About 60% of the dialogues have cross domain user goals that favor inter domain dependency and encourage natural transition across domains in conversation. We also provide a user simulator and several benchmark models for pipelined taskoriented dialogue systems, which will facilitate researchers to compare and evaluate their models on this corpus. The large size and rich annotation of CrossWOZ make it suitable to investigate a variety of tasks in cross domain dialogue modeling, such as dialogue state tracking, policy learning, user simulation, etc.","url":"https:\/\/aclanthology.org\/2020.tacl-1.19"},{"ID":"zmandar-etal-2021-financial","methods":["topline summarisers","abstractive"],"center_method":[null,null],"tasks":["summarising","financial narrative processing 2021 workshop"],"center_task":[null,null],"Goal":["Decent Work and Economic Growth"],"text":"The Financial Narrative Summarisation Shared Task FNS 2021. This paper presents the results and findings of the Financial Narrative Summarisation Shared Task on summarising UK annual reports. The shared task was organised as part of the Financial Narrative Processing 2021 Workshop (FNP 2021 Workshop). The shared task included one main task which is the use of either abstractive or extractive automatic summarisers to summarise long documents in terms of UK financial annual reports. This shared task is the second to target financial documents. The data for the shared task was created and collected from publicly available UK annual reports published by firms listed on the London Stock Exchange. A total number of 10 systems from 5 different teams participated in the shared task. In addition, we had two baseline and two topline summarisers to help evaluate the results of the participating teams and compare them to the state-of-the-art systems.","label":1,"title_clean":"The Financial Narrative Summarisation Shared Task FNS 2021","abstract_clean":"This paper presents the results and findings of the Financial Narrative Summarisation Shared Task on summarising UK annual reports. The shared task was organised as part of the Financial Narrative Processing 2021 Workshop (FNP 2021 Workshop). The shared task included one main task which is the use of either abstractive or extractive automatic summarisers to summarise long documents in terms of UK financial annual reports. This shared task is the second to target financial documents. The data for the shared task was created and collected from publicly available UK annual reports published by firms listed on the London Stock Exchange. A total number of 10 systems from 5 different teams participated in the shared task. In addition, we had two baseline and two topline summarisers to help evaluate the results of the participating teams and compare them to the state of the art systems.","url":"https:\/\/aclanthology.org\/2021.fnp-1.22"},{"ID":"zou-li-2021-lz1904","methods":["bidirectional long short term memory conditional random field","pretrained word embedding","recurrent neural networks","bi lstm crf","lstm","lz1904"],"center_method":[null,null,"recurrent neural networks",null,"lstm",null],"tasks":["toxic spans detection","machine translation","sequence tagging","classification"],"center_task":["toxic spans detection","machine translation",null,"classification"],"Goal":["Peace, Justice and Strong Institutions"],"text":"LZ1904 at SemEval-2021 Task 5: Bi-LSTM-CRF for Toxic Span Detection using Pretrained Word Embedding. Recurrent Neural Networks (RNN) have been widely used in various Natural Language Processing (NLP) tasks such as text classification, sequence tagging, and machine translation. Long Short Term Memory (LSTM), a special unit of RNN, has the advantage of memorizing past and even future information in a sentence (especially for bidirectional LSTM). In the shared task of detecting toxic spans in texts, we first apply pretrained word embedding (GloVe) to generate the word vectors after tokenization. Then we construct Bidirectional Long Short Term Memory-Conditional Random Field (Bi-LSTM-CRF) model by Baidu research to predict whether each word in the sentence is toxic or not. We tune hyperparameters of dropout rate, number of LSTM units, embedding size with 10 epochs and choose the epoch with best validation recall. Our model achieves an F1 score of 66.99% on test dataset.","label":1,"title_clean":"LZ1904 at SemEval 2021 Task 5: Bi LSTM CRF for Toxic Span Detection using Pretrained Word Embedding","abstract_clean":"Recurrent Neural Networks (RNN) have been widely used in various Natural Language Processing (NLP) tasks such as text classification, sequence tagging, and machine translation. Long Short Term Memory (LSTM), a special unit of RNN, has the advantage of memorizing past and even future information in a sentence (especially for bidirectional LSTM). In the shared task of detecting toxic spans in texts, we first apply pretrained word embedding (GloVe) to generate the word vectors after tokenization. Then we construct Bidirectional Long Short Term Memory Conditional Random Field (Bi LSTM CRF) model by Baidu research to predict whether each word in the sentence is toxic or not. We tune hyperparameters of dropout rate, number of LSTM units, embedding size with 10 epochs and choose the epoch with best validation recall. Our model achieves an F1 score of 66.99% on test dataset.","url":"https:\/\/aclanthology.org\/2021.semeval-1.138"}]