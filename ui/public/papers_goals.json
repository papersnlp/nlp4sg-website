[{"ID":"bhatia-etal-2021-automatic","title":"Automatic Classification of Neutralization Techniques in the Narrative of Climate Change Scepticism","abstract":"Neutralisation techniques, e.g. denial of responsibility and denial of victim, are used in the narrative of climate change scepticism to justify lack of action or to promote an alternative view. We first draw on social science to introduce the problem to the community of nlp, present the granularity of the coding schema and then collect manual annotations of neutralised techniques in text relating to climate change, and experiment with supervised and semi- supervised BERT-based models.","year":2021,"title_abstract":"Automatic Classification of Neutralization Techniques in the Narrative of Climate Change Scepticism Neutralisation techniques, e.g. denial of responsibility and denial of victim, are used in the narrative of climate change scepticism to justify lack of action or to promote an alternative view. We first draw on social science to introduce the problem to the community of nlp, present the granularity of the coding schema and then collect manual annotations of neutralised techniques in text relating to climate change, and experiment with supervised and semi- supervised BERT-based models.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.4429169893,"Goal":"Climate Action","Task":["Narrative of Climate Change","narrative of climate change scepticism","nlp","climate change"],"Method":["Automatic Classification of Neutralization Techniques","Neutralisation techniques","social science","coding schema","neutralised techniques","supervised and semi - supervised BERT - based models"]},{"ID":"stede-patz-2021-climate","title":"The Climate Change Debate and Natural Language Processing","abstract":"The debate around climate change (CC){---}its extent, its causes, and the necessary responses{---}is intense and of global importance. Yet, in the natural language processing (NLP) community, this domain has so far received little attention. In contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the {''}text-as-data{''} paradigm, seeking to employ quantitative methods for analyzing large amounts of CC-related text. Other research is qualitative in nature and studies details, nuances, actors, and motivations within CC discourses. Coming from both NLP and Political Science, and reviewing key works in both disciplines, we discuss how social science approaches to CC debates can inform advances in text-mining\/NLP, and how, in return, NLP can support policy-makers and activists in making sense of large-scale and complex CC discourses across multiple genres, channels, topics, and communities. This is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change.","year":2021,"title_abstract":"The Climate Change Debate and Natural Language Processing The debate around climate change (CC){---}its extent, its causes, and the necessary responses{---}is intense and of global importance. Yet, in the natural language processing (NLP) community, this domain has so far received little attention. In contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the {''}text-as-data{''} paradigm, seeking to employ quantitative methods for analyzing large amounts of CC-related text. Other research is qualitative in nature and studies details, nuances, actors, and motivations within CC discourses. Coming from both NLP and Political Science, and reviewing key works in both disciplines, we discuss how social science approaches to CC debates can inform advances in text-mining\/NLP, and how, in return, NLP can support policy-makers and activists in making sense of large-scale and complex CC discourses across multiple genres, channels, topics, and communities. This is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.3862949014,"Goal":"Climate Action","Task":["Climate Change Debate","Natural Language Processing","debate around climate change","natural language processing","social science disciplines","NLP and Political Science","CC debates","text - mining\/NLP"],"Method":["as - data{''} paradigm","quantitative methods","social science approaches","NLP"]},{"ID":"loffler-etal-2020-tag","title":"Tag Me If You Can! Semantic Annotation of Biodiversity Metadata with the {QEMP} Corpus and the {B}iodiv{T}agger","abstract":"Dataset Retrieval is gaining importance due to a large amount of research data and the great demand for reusing scientific data. Dataset Retrieval is mostly based on metadata, structured information about the primary data. Enriching these metadata with semantic annotations based on Linked Open Data (LOD) enables datasets, publications and authors to be connected and expands the search on semantically related terms. In this work, we introduce the BiodivTagger, an ontology-based Information Extraction pipeline, developed for metadata from biodiversity research. The system recognizes biological, physical and chemical processes, environmental terms, data parameters and phenotypes as well as materials and chemical compounds and links them to concepts in dedicated ontologies. To evaluate our pipeline, we created a gold standard of 50 metadata files (QEMP corpus) selected from five different data repositories in biodiversity research. To the best of our knowledge, this is the first annotated metadata corpus for biodiversity research data. The results reveal a mixed picture. While materials and data parameters are properly matched to ontological concepts in most cases, some ontological issues occurred for processes and environmental terms.","year":2020,"title_abstract":"Tag Me If You Can! Semantic Annotation of Biodiversity Metadata with the {QEMP} Corpus and the {B}iodiv{T}agger Dataset Retrieval is gaining importance due to a large amount of research data and the great demand for reusing scientific data. Dataset Retrieval is mostly based on metadata, structured information about the primary data. Enriching these metadata with semantic annotations based on Linked Open Data (LOD) enables datasets, publications and authors to be connected and expands the search on semantically related terms. In this work, we introduce the BiodivTagger, an ontology-based Information Extraction pipeline, developed for metadata from biodiversity research. The system recognizes biological, physical and chemical processes, environmental terms, data parameters and phenotypes as well as materials and chemical compounds and links them to concepts in dedicated ontologies. To evaluate our pipeline, we created a gold standard of 50 metadata files (QEMP corpus) selected from five different data repositories in biodiversity research. To the best of our knowledge, this is the first annotated metadata corpus for biodiversity research data. The results reveal a mixed picture. While materials and data parameters are properly matched to ontological concepts in most cases, some ontological issues occurred for processes and environmental terms.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.3628492355,"Goal":"Life on Land","Task":["Semantic Annotation","Retrieval","Dataset Retrieval","metadata","biodiversity research","biodiversity research"],"Method":["BiodivTagger","ontology - based Information Extraction pipeline"]},{"ID":"wittenburg-etal-2010-resource","title":"Resource and Service Centres as the Backbone for a Sustainable Service Infrastructure","abstract":"Currently, research infrastructures are being designed and established in many disciplines since they all suffer from an enormous fragmentation of their resources and tools. In the domain of language resources and tools the CLARIN initiative has been funded since 2008 to overcome many of the integration and interoperability hurdles. CLARIN can build on knowledge and work from many projects that were carried out during the last years and wants to build stable and robust services that can be used by researchers. Here service centres will play an important role that have the potential of being persistent and that adhere to criteria as they have been established by CLARIN. In the last year of the so-called preparatory phase these centres are currently developing four use cases that can demonstrate how the various pillars CLARIN has been working on can be integrated. All four use cases fulfil the criteria of being cross-national.","year":2010,"title_abstract":"Resource and Service Centres as the Backbone for a Sustainable Service Infrastructure Currently, research infrastructures are being designed and established in many disciplines since they all suffer from an enormous fragmentation of their resources and tools. In the domain of language resources and tools the CLARIN initiative has been funded since 2008 to overcome many of the integration and interoperability hurdles. CLARIN can build on knowledge and work from many projects that were carried out during the last years and wants to build stable and robust services that can be used by researchers. Here service centres will play an important role that have the potential of being persistent and that adhere to criteria as they have been established by CLARIN. In the last year of the so-called preparatory phase these centres are currently developing four use cases that can demonstrate how the various pillars CLARIN has been working on can be integrated. All four use cases fulfil the criteria of being cross-national.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.3627973199,"Goal":"Sustainable Cities and Communities","Task":["Sustainable Service Infrastructure","language resources","CLARIN initiative"],"Method":["Resource and Service Centres","CLARIN"]},{"ID":"alhafni-etal-2020-gender","title":"Gender-Aware Reinflection using Linguistically Enhanced Neural Models","abstract":"In this paper, we present an approach for sentence-level gender reinflection using linguistically enhanced sequence-to-sequence models. Our system takes an Arabic sentence and a given target gender as input and generates a gender-reinflected sentence based on the target gender. We formulate the problem as a user-aware grammatical error correction task and build an encoder-decoder architecture to jointly model reinflection for both masculine and feminine grammatical genders. We also show that adding linguistic features to our model leads to better reinflection results. The results on a blind test set using our best system show improvements over previous work, with a 3.6{\\%} absolute increase in M2 F0.5.","year":2020,"title_abstract":"Gender-Aware Reinflection using Linguistically Enhanced Neural Models In this paper, we present an approach for sentence-level gender reinflection using linguistically enhanced sequence-to-sequence models. Our system takes an Arabic sentence and a given target gender as input and generates a gender-reinflected sentence based on the target gender. We formulate the problem as a user-aware grammatical error correction task and build an encoder-decoder architecture to jointly model reinflection for both masculine and feminine grammatical genders. We also show that adding linguistic features to our model leads to better reinflection results. The results on a blind test set using our best system show improvements over previous work, with a 3.6{\\%} absolute increase in M2 F0.5.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.3593562841,"Goal":"Gender Equality","Task":["sentence - level gender reinflection","user - aware grammatical error correction task"],"Method":["Gender - Aware Reinflection","Linguistically Enhanced Neural Models","linguistically enhanced sequence - to - sequence models","encoder - decoder architecture"]},{"ID":"vaid-etal-2022-towards","title":"Towards Fine-grained Classification of Climate Change related Social Media Text","abstract":"With climate change becoming a cause of concern worldwide, it becomes essential to gauge people{'}s reactions. This can help educate and spread awareness about it and help leaders improve decision-making. This work explores the fine-grained classification and Stance detection of climate change-related social media text. Firstly, we create two datasets, ClimateStance and ClimateEng, consisting of 3777 tweets each, posted during the 2019 United Nations Framework Convention on Climate Change and comprehensively outline the dataset collection, annotation methodology, and dataset composition. Secondly, we propose the task of Climate Change stance detection based on our proposed ClimateStance dataset. Thirdly, we propose a fine-grained classification based on the ClimateEng dataset, classifying social media text into five categories: Disaster, Ocean\/Water, Agriculture\/Forestry, Politics, and General. We benchmark both the datasets for climate change stance detection and fine-grained classification using state-of-the-art methods in text classification. We also create a Reddit-based dataset for both the tasks, ClimateReddit, consisting of 6262 pseudo-labeled comments along with 329 manually annotated comments for the label. We then perform semi-supervised experiments for both the tasks and benchmark their results using the best-performing model for the supervised experiments. Lastly, we provide insights into the ClimateStance and ClimateReddit using part-of-speech tagging and named-entity recognition.","year":2022,"title_abstract":"Towards Fine-grained Classification of Climate Change related Social Media Text With climate change becoming a cause of concern worldwide, it becomes essential to gauge people{'}s reactions. This can help educate and spread awareness about it and help leaders improve decision-making. This work explores the fine-grained classification and Stance detection of climate change-related social media text. Firstly, we create two datasets, ClimateStance and ClimateEng, consisting of 3777 tweets each, posted during the 2019 United Nations Framework Convention on Climate Change and comprehensively outline the dataset collection, annotation methodology, and dataset composition. Secondly, we propose the task of Climate Change stance detection based on our proposed ClimateStance dataset. Thirdly, we propose a fine-grained classification based on the ClimateEng dataset, classifying social media text into five categories: Disaster, Ocean\/Water, Agriculture\/Forestry, Politics, and General. We benchmark both the datasets for climate change stance detection and fine-grained classification using state-of-the-art methods in text classification. We also create a Reddit-based dataset for both the tasks, ClimateReddit, consisting of 6262 pseudo-labeled comments along with 329 manually annotated comments for the label. We then perform semi-supervised experiments for both the tasks and benchmark their results using the best-performing model for the supervised experiments. Lastly, we provide insights into the ClimateStance and ClimateReddit using part-of-speech tagging and named-entity recognition.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.3573638201,"Goal":"Climate Action","Task":["Fine - grained Classification of Climate Change related Social Media Text","decision - making","fine - grained classification and Stance detection of climate change","annotation methodology","Climate Change stance detection","fine - grained classification","climate change stance detection","fine - grained classification","text classification","supervised","supervised experiments","part - of - speech tagging","named - entity recognition"],"Method":["Reddit","ClimateReddit"]},{"ID":"lynn-etal-2018-clpsych","title":"{CLP}sych 2018 Shared Task: Predicting Current and Future Psychological Health from Childhood Essays","abstract":"We describe the shared task for the CLPsych 2018 workshop, which focused on predicting current and future psychological health from an essay authored in childhood. Language-based predictions of a person{'}s current health have the potential to supplement traditional psychological assessment such as questionnaires, improving intake risk measurement and monitoring. Predictions of future psychological health can aid with both early detection and the development of preventative care. Research into the mental health trajectory of people, beginning from their childhood, has thus far been an area of little work within the NLP community. This shared task represents one of the first attempts to evaluate the use of early language to predict future health; this has the potential to support a wide variety of clinical health care tasks, from early assessment of lifetime risk for mental health problems, to optimal timing for targeted interventions aimed at both prevention and treatment.","year":2018,"title_abstract":"{CLP}sych 2018 Shared Task: Predicting Current and Future Psychological Health from Childhood Essays We describe the shared task for the CLPsych 2018 workshop, which focused on predicting current and future psychological health from an essay authored in childhood. Language-based predictions of a person{'}s current health have the potential to supplement traditional psychological assessment such as questionnaires, improving intake risk measurement and monitoring. Predictions of future psychological health can aid with both early detection and the development of preventative care. Research into the mental health trajectory of people, beginning from their childhood, has thus far been an area of little work within the NLP community. This shared task represents one of the first attempts to evaluate the use of early language to predict future health; this has the potential to support a wide variety of clinical health care tasks, from early assessment of lifetime risk for mental health problems, to optimal timing for targeted interventions aimed at both prevention and treatment.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.3409326375,"Goal":"Good Health and Well-Being","Task":["Predicting Current and Future Psychological Health","CLPsych 2018 workshop","predicting current and future psychological health","Language - based predictions","intake risk measurement","monitoring","Predictions of future psychological health","early detection","preventative care","NLP community","future health;","clinical health care tasks","early assessment of lifetime risk for mental health problems","targeted interventions","prevention","treatment"],"Method":["psychological assessment","questionnaires"]},{"ID":"conforti-etal-2020-natural","title":"Natural Language Processing for Achieving Sustainable Development: the Case of Neural Labelling to Enhance Community Profiling","abstract":"In recent years, there has been an increasing interest in the application of Artificial Intelligence {--} and especially Machine Learning {--} to the field of Sustainable Development (SD). However, until now, NLP has not been systematically applied in this context. In this paper, we show the high potential of NLP to enhance project sustainability. In particular, we focus on the case of community profiling in developing countries, where, in contrast to the developed world, a notable data gap exists. Here, NLP could help to address the cost and time barrier of structuring qualitative data that prohibits its widespread use and associated benefits. We propose the new extreme multi-class multi-label Automatic UserPerceived Value classification task. We release Stories2Insights, an expert-annotated dataset of interviews carried out in Uganda, we provide a detailed corpus analysis, and we implement a number of strong neural baselines to address the task. Experimental results show that the problem is challenging, and leaves considerable room for future research at the intersection of NLP and SD.","year":2020,"title_abstract":"Natural Language Processing for Achieving Sustainable Development: the Case of Neural Labelling to Enhance Community Profiling In recent years, there has been an increasing interest in the application of Artificial Intelligence {--} and especially Machine Learning {--} to the field of Sustainable Development (SD). However, until now, NLP has not been systematically applied in this context. In this paper, we show the high potential of NLP to enhance project sustainability. In particular, we focus on the case of community profiling in developing countries, where, in contrast to the developed world, a notable data gap exists. Here, NLP could help to address the cost and time barrier of structuring qualitative data that prohibits its widespread use and associated benefits. We propose the new extreme multi-class multi-label Automatic UserPerceived Value classification task. We release Stories2Insights, an expert-annotated dataset of interviews carried out in Uganda, we provide a detailed corpus analysis, and we implement a number of strong neural baselines to address the task. Experimental results show that the problem is challenging, and leaves considerable room for future research at the intersection of NLP and SD.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.3408303857,"Goal":"Sustainable Cities and Communities","Task":["Natural Language Processing","Sustainable Development","Community Profiling","Sustainable Development","project sustainability","community profiling","structuring qualitative data","extreme multi - class multi - label Automatic UserPerceived Value classification task","corpus analysis","NLP","SD"],"Method":["Neural Labelling","Artificial Intelligence","Machine Learning","NLP","NLP","NLP","neural baselines"]},{"ID":"dev-etal-2021-harms","title":"Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies","abstract":"Gender is widely discussed in the context of language tasks and when examining the stereotypes propagated by language models. However, current discussions primarily treat gender as binary, which can perpetuate harms such as the cyclical erasure of non-binary gender identities. These harms are driven by model and dataset biases, which are consequences of the non-recognition and lack of understanding of non-binary genders in society. In this paper, we explain the complexity of gender and language around it, and survey non-binary persons to understand harms associated with the treatment of gender as binary in English language technologies. We also detail how current language representations (e.g., GloVe, BERT) capture and perpetuate these harms and related challenges that need to be acknowledged and addressed for representations to equitably encode gender information.","year":2021,"title_abstract":"Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies Gender is widely discussed in the context of language tasks and when examining the stereotypes propagated by language models. However, current discussions primarily treat gender as binary, which can perpetuate harms such as the cyclical erasure of non-binary gender identities. These harms are driven by model and dataset biases, which are consequences of the non-recognition and lack of understanding of non-binary genders in society. In this paper, we explain the complexity of gender and language around it, and survey non-binary persons to understand harms associated with the treatment of gender as binary in English language technologies. We also detail how current language representations (e.g., GloVe, BERT) capture and perpetuate these harms and related challenges that need to be acknowledged and addressed for representations to equitably encode gender information.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.3394080997,"Goal":"Gender Equality","Task":["Gender Exclusivity","Non - Binary Representation","Gender","language tasks"],"Method":["Language Technologies","language models","language representations"]},{"ID":"luo-etal-2020-detecting","title":"Detecting Stance in Media On Global Warming","abstract":"Citing opinions is a powerful yet understudied strategy in argumentation. For example, an environmental activist might say, {``}Leading scientists agree that global warming is a serious concern,{''} framing a clause which affirms their own stance ({``}that global warming is serious{''}) as an opinion endorsed (''[scientists] agree{''}) by a reputable source ({``}leading{''}). In contrast, a global warming denier might frame the same clause as the opinion of an untrustworthy source with a predicate connoting doubt: {``}Mistaken scientists claim [...].'' Our work studies opinion-framing in the global warming (GW) debate, an increasingly partisan issue that has received little attention in NLP. We introduce DeSMOG, a dataset of stance-labeled GW sentences, and train a BERT classifier to study novel aspects of argumentation in how different sides of a debate represent their own and each other{'}s opinions. From 56K news articles, we find that similar linguistic devices for self-affirming and opponent-doubting discourse are used across GW-accepting and skeptic media, though GW-skeptical media shows more opponent-doubt. We also find that authors often characterize sources as hypocritical, by ascribing opinions expressing the author{'}s own view to source entities known to publicly endorse the opposing view. We release our stance dataset, model, and lexicons of framing devices for future work on opinion-framing and the automatic detection of GW stance.","year":2020,"title_abstract":"Detecting Stance in Media On Global Warming Citing opinions is a powerful yet understudied strategy in argumentation. For example, an environmental activist might say, {``}Leading scientists agree that global warming is a serious concern,{''} framing a clause which affirms their own stance ({``}that global warming is serious{''}) as an opinion endorsed (''[scientists] agree{''}) by a reputable source ({``}leading{''}). In contrast, a global warming denier might frame the same clause as the opinion of an untrustworthy source with a predicate connoting doubt: {``}Mistaken scientists claim [...].'' Our work studies opinion-framing in the global warming (GW) debate, an increasingly partisan issue that has received little attention in NLP. We introduce DeSMOG, a dataset of stance-labeled GW sentences, and train a BERT classifier to study novel aspects of argumentation in how different sides of a debate represent their own and each other{'}s opinions. From 56K news articles, we find that similar linguistic devices for self-affirming and opponent-doubting discourse are used across GW-accepting and skeptic media, though GW-skeptical media shows more opponent-doubt. We also find that authors often characterize sources as hypocritical, by ascribing opinions expressing the author{'}s own view to source entities known to publicly endorse the opposing view. We release our stance dataset, model, and lexicons of framing devices for future work on opinion-framing and the automatic detection of GW stance.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.3377008736,"Goal":"Climate Action","Task":["Detecting Stance in Media","Global Warming","argumentation","opinion - framing","global warming (GW) debate","partisan issue","NLP","argumentation","opinion - framing","automatic detection of GW stance"],"Method":["BERT classifier","linguistic devices","lexicons of framing devices"]},{"ID":"van-den-bogaert-etal-2020-cefat4cities","title":"{CEFAT}4{C}ities, a Natural Language Layer for the {ISA}2 Core Public Service Vocabulary","abstract":"The CEFAT4Cities project (2020-2022) will create a {``}Smart Cities natural language context{''} (a software layer that facilitates the conversion of natural-language administrative procedures, into machine-readable data sets) on top of the existing ISA2 interoperability layer for public services. Integration with the FIWARE\/ORION {``}Smart City{''} Context Broker, will make existing, paper-based, public services discoverable through {``}Smart City{''} frameworks, thus allowing for the development of more sophisticated and more user-friendly public services applications. An automated translation component will be included, to provide a solution that can be used by all EU Member States. As a result, the project will allow EU citizens and businesses to interact with public services on the city, national, regional and EU level, in their own language.","year":2020,"title_abstract":"{CEFAT}4{C}ities, a Natural Language Layer for the {ISA}2 Core Public Service Vocabulary The CEFAT4Cities project (2020-2022) will create a {``}Smart Cities natural language context{''} (a software layer that facilitates the conversion of natural-language administrative procedures, into machine-readable data sets) on top of the existing ISA2 interoperability layer for public services. Integration with the FIWARE\/ORION {``}Smart City{''} Context Broker, will make existing, paper-based, public services discoverable through {``}Smart City{''} frameworks, thus allowing for the development of more sophisticated and more user-friendly public services applications. An automated translation component will be included, to provide a solution that can be used by all EU Member States. As a result, the project will allow EU citizens and businesses to interact with public services on the city, national, regional and EU level, in their own language.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.336374402,"Goal":"Sustainable Cities and Communities","Task":["conversion of natural - language administrative procedures","public services","user - friendly public services applications"],"Method":["Natural Language Layer","software layer","ISA2 interoperability layer","City{''} Context Broker","City{''} frameworks","automated translation component"]},{"ID":"choubey-etal-2021-gfst","title":"{GFST}: {G}ender-Filtered Self-Training for More Accurate Gender in Translation","abstract":"Targeted evaluations have found that machine translation systems often output incorrect gender in translations, even when the gender is clear from context. Furthermore, these incorrectly gendered translations have the potential to reflect or amplify social biases. We propose gender-filtered self-training (GFST) to improve gender translation accuracy on unambiguously gendered inputs. Our GFST approach uses a source monolingual corpus and an initial model to generate gender-specific pseudo-parallel corpora which are then filtered and added to the training data. We evaluate GFST on translation from English into five languages, finding that it improves gender accuracy without damaging generic quality. We also show the viability of GFST on several experimental settings, including re-training from scratch, fine-tuning, controlling the gender balance of the data, forward translation, and back-translation.","year":2021,"title_abstract":"{GFST}: {G}ender-Filtered Self-Training for More Accurate Gender in Translation Targeted evaluations have found that machine translation systems often output incorrect gender in translations, even when the gender is clear from context. Furthermore, these incorrectly gendered translations have the potential to reflect or amplify social biases. We propose gender-filtered self-training (GFST) to improve gender translation accuracy on unambiguously gendered inputs. Our GFST approach uses a source monolingual corpus and an initial model to generate gender-specific pseudo-parallel corpora which are then filtered and added to the training data. We evaluate GFST on translation from English into five languages, finding that it improves gender accuracy without damaging generic quality. We also show the viability of GFST on several experimental settings, including re-training from scratch, fine-tuning, controlling the gender balance of the data, forward translation, and back-translation.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.3309888542,"Goal":"Gender Equality","Task":["Gender","Translation","machine translation","translation","translation","re - training","fine - tuning","translation","translation"],"Method":["Filtered Self - Training","gender - filtered self - training","GFST","GFST","GFST"]},{"ID":"doughman-etal-2021-gender","title":"Gender Bias in Text: Origin, Taxonomy, and Implications","abstract":"Gender inequality represents a considerable loss of human potential and perpetuates a culture of violence, higher gender wage gaps, and a lack of representation of women in higher and leadership positions. Applications powered by Artificial Intelligence (AI) are increasingly being used in the real world to provide critical decisions about who is going to be hired, granted a loan, admitted to college, etc. However, the main pillars of AI, Natural Language Processing (NLP) and Machine Learning (ML) have been shown to reflect and even amplify gender biases and stereotypes, which are mainly inherited from historical training data. In an effort to facilitate the identification and mitigation of gender bias in English text, we develop a comprehensive taxonomy that relies on the following gender bias types: Generic Pronouns, Sexism, Occupational Bias, Exclusionary Bias, and Semantics. We also provide a bottom-up overview of gender bias, from its societal origin to its spillover onto language. Finally, we link the societal implications of gender bias to their corresponding type(s) in the proposed taxonomy. The underlying motivation of our work is to help enable the technical community to identify and mitigate relevant biases from training corpora for improved fairness in NLP systems.","year":2021,"title_abstract":"Gender Bias in Text: Origin, Taxonomy, and Implications Gender inequality represents a considerable loss of human potential and perpetuates a culture of violence, higher gender wage gaps, and a lack of representation of women in higher and leadership positions. Applications powered by Artificial Intelligence (AI) are increasingly being used in the real world to provide critical decisions about who is going to be hired, granted a loan, admitted to college, etc. However, the main pillars of AI, Natural Language Processing (NLP) and Machine Learning (ML) have been shown to reflect and even amplify gender biases and stereotypes, which are mainly inherited from historical training data. In an effort to facilitate the identification and mitigation of gender bias in English text, we develop a comprehensive taxonomy that relies on the following gender bias types: Generic Pronouns, Sexism, Occupational Bias, Exclusionary Bias, and Semantics. We also provide a bottom-up overview of gender bias, from its societal origin to its spillover onto language. Finally, we link the societal implications of gender bias to their corresponding type(s) in the proposed taxonomy. The underlying motivation of our work is to help enable the technical community to identify and mitigate relevant biases from training corpora for improved fairness in NLP systems.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.330781877,"Goal":"Gender Equality","Task":["Gender Bias","Gender inequality","Artificial Intelligence","Machine Learning","identification and mitigation of gender bias","fairness","NLP systems"],"Method":["AI","Natural Language Processing"]},{"ID":"escude-font-costa-jussa-2019-equalizing","title":"Equalizing Gender Bias in Neural Machine Translation with Word Embeddings Techniques","abstract":"Neural machine translation has significantly pushed forward the quality of the field. However, there are remaining big issues with the output translations and one of them is fairness. Neural models are trained on large text corpora which contain biases and stereotypes. As a consequence, models inherit these social biases. Recent methods have shown results in reducing gender bias in other natural language processing tools such as word embeddings. We take advantage of the fact that word embeddings are used in neural machine translation to propose a method to equalize gender biases in neural machine translation using these representations. Specifically, we propose, experiment and analyze the integration of two debiasing techniques over GloVe embeddings in the Transformer translation architecture. We evaluate our proposed system on the WMT English-Spanish benchmark task, showing gains up to one BLEU point. As for the gender bias evaluation, we generate a test set of occupations and we show that our proposed system learns to equalize existing biases from the baseline system.","year":2019,"title_abstract":"Equalizing Gender Bias in Neural Machine Translation with Word Embeddings Techniques Neural machine translation has significantly pushed forward the quality of the field. However, there are remaining big issues with the output translations and one of them is fairness. Neural models are trained on large text corpora which contain biases and stereotypes. As a consequence, models inherit these social biases. Recent methods have shown results in reducing gender bias in other natural language processing tools such as word embeddings. We take advantage of the fact that word embeddings are used in neural machine translation to propose a method to equalize gender biases in neural machine translation using these representations. Specifically, we propose, experiment and analyze the integration of two debiasing techniques over GloVe embeddings in the Transformer translation architecture. We evaluate our proposed system on the WMT English-Spanish benchmark task, showing gains up to one BLEU point. As for the gender bias evaluation, we generate a test set of occupations and we show that our proposed system learns to equalize existing biases from the baseline system.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.3254457712,"Goal":"Gender Equality","Task":["Equalizing Gender Bias","Neural Machine Translation","Neural machine translation","gender bias","natural language processing tools","neural machine translation","neural machine translation","gender bias evaluation"],"Method":["Word Embeddings","Neural models","word embeddings","debiasing techniques","GloVe embeddings","Transformer translation architecture"]},{"ID":"basta-etal-2020-towards","title":"Towards Mitigating Gender Bias in a decoder-based Neural Machine Translation model by Adding Contextual Information","abstract":"Gender bias negatively impacts many natural language processing applications, including machine translation (MT). The motivation behind this work is to study whether recent proposed MT techniques are significantly contributing to attenuate biases in document-level and gender-balanced data. For the study, we consider approaches of adding the previous sentence and the speaker information, implemented in a decoder-based neural MT system. We show improvements both in translation quality (+1 BLEU point) as well as in gender bias mitigation on WinoMT (+5{\\%} accuracy).","year":2020,"title_abstract":"Towards Mitigating Gender Bias in a decoder-based Neural Machine Translation model by Adding Contextual Information Gender bias negatively impacts many natural language processing applications, including machine translation (MT). The motivation behind this work is to study whether recent proposed MT techniques are significantly contributing to attenuate biases in document-level and gender-balanced data. For the study, we consider approaches of adding the previous sentence and the speaker information, implemented in a decoder-based neural MT system. We show improvements both in translation quality (+1 BLEU point) as well as in gender bias mitigation on WinoMT (+5{\\%} accuracy).","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.3192572594,"Goal":"Gender Equality","Task":["Mitigating Gender Bias","natural language processing applications","machine translation","gender bias mitigation"],"Method":["decoder - based Neural Machine Translation model","MT techniques","decoder - based neural MT system"]},{"ID":"jain-etal-2021-generating","title":"Generating Gender Augmented Data for {NLP}","abstract":"Gender bias is a frequent occurrence in NLP-based applications, especially pronounced in gender-inflected languages. Bias can appear through associations of certain adjectives and animate nouns with the natural gender of referents, but also due to unbalanced grammatical gender frequencies of inflected words. This type of bias becomes more evident in generating conversational utterances where gender is not specified within the sentence, because most current NLP applications still work on a sentence-level context. As a step towards more inclusive NLP, this paper proposes an automatic and generalisable re-writing approach for short conversational sentences. The rewriting method can be applied to sentences that, without extra-sentential context, have multiple equivalent alternatives in terms of gender. The method can be applied both for creating gender balanced outputs as well as for creating gender balanced training data. The proposed approach is based on a neural machine translation system trained to {`}translate{'} from one gender alternative to another. Both the automatic and manual analysis of the approach show promising results with respect to the automatic generation of gender alternatives for conversational sentences in Spanish.","year":2021,"title_abstract":"Generating Gender Augmented Data for {NLP} Gender bias is a frequent occurrence in NLP-based applications, especially pronounced in gender-inflected languages. Bias can appear through associations of certain adjectives and animate nouns with the natural gender of referents, but also due to unbalanced grammatical gender frequencies of inflected words. This type of bias becomes more evident in generating conversational utterances where gender is not specified within the sentence, because most current NLP applications still work on a sentence-level context. As a step towards more inclusive NLP, this paper proposes an automatic and generalisable re-writing approach for short conversational sentences. The rewriting method can be applied to sentences that, without extra-sentential context, have multiple equivalent alternatives in terms of gender. The method can be applied both for creating gender balanced outputs as well as for creating gender balanced training data. The proposed approach is based on a neural machine translation system trained to {`}translate{'} from one gender alternative to another. Both the automatic and manual analysis of the approach show promising results with respect to the automatic generation of gender alternatives for conversational sentences in Spanish.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.3142097592,"Goal":"Gender Equality","Task":["Gender bias","NLP","generating conversational utterances","NLP","NLP","manual analysis","automatic generation of gender alternatives"],"Method":["automatic and generalisable re - writing approach","rewriting method","neural machine translation system"]},{"ID":"larson-2017-gender","title":"Gender as a Variable in Natural-Language Processing: Ethical Considerations","abstract":"Researchers and practitioners in natural-language processing (NLP) and related fields should attend to ethical principles in study design, ascription of categories\/variables to study participants, and reporting of findings or results. This paper discusses theoretical and ethical frameworks for using gender as a variable in NLP studies and proposes four guidelines for researchers and practitioners. The principles outlined here should guide practitioners, researchers, and peer reviewers, and they may be applicable to other social categories, such as race, applied to human beings connected to NLP research.","year":2017,"title_abstract":"Gender as a Variable in Natural-Language Processing: Ethical Considerations Researchers and practitioners in natural-language processing (NLP) and related fields should attend to ethical principles in study design, ascription of categories\/variables to study participants, and reporting of findings or results. This paper discusses theoretical and ethical frameworks for using gender as a variable in NLP studies and proposes four guidelines for researchers and practitioners. The principles outlined here should guide practitioners, researchers, and peer reviewers, and they may be applicable to other social categories, such as race, applied to human beings connected to NLP research.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.3131800592,"Goal":"Gender Equality","Task":["Natural - Language Processing","natural - language processing","NLP","NLP"],"Method":["theoretical and ethical frameworks"]},{"ID":"sun-etal-2019-mitigating","title":"Mitigating Gender Bias in Natural Language Processing: Literature Review","abstract":"As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in popularity, it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes. Although NLP models have shown success in modeling various applications, they propagate and may even amplify gender bias found in text corpora. While the study of bias in artificial intelligence is not new, methods to mitigate gender bias in NLP are relatively nascent. In this paper, we review contemporary studies on recognizing and mitigating gender bias in NLP. We discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias. Furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods. Finally, we discuss future studies for recognizing and mitigating gender bias in NLP.","year":2019,"title_abstract":"Mitigating Gender Bias in Natural Language Processing: Literature Review As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in popularity, it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes. Although NLP models have shown success in modeling various applications, they propagate and may even amplify gender bias found in text corpora. While the study of bias in artificial intelligence is not new, methods to mitigate gender bias in NLP are relatively nascent. In this paper, we review contemporary studies on recognizing and mitigating gender bias in NLP. We discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias. Furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods. Finally, we discuss future studies for recognizing and mitigating gender bias in NLP.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.3109840155,"Goal":"Gender Equality","Task":["Mitigating Gender Bias","Natural Language Processing","Natural Language Processing","bias","artificial intelligence","gender bias","NLP","recognizing and mitigating gender bias","NLP","gender bias","recognizing gender bias","recognizing and mitigating gender bias","NLP"],"Method":["Machine Learning","NLP models","representation bias","gender debiasing methods"]},{"ID":"choukri-arranz-2012-analytical","title":"An Analytical Model of Language Resource Sustainability","abstract":"This paper elaborates on a sustainability model for Language Resources, both at a descriptive and analytical level. The first part, devoted to the descriptive model, elaborates on the definition of this concept both from a general point of view and from the Human Language Technology and Language Resources perspective. The paper also intends to list an exhaustive number of factors that have an impact on this sustainability. These factors will be clustered into Pillars so as ease understanding as well as the prediction of LR sustainability itself. Rather than simply identifying a set of LRs that have been in use for a while and that one can consider as sustainable, the paper aims at first clarifying and (re)defining the concept of sustainability by also connecting it to other domains. Then it also presents a detailed decomposition of all dimensions of Language Resource features that can contribute and\/or have an impact on such sustainability. Such analysis will also help anticipate and forecast sustainability for a LR before taking any decisions concerning design and production.","year":2012,"title_abstract":"An Analytical Model of Language Resource Sustainability This paper elaborates on a sustainability model for Language Resources, both at a descriptive and analytical level. The first part, devoted to the descriptive model, elaborates on the definition of this concept both from a general point of view and from the Human Language Technology and Language Resources perspective. The paper also intends to list an exhaustive number of factors that have an impact on this sustainability. These factors will be clustered into Pillars so as ease understanding as well as the prediction of LR sustainability itself. Rather than simply identifying a set of LRs that have been in use for a while and that one can consider as sustainable, the paper aims at first clarifying and (re)defining the concept of sustainability by also connecting it to other domains. Then it also presents a detailed decomposition of all dimensions of Language Resource features that can contribute and\/or have an impact on such sustainability. Such analysis will also help anticipate and forecast sustainability for a LR before taking any decisions concerning design and production.","social_need":"Responsible Consumption and Production Ensure sustainable consumption and production patterns","cosine_similarity":0.3105169237,"Goal":"Responsible Consumption and Production","Task":["Language Resource Sustainability","Language Resources","Human Language Technology","prediction of LR sustainability","sustainability","sustainability","sustainability","design","production"],"Method":["Analytical Model","sustainability model","descriptive model","LRs"]},{"ID":"vanmassenhove-etal-2018-getting","title":"Getting Gender Right in Neural Machine Translation","abstract":"Speakers of different languages must attend to and encode strikingly different aspects of the world in order to use their language correctly (Sapir, 1921; Slobin, 1996). One such difference is related to the way gender is expressed in a language. Saying {``}I am happy{''} in English, does not encode any additional knowledge of the speaker that uttered the sentence. However, many other languages do have grammatical gender systems and so such knowledge would be encoded. In order to correctly translate such a sentence into, say, French, the inherent gender information needs to be retained\/recovered. The same sentence would become either {``}Je suis heureux{''}, for a male speaker or {``}Je suis heureuse{''} for a female one. Apart from morphological agreement, demographic factors (gender, age, etc.) also influence our use of language in terms of word choices or syntactic constructions (Tannen, 1991; Pennebaker et al., 2003). We integrate gender information into NMT systems. Our contribution is two-fold: (1) the compilation of large datasets with speaker information for 20 language pairs, and (2) a simple set of experiments that incorporate gender information into NMT for multiple language pairs. Our experiments show that adding a gender feature to an NMT system significantly improves the translation quality for some language pairs.","year":2018,"title_abstract":"Getting Gender Right in Neural Machine Translation Speakers of different languages must attend to and encode strikingly different aspects of the world in order to use their language correctly (Sapir, 1921; Slobin, 1996). One such difference is related to the way gender is expressed in a language. Saying {``}I am happy{''} in English, does not encode any additional knowledge of the speaker that uttered the sentence. However, many other languages do have grammatical gender systems and so such knowledge would be encoded. In order to correctly translate such a sentence into, say, French, the inherent gender information needs to be retained\/recovered. The same sentence would become either {``}Je suis heureux{''}, for a male speaker or {``}Je suis heureuse{''} for a female one. Apart from morphological agreement, demographic factors (gender, age, etc.) also influence our use of language in terms of word choices or syntactic constructions (Tannen, 1991; Pennebaker et al., 2003). We integrate gender information into NMT systems. Our contribution is two-fold: (1) the compilation of large datasets with speaker information for 20 language pairs, and (2) a simple set of experiments that incorporate gender information into NMT for multiple language pairs. Our experiments show that adding a gender feature to an NMT system significantly improves the translation quality for some language pairs.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.3097427189,"Goal":"Gender Equality","Task":["Neural Machine Translation","NMT","NMT"],"Method":["grammatical gender systems","NMT"]},{"ID":"van-waterschoot-etal-2020-bliss","title":"{BLISS}: An Agent for Collecting Spoken Dialogue Data about Health and Well-being","abstract":"An important objective in health-technology is the ability to gather information about people{'}s well-being. Structured interviews can be used to obtain this information, but are time-consuming and not scalable. Questionnaires provide an alternative way to extract such information, though typically lack depth. In this paper, we present our first prototype of the BLISS agent, an artificial intelligent agent which intends to automatically discover what makes people happy and healthy. The goal of Behaviour-based Language-Interactive Speaking Systems (BLISS) is to understand the motivations behind people{'}s happiness by conducting a personalized spoken dialogue based on a happiness model. We built our first prototype of the model to collect 55 spoken dialogues, in which the BLISS agent asked questions to users about their happiness and well-being. Apart from a description of the BLISS architecture, we also provide details about our dataset, which contains over 120 activities and 100 motivations and is made available for usage.","year":2020,"title_abstract":"{BLISS}: An Agent for Collecting Spoken Dialogue Data about Health and Well-being An important objective in health-technology is the ability to gather information about people{'}s well-being. Structured interviews can be used to obtain this information, but are time-consuming and not scalable. Questionnaires provide an alternative way to extract such information, though typically lack depth. In this paper, we present our first prototype of the BLISS agent, an artificial intelligent agent which intends to automatically discover what makes people happy and healthy. The goal of Behaviour-based Language-Interactive Speaking Systems (BLISS) is to understand the motivations behind people{'}s happiness by conducting a personalized spoken dialogue based on a happiness model. We built our first prototype of the model to collect 55 spoken dialogues, in which the BLISS agent asked questions to users about their happiness and well-being. Apart from a description of the BLISS architecture, we also provide details about our dataset, which contains over 120 activities and 100 motivations and is made available for usage.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.3091854751,"Goal":"Good Health and Well-Being","Task":["Collecting Spoken Dialogue Data","Health and Well - being","health - technology","Behaviour - based Language - Interactive Speaking Systems"],"Method":["Questionnaires","BLISS agent","artificial intelligent agent","happiness model","BLISS agent","BLISS architecture"]},{"ID":"bentivogli-etal-2020-gender","title":"Gender in Danger? Evaluating Speech Translation Technology on the {M}u{ST}-{SHE} Corpus","abstract":"Translating from languages without productive grammatical gender like English into gender-marked languages is a well-known difficulty for machines. This difficulty is also due to the fact that the training data on which models are built typically reflect the asymmetries of natural languages, gender bias included. Exclusively fed with textual data, machine translation is intrinsically constrained by the fact that the input sentence does not always contain clues about the gender identity of the referred human entities. But what happens with speech translation, where the input is an audio signal? Can audio provide additional information to reduce gender bias? We present the first thorough investigation of gender bias in speech translation, contributing with: i) the release of a benchmark useful for future studies, and ii) the comparison of different technologies (cascade and end-to-end) on two language directions (English-Italian\/French).","year":2020,"title_abstract":"Gender in Danger? Evaluating Speech Translation Technology on the {M}u{ST}-{SHE} Corpus Translating from languages without productive grammatical gender like English into gender-marked languages is a well-known difficulty for machines. This difficulty is also due to the fact that the training data on which models are built typically reflect the asymmetries of natural languages, gender bias included. Exclusively fed with textual data, machine translation is intrinsically constrained by the fact that the input sentence does not always contain clues about the gender identity of the referred human entities. But what happens with speech translation, where the input is an audio signal? Can audio provide additional information to reduce gender bias? We present the first thorough investigation of gender bias in speech translation, contributing with: i) the release of a benchmark useful for future studies, and ii) the comparison of different technologies (cascade and end-to-end) on two language directions (English-Italian\/French).","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.3090429306,"Goal":"Gender Equality","Task":["Gender in Danger?","machine translation","speech translation","gender bias","speech translation"],"Method":["Speech Translation Technology"]},{"ID":"costa-jussa-etal-2020-gebiotoolkit","title":"{G}e{B}io{T}oolkit: Automatic Extraction of Gender-Balanced Multilingual Corpus of {W}ikipedia Biographies","abstract":"We introduce GeBioToolkit, a tool for extracting multilingual parallel corpora at sentence level, with document and gender information from Wikipedia biographies. Despite the gender inequalities present in Wikipedia, the toolkit has been designed to extract corpus balanced in gender. While our toolkit is customizable to any number of languages (and different domains), in this work we present a corpus of 2,000 sentences in English, Spanish and Catalan, which has been post-edited by native speakers to become a high-quality dataset for machine translation evaluation. While GeBioCorpus aims at being one of the first non-synthetic gender-balanced test datasets, GeBioToolkit aims at paving the path to standardize procedures to produce gender-balanced datasets.","year":2020,"title_abstract":"{G}e{B}io{T}oolkit: Automatic Extraction of Gender-Balanced Multilingual Corpus of {W}ikipedia Biographies We introduce GeBioToolkit, a tool for extracting multilingual parallel corpora at sentence level, with document and gender information from Wikipedia biographies. Despite the gender inequalities present in Wikipedia, the toolkit has been designed to extract corpus balanced in gender. While our toolkit is customizable to any number of languages (and different domains), in this work we present a corpus of 2,000 sentences in English, Spanish and Catalan, which has been post-edited by native speakers to become a high-quality dataset for machine translation evaluation. While GeBioCorpus aims at being one of the first non-synthetic gender-balanced test datasets, GeBioToolkit aims at paving the path to standardize procedures to produce gender-balanced datasets.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.3077817261,"Goal":"Gender Equality","Task":["Automatic Extraction of Gender - Balanced Multilingual Corpus","extracting multilingual parallel corpora","machine translation evaluation"],"Method":["GeBioToolkit","GeBioCorpus","GeBioToolkit"]},{"ID":"rehm-etal-2008-metadata","title":"The Metadata-Database of a Next Generation Sustainability Web-Platform for Language Resources","abstract":"Our goal is to provide a web-based platform for the long-term preservation and distribution of a heterogeneous collection of linguistic resources. We discuss the corpus preprocessing and normalisation phase that results in sets of multi-rooted trees. At the same time we transform the original metadata records, just like the corpora annotated using different annotation approaches and exhibiting different levels of granularity, into the all-encompassing and highly flexible format eTEI for which we present editing and parsing tools. We also discuss the architecture of the sustainability platform. Its primary components are an XML database that contains corpus and metadata files and an SQL database that contains user accounts and access control lists. A staging area, whose structure, contents, and consistency can be checked using tools, is used to make sure that new resources about to be imported into the platform have the correct structure.","year":2008,"title_abstract":"The Metadata-Database of a Next Generation Sustainability Web-Platform for Language Resources Our goal is to provide a web-based platform for the long-term preservation and distribution of a heterogeneous collection of linguistic resources. We discuss the corpus preprocessing and normalisation phase that results in sets of multi-rooted trees. At the same time we transform the original metadata records, just like the corpora annotated using different annotation approaches and exhibiting different levels of granularity, into the all-encompassing and highly flexible format eTEI for which we present editing and parsing tools. We also discuss the architecture of the sustainability platform. Its primary components are an XML database that contains corpus and metadata files and an SQL database that contains user accounts and access control lists. A staging area, whose structure, contents, and consistency can be checked using tools, is used to make sure that new resources about to be imported into the platform have the correct structure.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.3077008724,"Goal":"Sustainable Cities and Communities","Task":["Language Resources","long - term preservation and distribution","normalisation phase","sustainability platform"],"Method":["Sustainability Web - Platform","web - based platform","corpus preprocessing","annotation approaches","editing and parsing tools","XML database","SQL database"]},{"ID":"guntuku-etal-2018-current","title":"Current and Future Psychological Health Prediction using Language and Socio-Demographics of Children for the {CLP}ysch 2018 Shared Task","abstract":"This article is a system description and report on the submission of a team from the University of Pennsylvania in the {'}CLPsych 2018{'} shared task. The goal of the shared task was to use childhood language as a marker for both current and future psychological health over individual lifetimes. Our system employs multiple textual features derived from the essays written and individuals{'} socio-demographic variables at the age of 11. We considered several word clustering approaches, and explore the use of linear regression based on different feature sets. Our approach showed best results for predicting distress at the age of 42 and for predicting current anxiety on Disattenuated Pearson Correlation, and ranked fourth in the future health prediction task. In addition to the subtasks presented, we attempted to provide insight into mental health aspects at different ages. Our findings indicate that misspellings, words with illegible letters and increased use of personal pronouns are correlated with poor mental health at age 11, while descriptions about future physical activity, family and friends are correlated with good mental health.","year":2018,"title_abstract":"Current and Future Psychological Health Prediction using Language and Socio-Demographics of Children for the {CLP}ysch 2018 Shared Task This article is a system description and report on the submission of a team from the University of Pennsylvania in the {'}CLPsych 2018{'} shared task. The goal of the shared task was to use childhood language as a marker for both current and future psychological health over individual lifetimes. Our system employs multiple textual features derived from the essays written and individuals{'} socio-demographic variables at the age of 11. We considered several word clustering approaches, and explore the use of linear regression based on different feature sets. Our approach showed best results for predicting distress at the age of 42 and for predicting current anxiety on Disattenuated Pearson Correlation, and ranked fourth in the future health prediction task. In addition to the subtasks presented, we attempted to provide insight into mental health aspects at different ages. Our findings indicate that misspellings, words with illegible letters and increased use of personal pronouns are correlated with poor mental health at age 11, while descriptions about future physical activity, family and friends are correlated with good mental health.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.3047590256,"Goal":"Good Health and Well-Being","Task":["Current and Future Psychological Health Prediction","current and future psychological health","predicting distress","predicting current anxiety","future health prediction task"],"Method":["word clustering approaches","linear regression"]},{"ID":"van-uytvanck-etal-2008-language","title":"Language-Sites: Accessing and Presenting Language Resources via Geographic Information Systems","abstract":"The emerging area of Geographic Information Systems (GIS) has proven to add an interesting dimension to many research projects. Within the language-sites initiative we have brought together a broad range of links to digital language corpora and resources. Via Google Earth\u0092s visually appealing 3D-interface users can spin the globe, zoom into an area they are interested in and access directly the relevant language resources. This paper focuses on several ways of relating the map and the online data (lexica, annotations, multimedia recordings, etc.). Furthermore, we discuss some of the implementation choices that have been made, including future challenges. In addition, we show how scholars (both linguists and anthropologists) are using GIS tools to fulfill their specific research needs by making use of practical examples. This illustrates how both scientists and the general public can benefit from geography-based access to digital language data.","year":2008,"title_abstract":"Language-Sites: Accessing and Presenting Language Resources via Geographic Information Systems The emerging area of Geographic Information Systems (GIS) has proven to add an interesting dimension to many research projects. Within the language-sites initiative we have brought together a broad range of links to digital language corpora and resources. Via Google Earth\u0092s visually appealing 3D-interface users can spin the globe, zoom into an area they are interested in and access directly the relevant language resources. This paper focuses on several ways of relating the map and the online data (lexica, annotations, multimedia recordings, etc.). Furthermore, we discuss some of the implementation choices that have been made, including future challenges. In addition, we show how scholars (both linguists and anthropologists) are using GIS tools to fulfill their specific research needs by making use of practical examples. This illustrates how both scientists and the general public can benefit from geography-based access to digital language data.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.3024297357,"Goal":"Sustainable Cities and Communities","Task":["Language - Sites","Accessing and Presenting Language Resources","Geographic Information Systems","language - sites initiative"],"Method":["Geographic Information Systems","GIS tools","geography - based access"]},{"ID":"garnerin-etal-2020-gender","title":"Gender Representation in Open Source Speech Resources","abstract":"With the rise of artificial intelligence (AI) and the growing use of deep-learning architectures, the question of ethics, transparency and fairness of AI systems has become a central concern within the research community. We address transparency and fairness in spoken language systems by proposing a study about gender representation in speech resources available through the Open Speech and Language Resource platform. We show that finding gender information in open source corpora is not straightforward and that gender balance depends on other corpus characteristics (elicited\/non elicited speech, low\/high resource language, speech task targeted). The paper ends with recommendations about metadata and gender information for researchers in order to assure better transparency of the speech systems built using such corpora.","year":2020,"title_abstract":"Gender Representation in Open Source Speech Resources With the rise of artificial intelligence (AI) and the growing use of deep-learning architectures, the question of ethics, transparency and fairness of AI systems has become a central concern within the research community. We address transparency and fairness in spoken language systems by proposing a study about gender representation in speech resources available through the Open Speech and Language Resource platform. We show that finding gender information in open source corpora is not straightforward and that gender balance depends on other corpus characteristics (elicited\/non elicited speech, low\/high resource language, speech task targeted). The paper ends with recommendations about metadata and gender information for researchers in order to assure better transparency of the speech systems built using such corpora.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.3022799194,"Goal":"Gender Equality","Task":["Gender Representation","artificial intelligence","AI","transparency","spoken language systems","gender representation"],"Method":["deep - learning architectures","speech systems"]},{"ID":"costa-jussa-de-jorge-2020-fine","title":"Fine-tuning Neural Machine Translation on Gender-Balanced Datasets","abstract":"Misrepresentation of certain communities in datasets is causing big disruptions in artificial intelligence applications. In this paper, we propose using an automatically extracted gender-balanced dataset parallel corpus from Wikipedia. This balanced set is used to perform fine-tuning techniques from a bigger model trained on unbalanced datasets to mitigate gender biases in neural machine translation.","year":2020,"title_abstract":"Fine-tuning Neural Machine Translation on Gender-Balanced Datasets Misrepresentation of certain communities in datasets is causing big disruptions in artificial intelligence applications. In this paper, we propose using an automatically extracted gender-balanced dataset parallel corpus from Wikipedia. This balanced set is used to perform fine-tuning techniques from a bigger model trained on unbalanced datasets to mitigate gender biases in neural machine translation.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.3011179566,"Goal":"Gender Equality","Task":["Fine - tuning Neural Machine Translation","artificial intelligence applications","neural machine translation"],"Method":["fine - tuning techniques","bigger model"]},{"ID":"savoldi-etal-2021-gender","title":"Gender Bias in Machine Translation","abstract":"AbstractMachine translation (MT) technology has facilitated our daily tasks by providing accessible shortcuts for gathering, processing, and communicating information. However, it can suffer from biases that harm users and society at large. As a relatively new field of inquiry, studies of gender bias in MT still lack cohesion. This advocates for a unified framework to ease future research. To this end, we: i) critically review current conceptualizations of bias in light of theoretical insights from related disciplines, ii) summarize previous analyses aimed at assessing gender bias in MT, iii) discuss the mitigating strategies proposed so far, and iv) point toward potential directions for future work.","year":2021,"title_abstract":"Gender Bias in Machine Translation AbstractMachine translation (MT) technology has facilitated our daily tasks by providing accessible shortcuts for gathering, processing, and communicating information. However, it can suffer from biases that harm users and society at large. As a relatively new field of inquiry, studies of gender bias in MT still lack cohesion. This advocates for a unified framework to ease future research. To this end, we: i) critically review current conceptualizations of bias in light of theoretical insights from related disciplines, ii) summarize previous analyses aimed at assessing gender bias in MT, iii) discuss the mitigating strategies proposed so far, and iv) point toward potential directions for future work.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.3001188636,"Goal":"Gender Equality","Task":["Gender Bias","Machine Translation","translation","communicating information","gender bias","MT","gender bias","MT"],"Method":["mitigating strategies"]},{"ID":"troles-schmid-2021-extending","title":"Extending Challenge Sets to Uncover Gender Bias in Machine Translation: Impact of Stereotypical Verbs and Adjectives","abstract":"Human gender bias is reflected in language and text production. Because state-of-the-art machine translation (MT) systems are trained on large corpora of text, mostly generated by humans, gender bias can also be found in MT. For instance when occupations are translated from a language like English, which mostly uses gender neutral words, to a language like German, which mostly uses a feminine and a masculine version for an occupation, a decision must be made by the MT System. Recent research showed that MT systems are biased towards stereotypical translation of occupations. In 2019 the first, and so far only, challenge set, explicitly designed to measure the extent of gender bias in MT systems has been published. In this set measurement of gender bias is solely based on the translation of occupations. With our paper we present an extension of this challenge set, called WiBeMT, which adds gender-biased adjectives and sentences with gender-biased verbs. The resulting challenge set consists of over 70, 000 sentences and has been translated with three commercial MT systems: DeepL Translator, Microsoft Translator, and Google Translate. Results show a gender bias for all three MT systems. This gender bias is to a great extent significantly influenced by adjectives and to a lesser extent by verbs.","year":2021,"title_abstract":"Extending Challenge Sets to Uncover Gender Bias in Machine Translation: Impact of Stereotypical Verbs and Adjectives Human gender bias is reflected in language and text production. Because state-of-the-art machine translation (MT) systems are trained on large corpora of text, mostly generated by humans, gender bias can also be found in MT. For instance when occupations are translated from a language like English, which mostly uses gender neutral words, to a language like German, which mostly uses a feminine and a masculine version for an occupation, a decision must be made by the MT System. Recent research showed that MT systems are biased towards stereotypical translation of occupations. In 2019 the first, and so far only, challenge set, explicitly designed to measure the extent of gender bias in MT systems has been published. In this set measurement of gender bias is solely based on the translation of occupations. With our paper we present an extension of this challenge set, called WiBeMT, which adds gender-biased adjectives and sentences with gender-biased verbs. The resulting challenge set consists of over 70, 000 sentences and has been translated with three commercial MT systems: DeepL Translator, Microsoft Translator, and Google Translate. Results show a gender bias for all three MT systems. This gender bias is to a great extent significantly influenced by adjectives and to a lesser extent by verbs.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2995145321,"Goal":"Gender Equality","Task":["Gender Bias","Machine Translation","language and text production","machine translation","MT","MT","MT","translation","MT","measurement of gender bias","translation of occupations","MT"],"Method":["WiBeMT","MT systems","DeepL Translator","Microsoft Translator","Google Translate"]},{"ID":"kumar-etal-2020-fair","title":"Fair Embedding Engine: A Library for Analyzing and Mitigating Gender Bias in Word Embeddings","abstract":"Non-contextual word embedding models have been shown to inherit human-like stereotypical biases of gender, race and religion from the training corpora. To counter this issue, a large body of research has emerged which aims to mitigate these biases while keeping the syntactic and semantic utility of embeddings intact. This paper describes Fair Embedding Engine (FEE), a library for analysing and mitigating gender bias in word embeddings. FEE combines various state of the art techniques for quantifying, visualising and mitigating gender bias in word embeddings under a standard abstraction. FEE will aid practitioners in fast track analysis of existing debiasing methods on their embedding models. Further, it will allow rapid prototyping of new methods by evaluating their performance on a suite of standard metrics.","year":2020,"title_abstract":"Fair Embedding Engine: A Library for Analyzing and Mitigating Gender Bias in Word Embeddings Non-contextual word embedding models have been shown to inherit human-like stereotypical biases of gender, race and religion from the training corpora. To counter this issue, a large body of research has emerged which aims to mitigate these biases while keeping the syntactic and semantic utility of embeddings intact. This paper describes Fair Embedding Engine (FEE), a library for analysing and mitigating gender bias in word embeddings. FEE combines various state of the art techniques for quantifying, visualising and mitigating gender bias in word embeddings under a standard abstraction. FEE will aid practitioners in fast track analysis of existing debiasing methods on their embedding models. Further, it will allow rapid prototyping of new methods by evaluating their performance on a suite of standard metrics.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2970628738,"Goal":"Gender Equality","Task":["Analyzing and Mitigating Gender Bias","Word Embeddings","analysing and mitigating gender bias","word embeddings","visualising and mitigating gender bias","word embeddings","fast track analysis"],"Method":["Fair Embedding Engine","Non - contextual word embedding models","Fair Embedding Engine","FEE","FEE","debiasing methods","embedding models"]},{"ID":"alhindi-etal-2021-fact","title":"What to Fact-Check: Guiding Check-Worthy Information Detection in News Articles through Argumentative Discourse Structure","abstract":"Most existing methods for automatic fact-checking start with a precompiled list of claims to verify. We investigate the understudied problem of determining what statements in news articles are worthy to fact-check. We annotate the argument structure of 95 news articles in the climate change domain that are fact-checked by climate scientists at climatefeedback.org. We release the first multi-layer annotated corpus for both argumentative discourse structure (argument types and relations) and for fact-checked statements in news articles. We discuss the connection between argument structure and check-worthy statements and develop several baseline models for detecting check-worthy statements in the climate change domain. Our preliminary results show that using information about argumentative discourse structure shows slight but statistically significant improvement over a baseline of local discourse structure.","year":2021,"title_abstract":"What to Fact-Check: Guiding Check-Worthy Information Detection in News Articles through Argumentative Discourse Structure Most existing methods for automatic fact-checking start with a precompiled list of claims to verify. We investigate the understudied problem of determining what statements in news articles are worthy to fact-check. We annotate the argument structure of 95 news articles in the climate change domain that are fact-checked by climate scientists at climatefeedback.org. We release the first multi-layer annotated corpus for both argumentative discourse structure (argument types and relations) and for fact-checked statements in news articles. We discuss the connection between argument structure and check-worthy statements and develop several baseline models for detecting check-worthy statements in the climate change domain. Our preliminary results show that using information about argumentative discourse structure shows slight but statistically significant improvement over a baseline of local discourse structure.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2969578505,"Goal":"Climate Action","Task":["Guiding Check - Worthy Information Detection","automatic fact - checking","fact - checked statements","detecting check - worthy statements","climate change domain"],"Method":["local discourse structure"]},{"ID":"renduchintala-williams-2022-investigating","title":"Investigating Failures of Automatic Translationin the Case of Unambiguous Gender","abstract":"Transformer-based models are the modern work horses for neural machine translation (NMT), reaching state of the art across several benchmarks. Despite their impressive accuracy, we observe a systemic and rudimentary class of errors made by current state-of-the-art NMT models with regards to translating from a language that doesn{'}t mark gender on nouns into others that do. We find that even when the surrounding context provides unambiguous evidence of the appropriate grammatical gender marking, no tested model was able to accurately gender occupation nouns systematically. We release an evaluation scheme and dataset for measuring the ability of NMT models to translate gender morphology correctly in unambiguous contexts across syntactically diverse sentences. Our dataset translates from an English source into 20 languages from several different language families. With the availability of this dataset, our hope is that the NMT community can iterate on solutions for this class of especially egregious errors.","year":2022,"title_abstract":"Investigating Failures of Automatic Translationin the Case of Unambiguous Gender Transformer-based models are the modern work horses for neural machine translation (NMT), reaching state of the art across several benchmarks. Despite their impressive accuracy, we observe a systemic and rudimentary class of errors made by current state-of-the-art NMT models with regards to translating from a language that doesn{'}t mark gender on nouns into others that do. We find that even when the surrounding context provides unambiguous evidence of the appropriate grammatical gender marking, no tested model was able to accurately gender occupation nouns systematically. We release an evaluation scheme and dataset for measuring the ability of NMT models to translate gender morphology correctly in unambiguous contexts across syntactically diverse sentences. Our dataset translates from an English source into 20 languages from several different language families. With the availability of this dataset, our hope is that the NMT community can iterate on solutions for this class of especially egregious errors.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.29630211,"Goal":"Gender Equality","Task":["Automatic Translationin","neural machine translation","NMT","grammatical gender marking","NMT","NMT"],"Method":["Unambiguous Gender Transformer - based models"]},{"ID":"stafanovics-etal-2020-mitigating","title":"Mitigating Gender Bias in Machine Translation with Target Gender Annotations","abstract":"When translating {``}The secretary asked for details.{''} to a language with grammatical gender, it might be necessary to determine the gender of the subject {``}secretary{''}. If the sentence does not contain the necessary information, it is not always possible to disambiguate. In such cases, machine translation systems select the most common translation option, which often corresponds to the stereotypical translations, thus potentially exacerbating prejudice and marginalisation of certain groups and people. We argue that the information necessary for an adequate translation can not always be deduced from the sentence being translated or even might depend on external knowledge. Therefore, in this work, we propose to decouple the task of acquiring the necessary information from the task of learning to translate correctly when such information is available. To that end, we present a method for training machine translation systems to use word-level annotations containing information about subject{'}s gender. To prepare training data, we annotate regular source language words with grammatical gender information of the corresponding target language words. Using such data to train machine translation systems reduces their reliance on gender stereotypes when information about the subject{'}s gender is available. Our experiments on five language pairs show that this allows improving accuracy on the WinoMT test set by up to 25.8 percentage points.","year":2020,"title_abstract":"Mitigating Gender Bias in Machine Translation with Target Gender Annotations When translating {``}The secretary asked for details.{''} to a language with grammatical gender, it might be necessary to determine the gender of the subject {``}secretary{''}. If the sentence does not contain the necessary information, it is not always possible to disambiguate. In such cases, machine translation systems select the most common translation option, which often corresponds to the stereotypical translations, thus potentially exacerbating prejudice and marginalisation of certain groups and people. We argue that the information necessary for an adequate translation can not always be deduced from the sentence being translated or even might depend on external knowledge. Therefore, in this work, we propose to decouple the task of acquiring the necessary information from the task of learning to translate correctly when such information is available. To that end, we present a method for training machine translation systems to use word-level annotations containing information about subject{'}s gender. To prepare training data, we annotate regular source language words with grammatical gender information of the corresponding target language words. Using such data to train machine translation systems reduces their reliance on gender stereotypes when information about the subject{'}s gender is available. Our experiments on five language pairs show that this allows improving accuracy on the WinoMT test set by up to 25.8 percentage points.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2959464788,"Goal":"Gender Equality","Task":["Mitigating Gender Bias","Machine Translation","machine translation","translation","translate","machine translation systems"],"Method":["machine translation systems"]},{"ID":"habash-etal-2019-automatic","title":"Automatic Gender Identification and Reinflection in {A}rabic","abstract":"The impressive progress in many Natural Language Processing (NLP) applications has increased the awareness of some of the biases these NLP systems have with regards to gender identities. In this paper, we propose an approach to extend biased single-output gender-blind NLP systems with gender-specific alternative reinflections. We focus on Arabic, a gender-marking morphologically rich language, in the context of machine translation (MT) from English, and for first-person-singular constructions only. Our contributions are the development of a system-independent gender-awareness wrapper, and the building of a corpus for training and evaluating first-person-singular gender identification and reinflection in Arabic. Our results successfully demonstrate the viability of this approach with 8{\\%} relative increase in Bleu score for first-person-singular feminine, and 5.3{\\%} comparable increase for first-person-singular masculine on top of a state-of-the-art gender-blind MT system on a held-out test set.","year":2019,"title_abstract":"Automatic Gender Identification and Reinflection in {A}rabic The impressive progress in many Natural Language Processing (NLP) applications has increased the awareness of some of the biases these NLP systems have with regards to gender identities. In this paper, we propose an approach to extend biased single-output gender-blind NLP systems with gender-specific alternative reinflections. We focus on Arabic, a gender-marking morphologically rich language, in the context of machine translation (MT) from English, and for first-person-singular constructions only. Our contributions are the development of a system-independent gender-awareness wrapper, and the building of a corpus for training and evaluating first-person-singular gender identification and reinflection in Arabic. Our results successfully demonstrate the viability of this approach with 8{\\%} relative increase in Bleu score for first-person-singular feminine, and 5.3{\\%} comparable increase for first-person-singular masculine on top of a state-of-the-art gender-blind MT system on a held-out test set.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2942878604,"Goal":"Gender Equality","Task":["Automatic Gender Identification","Reinflection","Natural Language Processing","NLP","machine translation","first - person - singular constructions","first - person - singular gender identification","reinflection"],"Method":["biased single - output gender - blind NLP systems","system - independent gender - awareness wrapper","gender - blind MT system"]},{"ID":"king-underwood-2006-evaluating","title":"Evaluating Symbiotic Systems: the challenge","abstract":"This paper looks at a class of systems which pose severe problems in evaluation design for current conventional approaches to evaluation. After describing the two conventional evaluation paradigms: the \u0093functionality paradigm\u0094 as typified by evaluation campaigns and the ISO inspired \u0093user-centred\u0094 paradigm typified by the work of the EAGLES and ISLE projects, it goes on to outline the problems posed by the evaluation of systems which are designed to work in critical interaction with a human expert user and to work over vast amounts of data. These systems pose problems for both paradigms although for different reasons. The primary aim of this paper is to provoke discussion and the search for solutions. We have no proven solutions at present. However, we describe a programme of exploratory research on which we have already embarked, which involves ground clearing work which we expect to result in a deep understanding of the systems and users, a pre-requisite for developing a general framework for evaluation in this field.","year":2006,"title_abstract":"Evaluating Symbiotic Systems: the challenge This paper looks at a class of systems which pose severe problems in evaluation design for current conventional approaches to evaluation. After describing the two conventional evaluation paradigms: the \u0093functionality paradigm\u0094 as typified by evaluation campaigns and the ISO inspired \u0093user-centred\u0094 paradigm typified by the work of the EAGLES and ISLE projects, it goes on to outline the problems posed by the evaluation of systems which are designed to work in critical interaction with a human expert user and to work over vast amounts of data. These systems pose problems for both paradigms although for different reasons. The primary aim of this paper is to provoke discussion and the search for solutions. We have no proven solutions at present. However, we describe a programme of exploratory research on which we have already embarked, which involves ground clearing work which we expect to result in a deep understanding of the systems and users, a pre-requisite for developing a general framework for evaluation in this field.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2935850024,"Goal":"Sustainable Cities and Communities","Task":["Evaluating Symbiotic Systems","evaluation design","evaluation","evaluation of systems","evaluation"],"Method":["evaluation paradigms","\u0093functionality paradigm\u0094","ISO inspired \u0093user - centred\u0094 paradigm"]},{"ID":"kaneko-bollegala-2019-gender","title":"Gender-preserving Debiasing for Pre-trained Word Embeddings","abstract":"Word embeddings learnt from massive text collections have demonstrated significant levels of discriminative biases such as gender, racial or ethnic biases, which in turn bias the down-stream NLP applications that use those word embeddings. Taking gender-bias as a working example, we propose a debiasing method that preserves non-discriminative gender-related information, while removing stereotypical discriminative gender biases from pre-trained word embeddings. Specifically, we consider four types of information: \\textit{feminine}, \\textit{masculine}, \\textit{gender-neutral} and \\textit{stereotypical}, which represent the relationship between gender vs. bias, and propose a debiasing method that (a) preserves the gender-related information in feminine and masculine words, (b) preserves the neutrality in gender-neutral words, and (c) removes the biases from stereotypical words. Experimental results on several previously proposed benchmark datasets show that our proposed method can debias pre-trained word embeddings better than existing SoTA methods proposed for debiasing word embeddings while preserving gender-related but non-discriminative information.","year":2019,"title_abstract":"Gender-preserving Debiasing for Pre-trained Word Embeddings Word embeddings learnt from massive text collections have demonstrated significant levels of discriminative biases such as gender, racial or ethnic biases, which in turn bias the down-stream NLP applications that use those word embeddings. Taking gender-bias as a working example, we propose a debiasing method that preserves non-discriminative gender-related information, while removing stereotypical discriminative gender biases from pre-trained word embeddings. Specifically, we consider four types of information: \\textit{feminine}, \\textit{masculine}, \\textit{gender-neutral} and \\textit{stereotypical}, which represent the relationship between gender vs. bias, and propose a debiasing method that (a) preserves the gender-related information in feminine and masculine words, (b) preserves the neutrality in gender-neutral words, and (c) removes the biases from stereotypical words. Experimental results on several previously proposed benchmark datasets show that our proposed method can debias pre-trained word embeddings better than existing SoTA methods proposed for debiasing word embeddings while preserving gender-related but non-discriminative information.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2917442322,"Goal":"Gender Equality","Task":["Pre - trained Word Embeddings","Word embeddings","down - stream NLP applications","debiasing word embeddings"],"Method":["Gender - preserving Debiasing","word embeddings","debiasing method","debiasing method","word embeddings","SoTA methods"]},{"ID":"wu-etal-2017-linguistic","title":"Linguistic Reflexes of Well-Being and Happiness in Echo","abstract":"Different theories posit different sources for feelings of well-being and happiness. Appraisal theory grounds our emotional responses in our goals and desires and their fulfillment, or lack of fulfillment. Self-Determination theory posits that the basis for well-being rests on our assessments of our competence, autonomy and social connection. And surveys that measure happiness empirically note that people require their basic needs to be met for food and shelter, but beyond that tend to be happiest when socializing, eating or having sex. We analyze a corpus of private micro-blogs from a well-being application called Echo, where users label each written post about daily events with a happiness score between 1 and 9. Our goal is to ground the linguistic descriptions of events that users experience in theories of well-being and happiness, and then examine the extent to which different theoretical accounts can explain the variance in the happiness scores. We show that recurrent event types, such as obligation and incompetence, which affect people{'}s feelings of well-being are not captured in current lexical or semantic resources.","year":2017,"title_abstract":"Linguistic Reflexes of Well-Being and Happiness in Echo Different theories posit different sources for feelings of well-being and happiness. Appraisal theory grounds our emotional responses in our goals and desires and their fulfillment, or lack of fulfillment. Self-Determination theory posits that the basis for well-being rests on our assessments of our competence, autonomy and social connection. And surveys that measure happiness empirically note that people require their basic needs to be met for food and shelter, but beyond that tend to be happiest when socializing, eating or having sex. We analyze a corpus of private micro-blogs from a well-being application called Echo, where users label each written post about daily events with a happiness score between 1 and 9. Our goal is to ground the linguistic descriptions of events that users experience in theories of well-being and happiness, and then examine the extent to which different theoretical accounts can explain the variance in the happiness scores. We show that recurrent event types, such as obligation and incompetence, which affect people{'}s feelings of well-being are not captured in current lexical or semantic resources.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.2914720476,"Goal":"Good Health and Well-Being","Task":["Linguistic Reflexes of Well - Being and Happiness","happiness"],"Method":["Appraisal theory","Self - Determination theory","lexical or semantic resources"]},{"ID":"mayfield-etal-2019-equity","title":"Equity Beyond Bias in Language Technologies for Education","abstract":"There is a long record of research on equity in schools. As machine learning researchers begin to study fairness and bias in earnest, language technologies in education have an unusually strong theoretical and applied foundation to build on. Here, we introduce concepts from culturally relevant pedagogy and other frameworks for teaching and learning, identifying future work on equity in NLP. We present case studies in a range of topics like intelligent tutoring systems, computer-assisted language learning, automated essay scoring, and sentiment analysis in classrooms, and provide an actionable agenda for research.","year":2019,"title_abstract":"Equity Beyond Bias in Language Technologies for Education There is a long record of research on equity in schools. As machine learning researchers begin to study fairness and bias in earnest, language technologies in education have an unusually strong theoretical and applied foundation to build on. Here, we introduce concepts from culturally relevant pedagogy and other frameworks for teaching and learning, identifying future work on equity in NLP. We present case studies in a range of topics like intelligent tutoring systems, computer-assisted language learning, automated essay scoring, and sentiment analysis in classrooms, and provide an actionable agenda for research.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.2907227278,"Goal":"Quality Education","Task":["Education","machine learning researchers","education","teaching and learning","NLP","intelligent tutoring systems","computer - assisted language learning","automated essay scoring","sentiment analysis"],"Method":["Language Technologies","language technologies","culturally relevant pedagogy"]},{"ID":"vanmassenhove-monti-2021-gender","title":"g{EN}der-{IT}: An Annotated {E}nglish-{I}talian Parallel Challenge Set for Cross-Linguistic Natural Gender Phenomena","abstract":"Languages differ in terms of the absence or presence of gender features, the number of gender classes and whether and where gender features are explicitly marked. These cross-linguistic differences can lead to ambiguities that are difficult to resolve, especially for sentence-level MT systems. The identification of ambiguity and its subsequent resolution is a challenging task for which currently there aren{'}t any specific resources or challenge sets available. In this paper, we introduce gENder-IT, an English{--}Italian challenge set focusing on the resolution of natural gender phenomena by providing word-level gender tags on the English source side and multiple gender alternative translations, where needed, on the Italian target side.","year":2021,"title_abstract":"g{EN}der-{IT}: An Annotated {E}nglish-{I}talian Parallel Challenge Set for Cross-Linguistic Natural Gender Phenomena Languages differ in terms of the absence or presence of gender features, the number of gender classes and whether and where gender features are explicitly marked. These cross-linguistic differences can lead to ambiguities that are difficult to resolve, especially for sentence-level MT systems. The identification of ambiguity and its subsequent resolution is a challenging task for which currently there aren{'}t any specific resources or challenge sets available. In this paper, we introduce gENder-IT, an English{--}Italian challenge set focusing on the resolution of natural gender phenomena by providing word-level gender tags on the English source side and multiple gender alternative translations, where needed, on the Italian target side.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2897545695,"Goal":"Gender Equality","Task":["Cross - Linguistic Natural Gender Phenomena Languages","MT","identification of ambiguity","resolution","resolution of natural gender phenomena"],"Method":["gENder - IT"]},{"ID":"zhao-etal-2018-learning","title":"Learning Gender-Neutral Word Embeddings","abstract":"Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.","year":2018,"title_abstract":"Learning Gender-Neutral Word Embeddings Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2877370119,"Goal":"Gender Equality","Task":["Learning Gender - Neutral Word Embeddings","Natural Language Processing","learning gender - neutral word embeddings"],"Method":["Word embedding models","training procedure","Gender - Neutral variant of GloVe","GloVe)","GN - GloVe","embedding model"]},{"ID":"saunders-etal-2022-first","title":"First the Worst: Finding Better Gender Translations During Beam Search","abstract":"Generating machine translations via beam search seeks the most likely output under a model. However, beam search has been shown to amplify demographic biases exhibited by a model. We aim to address this, focusing on gender bias resulting from systematic errors in grammatical gender translation. Almost all prior work on this problem adjusts the training data or the model itself. By contrast, our approach changes only the inference procedure. We constrain beam search to improve gender diversity in n-best lists, and rerank n-best lists using gender features obtained from the source sentence. Combining these strongly improves WinoMT gender translation accuracy for three language pairs without additional bilingual data or retraining. We also demonstrate our approach{'}s utility for consistently gendering named entities, and its flexibility to handle new gendered language beyond the binary.","year":2022,"title_abstract":"First the Worst: Finding Better Gender Translations During Beam Search Generating machine translations via beam search seeks the most likely output under a model. However, beam search has been shown to amplify demographic biases exhibited by a model. We aim to address this, focusing on gender bias resulting from systematic errors in grammatical gender translation. Almost all prior work on this problem adjusts the training data or the model itself. By contrast, our approach changes only the inference procedure. We constrain beam search to improve gender diversity in n-best lists, and rerank n-best lists using gender features obtained from the source sentence. Combining these strongly improves WinoMT gender translation accuracy for three language pairs without additional bilingual data or retraining. We also demonstrate our approach{'}s utility for consistently gendering named entities, and its flexibility to handle new gendered language beyond the binary.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2867290378,"Goal":"Gender Equality","Task":["machine translations","gender bias","grammatical gender translation","translation","consistently gendering named entities"],"Method":["Beam Search","beam search","beam search","inference procedure","beam search"]},{"ID":"bird-2022-local","title":"Local Languages, Third Spaces, and other High-Resource Scenarios","abstract":"How can language technology address the diverse situations of the world{'}s languages? In one view, languages exist on a resource continuum and the challenge is to scale existing solutions, bringing under-resourced languages into the high-resource world. In another view, presented here, the world{'}s language ecology includes standardised languages, local languages, and contact languages. These are often subsumed under the label of {``}under-resourced languages{''} even though they have distinct functions and prospects. I explore this position and propose some ecologically-aware language technology agendas.","year":2022,"title_abstract":"Local Languages, Third Spaces, and other High-Resource Scenarios How can language technology address the diverse situations of the world{'}s languages? In one view, languages exist on a resource continuum and the challenge is to scale existing solutions, bringing under-resourced languages into the high-resource world. In another view, presented here, the world{'}s language ecology includes standardised languages, local languages, and contact languages. These are often subsumed under the label of {``}under-resourced languages{''} even though they have distinct functions and prospects. I explore this position and propose some ecologically-aware language technology agendas.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.2858249545,"Goal":"Life Below Water","Task":["world{'}s language ecology","ecologically - aware language technology"],"Method":["language technology"]},{"ID":"jiang-etal-2017-comparing","title":"Comparing Attitudes to Climate Change in the Media using sentiment analysis based on {L}atent {D}irichlet {A}llocation","abstract":"News media typically present biased accounts of news stories, and different publications present different angles on the same event. In this research, we investigate how different publications differ in their approach to stories about climate change, by examining the sentiment and topics presented. To understand these attitudes, we find sentiment targets by combining Latent Dirichlet Allocation (LDA) with SentiWordNet, a general sentiment lexicon. Using LDA, we generate topics containing keywords which represent the sentiment targets, and then annotate the data using SentiWordNet before regrouping the articles based on topic similarity. Preliminary analysis identifies clearly different attitudes on the same issue presented in different news sources. Ongoing work is investigating how systematic these attitudes are between different publications, and how these may change over time.","year":2017,"title_abstract":"Comparing Attitudes to Climate Change in the Media using sentiment analysis based on {L}atent {D}irichlet {A}llocation News media typically present biased accounts of news stories, and different publications present different angles on the same event. In this research, we investigate how different publications differ in their approach to stories about climate change, by examining the sentiment and topics presented. To understand these attitudes, we find sentiment targets by combining Latent Dirichlet Allocation (LDA) with SentiWordNet, a general sentiment lexicon. Using LDA, we generate topics containing keywords which represent the sentiment targets, and then annotate the data using SentiWordNet before regrouping the articles based on topic similarity. Preliminary analysis identifies clearly different attitudes on the same issue presented in different news sources. Ongoing work is investigating how systematic these attitudes are between different publications, and how these may change over time.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2848234773,"Goal":"Climate Action","Task":["sentiment analysis"],"Method":["Latent Dirichlet Allocation","SentiWordNet","general sentiment lexicon","LDA","SentiWordNet"]},{"ID":"hirmer-etal-2021-building","title":"Building Representative Corpora from Illiterate Communities: A Reviewof Challenges and Mitigation Strategies for Developing Countries","abstract":"Most well-established data collection methods currently adopted in NLP depend on the as- sumption of speaker literacy. Consequently, the collected corpora largely fail to represent swathes of the global population, which tend to be some of the most vulnerable and marginalised people in society, and often live in rural developing areas. Such underrepresented groups are thus not only ignored when making modeling and system design decisions, but also prevented from benefiting from development outcomes achieved through data-driven NLP. This paper aims to address the under-representation of illiterate communities in NLP corpora: we identify potential biases and ethical issues that might arise when collecting data from rural communities with high illiteracy rates in Low-Income Countries, and propose a set of practical mitigation strategies to help future work.","year":2021,"title_abstract":"Building Representative Corpora from Illiterate Communities: A Reviewof Challenges and Mitigation Strategies for Developing Countries Most well-established data collection methods currently adopted in NLP depend on the as- sumption of speaker literacy. Consequently, the collected corpora largely fail to represent swathes of the global population, which tend to be some of the most vulnerable and marginalised people in society, and often live in rural developing areas. Such underrepresented groups are thus not only ignored when making modeling and system design decisions, but also prevented from benefiting from development outcomes achieved through data-driven NLP. This paper aims to address the under-representation of illiterate communities in NLP corpora: we identify potential biases and ethical issues that might arise when collecting data from rural communities with high illiteracy rates in Low-Income Countries, and propose a set of practical mitigation strategies to help future work.","social_need":"No Poverty End poverty in all its forms everywhere","cosine_similarity":0.2842176557,"Goal":"No Poverty","Task":["NLP","modeling and system design decisions"],"Method":["data collection methods","data - driven NLP","mitigation strategies"]},{"ID":"wang-etal-2020-double","title":"Double-Hard Debias: Tailoring Word Embeddings for Gender Bias Mitigation","abstract":"Word embeddings derived from human-generated corpora inherit strong gender bias which can be further amplified by downstream models. Some commonly adopted debiasing approaches, including the seminal Hard Debias algorithm, apply post-processing procedures that project pre-trained word embeddings into a subspace orthogonal to an inferred gender subspace. We discover that semantic-agnostic corpus regularities such as word frequency captured by the word embeddings negatively impact the performance of these algorithms. We propose a simple but effective technique, Double Hard Debias, which purifies the word embeddings against such corpus regularities prior to inferring and removing the gender subspace. Experiments on three bias mitigation benchmarks show that our approach preserves the distributional semantics of the pre-trained word embeddings while reducing gender bias to a significantly larger degree than prior approaches.","year":2020,"title_abstract":"Double-Hard Debias: Tailoring Word Embeddings for Gender Bias Mitigation Word embeddings derived from human-generated corpora inherit strong gender bias which can be further amplified by downstream models. Some commonly adopted debiasing approaches, including the seminal Hard Debias algorithm, apply post-processing procedures that project pre-trained word embeddings into a subspace orthogonal to an inferred gender subspace. We discover that semantic-agnostic corpus regularities such as word frequency captured by the word embeddings negatively impact the performance of these algorithms. We propose a simple but effective technique, Double Hard Debias, which purifies the word embeddings against such corpus regularities prior to inferring and removing the gender subspace. Experiments on three bias mitigation benchmarks show that our approach preserves the distributional semantics of the pre-trained word embeddings while reducing gender bias to a significantly larger degree than prior approaches.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2838883996,"Goal":"Gender Equality","Task":["Tailoring Word Embeddings","Gender Bias Mitigation","Word embeddings","bias mitigation benchmarks"],"Method":["Double - Hard Debias","downstream models","debiasing approaches","seminal Hard Debias algorithm","post - processing procedures","Double Hard Debias"]},{"ID":"marsi-etal-2017-marine","title":"Marine Variable Linker: Exploring Relations between Changing Variables in Marine Science Literature","abstract":"We report on a demonstration system for text mining of literature in marine science and related disciplines. It automatically extracts variables ({``}CO2{''}) involved in events of change\/increase\/decrease ({``}increasing CO2{''}), as well as co-occurrence and causal relations among these events ({``}increasing CO2 causes a decrease in pH in seawater{''}), resulting in a big knowledge graph. A web-based graphical user interface targeted at marine scientists facilitates searching, browsing and visualising events and their relations in an interactive way.","year":2017,"title_abstract":"Marine Variable Linker: Exploring Relations between Changing Variables in Marine Science Literature We report on a demonstration system for text mining of literature in marine science and related disciplines. It automatically extracts variables ({``}CO2{''}) involved in events of change\/increase\/decrease ({``}increasing CO2{''}), as well as co-occurrence and causal relations among these events ({``}increasing CO2 causes a decrease in pH in seawater{''}), resulting in a big knowledge graph. A web-based graphical user interface targeted at marine scientists facilitates searching, browsing and visualising events and their relations in an interactive way.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.281129241,"Goal":"Life Below Water","Task":["text mining of literature","marine science","marine scientists"],"Method":["Marine Variable Linker","web - based graphical user interface"]},{"ID":"hinrichs-etal-2010-sustainability","title":"Sustainability of Linguistic Data and Analysis in the Context of a Collaborative e{S}cience Environment","abstract":"For researchers, it is especially important that primary research data are preserved and made available on a long-term basis and to a wide variety of researchers. In order to ensure long-term availability of the archived data, it is imperative that the data to be stored is conformant with standardized data formats and best practices followed by the relevant research communities. Storing, managing, and accessing such standard-conformant data requires a repository-based infrastructure. Two projects at the University of T{\\\"u}bingen are realizing a collaborative eScience research environment with the help of eSciDoc for the university that supports long-term preservation of all kinds of data as well as a fine-grained and contextualized data management: the INF project and the BW-eSci(T) project. The task of the infrastructure (INF) project within the collaborative research centre {\\^a}\u0080\u009eEmergence of Meaning\u0093 (SFB 833) is to guarantee the long-term availability of the SFBs data. BW-eSci(T) is a joint project of the University of T{\\\"u}bingen and the Fachinformationszentrums (FIZ) Karlsruhe. The goal of this project is to develop a prototypical eScience research environment for the University of T{\\\"u}bingen.","year":2010,"title_abstract":"Sustainability of Linguistic Data and Analysis in the Context of a Collaborative e{S}cience Environment For researchers, it is especially important that primary research data are preserved and made available on a long-term basis and to a wide variety of researchers. In order to ensure long-term availability of the archived data, it is imperative that the data to be stored is conformant with standardized data formats and best practices followed by the relevant research communities. Storing, managing, and accessing such standard-conformant data requires a repository-based infrastructure. Two projects at the University of T{\\\"u}bingen are realizing a collaborative eScience research environment with the help of eSciDoc for the university that supports long-term preservation of all kinds of data as well as a fine-grained and contextualized data management: the INF project and the BW-eSci(T) project. The task of the infrastructure (INF) project within the collaborative research centre {\\^a}\u0080\u009eEmergence of Meaning\u0093 (SFB 833) is to guarantee the long-term availability of the SFBs data. BW-eSci(T) is a joint project of the University of T{\\\"u}bingen and the Fachinformationszentrums (FIZ) Karlsruhe. The goal of this project is to develop a prototypical eScience research environment for the University of T{\\\"u}bingen.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2804657221,"Goal":"Sustainable Cities and Communities","Task":["Analysis","eScience research environment","long - term preservation","fine - grained and contextualized data management","INF project","Meaning\u0093"],"Method":["eSciDoc","BW - eSci(T) project","SFBs","BW - eSci(T)"]},{"ID":"gonen-goldberg-2019-lipstick-pig","title":"Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them","abstract":"Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between {``}gender-neutralized{''} words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.","year":2019,"title_abstract":"Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between {``}gender-neutralized{''} words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2802814543,"Goal":"Gender Equality","Task":["NLP","word embeddings","gender - neutral modeling"],"Method":["Debiasing Methods","debiasing methods","bias removal techniques"]},{"ID":"renduchintala-etal-2021-gender","title":"Gender bias amplification during Speed-Quality optimization in Neural Machine Translation","abstract":"Is bias amplified when neural machine translation (NMT) models are optimized for speed and evaluated on generic test sets using BLEU? We investigate architectures and techniques commonly used to speed up decoding in Transformer-based models, such as greedy search, quantization, average attention networks (AANs) and shallow decoder models and show their effect on gendered noun translation. We construct a new gender bias test set, SimpleGEN, based on gendered noun phrases in which there is a single, unambiguous, correct answer. While we find minimal overall BLEU degradation as we apply speed optimizations, we observe that gendered noun translation performance degrades at a much faster rate.","year":2021,"title_abstract":"Gender bias amplification during Speed-Quality optimization in Neural Machine Translation Is bias amplified when neural machine translation (NMT) models are optimized for speed and evaluated on generic test sets using BLEU? We investigate architectures and techniques commonly used to speed up decoding in Transformer-based models, such as greedy search, quantization, average attention networks (AANs) and shallow decoder models and show their effect on gendered noun translation. We construct a new gender bias test set, SimpleGEN, based on gendered noun phrases in which there is a single, unambiguous, correct answer. While we find minimal overall BLEU degradation as we apply speed optimizations, we observe that gendered noun translation performance degrades at a much faster rate.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2776755989,"Goal":"Gender Equality","Task":["Gender bias amplification","Speed - Quality optimization","Neural Machine Translation","neural machine translation","decoding","gendered noun translation","translation"],"Method":["Transformer - based models","greedy search","quantization","average attention networks","shallow decoder models","speed optimizations"]},{"ID":"qian-etal-2019-reducing","title":"Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function","abstract":"Gender bias exists in natural language datasets, which neural language models tend to learn, resulting in biased text generation. In this research, we propose a debiasing approach based on the loss function modification. We introduce a new term to the loss function which attempts to equalize the probabilities of male and female words in the output. Using an array of bias evaluation metrics, we provide empirical evidence that our approach successfully mitigates gender bias in language models without increasing perplexity. In comparison to existing debiasing strategies, data augmentation, and word embedding debiasing, our method performs better in several aspects, especially in reducing gender bias in occupation words. Finally, we introduce a combination of data augmentation and our approach and show that it outperforms existing strategies in all bias evaluation metrics.","year":2019,"title_abstract":"Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function Gender bias exists in natural language datasets, which neural language models tend to learn, resulting in biased text generation. In this research, we propose a debiasing approach based on the loss function modification. We introduce a new term to the loss function which attempts to equalize the probabilities of male and female words in the output. Using an array of bias evaluation metrics, we provide empirical evidence that our approach successfully mitigates gender bias in language models without increasing perplexity. In comparison to existing debiasing strategies, data augmentation, and word embedding debiasing, our method performs better in several aspects, especially in reducing gender bias in occupation words. Finally, we introduce a combination of data augmentation and our approach and show that it outperforms existing strategies in all bias evaluation metrics.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2774900198,"Goal":"Gender Equality","Task":["Reducing Gender Bias","Word - Level Language Models","Gender bias","biased text generation"],"Method":["Gender - Equalizing Loss Function","neural language models","debiasing approach","loss function modification","loss function","language models","debiasing strategies","data augmentation","word embedding debiasing","data augmentation"]},{"ID":"gonen-etal-2019-grammatical-gender","title":"How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?","abstract":"Many natural languages assign grammatical gender also to inanimate nouns in the language. In such languages, words that relate to the gender-marked nouns are inflected to agree with the noun{'}s gender. We show that this affects the word representations of inanimate nouns, resulting in nouns with the same gender being closer to each other than nouns with different gender. While {``}embedding debiasing{''} methods fail to remove the effect, we demonstrate that a careful application of methods that neutralize grammatical gender signals from the words{'} context when training word embeddings is effective in removing it. Fixing the grammatical gender bias results in a positive effect on the quality of the resulting word embeddings, both in monolingual and cross lingual settings. We note that successfully removing gender signals, while achievable, is not trivial to do and that a language-specific morphological analyzer, together with careful usage of it, are essential for achieving good results.","year":2019,"title_abstract":"How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages? Many natural languages assign grammatical gender also to inanimate nouns in the language. In such languages, words that relate to the gender-marked nouns are inflected to agree with the noun{'}s gender. We show that this affects the word representations of inanimate nouns, resulting in nouns with the same gender being closer to each other than nouns with different gender. While {``}embedding debiasing{''} methods fail to remove the effect, we demonstrate that a careful application of methods that neutralize grammatical gender signals from the words{'} context when training word embeddings is effective in removing it. Fixing the grammatical gender bias results in a positive effect on the quality of the resulting word embeddings, both in monolingual and cross lingual settings. We note that successfully removing gender signals, while achievable, is not trivial to do and that a language-specific morphological analyzer, together with careful usage of it, are essential for achieving good results.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2774401903,"Goal":"Gender Equality","Task":["monolingual and cross lingual settings"],"Method":["Noun Representations","word representations","debiasing{''} methods","word embeddings","word embeddings","language - specific morphological analyzer"]},{"ID":"rios-etal-2020-quantifying","title":"Quantifying 60 Years of Gender Bias in Biomedical Research with Word Embeddings","abstract":"Gender bias in biomedical research can have an adverse impact on the health of real people. For example, there is evidence that heart disease-related funded research generally focuses on men. Health disparities can form between men and at-risk groups of women (i.e., elderly and low-income) if there is not an equal number of heart disease-related studies for both genders. In this paper, we study temporal bias in biomedical research articles by measuring gender differences in word embeddings. Specifically, we address multiple questions, including, How has gender bias changed over time in biomedical research, and what health-related concepts are the most biased? Overall, we find that traditional gender stereotypes have reduced over time. However, we also find that the embeddings of many medical conditions are as biased today as they were 60 years ago (e.g., concepts related to drug addiction and body dysmorphia).","year":2020,"title_abstract":"Quantifying 60 Years of Gender Bias in Biomedical Research with Word Embeddings Gender bias in biomedical research can have an adverse impact on the health of real people. For example, there is evidence that heart disease-related funded research generally focuses on men. Health disparities can form between men and at-risk groups of women (i.e., elderly and low-income) if there is not an equal number of heart disease-related studies for both genders. In this paper, we study temporal bias in biomedical research articles by measuring gender differences in word embeddings. Specifically, we address multiple questions, including, How has gender bias changed over time in biomedical research, and what health-related concepts are the most biased? Overall, we find that traditional gender stereotypes have reduced over time. However, we also find that the embeddings of many medical conditions are as biased today as they were 60 years ago (e.g., concepts related to drug addiction and body dysmorphia).","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2765909135,"Goal":"Gender Equality","Task":["biomedical research","biomedical research"],"Method":["Word Embeddings"]},{"ID":"gotze-boye-2016-spaceref","title":"{S}pace{R}ef: A corpus of street-level geographic descriptions","abstract":"This article describes SPACEREF, a corpus of street-level geographic descriptions. Pedestrians are walking a route in a (real) urban environment, describing their actions. Their position is automatically logged, their speech is manually transcribed, and their references to objects are manually annotated with respect to a crowdsourced geographic database. We describe how the data was collected and annotated, and how it has been used in the context of creating resources for an automatic pedestrian navigation system.","year":2016,"title_abstract":"{S}pace{R}ef: A corpus of street-level geographic descriptions This article describes SPACEREF, a corpus of street-level geographic descriptions. Pedestrians are walking a route in a (real) urban environment, describing their actions. Their position is automatically logged, their speech is manually transcribed, and their references to objects are manually annotated with respect to a crowdsourced geographic database. We describe how the data was collected and annotated, and how it has been used in the context of creating resources for an automatic pedestrian navigation system.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2764818072,"Goal":"Sustainable Cities and Communities","Task":["street - level geographic descriptions","automatic pedestrian navigation system"],"Method":["SPACEREF"]},{"ID":"simchon-gilead-2018-psychologically","title":"A Psychologically Informed Approach to {CLP}sych Shared Task 2018","abstract":"This paper describes our approach to the CLPsych 2018 Shared Task, in which we attempted to predict cross-sectional psychological health at age 11 and future psychological distress based on childhood essays. We attempted several modeling approaches and observed best cross-validated prediction accuracy with relatively simple models based on psychological theory. The models provided reasonable predictions in most outcomes. Notably, our model was especially successful in predicting out-of-sample psychological distress (across people and across time) at age 50.","year":2018,"title_abstract":"A Psychologically Informed Approach to {CLP}sych Shared Task 2018 This paper describes our approach to the CLPsych 2018 Shared Task, in which we attempted to predict cross-sectional psychological health at age 11 and future psychological distress based on childhood essays. We attempted several modeling approaches and observed best cross-validated prediction accuracy with relatively simple models based on psychological theory. The models provided reasonable predictions in most outcomes. Notably, our model was especially successful in predicting out-of-sample psychological distress (across people and across time) at age 50.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.2756687701,"Goal":"Good Health and Well-Being","Task":["Shared Task","CLPsych 2018 Shared Task","cross - sectional psychological health","out - of - sample psychological distress"],"Method":["Psychologically Informed Approach","modeling approaches","psychological theory"]},{"ID":"vanmassenhove-etal-2021-neutral","title":"{N}eu{T}ral {R}ewriter: {A} Rule-Based and Neural Approach to Automatic Rewriting into Gender Neutral Alternatives","abstract":"Recent years have seen an increasing need for gender-neutral and inclusive language. Within the field of NLP, there are various mono- and bilingual use cases where gender inclusive language is appropriate, if not preferred due to ambiguity or uncertainty in terms of the gender of referents. In this work, we present a rule-based and a neural approach to gender-neutral rewriting for English along with manually curated synthetic data (WinoBias+) and natural data (OpenSubtitles and Reddit) benchmarks. A detailed manual and automatic evaluation highlights how our NeuTral Rewriter, trained on data generated by the rule-based approach, obtains word error rates (WER) below 0.18{\\%} on synthetic, in-domain and out-domain test sets.","year":2021,"title_abstract":"{N}eu{T}ral {R}ewriter: {A} Rule-Based and Neural Approach to Automatic Rewriting into Gender Neutral Alternatives Recent years have seen an increasing need for gender-neutral and inclusive language. Within the field of NLP, there are various mono- and bilingual use cases where gender inclusive language is appropriate, if not preferred due to ambiguity or uncertainty in terms of the gender of referents. In this work, we present a rule-based and a neural approach to gender-neutral rewriting for English along with manually curated synthetic data (WinoBias+) and natural data (OpenSubtitles and Reddit) benchmarks. A detailed manual and automatic evaluation highlights how our NeuTral Rewriter, trained on data generated by the rule-based approach, obtains word error rates (WER) below 0.18{\\%} on synthetic, in-domain and out-domain test sets.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.275031507,"Goal":"Gender Equality","Task":["Automatic Rewriting","Gender Neutral Alternatives","NLP","gender - neutral rewriting"],"Method":["Rule - Based and Neural Approach","rule - based","neural approach","NeuTral Rewriter","rule - based approach"]},{"ID":"gonen-etal-2019-grammatical","title":"How Does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?","abstract":"Many natural languages assign grammatical gender also to inanimate nouns in the language. In such languages, words that relate to the gender-marked nouns are inflected to agree with the noun{'}s gender. We show that this affects the word representations of inanimate nouns, resulting in nouns with the same gender being closer to each other than nouns with different gender. While {``}embedding debiasing{''} methods fail to remove the effect, we demonstrate that a careful application of methods that neutralize grammatical gender signals from the words{'} context when training word embeddings is effective in removing it. Fixing the grammatical gender bias yields a positive effect on the quality of the resulting word embeddings, both in monolingual and cross-lingual settings. We note that successfully removing gender signals, while achievable, is not trivial to do and that a language-specific morphological analyzer, together with careful usage of it, are essential for achieving good results.","year":2019,"title_abstract":"How Does Grammatical Gender Affect Noun Representations in Gender-Marking Languages? Many natural languages assign grammatical gender also to inanimate nouns in the language. In such languages, words that relate to the gender-marked nouns are inflected to agree with the noun{'}s gender. We show that this affects the word representations of inanimate nouns, resulting in nouns with the same gender being closer to each other than nouns with different gender. While {``}embedding debiasing{''} methods fail to remove the effect, we demonstrate that a careful application of methods that neutralize grammatical gender signals from the words{'} context when training word embeddings is effective in removing it. Fixing the grammatical gender bias yields a positive effect on the quality of the resulting word embeddings, both in monolingual and cross-lingual settings. We note that successfully removing gender signals, while achievable, is not trivial to do and that a language-specific morphological analyzer, together with careful usage of it, are essential for achieving good results.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2748944759,"Goal":"Gender Equality","Task":["monolingual and cross - lingual settings"],"Method":["Noun Representations","word representations","debiasing{''} methods","word embeddings","word embeddings","language - specific morphological analyzer"]},{"ID":"vogt-andre-2006-improving","title":"Improving Automatic Emotion Recognition from Speech via Gender Differentiaion","abstract":"Feature extraction is still a disputed issue for the recognition of emotions from speech. Differences in features for male and female speakers are a well-known problem and it is established that gender-dependent emotion recognizers perform better than gender-independent ones. We propose a way to improve the discriminative quality of gender-dependent features: The emotion recognition system is preceded by an automatic gender detection that decides upon which of two gender-dependent emotion classifiers is used to classify an utterance. This framework was tested on two different databases, one with emotional speech produced by actors and one with spontaneous emotional speech from a Wizard-of-Oz setting. Gender detection achieved an accuracy of about 90 {\\%} and the combined gender and emotion recognition system improved the overall recognition rate of a gender-independent emotion recognition system by 2-4 {\\%}.","year":2006,"title_abstract":"Improving Automatic Emotion Recognition from Speech via Gender Differentiaion Feature extraction is still a disputed issue for the recognition of emotions from speech. Differences in features for male and female speakers are a well-known problem and it is established that gender-dependent emotion recognizers perform better than gender-independent ones. We propose a way to improve the discriminative quality of gender-dependent features: The emotion recognition system is preceded by an automatic gender detection that decides upon which of two gender-dependent emotion classifiers is used to classify an utterance. This framework was tested on two different databases, one with emotional speech produced by actors and one with spontaneous emotional speech from a Wizard-of-Oz setting. Gender detection achieved an accuracy of about 90 {\\%} and the combined gender and emotion recognition system improved the overall recognition rate of a gender-independent emotion recognition system by 2-4 {\\%}.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2748192549,"Goal":"Gender Equality","Task":["Automatic Emotion Recognition","recognition of emotions","automatic gender detection","Gender detection"],"Method":["Gender Differentiaion Feature extraction","gender - dependent emotion recognizers","emotion recognition system","gender - dependent emotion classifiers","gender and emotion recognition system","gender - independent emotion recognition system"]},{"ID":"oboronko-2000-wired","title":"Wired for peace and multi-language communication","abstract":"Our project Wired for Peace: Virtual Diplomacy in Northeast Asia (Http:\/\/www- neacd.ucsd.edu\/) has as its main aim to provide policymakers and researchers of the U.S., China, Russia, Japan, and Korea with Internet based tools to allow for continuous communication on issues of the regional security and cooperation. Since the very beginning of the project, we have understood that Web-based translation between English and Asian languages would be one of the most necessary tools for successful development of the project. With this understanding, we have partnered with Systran (www.systransoft.com), one of the leaders in MT field, in order to develop Internet-based tools for both synchronous and asynchronous translation of texts and discussions. This submission is a report on a work in progress.","year":2000,"title_abstract":"Wired for peace and multi-language communication Our project Wired for Peace: Virtual Diplomacy in Northeast Asia (Http:\/\/www- neacd.ucsd.edu\/) has as its main aim to provide policymakers and researchers of the U.S., China, Russia, Japan, and Korea with Internet based tools to allow for continuous communication on issues of the regional security and cooperation. Since the very beginning of the project, we have understood that Web-based translation between English and Asian languages would be one of the most necessary tools for successful development of the project. With this understanding, we have partnered with Systran (www.systransoft.com), one of the leaders in MT field, in order to develop Internet-based tools for both synchronous and asynchronous translation of texts and discussions. This submission is a report on a work in progress.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.2741247118,"Goal":"Peace, Justice and Strong Institutions","Task":["peace","multi - language communication","regional security","translation","MT field","synchronous and asynchronous translation of texts"],"Method":["Systran","systransoft","Internet - based tools"]},{"ID":"surana-chinagundi-2022-ginius","title":"gini{U}s @{LT}-{EDI}-{ACL}2022: Aasha: Transformers based Hope-{EDI}","abstract":"This paper describes team giniUs{'} submission to the Hope Speech Detection for Equality, Diversity and Inclusion Shared Task organised by LT-EDI ACL 2022. We have fine-tuned the Roberta-large pre-trained model and extracted the last four decoder layers to build a classifier. Our best result on the leaderboard achieve a weighted F1 score of 0.86 and a Macro F1 score of 0.51 for English. We have secured a rank of 4 for the English task. We have open-sourced our code implementations on GitHub to facilitate easy reproducibility by the scientific community.","year":2022,"title_abstract":"gini{U}s @{LT}-{EDI}-{ACL}2022: Aasha: Transformers based Hope-{EDI} This paper describes team giniUs{'} submission to the Hope Speech Detection for Equality, Diversity and Inclusion Shared Task organised by LT-EDI ACL 2022. We have fine-tuned the Roberta-large pre-trained model and extracted the last four decoder layers to build a classifier. Our best result on the leaderboard achieve a weighted F1 score of 0.86 and a Macro F1 score of 0.51 for English. We have secured a rank of 4 for the English task. We have open-sourced our code implementations on GitHub to facilitate easy reproducibility by the scientific community.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2729432583,"Goal":"Gender Equality","Task":["Hope Speech Detection","Equality","Inclusion","English task"],"Method":["Transformers","Roberta - large pre - trained model","decoder layers","classifier"]},{"ID":"saunders-byrne-2020-reducing","title":"Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem","abstract":"Training data for NLP tasks often exhibits gender bias in that fewer sentences refer to women than to men. In Neural Machine Translation (NMT) gender bias has been shown to reduce translation quality, particularly when the target language has grammatical gender. The recent WinoMT challenge set allows us to measure this effect directly (Stanovsky et al, 2019) Ideally we would reduce system bias by simply debiasing all data prior to training, but achieving this effectively is itself a challenge. Rather than attempt to create a {`}balanced{'} dataset, we use transfer learning on a small set of trusted, gender-balanced examples. This approach gives strong and consistent improvements in gender debiasing with much less computational cost than training from scratch. A known pitfall of transfer learning on new domains is {`}catastrophic forgetting{'}, which we address at adaptation and inference time. During adaptation we show that Elastic Weight Consolidation allows a performance trade-off between general translation quality and bias reduction. At inference time we propose a lattice-rescoring scheme which outperforms all systems evaluated in Stanovsky et al, 2019 on WinoMT with no degradation of general test set BLEU. We demonstrate our approach translating from English into three languages with varied linguistic properties and data availability.","year":2020,"title_abstract":"Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem Training data for NLP tasks often exhibits gender bias in that fewer sentences refer to women than to men. In Neural Machine Translation (NMT) gender bias has been shown to reduce translation quality, particularly when the target language has grammatical gender. The recent WinoMT challenge set allows us to measure this effect directly (Stanovsky et al, 2019) Ideally we would reduce system bias by simply debiasing all data prior to training, but achieving this effectively is itself a challenge. Rather than attempt to create a {`}balanced{'} dataset, we use transfer learning on a small set of trusted, gender-balanced examples. This approach gives strong and consistent improvements in gender debiasing with much less computational cost than training from scratch. A known pitfall of transfer learning on new domains is {`}catastrophic forgetting{'}, which we address at adaptation and inference time. During adaptation we show that Elastic Weight Consolidation allows a performance trade-off between general translation quality and bias reduction. At inference time we propose a lattice-rescoring scheme which outperforms all systems evaluated in Stanovsky et al, 2019 on WinoMT with no degradation of general test set BLEU. We demonstrate our approach translating from English into three languages with varied linguistic properties and data availability.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2723685801,"Goal":"Gender Equality","Task":["Reducing Gender Bias","Neural Machine Translation","Domain Adaptation Problem","NLP tasks","Neural Machine Translation","gender bias","gender debiasing","adaptation and inference time","adaptation","bias reduction"],"Method":["transfer learning","transfer learning","Elastic Weight Consolidation","lattice - rescoring scheme"]},{"ID":"sahai-sharma-2021-predicting","title":"Predicting and Explaining {F}rench Grammatical Gender","abstract":"Grammatical gender may be determined by semantics, orthography, phonology, or could even be arbitrary. Identifying patterns in the factors that govern noun genders can be useful for language learners, and for understanding innate linguistic sources of gender bias. Traditional manual rule-based approaches may be substituted by more accurate and scalable but harder-to-interpret computational approaches for predicting gender from typological information. In this work, we propose interpretable gender classification models for French, which obtain the best of both worlds. We present high accuracy neural approaches which are augmented by a novel global surrogate based approach for explaining predictions. We introduce {`}auxiliary attributes{'} to provide tunable explanation complexity.","year":2021,"title_abstract":"Predicting and Explaining {F}rench Grammatical Gender Grammatical gender may be determined by semantics, orthography, phonology, or could even be arbitrary. Identifying patterns in the factors that govern noun genders can be useful for language learners, and for understanding innate linguistic sources of gender bias. Traditional manual rule-based approaches may be substituted by more accurate and scalable but harder-to-interpret computational approaches for predicting gender from typological information. In this work, we propose interpretable gender classification models for French, which obtain the best of both worlds. We present high accuracy neural approaches which are augmented by a novel global surrogate based approach for explaining predictions. We introduce {`}auxiliary attributes{'} to provide tunable explanation complexity.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2723568976,"Goal":"Gender Equality","Task":["Predicting and Explaining {F}rench Grammatical Gender","language learners","innate linguistic sources of gender bias","predicting gender","explaining predictions"],"Method":["manual rule - based approaches","computational approaches","interpretable gender classification models","neural approaches","global surrogate based approach"]},{"ID":"savoldi-etal-2022-morphosyntactic","title":"Under the Morphosyntactic Lens: A Multifaceted Evaluation of Gender Bias in Speech Translation","abstract":"Gender bias is largely recognized as a problematic phenomenon affecting language technologies, with recent studies underscoring that it might surface differently across languages. However, most of current evaluation practices adopt a word-level focus on a narrow set of occupational nouns under synthetic conditions. Such protocols overlook key features of grammatical gender languages, which are characterized by morphosyntactic chains of gender agreement, marked on a variety of lexical items and parts-of-speech (POS). To overcome this limitation, we enrich the natural, gender-sensitive MuST-SHE corpus (Bentivogli et al., 2020) with two new linguistic annotation layers (POS and agreement chains), and explore to what extent different lexical categories and agreement phenomena are impacted by gender skews. Focusing on speech translation, we conduct a multifaceted evaluation on three language directions (English-French\/Italian\/Spanish), with models trained on varying amounts of data and different word segmentation techniques. By shedding light on model behaviours, gender bias, and its detection at several levels of granularity, our findings emphasize the value of dedicated analyses beyond aggregated overall results.","year":2022,"title_abstract":"Under the Morphosyntactic Lens: A Multifaceted Evaluation of Gender Bias in Speech Translation Gender bias is largely recognized as a problematic phenomenon affecting language technologies, with recent studies underscoring that it might surface differently across languages. However, most of current evaluation practices adopt a word-level focus on a narrow set of occupational nouns under synthetic conditions. Such protocols overlook key features of grammatical gender languages, which are characterized by morphosyntactic chains of gender agreement, marked on a variety of lexical items and parts-of-speech (POS). To overcome this limitation, we enrich the natural, gender-sensitive MuST-SHE corpus (Bentivogli et al., 2020) with two new linguistic annotation layers (POS and agreement chains), and explore to what extent different lexical categories and agreement phenomena are impacted by gender skews. Focusing on speech translation, we conduct a multifaceted evaluation on three language directions (English-French\/Italian\/Spanish), with models trained on varying amounts of data and different word segmentation techniques. By shedding light on model behaviours, gender bias, and its detection at several levels of granularity, our findings emphasize the value of dedicated analyses beyond aggregated overall results.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.271176666,"Goal":"Gender Equality","Task":["Multifaceted Evaluation of Gender Bias","Speech Translation","Gender bias","language technologies","speech translation","gender bias","detection"],"Method":["Morphosyntactic Lens","linguistic annotation layers","word segmentation techniques"]},{"ID":"levy-etal-2021-collecting-large","title":"Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation","abstract":"Recent works have found evidence of gender bias in models of machine translation and coreference resolution using mostly synthetic diagnostic datasets. While these quantify bias in a controlled experiment, they often do so on a small scale and consist mostly of artificial, out-of-distribution sentences. In this work, we find grammatical patterns indicating stereotypical and non-stereotypical gender-role assignments (e.g., female nurses versus male dancers) in corpora from three domains, resulting in a first large-scale gender bias dataset of 108K diverse real-world English sentences. We manually verify the quality of our corpus and use it to evaluate gender bias in various coreference resolution and machine translation models. We find that all tested models tend to over-rely on gender stereotypes when presented with natural inputs, which may be especially harmful when deployed in commercial systems. Finally, we show that our dataset lends itself to finetuning a coreference resolution model, finding it mitigates bias on a held out set. Our dataset and models are publicly available at github.com\/SLAB-NLP\/BUG. We hope they will spur future research into gender bias evaluation mitigation techniques in realistic settings.","year":2021,"title_abstract":"Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation Recent works have found evidence of gender bias in models of machine translation and coreference resolution using mostly synthetic diagnostic datasets. While these quantify bias in a controlled experiment, they often do so on a small scale and consist mostly of artificial, out-of-distribution sentences. In this work, we find grammatical patterns indicating stereotypical and non-stereotypical gender-role assignments (e.g., female nurses versus male dancers) in corpora from three domains, resulting in a first large-scale gender bias dataset of 108K diverse real-world English sentences. We manually verify the quality of our corpus and use it to evaluate gender bias in various coreference resolution and machine translation models. We find that all tested models tend to over-rely on gender stereotypes when presented with natural inputs, which may be especially harmful when deployed in commercial systems. Finally, we show that our dataset lends itself to finetuning a coreference resolution model, finding it mitigates bias on a held out set. Our dataset and models are publicly available at github.com\/SLAB-NLP\/BUG. We hope they will spur future research into gender bias evaluation mitigation techniques in realistic settings.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2708581686,"Goal":"Gender Equality","Task":["Coreference Resolution","Machine Translation","machine translation","coreference resolution"],"Method":["coreference resolution","machine translation models","coreference resolution model","gender bias evaluation mitigation techniques"]},{"ID":"zhu-etal-2021-dashboard","title":"A Dashboard for Mitigating the {COVID}-19 Misinfodemic","abstract":"This paper describes the current milestones achieved in our ongoing project that aims to understand the surveillance of, impact of and intervention on COVID-19 misinfodemic on Twitter. Specifically, it introduces a public dashboard which, in addition to displaying case counts in an interactive map and a navigational panel, also provides some unique features not found in other places. Particularly, the dashboard uses a curated catalog of COVID-19 related facts and debunks of misinformation, and it displays the most prevalent information from the catalog among Twitter users in user-selected U.S. geographic regions. The paper explains how to use BERT models to match tweets with the facts and misinformation and to detect their stance towards such information. The paper also discusses the results of preliminary experiments on analyzing the spatio-temporal spread of misinformation.","year":2021,"title_abstract":"A Dashboard for Mitigating the {COVID}-19 Misinfodemic This paper describes the current milestones achieved in our ongoing project that aims to understand the surveillance of, impact of and intervention on COVID-19 misinfodemic on Twitter. Specifically, it introduces a public dashboard which, in addition to displaying case counts in an interactive map and a navigational panel, also provides some unique features not found in other places. Particularly, the dashboard uses a curated catalog of COVID-19 related facts and debunks of misinformation, and it displays the most prevalent information from the catalog among Twitter users in user-selected U.S. geographic regions. The paper explains how to use BERT models to match tweets with the facts and misinformation and to detect their stance towards such information. The paper also discusses the results of preliminary experiments on analyzing the spatio-temporal spread of misinformation.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2686280012,"Goal":"Climate Action","Task":["{COVID} - 19","intervention","COVID - 19","spatio - temporal spread of misinformation"],"Method":["BERT models"]},{"ID":"ruiz-etal-2016-word","title":"More than Word Cooccurrence: Exploring Support and Opposition in International Climate Negotiations with Semantic Parsing","abstract":"Text analysis methods widely used in digital humanities often involve word co-occurrence, e.g. concept co-occurrence networks. These methods provide a useful corpus overview, but cannot determine the predicates that relate co-occurring concepts. Our goal was identifying propositions expressing the points supported or opposed by participants in international climate negotiations. Word co-occurrence methods were not sufficient, and an analysis based on open relation extraction had limited coverage for nominal predicates. We present a pipeline which identifies the points that different actors support and oppose, via a domain model with support\/opposition predicates, and analysis rules that exploit the output of semantic role labelling, syntactic dependencies and anaphora resolution. Entity linking and keyphrase extraction are also performed on the propositions related to each actor. A user interface allows examining the main concepts in points supported or opposed by each participant, which participants agree or disagree with each other, and about which issues. The system is an example of tools that digital humanities scholars are asking for, to render rich textual information (beyond word co-occurrence) more amenable to quantitative treatment. An evaluation of the tool was satisfactory.","year":2016,"title_abstract":"More than Word Cooccurrence: Exploring Support and Opposition in International Climate Negotiations with Semantic Parsing Text analysis methods widely used in digital humanities often involve word co-occurrence, e.g. concept co-occurrence networks. These methods provide a useful corpus overview, but cannot determine the predicates that relate co-occurring concepts. Our goal was identifying propositions expressing the points supported or opposed by participants in international climate negotiations. Word co-occurrence methods were not sufficient, and an analysis based on open relation extraction had limited coverage for nominal predicates. We present a pipeline which identifies the points that different actors support and oppose, via a domain model with support\/opposition predicates, and analysis rules that exploit the output of semantic role labelling, syntactic dependencies and anaphora resolution. Entity linking and keyphrase extraction are also performed on the propositions related to each actor. A user interface allows examining the main concepts in points supported or opposed by each participant, which participants agree or disagree with each other, and about which issues. The system is an example of tools that digital humanities scholars are asking for, to render rich textual information (beyond word co-occurrence) more amenable to quantitative treatment. An evaluation of the tool was satisfactory.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2684757113,"Goal":"Climate Action","Task":["Exploring Support and Opposition","International Climate Negotiations","digital humanities","semantic role labelling","Entity linking","keyphrase extraction","quantitative treatment"],"Method":["Word Cooccurrence","Semantic Parsing","Text analysis methods","concept co - occurrence networks","Word co - occurrence methods","open relation extraction","domain model","analysis rules","anaphora resolution"]},{"ID":"stanovsky-etal-2019-evaluating","title":"Evaluating Gender Bias in Machine Translation","abstract":"We present the first challenge set and evaluation protocol for the analysis of gender bias in machine translation (MT). Our approach uses two recent coreference resolution datasets composed of English sentences which cast participants into non-stereotypical gender roles (e.g., {``}The doctor asked the nurse to help her in the operation{''}). We devise an automatic gender bias evaluation method for eight target languages with grammatical gender, based on morphological analysis (e.g., the use of female inflection for the word {``}doctor{''}). Our analyses show that four popular industrial MT systems and two recent state-of-the-art academic MT models are significantly prone to gender-biased translation errors for all tested target languages. Our data and code are publicly available at https:\/\/github.com\/gabrielStanovsky\/mt{\\_}gender.","year":2019,"title_abstract":"Evaluating Gender Bias in Machine Translation We present the first challenge set and evaluation protocol for the analysis of gender bias in machine translation (MT). Our approach uses two recent coreference resolution datasets composed of English sentences which cast participants into non-stereotypical gender roles (e.g., {``}The doctor asked the nurse to help her in the operation{''}). We devise an automatic gender bias evaluation method for eight target languages with grammatical gender, based on morphological analysis (e.g., the use of female inflection for the word {``}doctor{''}). Our analyses show that four popular industrial MT systems and two recent state-of-the-art academic MT models are significantly prone to gender-biased translation errors for all tested target languages. Our data and code are publicly available at https:\/\/github.com\/gabrielStanovsky\/mt{\\_}gender.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2683019042,"Goal":"Gender Equality","Task":["Evaluating Gender Bias","Machine Translation","gender bias","machine translation","MT","MT","translation"],"Method":["automatic gender bias evaluation method","morphological analysis"]},{"ID":"gaido-etal-2020-breeding","title":"Breeding Gender-aware Direct Speech Translation Systems","abstract":"In automatic speech translation (ST), traditional cascade approaches involving separate transcription and translation steps are giving ground to increasingly competitive and more robust direct solutions. In particular, by translating speech audio data without intermediate transcription, direct ST models are able to leverage and preserve essential information present in the input (e.g.speaker{'}s vocal characteristics) that is otherwise lost in the cascade framework. Although such ability proved to be useful for gender translation, direct ST is nonetheless affected by gender bias just like its cascade counterpart, as well as machine translation and numerous other natural language processing applications. Moreover, direct ST systems that exclusively rely on vocal biometric features as a gender cue can be unsuitable or even potentially problematic for certain users. Going beyond speech signals, in this paper we compare different approaches to inform direct ST models about the speaker{'}s gender and test their ability to handle gender translation from English into Italian and French. To this aim, we manually annotated large datasets with speak-ers{'} gender information and used them for experiments reflecting different possible real-world scenarios. Our results show that gender-aware direct ST solutions can significantly outperform strong {--} but gender-unaware {--} direct ST models. In particular, the translation of gender-marked words can increase up to 30 points in accuracy while preserving overall translation quality.","year":2020,"title_abstract":"Breeding Gender-aware Direct Speech Translation Systems In automatic speech translation (ST), traditional cascade approaches involving separate transcription and translation steps are giving ground to increasingly competitive and more robust direct solutions. In particular, by translating speech audio data without intermediate transcription, direct ST models are able to leverage and preserve essential information present in the input (e.g.speaker{'}s vocal characteristics) that is otherwise lost in the cascade framework. Although such ability proved to be useful for gender translation, direct ST is nonetheless affected by gender bias just like its cascade counterpart, as well as machine translation and numerous other natural language processing applications. Moreover, direct ST systems that exclusively rely on vocal biometric features as a gender cue can be unsuitable or even potentially problematic for certain users. Going beyond speech signals, in this paper we compare different approaches to inform direct ST models about the speaker{'}s gender and test their ability to handle gender translation from English into Italian and French. To this aim, we manually annotated large datasets with speak-ers{'} gender information and used them for experiments reflecting different possible real-world scenarios. Our results show that gender-aware direct ST solutions can significantly outperform strong {--} but gender-unaware {--} direct ST models. In particular, the translation of gender-marked words can increase up to 30 points in accuracy while preserving overall translation quality.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2680515349,"Goal":"Gender Equality","Task":["Gender - aware Direct Speech Translation Systems","automatic speech translation","gender translation","direct ST","machine translation","natural language processing applications","ST","gender translation","translation"],"Method":["cascade approaches","transcription","translation steps","intermediate transcription","ST","cascade framework","cascade counterpart","direct ST models","gender - aware direct ST solutions","direct ST models"]},{"ID":"van-der-goot-etal-2018-bleaching","title":"Bleaching Text: Abstract Features for Cross-lingual Gender Prediction","abstract":"Gender prediction has typically focused on lexical and social network features, yielding good performance, but making systems highly language-, topic-, and platform dependent. Cross-lingual embeddings circumvent some of these limitations, but capture gender-specific style less. We propose an alternative: bleaching text, i.e., transforming lexical strings into more abstract features. This study provides evidence that such features allow for better transfer across languages. Moreover, we present a first study on the ability of humans to perform cross-lingual gender prediction. We find that human predictive power proves similar to that of our bleached models, and both perform better than lexical models.","year":2018,"title_abstract":"Bleaching Text: Abstract Features for Cross-lingual Gender Prediction Gender prediction has typically focused on lexical and social network features, yielding good performance, but making systems highly language-, topic-, and platform dependent. Cross-lingual embeddings circumvent some of these limitations, but capture gender-specific style less. We propose an alternative: bleaching text, i.e., transforming lexical strings into more abstract features. This study provides evidence that such features allow for better transfer across languages. Moreover, we present a first study on the ability of humans to perform cross-lingual gender prediction. We find that human predictive power proves similar to that of our bleached models, and both perform better than lexical models.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2679873705,"Goal":"Gender Equality","Task":["Cross - lingual Gender Prediction","Gender prediction","cross - lingual gender prediction"],"Method":["Cross - lingual embeddings","bleached models","lexical models"]},{"ID":"rico-etal-2020-inmigra3","title":"{INMIGRA}3: building a case for {NGO}s and {NMT}","abstract":"INMIGRA3 is a three-year project that builds on the work of two previous initi-atives: INMIGRA2-CM and CRISIS-MT . Together, they address the specific needs of NGOs in multilingual settings with a particular interest in migratory contexts. Work on INMIGRA3 concentrates in the analysis of how best can be NMT put to use for the purposes of translating NGOs documentation.","year":2020,"title_abstract":"{INMIGRA}3: building a case for {NGO}s and {NMT} INMIGRA3 is a three-year project that builds on the work of two previous initi-atives: INMIGRA2-CM and CRISIS-MT . Together, they address the specific needs of NGOs in multilingual settings with a particular interest in migratory contexts. Work on INMIGRA3 concentrates in the analysis of how best can be NMT put to use for the purposes of translating NGOs documentation.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.2676724792,"Goal":"Peace, Justice and Strong Institutions","Task":["multilingual settings","translating NGOs documentation"],"Method":["INMIGRA2 - CM","CRISIS - MT"]},{"ID":"gupta-etal-2022-iit","title":"{IIT} Dhanbad @{LT}-{EDI}-{ACL}2022- Hope Speech Detection for Equality, Diversity, and Inclusion","abstract":"Hope is considered significant for the wellbeing,recuperation and restoration of humanlife by health professionals. Hope speech reflectsthe belief that one can discover pathwaysto their desired objectives and become rousedto utilise those pathways. Hope speech offerssupport, reassurance, suggestions, inspirationand insight. Hate speech is a prevalent practicethat society has to struggle with everyday.The freedom of speech and ease of anonymitygranted by social media has also resulted inincitement to hatred. In this paper, we workto identify and promote positive and supportivecontent on these platforms. We work withseveral machine learning models to classify socialmedia comments as hope speech or nonhopespeech in English. This paper portraysour work for the Shared Task on Hope SpeechDetection for Equality, Diversity, and Inclusionat LT-EDI-ACL 2022.","year":2022,"title_abstract":"{IIT} Dhanbad @{LT}-{EDI}-{ACL}2022- Hope Speech Detection for Equality, Diversity, and Inclusion Hope is considered significant for the wellbeing,recuperation and restoration of humanlife by health professionals. Hope speech reflectsthe belief that one can discover pathwaysto their desired objectives and become rousedto utilise those pathways. Hope speech offerssupport, reassurance, suggestions, inspirationand insight. Hate speech is a prevalent practicethat society has to struggle with everyday.The freedom of speech and ease of anonymitygranted by social media has also resulted inincitement to hatred. In this paper, we workto identify and promote positive and supportivecontent on these platforms. We work withseveral machine learning models to classify socialmedia comments as hope speech or nonhopespeech in English. This paper portraysour work for the Shared Task on Hope SpeechDetection for Equality, Diversity, and Inclusionat LT-EDI-ACL 2022.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2664173841,"Goal":"Gender Equality","Task":["Hope Speech Detection","Equality","Hope","recuperation","restoration of humanlife","Hope SpeechDetection","Inclusionat LT - EDI - ACL"],"Method":["machine learning models"]},{"ID":"spyns-etal-2008-dutch","title":"The {D}utch-{F}lemish Comprehensive Approach to {HLT} Stimulation and Innovation: {STEVIN}, {HLT} Agency and beyond","abstract":"This paper shows how a research and industry stimulation programme on human language technologies (HLT) for Dutch can be \u0093enhanced\u0094 with more specific innovation policy aspects to support the take-up by the HLT industry in the Netherlands and Flanders. Important to note is the distinction between the HLT programme itself (called STEVIN) with its specific related committees and actions and the overall policy instruments (HLT Agency, HLT steering board?) that try to span the entire domain of HLT for Dutch and have a more permanent character. The establishment of a pricing committee and a PR {\\&} communication working group is explained as a consequence of adopting the notion of \u0093innovation system\u0094 as a theoretical framework. It means that a stronger emphasis is put on improving knowledge transfer and exchange amongst actors in the field. Therefore, the focus at the programme management level is shifting from the projects\u0092 research activities producing results to gathering the results, making them available at a certain cost and advertising them through the appropriate channels to the appropriate potential customers. Our conclusion is that this policy stimulates the transfer from academia to industry though it is too soon for an in-depth assessment of the STEVIN programme and other HLT innovation policy instruments.","year":2008,"title_abstract":"The {D}utch-{F}lemish Comprehensive Approach to {HLT} Stimulation and Innovation: {STEVIN}, {HLT} Agency and beyond This paper shows how a research and industry stimulation programme on human language technologies (HLT) for Dutch can be \u0093enhanced\u0094 with more specific innovation policy aspects to support the take-up by the HLT industry in the Netherlands and Flanders. Important to note is the distinction between the HLT programme itself (called STEVIN) with its specific related committees and actions and the overall policy instruments (HLT Agency, HLT steering board?) that try to span the entire domain of HLT for Dutch and have a more permanent character. The establishment of a pricing committee and a PR {\\&} communication working group is explained as a consequence of adopting the notion of \u0093innovation system\u0094 as a theoretical framework. It means that a stronger emphasis is put on improving knowledge transfer and exchange amongst actors in the field. Therefore, the focus at the programme management level is shifting from the projects\u0092 research activities producing results to gathering the results, making them available at a certain cost and advertising them through the appropriate channels to the appropriate potential customers. Our conclusion is that this policy stimulates the transfer from academia to industry though it is too soon for an in-depth assessment of the STEVIN programme and other HLT innovation policy instruments.","social_need":"Industry, Innovation and Infrastrucure Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation","cosine_similarity":0.2663098872,"Goal":"Industry, Innovation and Infrastrucure","Task":["Innovation","research and industry stimulation programme","human language technologies","HLT industry","HLT","knowledge transfer"],"Method":["HLT programme","STEVIN)","HLT","pricing committee","PR","\u0093innovation system\u0094","theoretical framework","STEVIN programme","HLT innovation policy instruments"]},{"ID":"ionita-etal-2019-resolving","title":"Resolving Gendered Ambiguous Pronouns with {BERT}","abstract":"Pronoun resolution is part of coreference resolution, the task of pairing an expression to its referring entity. This is an important task for natural language understanding and a necessary component of machine translation systems, chat bots and assistants. Neural machine learning systems perform far from ideally in this task, reaching as low as 73{\\%} F1 scores on modern benchmark datasets. Moreover, they tend to perform better for masculine pronouns than for feminine ones. Thus, the problem is both challenging and important for NLP researchers and practitioners. In this project, we describe our BERT-based approach to solving the problem of gender-balanced pronoun resolution. We are able to reach 92{\\%} F1 score and a much lower gender bias on the benchmark dataset shared by Google AI Language team.","year":2019,"title_abstract":"Resolving Gendered Ambiguous Pronouns with {BERT} Pronoun resolution is part of coreference resolution, the task of pairing an expression to its referring entity. This is an important task for natural language understanding and a necessary component of machine translation systems, chat bots and assistants. Neural machine learning systems perform far from ideally in this task, reaching as low as 73{\\%} F1 scores on modern benchmark datasets. Moreover, they tend to perform better for masculine pronouns than for feminine ones. Thus, the problem is both challenging and important for NLP researchers and practitioners. In this project, we describe our BERT-based approach to solving the problem of gender-balanced pronoun resolution. We are able to reach 92{\\%} F1 score and a much lower gender bias on the benchmark dataset shared by Google AI Language team.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2654666305,"Goal":"Gender Equality","Task":["Resolving Gendered Ambiguous Pronouns","Pronoun resolution","coreference resolution","natural language understanding","machine translation systems","chat bots","assistants","NLP researchers","gender - balanced pronoun resolution"],"Method":["Neural machine learning systems","BERT - based approach"]},{"ID":"bethard-etal-2019-inferring","title":"Inferring missing metadata from environmental policy texts","abstract":"The National Environmental Policy Act (NEPA) provides a trove of data on how environmental policy decisions have been made in the United States over the last 50 years. Unfortunately, there is no central database for this information and it is too voluminous to assess manually. We describe our efforts to enable systematic research over US environmental policy by extracting and organizing metadata from the text of NEPA documents. Our contributions include collecting more than 40,000 NEPA-related documents, and evaluating rule-based baselines that establish the difficulty of three important tasks: identifying lead agencies, aligning document versions, and detecting reused text.","year":2019,"title_abstract":"Inferring missing metadata from environmental policy texts The National Environmental Policy Act (NEPA) provides a trove of data on how environmental policy decisions have been made in the United States over the last 50 years. Unfortunately, there is no central database for this information and it is too voluminous to assess manually. We describe our efforts to enable systematic research over US environmental policy by extracting and organizing metadata from the text of NEPA documents. Our contributions include collecting more than 40,000 NEPA-related documents, and evaluating rule-based baselines that establish the difficulty of three important tasks: identifying lead agencies, aligning document versions, and detecting reused text.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2653891146,"Goal":"Climate Action","Task":["Inferring missing metadata","environmental policy decisions","US environmental policy","organizing metadata"],"Method":["rule - based baselines"]},{"ID":"subramanian-etal-2019-target","title":"Target Based Speech Act Classification in Political Campaign Text","abstract":"We study pragmatics in political campaign text, through analysis of speech acts and the target of each utterance. We propose a new annotation schema incorporating domain-specific speech acts, such as commissive-action, and present a novel annotated corpus of media releases and speech transcripts from the 2016 Australian election cycle. We show how speech acts and target referents can be modeled as sequential classification, and evaluate several techniques, exploiting contextualized word representations, semi-supervised learning, task dependencies and speaker meta-data.","year":2019,"title_abstract":"Target Based Speech Act Classification in Political Campaign Text We study pragmatics in political campaign text, through analysis of speech acts and the target of each utterance. We propose a new annotation schema incorporating domain-specific speech acts, such as commissive-action, and present a novel annotated corpus of media releases and speech transcripts from the 2016 Australian election cycle. We show how speech acts and target referents can be modeled as sequential classification, and evaluate several techniques, exploiting contextualized word representations, semi-supervised learning, task dependencies and speaker meta-data.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2651677132,"Goal":"Climate Action","Task":["Target Based Speech Act Classification","analysis of speech acts","sequential classification"],"Method":["annotation schema","contextualized word representations","semi - supervised learning"]},{"ID":"yeo-chen-2020-defining","title":"Defining and Evaluating Fair Natural Language Generation","abstract":"Our work focuses on the biases that emerge in the natural language generation (NLG) task of sentence completion. In this paper, we introduce a mathematical framework of fairness for NLG followed by an evaluation of gender biases in two state-of-the-art language models. Our analysis provides a theoretical formulation for biases in NLG and empirical evidence that existing language generation models embed gender bias.","year":2020,"title_abstract":"Defining and Evaluating Fair Natural Language Generation Our work focuses on the biases that emerge in the natural language generation (NLG) task of sentence completion. In this paper, we introduce a mathematical framework of fairness for NLG followed by an evaluation of gender biases in two state-of-the-art language models. Our analysis provides a theoretical formulation for biases in NLG and empirical evidence that existing language generation models embed gender bias.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2649067342,"Goal":"Gender Equality","Task":["Fair Natural Language Generation","natural language generation (NLG) task of sentence completion","fairness","NLG","biases","NLG"],"Method":["language models","language generation models"]},{"ID":"wang-etal-2020-public","title":"Public Sentiment on Governmental {COVID}-19 Measures in {D}utch Social Media","abstract":"Public sentiment (the opinion, attitude or feeling that the public expresses) is a factor of interest for government, as it directly influences the implementation of policies. Given the unprecedented nature of the COVID-19 crisis, having an up-to-date representation of public sentiment on governmental measures and announcements is crucial. In this paper, we analyse Dutch public sentiment on governmental COVID-19 measures from text data collected across three online media sources (Twitter, Reddit and Nu.nl) from February to September 2020. We apply sentiment analysis methods to analyse polarity over time, as well as to identify stance towards two specific pandemic policies regarding social distancing and wearing face masks. The presented preliminary results provide valuable insights into the narratives shown in vast social media text data, which help understand the influence of COVID-19 measures on the general public.","year":2020,"title_abstract":"Public Sentiment on Governmental {COVID}-19 Measures in {D}utch Social Media Public sentiment (the opinion, attitude or feeling that the public expresses) is a factor of interest for government, as it directly influences the implementation of policies. Given the unprecedented nature of the COVID-19 crisis, having an up-to-date representation of public sentiment on governmental measures and announcements is crucial. In this paper, we analyse Dutch public sentiment on governmental COVID-19 measures from text data collected across three online media sources (Twitter, Reddit and Nu.nl) from February to September 2020. We apply sentiment analysis methods to analyse polarity over time, as well as to identify stance towards two specific pandemic policies regarding social distancing and wearing face masks. The presented preliminary results provide valuable insights into the narratives shown in vast social media text data, which help understand the influence of COVID-19 measures on the general public.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2648079395,"Goal":"Climate Action","Task":["governmental measures","pandemic policies","social distancing"],"Method":["sentiment analysis methods"]},{"ID":"min-etal-2021-excavatorcovid","title":"{E}xcavator{C}ovid: Extracting Events and Relations from Text Corpora for Temporal and Causal Analysis for {COVID}-19","abstract":"Timely responses from policy makers to mitigate the impact of the COVID-19 pandemic rely on a comprehensive grasp of events, their causes, and their impacts. These events are reported at such a speed and scale as to be overwhelming. In this paper, we present ExcavatorCovid, a machine reading system that ingests open-source text documents (e.g., news and scientific publications), extracts COVID-19 related events and relations between them, and builds a Temporal and Causal Analysis Graph (TCAG). Excavator will help government agencies alleviate the information overload, understand likely downstream effects of political and economic decisions and events related to the pandemic, and respond in a timely manner to mitigate the impact of COVID-19. We expect the utility of Excavator to outlive the COVID-19 pandemic: analysts and decision makers will be empowered by Excavator to better understand and solve complex problems in the future. A demonstration video is available at https:\/\/vimeo.com\/528619007.","year":2021,"title_abstract":"{E}xcavator{C}ovid: Extracting Events and Relations from Text Corpora for Temporal and Causal Analysis for {COVID}-19 Timely responses from policy makers to mitigate the impact of the COVID-19 pandemic rely on a comprehensive grasp of events, their causes, and their impacts. These events are reported at such a speed and scale as to be overwhelming. In this paper, we present ExcavatorCovid, a machine reading system that ingests open-source text documents (e.g., news and scientific publications), extracts COVID-19 related events and relations between them, and builds a Temporal and Causal Analysis Graph (TCAG). Excavator will help government agencies alleviate the information overload, understand likely downstream effects of political and economic decisions and events related to the pandemic, and respond in a timely manner to mitigate the impact of COVID-19. We expect the utility of Excavator to outlive the COVID-19 pandemic: analysts and decision makers will be empowered by Excavator to better understand and solve complex problems in the future. A demonstration video is available at https:\/\/vimeo.com\/528619007.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2645140588,"Goal":"Climate Action","Task":["Extracting Events and Relations","Temporal and Causal Analysis","COVID - 19 pandemic","COVID - 19 pandemic","decision makers"],"Method":["ExcavatorCovid","machine reading system","Temporal and Causal Analysis Graph","Excavator","Excavator","Excavator"]},{"ID":"bartl-leavy-2022-inferring","title":"Inferring Gender: A Scalable Methodology for Gender Detection with Online Lexical Databases","abstract":"This paper presents a new method for automatic detection of gendered terms in large-scale language datasets. Currently, the evaluation of gender bias in natural language processing relies on the use of manually compiled lexicons of gendered expressions, such as pronouns and words that imply gender. However, manual compilation of lists with lexical gender can lead to static information if lists are not periodically updated and often involve value judgements by individual annotators and researchers. Moreover, terms not included in the lexicons fall out of the range of analysis.To address these issues, we devised a scalable dictionary-based method to automatically detect lexical gender that can provide a dynamic, up-to-date analysis with high coverage. Our approach reaches over 80{\\%} accuracy in determining the lexical gender of words retrieved randomly from a Wikipedia sample and when testing on a list of gendered words used in previous research.","year":2022,"title_abstract":"Inferring Gender: A Scalable Methodology for Gender Detection with Online Lexical Databases This paper presents a new method for automatic detection of gendered terms in large-scale language datasets. Currently, the evaluation of gender bias in natural language processing relies on the use of manually compiled lexicons of gendered expressions, such as pronouns and words that imply gender. However, manual compilation of lists with lexical gender can lead to static information if lists are not periodically updated and often involve value judgements by individual annotators and researchers. Moreover, terms not included in the lexicons fall out of the range of analysis.To address these issues, we devised a scalable dictionary-based method to automatically detect lexical gender that can provide a dynamic, up-to-date analysis with high coverage. Our approach reaches over 80{\\%} accuracy in determining the lexical gender of words retrieved randomly from a Wikipedia sample and when testing on a list of gendered words used in previous research.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2644155622,"Goal":"Gender Equality","Task":["Inferring Gender","Gender Detection","automatic detection of gendered terms","gender bias","natural language processing","manual compilation of lists","lexical gender","dynamic , up - to - date analysis"],"Method":["Scalable Methodology","dictionary - based method"]},{"ID":"raby-1998-system","title":"System demonstration: {SYSTRAN} {E}nterprise","abstract":"SYSTRAN\u00ae Enterprise responds to the demands of today{'}s fast paced international business environment and is tailored for use on an intranet, extranet or LAN.","year":1998,"title_abstract":"System demonstration: {SYSTRAN} {E}nterprise SYSTRAN\u00ae Enterprise responds to the demands of today{'}s fast paced international business environment and is tailored for use on an intranet, extranet or LAN.","social_need":"Industry, Innovation and Infrastrucure Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation","cosine_similarity":0.2639932036,"Goal":"Industry, Innovation and Infrastrucure","Task":["System demonstration"],"Method":["SYSTRAN\u00ae Enterprise"]},{"ID":"wisniewski-etal-2021-screening","title":"Screening Gender Transfer in Neural Machine Translation","abstract":"This paper aims at identifying the information flow in state-of-the-art machine translation systems, taking as example the transfer of gender when translating from French into English. Using a controlled set of examples, we experiment several ways to investigate how gender information circulates in a encoder-decoder architecture considering both probing techniques as well as interventions on the internal representations used in the MT system. Our results show that gender information can be found in all token representations built by the encoder and the decoder and lead us to conclude that there are multiple pathways for gender transfer.","year":2021,"title_abstract":"Screening Gender Transfer in Neural Machine Translation This paper aims at identifying the information flow in state-of-the-art machine translation systems, taking as example the transfer of gender when translating from French into English. Using a controlled set of examples, we experiment several ways to investigate how gender information circulates in a encoder-decoder architecture considering both probing techniques as well as interventions on the internal representations used in the MT system. Our results show that gender information can be found in all token representations built by the encoder and the decoder and lead us to conclude that there are multiple pathways for gender transfer.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2635755241,"Goal":"Gender Equality","Task":["Gender Transfer","Neural Machine Translation","machine translation","transfer of gender","MT","gender transfer"],"Method":["encoder - decoder architecture","probing techniques","internal representations","token representations","encoder","decoder"]},{"ID":"grypari-etal-2020-research","title":"Research {\\&} Innovation Activities{'} Impact Assessment: The {D}ata4{I}mpact System","abstract":"Cat. 2 Show-case: We present the Data4Impact (D4I) platform, a novel end-to-end system for evidence-based, timely and accurate monitoring and evaluation of research and innovation (R{\\&}I) activities. Using the latest technological advances in Human Language Technology (HLT) and our data-driven methodology, we build a novel set of indicators in order to track funded projects and their impact on science, the economy and the society as a whole, during and after the project life-cycle. We develop our methodology by targeting Health-related EC projects from 2007 to 2019 to produce solutions that meet the needs of stakeholders (mainly policy-makers and research funders). Various D4I text analytics workflows process datasets and their metadata, extract valuable insights and estimate intermediate results and metrics, culminating in a set of robust indicators that the users can interact with through our dashboard, the D4I Monitor (available at monitor.data4impact.eu). Therefore, our approach, which can be generalized to different contexts, is multidimensional (technology, tools, indicators, dashboard) and the resulting system can provide an innovative solution for public administrators in their policy-making needs related to RDI funding allocation.","year":2020,"title_abstract":"Research {\\&} Innovation Activities{'} Impact Assessment: The {D}ata4{I}mpact System Cat. 2 Show-case: We present the Data4Impact (D4I) platform, a novel end-to-end system for evidence-based, timely and accurate monitoring and evaluation of research and innovation (R{\\&}I) activities. Using the latest technological advances in Human Language Technology (HLT) and our data-driven methodology, we build a novel set of indicators in order to track funded projects and their impact on science, the economy and the society as a whole, during and after the project life-cycle. We develop our methodology by targeting Health-related EC projects from 2007 to 2019 to produce solutions that meet the needs of stakeholders (mainly policy-makers and research funders). Various D4I text analytics workflows process datasets and their metadata, extract valuable insights and estimate intermediate results and metrics, culminating in a set of robust indicators that the users can interact with through our dashboard, the D4I Monitor (available at monitor.data4impact.eu). Therefore, our approach, which can be generalized to different contexts, is multidimensional (technology, tools, indicators, dashboard) and the resulting system can provide an innovative solution for public administrators in their policy-making needs related to RDI funding allocation.","social_need":"Industry, Innovation and Infrastrucure Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation","cosine_similarity":0.2632150948,"Goal":"Industry, Innovation and Infrastrucure","Task":["Impact Assessment","evidence - based , timely and accurate monitoring and evaluation of research and innovation (R{\\&}I) activities","policy - making needs","RDI funding allocation"],"Method":["{D}ata4{I}mpact System","Data4Impact (D4I) platform","end system","Human Language Technology","data - driven methodology","dashboard","D4I Monitor"]},{"ID":"chaloner-maldonado-2019-measuring","title":"Measuring Gender Bias in Word Embeddings across Domains and Discovering New Gender Bias Word Categories","abstract":"Prior work has shown that word embeddings capture human stereotypes, including gender bias. However, there is a lack of studies testing the presence of specific gender bias categories in word embeddings across diverse domains. This paper aims to fill this gap by applying the WEAT bias detection method to four sets of word embeddings trained on corpora from four different domains: news, social networking, biomedical and a gender-balanced corpus extracted from Wikipedia (GAP). We find that some domains are definitely more prone to gender bias than others, and that the categories of gender bias present also vary for each set of word embeddings. We detect some gender bias in GAP. We also propose a simple but novel method for discovering new bias categories by clustering word embeddings. We validate this method through WEAT{'}s hypothesis testing mechanism and find it useful for expanding the relatively small set of well-known gender bias word categories commonly used in the literature.","year":2019,"title_abstract":"Measuring Gender Bias in Word Embeddings across Domains and Discovering New Gender Bias Word Categories Prior work has shown that word embeddings capture human stereotypes, including gender bias. However, there is a lack of studies testing the presence of specific gender bias categories in word embeddings across diverse domains. This paper aims to fill this gap by applying the WEAT bias detection method to four sets of word embeddings trained on corpora from four different domains: news, social networking, biomedical and a gender-balanced corpus extracted from Wikipedia (GAP). We find that some domains are definitely more prone to gender bias than others, and that the categories of gender bias present also vary for each set of word embeddings. We detect some gender bias in GAP. We also propose a simple but novel method for discovering new bias categories by clustering word embeddings. We validate this method through WEAT{'}s hypothesis testing mechanism and find it useful for expanding the relatively small set of well-known gender bias word categories commonly used in the literature.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2630993724,"Goal":"Gender Equality","Task":["Measuring Gender Bias","Word Embeddings","discovering new bias categories","clustering word embeddings"],"Method":["word embeddings","WEAT bias detection method","WEAT{'}s hypothesis testing mechanism"]},{"ID":"gonen-goldberg-2019-lipstick","title":"Lipstick on a Pig: {D}ebiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them","abstract":"Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between {``}gender-neutralized{''} words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.","year":2019,"title_abstract":"Lipstick on a Pig: {D}ebiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between {``}gender-neutralized{''} words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2628781199,"Goal":"Gender Equality","Task":["NLP","gender - neutral modeling"],"Method":["{D}ebiasing Methods","Word embeddings","word embedding models","debiasing methods","bias removal techniques"]},{"ID":"basta-etal-2019-evaluating","title":"Evaluating the Underlying Gender Bias in Contextualized Word Embeddings","abstract":"Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.","year":2019,"title_abstract":"Evaluating the Underlying Gender Bias in Contextualized Word Embeddings Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2625030875,"Goal":"Gender Equality","Task":["Contextualized Word Embeddings","Gender bias","natural language processing applications","word embedding computation"],"Method":["Word embeddings","contextualized word embeddings","word embedding techniques","word vector representations","contextualized word embeddings"]},{"ID":"ramesh-etal-2021-evaluating","title":"Evaluating Gender Bias in {H}indi-{E}nglish Machine Translation","abstract":"With language models being deployed increasingly in the real world, it is essential to address the issue of the fairness of their outputs. The word embedding representations of these language models often implicitly draw unwanted associations that form a social bias within the model. The nature of gendered languages like Hindi, poses an additional problem to the quantification and mitigation of bias, owing to the change in the form of the words in the sentence, based on the gender of the subject. Additionally, there is sparse work done in the realm of measuring and debiasing systems for Indic languages. In our work, we attempt to evaluate and quantify the gender bias within a Hindi-English machine translation system. We implement a modified version of the existing TGBI metric based on the grammatical considerations for Hindi. We also compare and contrast the resulting bias measurements across multiple metrics for pre-trained embeddings and the ones learned by our machine translation model.","year":2021,"title_abstract":"Evaluating Gender Bias in {H}indi-{E}nglish Machine Translation With language models being deployed increasingly in the real world, it is essential to address the issue of the fairness of their outputs. The word embedding representations of these language models often implicitly draw unwanted associations that form a social bias within the model. The nature of gendered languages like Hindi, poses an additional problem to the quantification and mitigation of bias, owing to the change in the form of the words in the sentence, based on the gender of the subject. Additionally, there is sparse work done in the realm of measuring and debiasing systems for Indic languages. In our work, we attempt to evaluate and quantify the gender bias within a Hindi-English machine translation system. We implement a modified version of the existing TGBI metric based on the grammatical considerations for Hindi. We also compare and contrast the resulting bias measurements across multiple metrics for pre-trained embeddings and the ones learned by our machine translation model.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2624799609,"Goal":"Gender Equality","Task":["Evaluating Gender Bias","quantification and mitigation of bias","measuring and debiasing systems","Indic languages","machine translation system"],"Method":["language models","word embedding representations","language models","TGBI metric","machine translation model"]},{"ID":"gonen-etal-2022-analyzing","title":"Analyzing Gender Representation in Multilingual Models","abstract":"Multilingual language models were shown to allow for nontrivial transfer across scripts and languages. In this work, we study the structure of the internal representations that enable this transfer. We focus on the representations of gender distinctions as a practical case study, and examine the extent to which the gender concept is encoded in shared subspaces across different languages. Our analysis shows that gender representations consist of several prominent components that are shared across languages, alongside language-specific components. The existence of language-independent and language-specific components provides an explanation for an intriguing empirical observation we make{''}:'' while gender classification transfers well across languages, interventions for gender removal trained on a single language do not transfer easily to others.","year":2022,"title_abstract":"Analyzing Gender Representation in Multilingual Models Multilingual language models were shown to allow for nontrivial transfer across scripts and languages. In this work, we study the structure of the internal representations that enable this transfer. We focus on the representations of gender distinctions as a practical case study, and examine the extent to which the gender concept is encoded in shared subspaces across different languages. Our analysis shows that gender representations consist of several prominent components that are shared across languages, alongside language-specific components. The existence of language-independent and language-specific components provides an explanation for an intriguing empirical observation we make{''}:'' while gender classification transfers well across languages, interventions for gender removal trained on a single language do not transfer easily to others.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2624497712,"Goal":"Gender Equality","Task":["Analyzing Gender Representation","representations of gender distinctions","gender classification","gender removal"],"Method":["Multilingual Models","Multilingual language models","internal representations","gender representations","language - specific components"]},{"ID":"bartie-etal-2016-real","title":"The {REAL} Corpus: A Crowd-Sourced Corpus of Human Generated and Evaluated Spatial References to Real-World Urban Scenes","abstract":"Our interest is in people{'}s capacity to efficiently and effectively describe geographic objects in urban scenes. The broader ambition is to develop spatial models capable of equivalent functionality able to construct such referring expressions. To that end we present a newly crowd-sourced data set of natural language references to objects anchored in complex urban scenes (In short: The REAL Corpus \u2015 Referring Expressions Anchored Language). The REAL corpus contains a collection of images of real-world urban scenes together with verbal descriptions of target objects generated by humans, paired with data on how successful other people were able to identify the same object based on these descriptions. In total, the corpus contains 32 images with on average 27 descriptions per image and 3 verifications for each description. In addition, the corpus is annotated with a variety of linguistically motivated features. The paper highlights issues posed by collecting data using crowd-sourcing with an unrestricted input format, as well as using real-world urban scenes.","year":2016,"title_abstract":"The {REAL} Corpus: A Crowd-Sourced Corpus of Human Generated and Evaluated Spatial References to Real-World Urban Scenes Our interest is in people{'}s capacity to efficiently and effectively describe geographic objects in urban scenes. The broader ambition is to develop spatial models capable of equivalent functionality able to construct such referring expressions. To that end we present a newly crowd-sourced data set of natural language references to objects anchored in complex urban scenes (In short: The REAL Corpus \u2015 Referring Expressions Anchored Language). The REAL corpus contains a collection of images of real-world urban scenes together with verbal descriptions of target objects generated by humans, paired with data on how successful other people were able to identify the same object based on these descriptions. In total, the corpus contains 32 images with on average 27 descriptions per image and 3 verifications for each description. In addition, the corpus is annotated with a variety of linguistically motivated features. The paper highlights issues posed by collecting data using crowd-sourcing with an unrestricted input format, as well as using real-world urban scenes.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2622230351,"Goal":"Sustainable Cities and Communities","Task":["geographic objects in urban scenes"],"Method":["spatial models","crowd - sourcing"]},{"ID":"vargas-cotterell-2020-exploring","title":"Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation","abstract":"Bolukbasi et al. (2016) presents one of the first gender bias mitigation techniques for word embeddings. Their method takes pre-trained word embeddings as input and attempts to isolate a linear subspace that captures most of the gender bias in the embeddings. As judged by an analogical evaluation task, their method virtually eliminates gender bias in the embeddings. However, an implicit and untested assumption of their method is that the bias subspace is actually linear. In this work, we generalize their method to a kernelized, non-linear version. We take inspiration from kernel principal component analysis and derive a non-linear bias isolation technique. We discuss and overcome some of the practical drawbacks of our method for non-linear gender bias mitigation in word embeddings and analyze empirically whether the bias subspace is actually linear. Our analysis shows that gender bias is in fact well captured by a linear subspace, justifying the assumption of Bolukbasi et al. (2016).","year":2020,"title_abstract":"Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation Bolukbasi et al. (2016) presents one of the first gender bias mitigation techniques for word embeddings. Their method takes pre-trained word embeddings as input and attempts to isolate a linear subspace that captures most of the gender bias in the embeddings. As judged by an analogical evaluation task, their method virtually eliminates gender bias in the embeddings. However, an implicit and untested assumption of their method is that the bias subspace is actually linear. In this work, we generalize their method to a kernelized, non-linear version. We take inspiration from kernel principal component analysis and derive a non-linear bias isolation technique. We discuss and overcome some of the practical drawbacks of our method for non-linear gender bias mitigation in word embeddings and analyze empirically whether the bias subspace is actually linear. Our analysis shows that gender bias is in fact well captured by a linear subspace, justifying the assumption of Bolukbasi et al. (2016).","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2616049945,"Goal":"Gender Equality","Task":["Gender Bias Mitigation","word embeddings","analogical evaluation task","non - linear gender bias mitigation","word embeddings"],"Method":["Linear Subspace Hypothesis","gender bias mitigation techniques","linear subspace","kernelized , non - linear version","kernel principal component analysis","non - linear bias isolation technique","linear subspace"]},{"ID":"kocmi-etal-2020-gender","title":"Gender Coreference and Bias Evaluation at {WMT} 2020","abstract":"Gender bias in machine translation can manifest when choosing gender inflections based on spurious gender correlations. For example, always translating doctors as men and nurses as women. This can be particularly harmful as models become more popular and deployed within commercial systems. Our work presents the largest evidence for the phenomenon in more than 19 systems submitted to the WMT over four diverse target languages: Czech, German, Polish, and Russian. To achieve this, we use WinoMT, a recent automatic test suite which examines gender coreference and bias when translating from English to languages with grammatical gender. We extend WinoMT to handle two new languages tested in WMT: Polish and Czech. We find that all systems consistently use spurious correlations in the data rather than meaningful contextual information.","year":2020,"title_abstract":"Gender Coreference and Bias Evaluation at {WMT} 2020 Gender bias in machine translation can manifest when choosing gender inflections based on spurious gender correlations. For example, always translating doctors as men and nurses as women. This can be particularly harmful as models become more popular and deployed within commercial systems. Our work presents the largest evidence for the phenomenon in more than 19 systems submitted to the WMT over four diverse target languages: Czech, German, Polish, and Russian. To achieve this, we use WinoMT, a recent automatic test suite which examines gender coreference and bias when translating from English to languages with grammatical gender. We extend WinoMT to handle two new languages tested in WMT: Polish and Czech. We find that all systems consistently use spurious correlations in the data rather than meaningful contextual information.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2605246902,"Goal":"Gender Equality","Task":["Gender Coreference","Bias Evaluation","Gender bias","machine translation","gender coreference"],"Method":["WinoMT","automatic test suite","WinoMT"]},{"ID":"alshahrani-etal-2022-roadblocks","title":"Roadblocks in Gender Bias Measurement for Diachronic Corpora","abstract":"The use of word embeddings is an important NLP technique for extracting meaningful conclusions from corpora of human text. One important question that has been raised about word embeddings is the degree of gender bias learned from corpora. Bolukbasi et al. (2016) proposed an important technique for quantifying gender bias in word embeddings that, at its heart, is lexically based and relies on sets of highly gendered word pairs (e.g., mother\/father and madam\/sir) and a list of professions words (e.g., doctor and nurse). In this paper, we document problems that arise with this method to quantify gender bias in diachronic corpora. Focusing on Arabic and Chinese corpora, in particular, we document clear changes in profession words used over time and, somewhat surprisingly, even changes in the simpler gendered defining set word pairs. We further document complications in languages such as Arabic, where many words are highly polysemous\/homonymous, especially female professions words.","year":2022,"title_abstract":"Roadblocks in Gender Bias Measurement for Diachronic Corpora The use of word embeddings is an important NLP technique for extracting meaningful conclusions from corpora of human text. One important question that has been raised about word embeddings is the degree of gender bias learned from corpora. Bolukbasi et al. (2016) proposed an important technique for quantifying gender bias in word embeddings that, at its heart, is lexically based and relies on sets of highly gendered word pairs (e.g., mother\/father and madam\/sir) and a list of professions words (e.g., doctor and nurse). In this paper, we document problems that arise with this method to quantify gender bias in diachronic corpora. Focusing on Arabic and Chinese corpora, in particular, we document clear changes in profession words used over time and, somewhat surprisingly, even changes in the simpler gendered defining set word pairs. We further document complications in languages such as Arabic, where many words are highly polysemous\/homonymous, especially female professions words.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.259976089,"Goal":"Gender Equality","Task":["Gender Bias Measurement","extracting meaningful conclusions","word embeddings","quantifying gender bias","word embeddings","gender bias"],"Method":["word embeddings","NLP technique"]},{"ID":"webster-etal-2018-mind","title":"Mind the {GAP}: A Balanced Corpus of Gendered Ambiguous Pronouns","abstract":"Coreference resolution is an important task for natural language understanding, and the resolution of ambiguous pronouns a longstanding challenge. Nonetheless, existing corpora do not capture ambiguous pronouns in sufficient volume or diversity to accurately indicate the practical utility of models. Furthermore, we find gender bias in existing corpora and systems favoring masculine entities. To address this, we present and release GAP, a gender-balanced labeled corpus of 8,908 ambiguous pronoun{--}name pairs sampled to provide diverse coverage of challenges posed by real-world text. We explore a range of baselines that demonstrate the complexity of the challenge, the best achieving just 66.9{\\%} F1. We show that syntactic structure and continuous neural models provide promising, complementary cues for approaching the challenge.","year":2018,"title_abstract":"Mind the {GAP}: A Balanced Corpus of Gendered Ambiguous Pronouns Coreference resolution is an important task for natural language understanding, and the resolution of ambiguous pronouns a longstanding challenge. Nonetheless, existing corpora do not capture ambiguous pronouns in sufficient volume or diversity to accurately indicate the practical utility of models. Furthermore, we find gender bias in existing corpora and systems favoring masculine entities. To address this, we present and release GAP, a gender-balanced labeled corpus of 8,908 ambiguous pronoun{--}name pairs sampled to provide diverse coverage of challenges posed by real-world text. We explore a range of baselines that demonstrate the complexity of the challenge, the best achieving just 66.9{\\%} F1. We show that syntactic structure and continuous neural models provide promising, complementary cues for approaching the challenge.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2591412067,"Goal":"Gender Equality","Task":["Coreference resolution","natural language understanding","resolution of ambiguous pronouns"],"Method":["GAP","continuous neural models"]},{"ID":"attree-2019-gendered","title":"Gendered Ambiguous Pronouns Shared Task: Boosting Model Confidence by Evidence Pooling","abstract":"This paper presents a strong set of results for resolving gendered ambiguous pronouns on the Gendered Ambiguous Pronouns shared task. The model presented here draws upon the strengths of state-of-the-art language and coreference resolution models, and introduces a novel evidence-based deep learning architecture. Injecting evidence from the coreference models compliments the base architecture, and analysis shows that the model is not hindered by their weaknesses, specifically gender bias. The modularity and simplicity of the architecture make it very easy to extend for further improvement and applicable to other NLP problems. Evaluation on GAP test data results in a state-of-the-art performance at 92.5{\\%} F1 (gender bias of 0.97), edging closer to the human performance of 96.6{\\%}. The end-to-end solution presented here placed 1st in the Kaggle competition, winning by a significant lead.","year":2019,"title_abstract":"Gendered Ambiguous Pronouns Shared Task: Boosting Model Confidence by Evidence Pooling This paper presents a strong set of results for resolving gendered ambiguous pronouns on the Gendered Ambiguous Pronouns shared task. The model presented here draws upon the strengths of state-of-the-art language and coreference resolution models, and introduces a novel evidence-based deep learning architecture. Injecting evidence from the coreference models compliments the base architecture, and analysis shows that the model is not hindered by their weaknesses, specifically gender bias. The modularity and simplicity of the architecture make it very easy to extend for further improvement and applicable to other NLP problems. Evaluation on GAP test data results in a state-of-the-art performance at 92.5{\\%} F1 (gender bias of 0.97), edging closer to the human performance of 96.6{\\%}. The end-to-end solution presented here placed 1st in the Kaggle competition, winning by a significant lead.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2577838898,"Goal":"Gender Equality","Task":["Gendered Ambiguous Pronouns Shared Task","Boosting Model Confidence","resolving gendered ambiguous pronouns","NLP problems","Kaggle competition"],"Method":["Evidence Pooling","language and coreference resolution models","evidence - based deep learning architecture","coreference models","end solution"]},{"ID":"spyns-dhalleweyn-2010-flemish","title":"{F}lemish-{D}utch {HLT} Policy: Evolving to New Forms of Collaboration","abstract":"In the last decade, the Dutch Language Union has taken a serious interest in digital language resources and human language technologies (HLT), because they are crucial for a language to be able to survive in the information society. In this paper we report on the current state of the joint Flemish-Dutch efforts in the field of HLT for Dutch (HLTD) and how follow-up activities are being prepared. We explain the overall mechanism of evaluating an R{\\&}D programme and the role of evaluation in the policy cycle to establish new R{\\&}D funding activities. This is applied to the joint Flemish-Dutch STEVIN programme. Outcomes of the STEVIN scientific midterm review are shortly discussed as the overall final evaluation is currently still on-going. As part of preparing for future policy plans, an HLTD forecast is presented. Also new opportunities are outlined, in particular in the context of the European CLARIN infrastructure project that can lead to new avenues for joint Flemish-Dutch cooperation on HLTD.","year":2010,"title_abstract":"{F}lemish-{D}utch {HLT} Policy: Evolving to New Forms of Collaboration In the last decade, the Dutch Language Union has taken a serious interest in digital language resources and human language technologies (HLT), because they are crucial for a language to be able to survive in the information society. In this paper we report on the current state of the joint Flemish-Dutch efforts in the field of HLT for Dutch (HLTD) and how follow-up activities are being prepared. We explain the overall mechanism of evaluating an R{\\&}D programme and the role of evaluation in the policy cycle to establish new R{\\&}D funding activities. This is applied to the joint Flemish-Dutch STEVIN programme. Outcomes of the STEVIN scientific midterm review are shortly discussed as the overall final evaluation is currently still on-going. As part of preparing for future policy plans, an HLTD forecast is presented. Also new opportunities are outlined, in particular in the context of the European CLARIN infrastructure project that can lead to new avenues for joint Flemish-Dutch cooperation on HLTD.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.2577158809,"Goal":"Partnership for the Goals","Task":["human language technologies","HLT","R{\\&}D programme","evaluation","policy cycle","STEVIN programme","STEVIN scientific midterm review","policy plans","CLARIN infrastructure project"],"Method":["HLTD"]},{"ID":"scelsi-etal-2021-principled","title":"Principled Analysis of Energy Discourse across Domains with Thesaurus-based Automatic Topic Labeling","abstract":"With the increasing impact of Natural Language Processing tools like topic models in social science research, the experimental rigor and comparability of models and datasets has come under scrutiny. Especially when contributing to research on topics with worldwide impacts like energy policy, objective analyses and reliable datasets are necessary. We contribute toward this goal in two ways: first, we release two diachronic corpora covering 23 years of energy discussions in the U.S. Energy Information Administration. Secondly, we propose a simple and theoretically sound method for automatic topic labelling drawing on political thesauri. We empirically evaluate the quality of our labels, and apply our labelling to topics induced by diachronic topic models on our energy corpora, and present a detailed analysis.","year":2021,"title_abstract":"Principled Analysis of Energy Discourse across Domains with Thesaurus-based Automatic Topic Labeling With the increasing impact of Natural Language Processing tools like topic models in social science research, the experimental rigor and comparability of models and datasets has come under scrutiny. Especially when contributing to research on topics with worldwide impacts like energy policy, objective analyses and reliable datasets are necessary. We contribute toward this goal in two ways: first, we release two diachronic corpora covering 23 years of energy discussions in the U.S. Energy Information Administration. Secondly, we propose a simple and theoretically sound method for automatic topic labelling drawing on political thesauri. We empirically evaluate the quality of our labels, and apply our labelling to topics induced by diachronic topic models on our energy corpora, and present a detailed analysis.","social_need":"Affordable and Clean Energy Ensure access to affordable, reliable, sustainable and modern energy for all","cosine_similarity":0.2568682432,"Goal":"Affordable and Clean Energy","Task":["Principled Analysis of Energy Discourse","social science research","energy policy","automatic topic labelling"],"Method":["Thesaurus - based Automatic Topic Labeling","Natural Language Processing tools","topic models","labelling","diachronic topic models"]},{"ID":"yoo-etal-2021-empathy","title":"Empathy and Hope: Resource Transfer to Model Inter-country Social Media Dynamics","abstract":"The ongoing COVID-19 pandemic resulted in significant ramifications for international relations ranging from travel restrictions, global ceasefires, and international vaccine production and sharing agreements. Amidst a wave of infections in India that resulted in a systemic breakdown of healthcare infrastructure, a social welfare organization based in Pakistan offered to procure medical-grade oxygen to assist India - a nation which was involved in four wars with Pakistan in the past few decades. In this paper, we focus on Pakistani Twitter users{'} response to the ongoing healthcare crisis in India. While {\\#}IndiaNeedsOxygen and {\\#}PakistanStandsWithIndia featured among the top-trending hashtags in Pakistan, divisive hashtags such as {\\#}EndiaSaySorryToKashmir simultaneously started trending. Against the backdrop of a contentious history including four wars, divisive content of this nature, especially when a country is facing an unprecedented healthcare crisis, fuels further deterioration of relations. In this paper, we define a new task of detecting \\textit{supportive} content and demonstrate that existing \\textit{NLP for social impact} tools can be effectively harnessed for such tasks within a quick turnaround time. We also release the first publicly available data set at the intersection of geopolitical relations and a raging pandemic in the context of India and Pakistan.","year":2021,"title_abstract":"Empathy and Hope: Resource Transfer to Model Inter-country Social Media Dynamics The ongoing COVID-19 pandemic resulted in significant ramifications for international relations ranging from travel restrictions, global ceasefires, and international vaccine production and sharing agreements. Amidst a wave of infections in India that resulted in a systemic breakdown of healthcare infrastructure, a social welfare organization based in Pakistan offered to procure medical-grade oxygen to assist India - a nation which was involved in four wars with Pakistan in the past few decades. In this paper, we focus on Pakistani Twitter users{'} response to the ongoing healthcare crisis in India. While {\\#}IndiaNeedsOxygen and {\\#}PakistanStandsWithIndia featured among the top-trending hashtags in Pakistan, divisive hashtags such as {\\#}EndiaSaySorryToKashmir simultaneously started trending. Against the backdrop of a contentious history including four wars, divisive content of this nature, especially when a country is facing an unprecedented healthcare crisis, fuels further deterioration of relations. In this paper, we define a new task of detecting \\textit{supportive} content and demonstrate that existing \\textit{NLP for social impact} tools can be effectively harnessed for such tasks within a quick turnaround time. We also release the first publicly available data set at the intersection of geopolitical relations and a raging pandemic in the context of India and Pakistan.","social_need":"No Poverty End poverty in all its forms everywhere","cosine_similarity":0.256354332,"Goal":"No Poverty","Task":["Resource Transfer","Inter - country Social Media Dynamics","COVID - 19 pandemic","international relations","international vaccine production and sharing agreements","detecting \\textit{supportive} content"],"Method":["\\textit{NLP","social impact} tools"]},{"ID":"spyns-van-veenendaal-2014-decade","title":"A decade of {HLT} Agency activities in the Low Countries: from resource maintenance ({BLARK}) to service offerings ({BLAISE})","abstract":"In this paper we report on the Flemish-Dutch Agency for Human Language Technologies (HLT Agency or TST-Centrale in Dutch) in the Low Countries. We present its activities in its first decade of existence. The main goal of the HLT Agency is to ensure the sustainability of linguistic resources for Dutch. 10 years after its inception, the HLT Agency faces new challenges and opportunities. An important contextual factor is the rise of the infrastructure networks and proliferation of resource centres. We summarise some lessons learnt and we propose as future work to define and build for Dutch (which by extension can apply to any national language) a set of Basic LAnguage Infrastructure SErvices (BLAISE). As a conclusion, we state that the HLT Agency, also by its peculiar institutional status, has fulfilled and still is fulfilling an important role in maintaining Dutch as a digitally fully fledged functional language.","year":2014,"title_abstract":"A decade of {HLT} Agency activities in the Low Countries: from resource maintenance ({BLARK}) to service offerings ({BLAISE}) In this paper we report on the Flemish-Dutch Agency for Human Language Technologies (HLT Agency or TST-Centrale in Dutch) in the Low Countries. We present its activities in its first decade of existence. The main goal of the HLT Agency is to ensure the sustainability of linguistic resources for Dutch. 10 years after its inception, the HLT Agency faces new challenges and opportunities. An important contextual factor is the rise of the infrastructure networks and proliferation of resource centres. We summarise some lessons learnt and we propose as future work to define and build for Dutch (which by extension can apply to any national language) a set of Basic LAnguage Infrastructure SErvices (BLAISE). As a conclusion, we state that the HLT Agency, also by its peculiar institutional status, has fulfilled and still is fulfilling an important role in maintaining Dutch as a digitally fully fledged functional language.","social_need":"Industry, Innovation and Infrastrucure Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation","cosine_similarity":0.2561523914,"Goal":"Industry, Innovation and Infrastrucure","Task":["resource maintenance","service offerings","Human Language Technologies"],"Method":["TST - Centrale","HLT Agency","HLT Agency","HLT Agency"]},{"ID":"hicks-etal-2016-analysis","title":"An Analysis of {W}ord{N}et{'}s Coverage of Gender Identity Using {T}witter and The National Transgender Discrimination Survey","abstract":"While gender identities in the Western world are typically regarded as binary, our previous work (Hicks et al., 2015) shows that there is more lexical variety of gender identity and the way people identify their gender. There is also a growing need to lexically represent this variety of gender identities. In our previous work, we developed a set of tools and approaches for analyzing Twitter data as a basis for generating hypotheses on language used to identify gender and discuss gender-related issues across geographic regions and population groups in the U.S.A. In this paper we analyze the coverage and relative frequency of the word forms in our Twitter analysis with respect to the National Transgender Discrimination Survey data set, one of the most comprehensive data sets on transgender, gender non-conforming, and gender variant people in the U.S.A. We then analyze the coverage of WordNet, a widely used lexical database, with respect to these identities and discuss some key considerations and next steps for adding gender identity words and their meanings to WordNet.","year":2016,"title_abstract":"An Analysis of {W}ord{N}et{'}s Coverage of Gender Identity Using {T}witter and The National Transgender Discrimination Survey While gender identities in the Western world are typically regarded as binary, our previous work (Hicks et al., 2015) shows that there is more lexical variety of gender identity and the way people identify their gender. There is also a growing need to lexically represent this variety of gender identities. In our previous work, we developed a set of tools and approaches for analyzing Twitter data as a basis for generating hypotheses on language used to identify gender and discuss gender-related issues across geographic regions and population groups in the U.S.A. In this paper we analyze the coverage and relative frequency of the word forms in our Twitter analysis with respect to the National Transgender Discrimination Survey data set, one of the most comprehensive data sets on transgender, gender non-conforming, and gender variant people in the U.S.A. We then analyze the coverage of WordNet, a widely used lexical database, with respect to these identities and discuss some key considerations and next steps for adding gender identity words and their meanings to WordNet.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2560965717,"Goal":"Gender Equality","Task":["language","Twitter analysis"],"Method":["{T}witter","WordNet"]},{"ID":"parikh-etal-2019-multi","title":"Multi-label Categorization of Accounts of Sexism using a Neural Framework","abstract":"Sexism, an injustice that subjects women and girls to enormous suffering, manifests in blatant as well as subtle ways. In the wake of growing documentation of experiences of sexism on the web, the automatic categorization of accounts of sexism has the potential to assist social scientists and policy makers in utilizing such data to study and counter sexism better. The existing work on sexism classification, which is different from sexism detection, has certain limitations in terms of the categories of sexism used and\/or whether they can co-occur. To the best of our knowledge, this is the first work on the multi-label classification of sexism of any kind(s), and we contribute the largest dataset for sexism categorization. We develop a neural solution for this multi-label classification that can combine sentence representations obtained using models such as BERT with distributional and linguistic word embeddings using a flexible, hierarchical architecture involving recurrent components and optional convolutional ones. Further, we leverage unlabeled accounts of sexism to infuse domain-specific elements into our framework. The best proposed method outperforms several deep learning as well as traditional machine learning baselines by an appreciable margin.","year":2019,"title_abstract":"Multi-label Categorization of Accounts of Sexism using a Neural Framework Sexism, an injustice that subjects women and girls to enormous suffering, manifests in blatant as well as subtle ways. In the wake of growing documentation of experiences of sexism on the web, the automatic categorization of accounts of sexism has the potential to assist social scientists and policy makers in utilizing such data to study and counter sexism better. The existing work on sexism classification, which is different from sexism detection, has certain limitations in terms of the categories of sexism used and\/or whether they can co-occur. To the best of our knowledge, this is the first work on the multi-label classification of sexism of any kind(s), and we contribute the largest dataset for sexism categorization. We develop a neural solution for this multi-label classification that can combine sentence representations obtained using models such as BERT with distributional and linguistic word embeddings using a flexible, hierarchical architecture involving recurrent components and optional convolutional ones. Further, we leverage unlabeled accounts of sexism to infuse domain-specific elements into our framework. The best proposed method outperforms several deep learning as well as traditional machine learning baselines by an appreciable margin.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2553234696,"Goal":"Gender Equality","Task":["Multi - label Categorization of Accounts of Sexism","automatic categorization of accounts of sexism","social scientists","policy makers","sexism classification","sexism detection","multi - label classification of sexism","sexism categorization","multi - label classification"],"Method":["Neural Framework","neural solution","sentence representations","BERT","distributional and linguistic word embeddings","hierarchical architecture","recurrent components","convolutional ones","deep learning","machine learning baselines"]},{"ID":"saumya-mishra-2021-iiit","title":"{IIIT}{\\_}{DWD}@{LT}-{EDI}-{EACL}2021: Hope Speech Detection in {Y}ou{T}ube multilingual comments","abstract":"Language as a significant part of communication should be inclusive of equality and diversity. The internet user{'}s language has a huge influence on peer users all over the world. People express their views through language on virtual platforms like Facebook, Twitter, YouTube etc. People admire the success of others, pray for their well-being, and encourage on their failure. Such inspirational comments are hope speech comments. At the same time, a group of users promotes discrimination based on gender, racial, sexual orientation, persons with disability, and other minorities. The current paper aims to identify hope speech comments which are very important to move on in life. Various machine learning and deep learning based models (such as support vector machine, logistics regression, convolutional neural network, recurrent neural network) are employed to identify the hope speech in the given YouTube comments. The YouTube comments are available in English, Tamil and Malayalam languages and are part of the task {``}EACL-2021:Hope Speech Detection for Equality, Diversity and Inclusion{''}.","year":2021,"title_abstract":"{IIIT}{\\_}{DWD}@{LT}-{EDI}-{EACL}2021: Hope Speech Detection in {Y}ou{T}ube multilingual comments Language as a significant part of communication should be inclusive of equality and diversity. The internet user{'}s language has a huge influence on peer users all over the world. People express their views through language on virtual platforms like Facebook, Twitter, YouTube etc. People admire the success of others, pray for their well-being, and encourage on their failure. Such inspirational comments are hope speech comments. At the same time, a group of users promotes discrimination based on gender, racial, sexual orientation, persons with disability, and other minorities. The current paper aims to identify hope speech comments which are very important to move on in life. Various machine learning and deep learning based models (such as support vector machine, logistics regression, convolutional neural network, recurrent neural network) are employed to identify the hope speech in the given YouTube comments. The YouTube comments are available in English, Tamil and Malayalam languages and are part of the task {``}EACL-2021:Hope Speech Detection for Equality, Diversity and Inclusion{''}.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2551350892,"Goal":"Gender Equality","Task":["Hope Speech Detection","communication","Hope Speech Detection","Equality"],"Method":["machine learning","deep learning based models","support vector machine","logistics regression","convolutional neural network","recurrent neural network)"]},{"ID":"roy-etal-2020-gender","title":"Gender Detection from Human Voice Using Tensor Analysis","abstract":"Speech-based communication is one of the most preferred modes of communication for humans. The human voice contains several important information and clues that help in interpreting the voice message. The gender of the speaker can be accurately guessed by a person based on the received voice of a speaker. The knowledge of the speaker{'}s gender can be a great aid to design accurate speech recognition systems. GMM based classifier is a popular choice used for gender detection. In this paper, we propose a Tensor-based approach for detecting the gender of a speaker and discuss its implementation details for low resourceful languages. Experiments were conducted using the TIMIT and SHRUTI dataset. An average gender detection accuracy of 91{\\%} is recorded. Analysis of the results with the proposed method is presented in this paper.","year":2020,"title_abstract":"Gender Detection from Human Voice Using Tensor Analysis Speech-based communication is one of the most preferred modes of communication for humans. The human voice contains several important information and clues that help in interpreting the voice message. The gender of the speaker can be accurately guessed by a person based on the received voice of a speaker. The knowledge of the speaker{'}s gender can be a great aid to design accurate speech recognition systems. GMM based classifier is a popular choice used for gender detection. In this paper, we propose a Tensor-based approach for detecting the gender of a speaker and discuss its implementation details for low resourceful languages. Experiments were conducted using the TIMIT and SHRUTI dataset. An average gender detection accuracy of 91{\\%} is recorded. Analysis of the results with the proposed method is presented in this paper.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2551130652,"Goal":"Gender Equality","Task":["Gender Detection","Speech - based communication","communication","gender detection","detecting the gender of a speaker"],"Method":["Tensor Analysis","speech recognition systems","GMM based classifier","Tensor - based approach"]},{"ID":"devinney-etal-2020-semi","title":"Semi-Supervised Topic Modeling for Gender Bias Discovery in {E}nglish and {S}wedish","abstract":"Gender bias has been identified in many models for Natural Language Processing, stemming from implicit biases in the text corpora used to train the models. Such corpora are too large to closely analyze for biased or stereotypical content. Thus, we argue for a combination of quantitative and qualitative methods, where the quantitative part produces a view of the data of a size suitable for qualitative analysis. We investigate the usefulness of semi-supervised topic modeling for the detection and analysis of gender bias in three corpora (mainstream news articles in English and Swedish, and LGBTQ+ web content in English). We compare differences in topic models for three gender categories (masculine, feminine, and nonbinary or neutral) in each corpus. We find that in all corpora, genders are treated differently and that these differences tend to correspond to hegemonic ideas of gender.","year":2020,"title_abstract":"Semi-Supervised Topic Modeling for Gender Bias Discovery in {E}nglish and {S}wedish Gender bias has been identified in many models for Natural Language Processing, stemming from implicit biases in the text corpora used to train the models. Such corpora are too large to closely analyze for biased or stereotypical content. Thus, we argue for a combination of quantitative and qualitative methods, where the quantitative part produces a view of the data of a size suitable for qualitative analysis. We investigate the usefulness of semi-supervised topic modeling for the detection and analysis of gender bias in three corpora (mainstream news articles in English and Swedish, and LGBTQ+ web content in English). We compare differences in topic models for three gender categories (masculine, feminine, and nonbinary or neutral) in each corpus. We find that in all corpora, genders are treated differently and that these differences tend to correspond to hegemonic ideas of gender.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2544139028,"Goal":"Gender Equality","Task":["Gender Bias Discovery","Natural Language Processing","qualitative analysis","detection and analysis of gender bias"],"Method":["Semi - Supervised Topic Modeling","quantitative and qualitative methods","quantitative part","semi - supervised topic modeling","topic models"]},{"ID":"hitti-etal-2019-proposed","title":"Proposed Taxonomy for Gender Bias in Text; A Filtering Methodology for the Gender Generalization Subtype","abstract":"The purpose of this paper is to present an empirical study on gender bias in text. Current research in this field is focused on detecting and correcting for gender bias in existing machine learning models rather than approaching the issue at the dataset level. The underlying motivation is to create a dataset which could enable machines to learn to differentiate bias writing from non-bias writing. A taxonomy is proposed for structural and contextual gender biases which can manifest themselves in text. A methodology is proposed to fetch one type of structural gender bias, Gender Generalization. We explore the IMDB movie review dataset and 9 different corpora from Project Gutenberg. By filtering out irrelevant sentences, the remaining pool of candidate sentences are sent for human validation. A total of 6123 judgments are made on 1627 sentences and after a quality check on randomly selected sentences we obtain an accuracy of 75{\\%}. Out of the 1627 sentences, 808 sentence were labeled as Gender Generalizations. The inter-rater reliability amongst labelers was of 61.14{\\%}.","year":2019,"title_abstract":"Proposed Taxonomy for Gender Bias in Text; A Filtering Methodology for the Gender Generalization Subtype The purpose of this paper is to present an empirical study on gender bias in text. Current research in this field is focused on detecting and correcting for gender bias in existing machine learning models rather than approaching the issue at the dataset level. The underlying motivation is to create a dataset which could enable machines to learn to differentiate bias writing from non-bias writing. A taxonomy is proposed for structural and contextual gender biases which can manifest themselves in text. A methodology is proposed to fetch one type of structural gender bias, Gender Generalization. We explore the IMDB movie review dataset and 9 different corpora from Project Gutenberg. By filtering out irrelevant sentences, the remaining pool of candidate sentences are sent for human validation. A total of 6123 judgments are made on 1627 sentences and after a quality check on randomly selected sentences we obtain an accuracy of 75{\\%}. Out of the 1627 sentences, 808 sentence were labeled as Gender Generalizations. The inter-rater reliability amongst labelers was of 61.14{\\%}.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2543767393,"Goal":"Gender Equality","Task":["Gender Bias","Gender Generalization Subtype","gender bias","gender bias","structural and contextual gender biases","structural gender bias","Gender Generalization","human validation"],"Method":["Filtering Methodology","machine learning models"]},{"ID":"ishibashi-etal-2020-reflection","title":"Reflection-based Word Attribute Transfer","abstract":"Word embeddings, which often represent such analogic relations as king - man + woman queen, can be used to change a word{'}s attribute, including its gender. For transferring king into queen in this analogy-based manner, we subtract a difference vector man - woman based on the knowledge that king is male. However, developing such knowledge is very costly for words and attributes. In this work, we propose a novel method for word attribute transfer based on reflection mappings without such an analogy operation. Experimental results show that our proposed method can transfer the word attributes of the given words without changing the words that do not have the target attributes.","year":2020,"title_abstract":"Reflection-based Word Attribute Transfer Word embeddings, which often represent such analogic relations as king - man + woman queen, can be used to change a word{'}s attribute, including its gender. For transferring king into queen in this analogy-based manner, we subtract a difference vector man - woman based on the knowledge that king is male. However, developing such knowledge is very costly for words and attributes. In this work, we propose a novel method for word attribute transfer based on reflection mappings without such an analogy operation. Experimental results show that our proposed method can transfer the word attributes of the given words without changing the words that do not have the target attributes.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2543455362,"Goal":"Gender Equality","Task":["Word embeddings","word attribute transfer"],"Method":["Reflection - based Word Attribute Transfer","analogy - based manner","reflection mappings","analogy operation"]},{"ID":"laurent-etal-2010-ad","title":"Ad-hoc Evaluations Along the Lifecycle of Industrial Spoken Dialogue Systems: Heading to Harmonisation?","abstract":"With a view to rationalise the evaluation process within the Orange Labs spoken dialogue system projects, a field audit has been realised among the various related professionals. The article presents the study's main conclusions and draws work perspectives to enhance the evaluation process in such a complex organisation. We first present the typical spoken dialogue system project lifecycle and the involved communities of stakeholders. We then sketch a map of indicators used across the teams. It shows that each professional category designs its evaluation metrics according to a case-by-case strategy, each one targeting different goals and methodologies. And last, we identify weaknesses in the evaluation process is handled by the various teams. Among others, we mention: the dependency on the design and exploitation tools that may not be suitable for an adequate collection of relevant indicators, the need to refine some indicators' definition and analysis to obtain valuable information for system enhancement, the sharing issue that advocates for a common definition of indicators across the teams and, as a consequence, the need for shared applications that support and encourage such a rationalisation.","year":2010,"title_abstract":"Ad-hoc Evaluations Along the Lifecycle of Industrial Spoken Dialogue Systems: Heading to Harmonisation? With a view to rationalise the evaluation process within the Orange Labs spoken dialogue system projects, a field audit has been realised among the various related professionals. The article presents the study's main conclusions and draws work perspectives to enhance the evaluation process in such a complex organisation. We first present the typical spoken dialogue system project lifecycle and the involved communities of stakeholders. We then sketch a map of indicators used across the teams. It shows that each professional category designs its evaluation metrics according to a case-by-case strategy, each one targeting different goals and methodologies. And last, we identify weaknesses in the evaluation process is handled by the various teams. Among others, we mention: the dependency on the design and exploitation tools that may not be suitable for an adequate collection of relevant indicators, the need to refine some indicators' definition and analysis to obtain valuable information for system enhancement, the sharing issue that advocates for a common definition of indicators across the teams and, as a consequence, the need for shared applications that support and encourage such a rationalisation.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.2539329529,"Goal":"Partnership for the Goals","Task":["Ad - hoc Evaluations","Spoken Dialogue Systems","Harmonisation?","evaluation process","Orange Labs spoken dialogue system projects","evaluation process","analysis","system enhancement","sharing issue"],"Method":["case strategy","evaluation process","exploitation tools"]},{"ID":"bordia-bowman-2019-identifying","title":"Identifying and Reducing Gender Bias in Word-Level Language Models","abstract":"Many text corpora exhibit socially problematic biases, which can be propagated or amplified in the models trained on such data. For example, doctor cooccurs more frequently with male pronouns than female pronouns. In this study we (i) propose a metric to measure gender bias; (ii) measure bias in a text corpus and the text generated from a recurrent neural network language model trained on the text corpus; (iii) propose a regularization loss term for the language model that minimizes the projection of encoder-trained embeddings onto an embedding subspace that encodes gender; (iv) finally, evaluate efficacy of our proposed method on reducing gender bias. We find this regularization method to be effective in reducing gender bias up to an optimal weight assigned to the loss term, beyond which the model becomes unstable as the perplexity increases. We replicate this study on three training corpora{---}Penn Treebank, WikiText-2, and CNN\/Daily Mail{---}resulting in similar conclusions.","year":2019,"title_abstract":"Identifying and Reducing Gender Bias in Word-Level Language Models Many text corpora exhibit socially problematic biases, which can be propagated or amplified in the models trained on such data. For example, doctor cooccurs more frequently with male pronouns than female pronouns. In this study we (i) propose a metric to measure gender bias; (ii) measure bias in a text corpus and the text generated from a recurrent neural network language model trained on the text corpus; (iii) propose a regularization loss term for the language model that minimizes the projection of encoder-trained embeddings onto an embedding subspace that encodes gender; (iv) finally, evaluate efficacy of our proposed method on reducing gender bias. We find this regularization method to be effective in reducing gender bias up to an optimal weight assigned to the loss term, beyond which the model becomes unstable as the perplexity increases. We replicate this study on three training corpora{---}Penn Treebank, WikiText-2, and CNN\/Daily Mail{---}resulting in similar conclusions.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2537873387,"Goal":"Gender Equality","Task":["Identifying and Reducing Gender Bias","Word - Level Language Models","gender bias;","reducing gender bias"],"Method":["recurrent neural network language model","regularization loss term","language model","projection of encoder - trained embeddings","embedding subspace","regularization method"]},{"ID":"garimella-mihalcea-2016-zooming","title":"Zooming in on Gender Differences in Social Media","abstract":"Men are from Mars and women are from Venus - or so the genre of relationship literature would have us believe. But there is some truth in this idea, and researchers in fields as diverse as psychology, sociology, and linguistics have explored ways to better understand the differences between genders. In this paper, we take another look at the problem of gender discrimination and attempt to move beyond the typical surface-level text classification approach, by (1) identifying semantic and psycholinguistic word classes that reflect systematic differences between men and women and (2) finding differences between genders in the ways they use the same words. We describe several experiments and report results on a large collection of blogs authored by men and women.","year":2016,"title_abstract":"Zooming in on Gender Differences in Social Media Men are from Mars and women are from Venus - or so the genre of relationship literature would have us believe. But there is some truth in this idea, and researchers in fields as diverse as psychology, sociology, and linguistics have explored ways to better understand the differences between genders. In this paper, we take another look at the problem of gender discrimination and attempt to move beyond the typical surface-level text classification approach, by (1) identifying semantic and psycholinguistic word classes that reflect systematic differences between men and women and (2) finding differences between genders in the ways they use the same words. We describe several experiments and report results on a large collection of blogs authored by men and women.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2529405951,"Goal":"Gender Equality","Task":["psychology","sociology","linguistics","gender discrimination"],"Method":["surface - level text classification approach"]},{"ID":"martin-2017-community2vec","title":"community2vec: Vector representations of online communities encode semantic relationships","abstract":"Vector embeddings of words have been shown to encode meaningful semantic relationships that enable solving of complex analogies. This vector embedding concept has been extended successfully to many different domains and in this paper we both create and visualize vector representations of an unstructured collection of online communities based on user participation. Further, we quantitatively and qualitatively show that these representations allow solving of semantically meaningful community analogies and also other more general types of relationships. These results could help improve community recommendation engines and also serve as a tool for sociological studies of community relatedness.","year":2017,"title_abstract":"community2vec: Vector representations of online communities encode semantic relationships Vector embeddings of words have been shown to encode meaningful semantic relationships that enable solving of complex analogies. This vector embedding concept has been extended successfully to many different domains and in this paper we both create and visualize vector representations of an unstructured collection of online communities based on user participation. Further, we quantitatively and qualitatively show that these representations allow solving of semantically meaningful community analogies and also other more general types of relationships. These results could help improve community recommendation engines and also serve as a tool for sociological studies of community relatedness.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2525457144,"Goal":"Sustainable Cities and Communities","Task":["semantically meaningful community analogies","sociological studies of community relatedness"],"Method":["community2vec","Vector representations of online communities","Vector embeddings of words","vector embedding concept","vector representations","community recommendation engines"]},{"ID":"saakyan-etal-2021-covid","title":"{COVID}-Fact: Fact Extraction and Verification of Real-World Claims on {COVID}-19 Pandemic","abstract":"We introduce a FEVER-like dataset COVID-Fact of 4,086 claims concerning the COVID-19 pandemic. The dataset contains claims, evidence for the claims, and contradictory claims refuted by the evidence. Unlike previous approaches, we automatically detect true claims and their source articles and then generate counter-claims using automatic methods rather than employing human annotators. Along with our constructed resource, we formally present the task of identifying relevant evidence for the claims and verifying whether the evidence refutes or supports a given claim. In addition to scientific claims, our data contains simplified general claims from media sources, making it better suited for detecting general misinformation regarding COVID-19. Our experiments indicate that COVID-Fact will provide a challenging testbed for the development of new systems and our approach will reduce the costs of building domain-specific datasets for detecting misinformation.","year":2021,"title_abstract":"{COVID}-Fact: Fact Extraction and Verification of Real-World Claims on {COVID}-19 Pandemic We introduce a FEVER-like dataset COVID-Fact of 4,086 claims concerning the COVID-19 pandemic. The dataset contains claims, evidence for the claims, and contradictory claims refuted by the evidence. Unlike previous approaches, we automatically detect true claims and their source articles and then generate counter-claims using automatic methods rather than employing human annotators. Along with our constructed resource, we formally present the task of identifying relevant evidence for the claims and verifying whether the evidence refutes or supports a given claim. In addition to scientific claims, our data contains simplified general claims from media sources, making it better suited for detecting general misinformation regarding COVID-19. Our experiments indicate that COVID-Fact will provide a challenging testbed for the development of new systems and our approach will reduce the costs of building domain-specific datasets for detecting misinformation.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2509738803,"Goal":"Climate Action","Task":["Fact Extraction","Verification of Real - World Claims","detecting general misinformation","COVID - 19","detecting misinformation"],"Method":["automatic methods"]},{"ID":"fergadis-etal-2021-argumentation","title":"Argumentation Mining in Scientific Literature for Sustainable Development","abstract":"Science, technology and innovation (STI) policies have evolved in the past decade. We are now progressing towards policies that are more aligned with sustainable development through integrating social, economic and environmental dimensions. In this new policy environment, the need to keep track of innovation from its conception in Science and Research has emerged. Argumentation mining, an interdisciplinary NLP field, gives rise to the required technologies. In this study, we present the first STI-driven multidisciplinary corpus of scientific abstracts annotated for argumentative units (AUs) on the sustainable development goals (SDGs) set by the United Nations (UN). AUs are the sentences conveying the Claim(s) reported in the author{'}s original research and the Evidence provided for support. We also present a set of strong, BERT-based neural baselines achieving an f1-score of 70.0 for Claim and 62.4 for Evidence identification evaluated with 10-fold cross-validation. To demonstrate the effectiveness of our models, we experiment with different test sets showing comparable performance across various SDG policy domains. Our dataset and models are publicly available for research purposes.","year":2021,"title_abstract":"Argumentation Mining in Scientific Literature for Sustainable Development Science, technology and innovation (STI) policies have evolved in the past decade. We are now progressing towards policies that are more aligned with sustainable development through integrating social, economic and environmental dimensions. In this new policy environment, the need to keep track of innovation from its conception in Science and Research has emerged. Argumentation mining, an interdisciplinary NLP field, gives rise to the required technologies. In this study, we present the first STI-driven multidisciplinary corpus of scientific abstracts annotated for argumentative units (AUs) on the sustainable development goals (SDGs) set by the United Nations (UN). AUs are the sentences conveying the Claim(s) reported in the author{'}s original research and the Evidence provided for support. We also present a set of strong, BERT-based neural baselines achieving an f1-score of 70.0 for Claim and 62.4 for Evidence identification evaluated with 10-fold cross-validation. To demonstrate the effectiveness of our models, we experiment with different test sets showing comparable performance across various SDG policy domains. Our dataset and models are publicly available for research purposes.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.2507723272,"Goal":"Partnership for the Goals","Task":["Argumentation Mining","Sustainable Development Science","technology","innovation","Research","Argumentation mining","interdisciplinary NLP field","Claim","Evidence identification","SDG"],"Method":["BERT - based neural baselines"]},{"ID":"garcia-martinez-etal-2020-eco","title":"{E}co.pangeamt: Industrializing Neural {MT}","abstract":"Eco is Pangeanic{'}s customer portal for generic or specialized translation services (machine translation and post-editing, generic API MT and custom API MT). Users can request the processing (translation) of files in different formats. Moreover, a client user can manage the engines and models allowing their cloning and retraining.","year":2020,"title_abstract":"{E}co.pangeamt: Industrializing Neural {MT} Eco is Pangeanic{'}s customer portal for generic or specialized translation services (machine translation and post-editing, generic API MT and custom API MT). Users can request the processing (translation) of files in different formats. Moreover, a client user can manage the engines and models allowing their cloning and retraining.","social_need":"Industry, Innovation and Infrastrucure Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation","cosine_similarity":0.2506477237,"Goal":"Industry, Innovation and Infrastrucure","Task":["generic or specialized translation services","translation","post - editing","generic API MT","MT)"],"Method":["Industrializing Neural","Eco"]},{"ID":"williams-etal-2019-quantifying","title":"Quantifying the Semantic Core of Gender Systems","abstract":"Many of the world{'}s languages employ grammatical gender on the lexeme. For instance, in Spanish, house {``}casa{''} is feminine, whereas the word for paper {``}papel{''} is masculine. To a speaker of a genderless language, this categorization seems to exist with neither rhyme nor reason. But, is the association of nouns to gender classes truly arbitrary? In this work, we present the first large-scale investigation of the arbitrariness of gender assignment that uses canonical correlation analysis as a method for correlating the gender of inanimate nouns with their lexical semantic meaning. We find that the gender systems of 18 languages exhibit a significant correlation with an externally grounded definition of lexical semantics.","year":2019,"title_abstract":"Quantifying the Semantic Core of Gender Systems Many of the world{'}s languages employ grammatical gender on the lexeme. For instance, in Spanish, house {``}casa{''} is feminine, whereas the word for paper {``}papel{''} is masculine. To a speaker of a genderless language, this categorization seems to exist with neither rhyme nor reason. But, is the association of nouns to gender classes truly arbitrary? In this work, we present the first large-scale investigation of the arbitrariness of gender assignment that uses canonical correlation analysis as a method for correlating the gender of inanimate nouns with their lexical semantic meaning. We find that the gender systems of 18 languages exhibit a significant correlation with an externally grounded definition of lexical semantics.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2497040778,"Goal":"Gender Equality","Task":["Semantic Core of Gender Systems","arbitrariness of gender assignment","lexical semantics"],"Method":["canonical correlation analysis","gender systems"]},{"ID":"schmahl-etal-2020-wikipedia","title":"Is {W}ikipedia succeeding in reducing gender bias? Assessing changes in gender bias in {W}ikipedia using word embeddings","abstract":"Large text corpora used for creating word embeddings (vectors which represent word meanings) often contain stereotypical gender biases. As a result, such unwanted biases will typically also be present in word embeddings derived from such corpora and downstream applications in the field of natural language processing (NLP). To minimize the effect of gender bias in these settings, more insight is needed when it comes to where and how biases manifest themselves in the text corpora employed. This paper contributes by showing how gender bias in word embeddings from Wikipedia has developed over time. Quantifying the gender bias over time shows that art related words have become more female biased. Family and science words have stereotypical biases towards respectively female and male words. These biases seem to have decreased since 2006, but these changes are not more extreme than those seen in random sets of words. Career related words are more strongly associated with male than with female, this difference has only become smaller in recently written articles. These developments provide additional understanding of what can be done to make Wikipedia more gender neutral and how important time of writing can be when considering biases in word embeddings trained from Wikipedia or from other text corpora.","year":2020,"title_abstract":"Is {W}ikipedia succeeding in reducing gender bias? Assessing changes in gender bias in {W}ikipedia using word embeddings Large text corpora used for creating word embeddings (vectors which represent word meanings) often contain stereotypical gender biases. As a result, such unwanted biases will typically also be present in word embeddings derived from such corpora and downstream applications in the field of natural language processing (NLP). To minimize the effect of gender bias in these settings, more insight is needed when it comes to where and how biases manifest themselves in the text corpora employed. This paper contributes by showing how gender bias in word embeddings from Wikipedia has developed over time. Quantifying the gender bias over time shows that art related words have become more female biased. Family and science words have stereotypical biases towards respectively female and male words. These biases seem to have decreased since 2006, but these changes are not more extreme than those seen in random sets of words. Career related words are more strongly associated with male than with female, this difference has only become smaller in recently written articles. These developments provide additional understanding of what can be done to make Wikipedia more gender neutral and how important time of writing can be when considering biases in word embeddings trained from Wikipedia or from other text corpora.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2496356964,"Goal":"Gender Equality","Task":["gender bias? Assessing changes in gender bias","word embeddings","natural language processing"],"Method":["word embeddings"]},{"ID":"calzolari-soria-2010-preparing","title":"Preparing the field for an Open Resource Infrastructure: the role of the {FL}a{R}e{N}et Network of Excellence","abstract":"In order to overcome the fragmentation that affects the field of Language Resources and Technologies, an Open and Distributed Resource Infrastructure is the necessary step for building on each other achievements, integrating resources and technologies and avoiding dispersed or conflicting efforts. Since this endeavour represents a true cultural turnpoint in the LRs field, it needs a careful preparation, both in terms of acceptance by the community and thoughtful investigation of the various technical, organisational and practical aspects implied. To achieve this, we need to act as a community able to join forces on a set of shared priorities and we need to act at a worldwide level. FLaReNet \u2015 Fostering Language Resources Network \u2015 is a Thematic Network funded under the EU eContent program that aims at developing the needed common vision and fostering a European and International strategy for consolidating the sector, thus enhancing competitiveness at EU level and worldwide. In this paper we present the activities undertaken by FLaReNet in order to prepare and support the establishment of such an Infrastructure, which is becoming now a reality within the new MetaNet initiative.","year":2010,"title_abstract":"Preparing the field for an Open Resource Infrastructure: the role of the {FL}a{R}e{N}et Network of Excellence In order to overcome the fragmentation that affects the field of Language Resources and Technologies, an Open and Distributed Resource Infrastructure is the necessary step for building on each other achievements, integrating resources and technologies and avoiding dispersed or conflicting efforts. Since this endeavour represents a true cultural turnpoint in the LRs field, it needs a careful preparation, both in terms of acceptance by the community and thoughtful investigation of the various technical, organisational and practical aspects implied. To achieve this, we need to act as a community able to join forces on a set of shared priorities and we need to act at a worldwide level. FLaReNet \u2015 Fostering Language Resources Network \u2015 is a Thematic Network funded under the EU eContent program that aims at developing the needed common vision and fostering a European and International strategy for consolidating the sector, thus enhancing competitiveness at EU level and worldwide. In this paper we present the activities undertaken by FLaReNet in order to prepare and support the establishment of such an Infrastructure, which is becoming now a reality within the new MetaNet initiative.","social_need":"Industry, Innovation and Infrastrucure Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation","cosine_similarity":0.2496173382,"Goal":"Industry, Innovation and Infrastrucure","Task":["Open Resource Infrastructure","Language Resources","LRs field"],"Method":["{FL}a{R}e{N}et Network","FLaReNet","Language Resources Network","Thematic Network","FLaReNet","MetaNet initiative"]},{"ID":"zmigrod-etal-2019-counterfactual","title":"Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology","abstract":"Gender stereotypes are manifest in most of the world{'}s languages and are consequently propagated or amplified by NLP systems. Although research has focused on mitigating gender stereotypes in English, the approaches that are commonly employed produce ungrammatical sentences in morphologically rich languages. We present a novel approach for converting between masculine-inflected and feminine-inflected sentences in such languages. For Spanish and Hebrew, our approach achieves F1 scores of 82{\\%} and 73{\\%} at the level of tags and accuracies of 90{\\%} and 87{\\%} at the level of forms. By evaluating our approach using four different languages, we show that, on average, it reduces gender stereotyping by a factor of 2.5 without any sacrifice to grammaticality.","year":2019,"title_abstract":"Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology Gender stereotypes are manifest in most of the world{'}s languages and are consequently propagated or amplified by NLP systems. Although research has focused on mitigating gender stereotypes in English, the approaches that are commonly employed produce ungrammatical sentences in morphologically rich languages. We present a novel approach for converting between masculine-inflected and feminine-inflected sentences in such languages. For Spanish and Hebrew, our approach achieves F1 scores of 82{\\%} and 73{\\%} at the level of tags and accuracies of 90{\\%} and 87{\\%} at the level of forms. By evaluating our approach using four different languages, we show that, on average, it reduces gender stereotyping by a factor of 2.5 without any sacrifice to grammaticality.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2490255684,"Goal":"Gender Equality","Task":["Counterfactual Data Augmentation","Mitigating Gender Stereotypes","gender stereotyping"],"Method":["NLP systems"]},{"ID":"puranik-etal-2021-iiitt","title":"{IIITT}@{LT}-{EDI}-{EACL}2021-Hope Speech Detection: There is always hope in Transformers","abstract":"In a world with serious challenges like climate change, religious and political conflicts, global pandemics, terrorism, and racial discrimination, an internet full of hate speech, abusive and offensive content is the last thing we desire for. In this paper, we work to identify and promote positive and supportive content on these platforms. We work with several transformer-based models to classify social media comments as hope speech or not hope speech in English, Malayalam, and Tamil languages. This paper portrays our work for the Shared Task on Hope Speech Detection for Equality, Diversity, and Inclusion at LT-EDI 2021- EACL 2021. The codes for our best submission can be viewed.","year":2021,"title_abstract":"{IIITT}@{LT}-{EDI}-{EACL}2021-Hope Speech Detection: There is always hope in Transformers In a world with serious challenges like climate change, religious and political conflicts, global pandemics, terrorism, and racial discrimination, an internet full of hate speech, abusive and offensive content is the last thing we desire for. In this paper, we work to identify and promote positive and supportive content on these platforms. We work with several transformer-based models to classify social media comments as hope speech or not hope speech in English, Malayalam, and Tamil languages. This paper portrays our work for the Shared Task on Hope Speech Detection for Equality, Diversity, and Inclusion at LT-EDI 2021- EACL 2021. The codes for our best submission can be viewed.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2489597648,"Goal":"Gender Equality","Task":["Hope Speech Detection","Hope Speech Detection","Equality","Inclusion","LT - EDI"],"Method":["transformer - based models"]},{"ID":"chang-etal-2017-affordable","title":"Affordable On-line Dialogue Policy Learning","abstract":"The key to building an evolvable dialogue system in real-world scenarios is to ensure an affordable on-line dialogue policy learning, which requires the on-line learning process to be safe, efficient and economical. But in reality, due to the scarcity of real interaction data, the dialogue system usually grows slowly. Besides, the poor initial dialogue policy easily leads to bad user experience and incurs a failure of attracting users to contribute training data, so that the learning process is unsustainable. To accurately depict this, two quantitative metrics are proposed to assess safety and efficiency issues. For solving the unsustainable learning problem, we proposed a complete companion teaching framework incorporating the guidance from the human teacher. Since the human teaching is expensive, we compared various teaching schemes answering the question how and when to teach, to economically utilize teaching budget, so that make the online learning process affordable.","year":2017,"title_abstract":"Affordable On-line Dialogue Policy Learning The key to building an evolvable dialogue system in real-world scenarios is to ensure an affordable on-line dialogue policy learning, which requires the on-line learning process to be safe, efficient and economical. But in reality, due to the scarcity of real interaction data, the dialogue system usually grows slowly. Besides, the poor initial dialogue policy easily leads to bad user experience and incurs a failure of attracting users to contribute training data, so that the learning process is unsustainable. To accurately depict this, two quantitative metrics are proposed to assess safety and efficiency issues. For solving the unsustainable learning problem, we proposed a complete companion teaching framework incorporating the guidance from the human teacher. Since the human teaching is expensive, we compared various teaching schemes answering the question how and when to teach, to economically utilize teaching budget, so that make the online learning process affordable.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.2486518323,"Goal":"Quality Education","Task":["On - line Dialogue Policy Learning","real - world scenarios","on - line dialogue policy learning","on - line learning process","unsustainable learning problem","human teaching"],"Method":["evolvable dialogue system","dialogue system","initial dialogue policy","learning process","companion teaching framework","teaching schemes","online learning process"]},{"ID":"bender-etal-2020-integrating","title":"Integrating Ethics into the {NLP} Curriculum","abstract":"To raise awareness among future NLP practitioners and prevent inertia in the field, we need to place ethics in the curriculum for all NLP students{---}not as an elective, but as a core part of their education. Our goal in this tutorial is to empower NLP researchers and practitioners with tools and resources to teach others about how to ethically apply NLP techniques. We will present both high-level strategies for developing an ethics-oriented curriculum, based on experience and best practices, as well as specific sample exercises that can be brought to a classroom. This highly interactive work session will culminate in a shared online resource page that pools lesson plans, assignments, exercise ideas, reading suggestions, and ideas from the attendees. Though the tutorial will focus particularly on examples for university classrooms, we believe these ideas can extend to company-internal workshops or tutorials in a variety of organizations. In this setting, a key lesson is that there is no single approach to ethical NLP: each project requires thoughtful consideration about what steps can be taken to best support people affected by that project. However, we can learn (and teach) what issues to be aware of, what questions to ask, and what strategies are available to mitigate harm.","year":2020,"title_abstract":"Integrating Ethics into the {NLP} Curriculum To raise awareness among future NLP practitioners and prevent inertia in the field, we need to place ethics in the curriculum for all NLP students{---}not as an elective, but as a core part of their education. Our goal in this tutorial is to empower NLP researchers and practitioners with tools and resources to teach others about how to ethically apply NLP techniques. We will present both high-level strategies for developing an ethics-oriented curriculum, based on experience and best practices, as well as specific sample exercises that can be brought to a classroom. This highly interactive work session will culminate in a shared online resource page that pools lesson plans, assignments, exercise ideas, reading suggestions, and ideas from the attendees. Though the tutorial will focus particularly on examples for university classrooms, we believe these ideas can extend to company-internal workshops or tutorials in a variety of organizations. In this setting, a key lesson is that there is no single approach to ethical NLP: each project requires thoughtful consideration about what steps can be taken to best support people affected by that project. However, we can learn (and teach) what issues to be aware of, what questions to ask, and what strategies are available to mitigate harm.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.2486146837,"Goal":"Quality Education","Task":["NLP","NLP","ethics - oriented curriculum","ethical NLP"],"Method":["NLP","NLP techniques"]},{"ID":"sorato-etal-2021-using","title":"Using Word Embeddings to Quantify Ethnic Stereotypes in 12 years of {S}panish News","abstract":"The current study provides a diachronic analysis of the stereotypical portrayals concerning seven of the most prominent foreign nationalities living in Spain in a Spanish news outlet. We use 12 years (2007-2018) of news articles to train word embedding models to quantify the association of such outgroups with drug use, prostitution, crimes, and poverty concepts. Then, we investigate the effects of sociopolitical variables on the computed bias series, such as the outgroup size in the host country and the rate of the population receiving unemployment benefits. Our findings indicate that the texts exhibit bias against foreign-born people, especially in the case of outgroups for which the country of origin has a lower Gross Domestic Product per capita (PPP) than Spain.","year":2021,"title_abstract":"Using Word Embeddings to Quantify Ethnic Stereotypes in 12 years of {S}panish News The current study provides a diachronic analysis of the stereotypical portrayals concerning seven of the most prominent foreign nationalities living in Spain in a Spanish news outlet. We use 12 years (2007-2018) of news articles to train word embedding models to quantify the association of such outgroups with drug use, prostitution, crimes, and poverty concepts. Then, we investigate the effects of sociopolitical variables on the computed bias series, such as the outgroup size in the host country and the rate of the population receiving unemployment benefits. Our findings indicate that the texts exhibit bias against foreign-born people, especially in the case of outgroups for which the country of origin has a lower Gross Domestic Product per capita (PPP) than Spain.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.2481065691,"Goal":"Reduced Inequalities","Task":["Ethnic Stereotypes","diachronic analysis","stereotypical portrayals"],"Method":["Word Embeddings","word embedding models"]},{"ID":"chiril-etal-2021-nice-wife","title":"{``}Be nice to your wife! The restaurants are closed{''}: Can Gender Stereotype Detection Improve Sexism Classification?","abstract":"In this paper, we focus on the detection of sexist hate speech against women in tweets studying for the first time the impact of gender stereotype detection on sexism classification. We propose: (1) the first dataset annotated for gender stereotype detection, (2) a new method for data augmentation based on sentence similarity with multilingual external datasets, and (3) a set of deep learning experiments first to detect gender stereotypes and then, to use this auxiliary task for sexism detection. Although the presence of stereotypes does not necessarily entail hateful content, our results show that sexism classification can definitively benefit from gender stereotype detection.","year":2021,"title_abstract":"{``}Be nice to your wife! The restaurants are closed{''}: Can Gender Stereotype Detection Improve Sexism Classification? In this paper, we focus on the detection of sexist hate speech against women in tweets studying for the first time the impact of gender stereotype detection on sexism classification. We propose: (1) the first dataset annotated for gender stereotype detection, (2) a new method for data augmentation based on sentence similarity with multilingual external datasets, and (3) a set of deep learning experiments first to detect gender stereotypes and then, to use this auxiliary task for sexism detection. Although the presence of stereotypes does not necessarily entail hateful content, our results show that sexism classification can definitively benefit from gender stereotype detection.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2472648025,"Goal":"Gender Equality","Task":["Gender Stereotype Detection","Sexism Classification?","detection of sexist hate speech","gender stereotype detection","sexism classification","gender stereotype detection","data augmentation","sentence similarity","auxiliary task","sexism detection","sexism classification"],"Method":["deep learning","gender stereotype detection"]},{"ID":"singh-motlicek-2022-idiap","title":"{IDIAP} Submission@{LT}-{EDI}-{ACL}2022 : Hope Speech Detection for Equality, Diversity and Inclusion","abstract":"Social media platforms have been provoking masses of people. The individual comments affect a prevalent way of thinking by moving away from preoccupation with discrimination, loneliness, or influence in building confidence, support, and good qualities. This paper aims to identify hope in these social media posts. Hope significantly impacts the well-being of people, as suggested by health professionals. It reflects the belief to achieve an objective, discovers a new path, or become motivated to formulate pathways.In this paper we classify given a social media post, hope speech or not hope speech, using ensembled voting of BERT, ERNIE 2.0 and RoBERTa for English language with 0.54 macro F1-score ($2^{st}$ rank). For non-English languages Malayalam, Spanish and Tamil we utilized XLM RoBERTA with 0.50, 0.81, 0.3 macro F1 score ($1^{st}$, $1^{st}$,$3^{rd}$ rank) respectively. For Kannada, we use Multilingual BERT with 0.32 F1 score($5^{th}$)position. We release our code-base here: https:\/\/github.com\/Muskaan-Singh\/Hate-Speech-detection.git.","year":2022,"title_abstract":"{IDIAP} Submission@{LT}-{EDI}-{ACL}2022 : Hope Speech Detection for Equality, Diversity and Inclusion Social media platforms have been provoking masses of people. The individual comments affect a prevalent way of thinking by moving away from preoccupation with discrimination, loneliness, or influence in building confidence, support, and good qualities. This paper aims to identify hope in these social media posts. Hope significantly impacts the well-being of people, as suggested by health professionals. It reflects the belief to achieve an objective, discovers a new path, or become motivated to formulate pathways.In this paper we classify given a social media post, hope speech or not hope speech, using ensembled voting of BERT, ERNIE 2.0 and RoBERTa for English language with 0.54 macro F1-score ($2^{st}$ rank). For non-English languages Malayalam, Spanish and Tamil we utilized XLM RoBERTA with 0.50, 0.81, 0.3 macro F1 score ($1^{st}$, $1^{st}$,$3^{rd}$ rank) respectively. For Kannada, we use Multilingual BERT with 0.32 F1 score($5^{th}$)position. We release our code-base here: https:\/\/github.com\/Muskaan-Singh\/Hate-Speech-detection.git.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2462229133,"Goal":"Gender Equality","Task":["Hope Speech Detection","Equality"],"Method":["ensembled voting","BERT","XLM RoBERTA"]},{"ID":"ghazouani-etal-2019-assessing","title":"Assessing socioeconomic status of {T}witter users: A survey","abstract":"Every day, the emotion and opinion of different people across the world are reflected in the form of short messages using microblogging platforms. Despite the existence of enormous potential introduced by this data source, the Twitter community is still ambiguous and is not fully explored yet. While there are a huge number of studies examining the possibilities of inferring gender and age, there exist hardly researches on socioeconomic status (SES) inference of Twitter users. As socioeconomic status is essential to treating diverse questions linked to human behavior in several fields (sociology, demography, public health, etc.), we conducted a comprehensive literature review of SES studies, inference methods, and metrics. With reference to the research on literature{'}s results, we came to outline the most critical challenges for researchers. To the best of our knowledge, this paper is the first review that introduces the different aspects of SES inference. Indeed, this article provides the benefits for practitioners who aim to process and explore Twitter SES inference.","year":2019,"title_abstract":"Assessing socioeconomic status of {T}witter users: A survey Every day, the emotion and opinion of different people across the world are reflected in the form of short messages using microblogging platforms. Despite the existence of enormous potential introduced by this data source, the Twitter community is still ambiguous and is not fully explored yet. While there are a huge number of studies examining the possibilities of inferring gender and age, there exist hardly researches on socioeconomic status (SES) inference of Twitter users. As socioeconomic status is essential to treating diverse questions linked to human behavior in several fields (sociology, demography, public health, etc.), we conducted a comprehensive literature review of SES studies, inference methods, and metrics. With reference to the research on literature{'}s results, we came to outline the most critical challenges for researchers. To the best of our knowledge, this paper is the first review that introduces the different aspects of SES inference. Indeed, this article provides the benefits for practitioners who aim to process and explore Twitter SES inference.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.2459492385,"Goal":"Reduced Inequalities","Task":["Assessing socioeconomic status","socioeconomic status","inference","demography","public health","SES studies","SES inference","Twitter SES inference"],"Method":["microblogging platforms","inference methods"]},{"ID":"shin-etal-2020-neutralizing","title":"Neutralizing Gender Bias in Word Embeddings with Latent Disentanglement and Counterfactual Generation","abstract":"Recent research demonstrates that word embeddings, trained on the human-generated corpus, have strong gender biases in embedding spaces, and these biases can result in the discriminative results from the various downstream tasks. Whereas the previous methods project word embeddings into a linear subspace for debiasing, we introduce a Latent Disentanglement method with a siamese auto-encoder structure with an adapted gradient reversal layer. Our structure enables the separation of the semantic latent information and gender latent information of given word into the disjoint latent dimensions. Afterwards, we introduce a Counterfactual Generation to convert the gender information of words, so the original and the modified embeddings can produce a gender-neutralized word embedding after geometric alignment regularization, without loss of semantic information. From the various quantitative and qualitative debiasing experiments, our method shows to be better than existing debiasing methods in debiasing word embeddings. In addition, Our method shows the ability to preserve semantic information during debiasing by minimizing the semantic information losses for extrinsic NLP downstream tasks.","year":2020,"title_abstract":"Neutralizing Gender Bias in Word Embeddings with Latent Disentanglement and Counterfactual Generation Recent research demonstrates that word embeddings, trained on the human-generated corpus, have strong gender biases in embedding spaces, and these biases can result in the discriminative results from the various downstream tasks. Whereas the previous methods project word embeddings into a linear subspace for debiasing, we introduce a Latent Disentanglement method with a siamese auto-encoder structure with an adapted gradient reversal layer. Our structure enables the separation of the semantic latent information and gender latent information of given word into the disjoint latent dimensions. Afterwards, we introduce a Counterfactual Generation to convert the gender information of words, so the original and the modified embeddings can produce a gender-neutralized word embedding after geometric alignment regularization, without loss of semantic information. From the various quantitative and qualitative debiasing experiments, our method shows to be better than existing debiasing methods in debiasing word embeddings. In addition, Our method shows the ability to preserve semantic information during debiasing by minimizing the semantic information losses for extrinsic NLP downstream tasks.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2456128746,"Goal":"Gender Equality","Task":["Neutralizing Gender Bias","Word Embeddings","downstream tasks","debiasing","debiasing","debiasing word embeddings","debiasing","extrinsic NLP downstream tasks"],"Method":["Latent Disentanglement","Counterfactual Generation","word embeddings","Latent Disentanglement method","siamese auto - encoder structure","gradient reversal layer","Counterfactual Generation","gender - neutralized word embedding","geometric alignment regularization","debiasing methods"]},{"ID":"wallis-etal-2019-exploiting","title":"Exploiting parsed corpora in grammar teaching","abstract":"The principal barrier to the uptake of technologies in schools is not technological, but social and political. Teachers must be convinced of the pedagogical benefits of a particular curriculum before they will agree to learn the means to teach it. The teaching of formal grammar to first language students in schools is no exception to this rule. Over the last three decades, most schools in England have been legally required to teach grammatical subject knowledge, i.e. linguistic knowledge of grammar terms and structure, to children age five and upwards as part of the national curriculum in English. A mandatory set of curriculum specifications for England and Wales was published in 2014, and elsewhere similar requirements were imposed. However, few current English school teachers were taught grammar themselves, and the dominant view has long been in favour of {`}real books{'} rather than the teaching of a formal grammar. English grammar teaching thus faces multiple challenges: to convince teachers of the value of grammar in their own teaching, to teach the teachers the knowledge they need, and to develop relevant resources to use in the classroom. Alongside subject knowledge, teachers need pedagogical knowledge {--} how to teach grammar effectively and how to integrate this teaching into other kinds of language learning. The paper introduces the Englicious1 web platform for schools, and summarises its development and impact since publication. Englicious draws data from the fully-parsed British Component of the International Corpus of English, ICE-GB. The corpus offers plentiful examples of genuine natural language, speech and writing, with context and potentially audio playback. However, corpus examples may be ageinappropriate or over-complex, and without grammar training, teachers are insufficiently equipped to use them. In the absence of grammatical knowledge among teachers, it is insufficient simply to give teachers and children access to a corpus. Whereas so-called {`}classroom concordancing{'} approaches offer access to tools and encourage bottom-up learning, Englicious approaches the question of grammar teaching in a concept-driven, top-down way. It contains a modular series of professional development resources, lessons and exercises focused on each concept in turn, in which corpus examples are used extensively. Teachers must be able to discuss with a class why, for instance, work is a noun in a particular sentence, rather than merely report that it is. The paper describes the development of Englicious from secondary to primary, and outlines some of the practical challenges facing the design of this type of teaching resource. A key question, the {`}selection problem{'}, concerns how tools parameterise the selection of relevant examples for teaching purposes. Finally we discuss curricula for teaching teachers and the evaluation of the effectiveness of the intervention.","year":2019,"title_abstract":"Exploiting parsed corpora in grammar teaching The principal barrier to the uptake of technologies in schools is not technological, but social and political. Teachers must be convinced of the pedagogical benefits of a particular curriculum before they will agree to learn the means to teach it. The teaching of formal grammar to first language students in schools is no exception to this rule. Over the last three decades, most schools in England have been legally required to teach grammatical subject knowledge, i.e. linguistic knowledge of grammar terms and structure, to children age five and upwards as part of the national curriculum in English. A mandatory set of curriculum specifications for England and Wales was published in 2014, and elsewhere similar requirements were imposed. However, few current English school teachers were taught grammar themselves, and the dominant view has long been in favour of {`}real books{'} rather than the teaching of a formal grammar. English grammar teaching thus faces multiple challenges: to convince teachers of the value of grammar in their own teaching, to teach the teachers the knowledge they need, and to develop relevant resources to use in the classroom. Alongside subject knowledge, teachers need pedagogical knowledge {--} how to teach grammar effectively and how to integrate this teaching into other kinds of language learning. The paper introduces the Englicious1 web platform for schools, and summarises its development and impact since publication. Englicious draws data from the fully-parsed British Component of the International Corpus of English, ICE-GB. The corpus offers plentiful examples of genuine natural language, speech and writing, with context and potentially audio playback. However, corpus examples may be ageinappropriate or over-complex, and without grammar training, teachers are insufficiently equipped to use them. In the absence of grammatical knowledge among teachers, it is insufficient simply to give teachers and children access to a corpus. Whereas so-called {`}classroom concordancing{'} approaches offer access to tools and encourage bottom-up learning, Englicious approaches the question of grammar teaching in a concept-driven, top-down way. It contains a modular series of professional development resources, lessons and exercises focused on each concept in turn, in which corpus examples are used extensively. Teachers must be able to discuss with a class why, for instance, work is a noun in a particular sentence, rather than merely report that it is. The paper describes the development of Englicious from secondary to primary, and outlines some of the practical challenges facing the design of this type of teaching resource. A key question, the {`}selection problem{'}, concerns how tools parameterise the selection of relevant examples for teaching purposes. Finally we discuss curricula for teaching teachers and the evaluation of the effectiveness of the intervention.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.2451987714,"Goal":"Quality Education","Task":["grammar teaching","formal grammar","English grammar teaching","grammar","language learning","bottom - up learning","grammar teaching","concept - driven , top - down way","professional development resources","teaching resource","{`}selection problem{'}","teaching purposes","teaching teachers"],"Method":["formal grammar","Englicious1 web platform","Englicious","grammar training","concordancing{'} approaches","Englicious","Englicious"]},{"ID":"prokopalo-etal-2020-evaluation-lifelong","title":"Evaluation of Lifelong Learning Systems","abstract":"Current intelligent systems need the expensive support of machine learning experts to sustain their performance level when used on a daily basis. To reduce this cost, i.e. remaining free from any machine learning expert, it is reasonable to implement lifelong (or continuous) learning intelligent systems that will continuously adapt their model when facing changing execution conditions. In this work, the systems are allowed to refer to human domain experts who can provide the system with relevant knowledge about the task. Nowadays, the fast growth of lifelong learning systems development rises the question of their evaluation. In this article we propose a generic evaluation methodology for the specific case of lifelong learning systems. Two steps will be considered. First, the evaluation of human-assisted learning (including active and\/or interactive learning) outside the context of lifelong learning. Second, the system evaluation across time, with propositions of how a lifelong learning intelligent system should be evaluated when including human assisted learning or not.","year":2020,"title_abstract":"Evaluation of Lifelong Learning Systems Current intelligent systems need the expensive support of machine learning experts to sustain their performance level when used on a daily basis. To reduce this cost, i.e. remaining free from any machine learning expert, it is reasonable to implement lifelong (or continuous) learning intelligent systems that will continuously adapt their model when facing changing execution conditions. In this work, the systems are allowed to refer to human domain experts who can provide the system with relevant knowledge about the task. Nowadays, the fast growth of lifelong learning systems development rises the question of their evaluation. In this article we propose a generic evaluation methodology for the specific case of lifelong learning systems. Two steps will be considered. First, the evaluation of human-assisted learning (including active and\/or interactive learning) outside the context of lifelong learning. Second, the system evaluation across time, with propositions of how a lifelong learning intelligent system should be evaluated when including human assisted learning or not.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.2451629192,"Goal":"Quality Education","Task":["Evaluation of Lifelong Learning Systems","lifelong learning systems development","evaluation","lifelong learning systems","human - assisted learning","active and\/or interactive learning)","lifelong learning","human assisted learning"],"Method":["intelligent systems","machine learning experts","machine learning expert","lifelong (or continuous) learning intelligent systems","evaluation methodology","lifelong learning intelligent system"]},{"ID":"adebara-abdul-mageed-2022-towards","title":"Towards Afrocentric {NLP} for {A}frican Languages: Where We Are and Where We Can Go","abstract":"Aligning with ACL 2022 special Theme on {``}Language Diversity: from Low Resource to Endangered Languages{''}, we discuss the major linguistic and sociopolitical challenges facing development of NLP technologies for African languages. Situating African languages in a typological framework, we discuss how the particulars of these languages can be harnessed. To facilitate future research, we also highlight current efforts, communities, venues, datasets, and tools. Our main objective is to motivate and advocate for an Afrocentric approach to technology development. With this in mind, we recommend \\textit{what} technologies to build and \\textit{how} to build, evaluate, and deploy them based on the needs of local African communities.","year":2022,"title_abstract":"Towards Afrocentric {NLP} for {A}frican Languages: Where We Are and Where We Can Go Aligning with ACL 2022 special Theme on {``}Language Diversity: from Low Resource to Endangered Languages{''}, we discuss the major linguistic and sociopolitical challenges facing development of NLP technologies for African languages. Situating African languages in a typological framework, we discuss how the particulars of these languages can be harnessed. To facilitate future research, we also highlight current efforts, communities, venues, datasets, and tools. Our main objective is to motivate and advocate for an Afrocentric approach to technology development. With this in mind, we recommend \\textit{what} technologies to build and \\textit{how} to build, evaluate, and deploy them based on the needs of local African communities.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2448309064,"Goal":"Sustainable Cities and Communities","Task":["linguistic and sociopolitical challenges","technology development"],"Method":["Afrocentric","NLP technologies","typological framework","Afrocentric approach"]},{"ID":"chamberlain-etal-2020-speaking","title":"Speaking Outside the Box: Exploring the Benefits of Unconstrained Input in Crowdsourcing and Citizen Science Platforms","abstract":"Crowdsourcing approaches provide a difficult design challenge for developers. There is a trade-off between the efficiency of the task to be done and the reward given to the user for participating, whether it be altruism, social enhancement, entertainment or money. This paper explores how crowdsourcing and citizen science systems collect data and complete tasks, illustrated by a case study from the online language game-with-a-purpose Phrase Detectives. The game was originally developed to be a constrained interface to prevent player collusion, but subsequently benefited from posthoc analysis of over 76k unconstrained inputs from users. Understanding the interface design and task deconstruction are critical for enabling users to participate in such systems and the paper concludes with a discussion of the idea that social networks can be viewed as form of citizen science platform with both constrained and unconstrained inputs making for a highly complex dataset.","year":2020,"title_abstract":"Speaking Outside the Box: Exploring the Benefits of Unconstrained Input in Crowdsourcing and Citizen Science Platforms Crowdsourcing approaches provide a difficult design challenge for developers. There is a trade-off between the efficiency of the task to be done and the reward given to the user for participating, whether it be altruism, social enhancement, entertainment or money. This paper explores how crowdsourcing and citizen science systems collect data and complete tasks, illustrated by a case study from the online language game-with-a-purpose Phrase Detectives. The game was originally developed to be a constrained interface to prevent player collusion, but subsequently benefited from posthoc analysis of over 76k unconstrained inputs from users. Understanding the interface design and task deconstruction are critical for enabling users to participate in such systems and the paper concludes with a discussion of the idea that social networks can be viewed as form of citizen science platform with both constrained and unconstrained inputs making for a highly complex dataset.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2447776198,"Goal":"Sustainable Cities and Communities","Task":["Crowdsourcing","online language game - with - a - purpose Phrase Detectives","player collusion","interface design","task deconstruction"],"Method":["Crowdsourcing approaches","crowdsourcing","citizen science systems","posthoc analysis","citizen science platform"]},{"ID":"zhu-2022-lps","title":"{LPS}@{LT}-{EDI}-{ACL}2022:An Ensemble Approach about Hope Speech Detection","abstract":"The task shared by sponsor about Hope Speech Detection for Equality, Diversity, and Inclusion at LT-EDI-ACL-2022.The goal of this task is to identify whether a given comment contains hope speech or not,and hope is considered significant for the well-being, recuperation and restoration of human life.Our work aims to change the prevalent way of thinking by moving away from a preoccupation with discrimination, loneliness or the worst things in life to building the confidence, support and good qualities based on comments by individuals. In response to the need to detect equality, diversity and inclusion of hope speech in a multilingual environment, we built an integration model and achieved well performance on multiple datasets presented by the sponsor and the specific results can be referred to the experimental results section.","year":2022,"title_abstract":"{LPS}@{LT}-{EDI}-{ACL}2022:An Ensemble Approach about Hope Speech Detection The task shared by sponsor about Hope Speech Detection for Equality, Diversity, and Inclusion at LT-EDI-ACL-2022.The goal of this task is to identify whether a given comment contains hope speech or not,and hope is considered significant for the well-being, recuperation and restoration of human life.Our work aims to change the prevalent way of thinking by moving away from a preoccupation with discrimination, loneliness or the worst things in life to building the confidence, support and good qualities based on comments by individuals. In response to the need to detect equality, diversity and inclusion of hope speech in a multilingual environment, we built an integration model and achieved well performance on multiple datasets presented by the sponsor and the specific results can be referred to the experimental results section.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2439877391,"Goal":"Gender Equality","Task":["Hope Speech Detection","Hope Speech Detection","Equality","LT - EDI - ACL","recuperation","restoration of human life"],"Method":["Ensemble Approach","integration model"]},{"ID":"funke-etal-2018-interactive","title":"Interactive health insight miner: an adaptive, semantic-based approach","abstract":"E-health applications aim to support the user in adopting healthy habits. An important feature is to provide insights into the user{'}s lifestyle. To actively engage the user in the insight mining process, we propose an ontology-based framework with a Controlled Natural Language interface, which enables the user to ask for specific insights and to customize personal information.","year":2018,"title_abstract":"Interactive health insight miner: an adaptive, semantic-based approach E-health applications aim to support the user in adopting healthy habits. An important feature is to provide insights into the user{'}s lifestyle. To actively engage the user in the insight mining process, we propose an ontology-based framework with a Controlled Natural Language interface, which enables the user to ask for specific insights and to customize personal information.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.2438309789,"Goal":"Good Health and Well-Being","Task":["E - health applications","adopting healthy habits","insight mining process"],"Method":["Interactive health insight miner","adaptive , semantic - based approach","ontology - based framework"]},{"ID":"cercas-curry-etal-2020-conversational","title":"Conversational Assistants and Gender Stereotypes: Public Perceptions and Desiderata for Voice Personas","abstract":"Conversational voice assistants are rapidly developing from purely transactional systems to social companions with {``}personality{''}. UNESCO recently stated that the female and submissive personality of current digital assistants gives rise for concern as it reinforces gender stereotypes. In this work, we present results from a participatory design workshop, where we invite people to submit their preferences for a what their ideal persona might look like, both in drawings as well as in a multiple choice questionnaire. We find no clear consensus which suggests that one possible solution is to let people configure\/personalise their assistants. We then outline a multi-disciplinary project of how we plan to address the complex question of gender and stereotyping in digital assistants.","year":2020,"title_abstract":"Conversational Assistants and Gender Stereotypes: Public Perceptions and Desiderata for Voice Personas Conversational voice assistants are rapidly developing from purely transactional systems to social companions with {``}personality{''}. UNESCO recently stated that the female and submissive personality of current digital assistants gives rise for concern as it reinforces gender stereotypes. In this work, we present results from a participatory design workshop, where we invite people to submit their preferences for a what their ideal persona might look like, both in drawings as well as in a multiple choice questionnaire. We find no clear consensus which suggests that one possible solution is to let people configure\/personalise their assistants. We then outline a multi-disciplinary project of how we plan to address the complex question of gender and stereotyping in digital assistants.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2434297353,"Goal":"Gender Equality","Task":["Gender Stereotypes","Voice Personas","participatory design workshop","gender and stereotyping","digital assistants"],"Method":["Conversational Assistants","Conversational voice assistants","transactional systems"]},{"ID":"jang-etal-2020-exploratory","title":"Exploratory Analysis of {COVID}-19 Related Tweets in {N}orth {A}merica to Inform Public Health Institutes","abstract":"Social media is a rich source where we can learn about people{'}s reactions to social issues. As COVID-19 has significantly impacted on people{'}s lives, it is essential to capture how people react to public health interventions and understand their concerns. In this paper, we aim to investigate people{'}s reactions and concerns about COVID-19 in North America, especially focusing on Canada. We analyze COVID-19 related tweets using topic modeling and aspect-based sentiment analysis, and interpret the results with public health experts. We compare timeline of topics discussed with timing of implementation of public health interventions for COVID-19. We also examine people{'}s sentiment about COVID-19 related issues. We discuss how the results can be helpful for public health agencies when designing a policy for new interventions. Our work shows how Natural Language Processing (NLP) techniques could be applied to public health questions with domain expert involvement.","year":2020,"title_abstract":"Exploratory Analysis of {COVID}-19 Related Tweets in {N}orth {A}merica to Inform Public Health Institutes Social media is a rich source where we can learn about people{'}s reactions to social issues. As COVID-19 has significantly impacted on people{'}s lives, it is essential to capture how people react to public health interventions and understand their concerns. In this paper, we aim to investigate people{'}s reactions and concerns about COVID-19 in North America, especially focusing on Canada. We analyze COVID-19 related tweets using topic modeling and aspect-based sentiment analysis, and interpret the results with public health experts. We compare timeline of topics discussed with timing of implementation of public health interventions for COVID-19. We also examine people{'}s sentiment about COVID-19 related issues. We discuss how the results can be helpful for public health agencies when designing a policy for new interventions. Our work shows how Natural Language Processing (NLP) techniques could be applied to public health questions with domain expert involvement.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.24328731,"Goal":"Climate Action","Task":["Exploratory Analysis","Public Health","COVID - 19","COVID - 19","public health questions"],"Method":["topic modeling","aspect - based sentiment analysis","public health interventions","Natural Language Processing"]},{"ID":"b-etal-2021-overview","title":"An Overview of Fairness in Data {--} Illuminating the Bias in Data Pipeline","abstract":"Data in general encodes human biases by default; being aware of this is a good start, and the research around how to handle it is ongoing. The term {`}bias{'} is extensively used in various contexts in NLP systems. In our research the focus is specific to biases such as gender, racism, religion, demographic and other intersectional views on biases that prevail in text processing systems responsible for systematically discriminating specific population, which is not ethical in NLP. These biases exacerbate the lack of equality, diversity and inclusion of specific population while utilizing the NLP applications. The tools and technology at the intermediate level utilize biased data, and transfer or amplify this bias to the downstream applications. However, it is not enough to be colourblind, gender-neutral alone when designing a unbiased technology {--} instead, we should take a conscious effort by designing a unified framework to measure and benchmark the bias. In this paper, we recommend six measures and one augment measure based on the observations of the bias in data, annotations, text representations and debiasing techniques.","year":2021,"title_abstract":"An Overview of Fairness in Data {--} Illuminating the Bias in Data Pipeline Data in general encodes human biases by default; being aware of this is a good start, and the research around how to handle it is ongoing. The term {`}bias{'} is extensively used in various contexts in NLP systems. In our research the focus is specific to biases such as gender, racism, religion, demographic and other intersectional views on biases that prevail in text processing systems responsible for systematically discriminating specific population, which is not ethical in NLP. These biases exacerbate the lack of equality, diversity and inclusion of specific population while utilizing the NLP applications. The tools and technology at the intermediate level utilize biased data, and transfer or amplify this bias to the downstream applications. However, it is not enough to be colourblind, gender-neutral alone when designing a unbiased technology {--} instead, we should take a conscious effort by designing a unified framework to measure and benchmark the bias. In this paper, we recommend six measures and one augment measure based on the observations of the bias in data, annotations, text representations and debiasing techniques.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2432015836,"Goal":"Gender Equality","Task":["NLP","NLP","text representations"],"Method":["text processing systems","NLP applications","unbiased technology","debiasing techniques"]},{"ID":"wang-etal-2021-gender","title":"Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search","abstract":"Internet search affects people{'}s cognition of the world, so mitigating biases in search results and learning fair models is imperative for social good. We study a unique gender bias in image search in this work: the search images are often gender-imbalanced for gender-neutral natural language queries. We diagnose two typical image search models, the specialized model trained on in-domain datasets and the generalized representation model pre-trained on massive image and text data across the internet. Both models suffer from severe gender bias. Therefore, we introduce two novel debiasing approaches: an in-processing fair sampling method to address the gender imbalance issue for training models, and a post-processing feature clipping method base on mutual information to debias multimodal representations of pre-trained models. Extensive experiments on MS-COCO and Flickr30K benchmarks show that our methods significantly reduce the gender bias in image search models.","year":2021,"title_abstract":"Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search Internet search affects people{'}s cognition of the world, so mitigating biases in search results and learning fair models is imperative for social good. We study a unique gender bias in image search in this work: the search images are often gender-imbalanced for gender-neutral natural language queries. We diagnose two typical image search models, the specialized model trained on in-domain datasets and the generalized representation model pre-trained on massive image and text data across the internet. Both models suffer from severe gender bias. Therefore, we introduce two novel debiasing approaches: an in-processing fair sampling method to address the gender imbalance issue for training models, and a post-processing feature clipping method base on mutual information to debias multimodal representations of pre-trained models. Extensive experiments on MS-COCO and Flickr30K benchmarks show that our methods significantly reduce the gender bias in image search models.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2431424707,"Goal":"Gender Equality","Task":["Gender Bias","Image Search","Internet search","image search","gender imbalance issue","training models","image search models"],"Method":["fair models","image search models","specialized model","generalized representation model","debiasing approaches","in - processing fair sampling method","post - processing feature clipping method","multimodal representations","trained models"]},{"ID":"lucas-etal-2006-information","title":"The Information Commons Gazetteer","abstract":"Advances in location aware computing and the convergence of geographic and textual information systems will require a comprehensive, extensible, information rich framework called the Information Commons Gazetteer that can be freely disseminated to small devices in a modular fashion. This paper describes the infrastructure and datasets used to create such a resource. The Gazetteer makes use of MAYA Design's Universal Database Architecture; a peer-to-peer system based upon bundles of attribute-value pairs with universally unique identity, and sophisticated indexing and data fusion tools. The Gazetteer primarily constitutes publicly available geographic information from various agencies that is organized into a well-defined scalable hierarchy of worldwide administrative divisions and populated places. The data from various sources are imported into the commons incrementally and are fused with existing data in an iterative process allowing for rich information to evolve over time. Such a flexible and distributed public resource of the geographic places and place names allows for both researchers and practitioners to realize location aware computing in an efficient and useful way in the near future by eliminating redundant time consuming fusion of disparate sources.","year":2006,"title_abstract":"The Information Commons Gazetteer Advances in location aware computing and the convergence of geographic and textual information systems will require a comprehensive, extensible, information rich framework called the Information Commons Gazetteer that can be freely disseminated to small devices in a modular fashion. This paper describes the infrastructure and datasets used to create such a resource. The Gazetteer makes use of MAYA Design's Universal Database Architecture; a peer-to-peer system based upon bundles of attribute-value pairs with universally unique identity, and sophisticated indexing and data fusion tools. The Gazetteer primarily constitutes publicly available geographic information from various agencies that is organized into a well-defined scalable hierarchy of worldwide administrative divisions and populated places. The data from various sources are imported into the commons incrementally and are fused with existing data in an iterative process allowing for rich information to evolve over time. Such a flexible and distributed public resource of the geographic places and place names allows for both researchers and practitioners to realize location aware computing in an efficient and useful way in the near future by eliminating redundant time consuming fusion of disparate sources.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2431101799,"Goal":"Sustainable Cities and Communities","Task":["location aware computing","geographic and textual information systems","Information Commons Gazetteer","location aware computing"],"Method":["Information Commons Gazetteer","information rich framework","Gazetteer","MAYA Design's Universal Database Architecture;","peer - to - peer system","indexing and data fusion tools"]},{"ID":"de-jong-etal-2020-interoperability","title":"Interoperability in an Infrastructure Enabling Multidisciplinary Research: The case of {CLARIN}","abstract":"CLARIN is a European Research Infrastructure providing access to language resources and technologies for researchers in the humanities and social sciences. It supports the use and study of language data in general and aims to increase the potential for comparative research of cultural and societal phenomena across the boundaries of languages and disciplines, all in line with the European agenda for Open Science. Data infrastructures such as CLARIN have recently embarked on the emerging frameworks for the federation of infrastructural services, such as the European Open Science Cloud and the integration of services resulting from multidisciplinary collaboration in federated services for the wider SSH domain. In this paper we describe the interoperability requirements that arise through the existing ambitions and the emerging frameworks. The interoperability theme will be addressed at several levels, including organisation and ecosystem, design of workflow services, data curation, performance measurement and collaboration.","year":2020,"title_abstract":"Interoperability in an Infrastructure Enabling Multidisciplinary Research: The case of {CLARIN} CLARIN is a European Research Infrastructure providing access to language resources and technologies for researchers in the humanities and social sciences. It supports the use and study of language data in general and aims to increase the potential for comparative research of cultural and societal phenomena across the boundaries of languages and disciplines, all in line with the European agenda for Open Science. Data infrastructures such as CLARIN have recently embarked on the emerging frameworks for the federation of infrastructural services, such as the European Open Science Cloud and the integration of services resulting from multidisciplinary collaboration in federated services for the wider SSH domain. In this paper we describe the interoperability requirements that arise through the existing ambitions and the emerging frameworks. The interoperability theme will be addressed at several levels, including organisation and ecosystem, design of workflow services, data curation, performance measurement and collaboration.","social_need":"Industry, Innovation and Infrastrucure Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation","cosine_similarity":0.2422960997,"Goal":"Industry, Innovation and Infrastrucure","Task":["Interoperability","Multidisciplinary Research","comparative research of cultural and societal phenomena","Open Science","federation of infrastructural services","SSH domain","interoperability theme","organisation and ecosystem","workflow services","data curation","performance measurement","collaboration"],"Method":["CLARIN","Data infrastructures","CLARIN","Open Science Cloud","federated services"]},{"ID":"hoyle-etal-2019-unsupervised","title":"Unsupervised Discovery of Gendered Language through Latent-Variable Modeling","abstract":"Studying the ways in which language is gendered has long been an area of interest in sociolinguistics. Studies have explored, for example, the speech of male and female characters in film and the language used to describe male and female politicians. In this paper, we aim not to merely study this phenomenon qualitatively, but instead to quantify the degree to which the language used to describe men and women is different and, moreover, different in a positive or negative way. To that end, we introduce a generative latent-variable model that jointly represents adjective (or verb) choice, with its sentiment, given the natural gender of a head (or dependent) noun. We find that there are significant differences between descriptions of male and female nouns and that these differences align with common gender stereotypes: Positive adjectives used to describe women are more often related to their bodies than adjectives used to describe men.","year":2019,"title_abstract":"Unsupervised Discovery of Gendered Language through Latent-Variable Modeling Studying the ways in which language is gendered has long been an area of interest in sociolinguistics. Studies have explored, for example, the speech of male and female characters in film and the language used to describe male and female politicians. In this paper, we aim not to merely study this phenomenon qualitatively, but instead to quantify the degree to which the language used to describe men and women is different and, moreover, different in a positive or negative way. To that end, we introduce a generative latent-variable model that jointly represents adjective (or verb) choice, with its sentiment, given the natural gender of a head (or dependent) noun. We find that there are significant differences between descriptions of male and female nouns and that these differences align with common gender stereotypes: Positive adjectives used to describe women are more often related to their bodies than adjectives used to describe men.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2415641844,"Goal":"Gender Equality","Task":["Unsupervised Discovery of Gendered Language"],"Method":["Latent - Variable Modeling","generative latent - variable model"]},{"ID":"mandravickaite-krilavicius-2017-stylometric","title":"Stylometric Analysis of Parliamentary Speeches: Gender Dimension","abstract":"Relation between gender and language has been studied by many authors, however, there is still some uncertainty left regarding gender influence on language usage in the professional environment. Often, the studied data sets are too small or texts of individual authors are too short in order to capture differences of language usage wrt gender successfully. This study draws from a larger corpus of speeches transcripts of the Lithuanian Parliament (1990-2013) to explore language differences of political debates by gender via stylometric analysis. Experimental set up consists of stylistic features that indicate lexical style and do not require external linguistic tools, namely the most frequent words, in combination with unsupervised machine learning algorithms. Results show that gender differences in the language use remain in professional environment not only in usage of function words, preferred linguistic constructions, but in the presented topics as well.","year":2017,"title_abstract":"Stylometric Analysis of Parliamentary Speeches: Gender Dimension Relation between gender and language has been studied by many authors, however, there is still some uncertainty left regarding gender influence on language usage in the professional environment. Often, the studied data sets are too small or texts of individual authors are too short in order to capture differences of language usage wrt gender successfully. This study draws from a larger corpus of speeches transcripts of the Lithuanian Parliament (1990-2013) to explore language differences of political debates by gender via stylometric analysis. Experimental set up consists of stylistic features that indicate lexical style and do not require external linguistic tools, namely the most frequent words, in combination with unsupervised machine learning algorithms. Results show that gender differences in the language use remain in professional environment not only in usage of function words, preferred linguistic constructions, but in the presented topics as well.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2414189875,"Goal":"Gender Equality","Task":["language usage"],"Method":["Stylometric Analysis","stylometric analysis","unsupervised machine learning algorithms"]},{"ID":"stowe-etal-2018-developing","title":"Developing and Evaluating Annotation Procedures for {T}witter Data during Hazard Events","abstract":"When a hazard such as a hurricane threatens, people are forced to make a wide variety of decisions, and the information they receive and produce can influence their own and others{'} actions. As social media grows more popular, an increasing number of people are using social media platforms to obtain and share information about approaching threats and discuss their interpretations of the threat and their protective decisions. This work aims to improve understanding of natural disasters through social media and provide an annotation scheme to identify themes in user{'}s social media behavior and facilitate efforts in supervised machine learning. To that end, this work has three contributions: (1) the creation of an annotation scheme to consistently identify hazard-related themes in Twitter, (2) an overview of agreement rates and difficulties in identifying annotation categories, and (3) a public release of both the dataset and guidelines developed from this scheme.","year":2018,"title_abstract":"Developing and Evaluating Annotation Procedures for {T}witter Data during Hazard Events When a hazard such as a hurricane threatens, people are forced to make a wide variety of decisions, and the information they receive and produce can influence their own and others{'} actions. As social media grows more popular, an increasing number of people are using social media platforms to obtain and share information about approaching threats and discuss their interpretations of the threat and their protective decisions. This work aims to improve understanding of natural disasters through social media and provide an annotation scheme to identify themes in user{'}s social media behavior and facilitate efforts in supervised machine learning. To that end, this work has three contributions: (1) the creation of an annotation scheme to consistently identify hazard-related themes in Twitter, (2) an overview of agreement rates and difficulties in identifying annotation categories, and (3) a public release of both the dataset and guidelines developed from this scheme.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2409112453,"Goal":"Climate Action","Task":["{T}witter","Hazard Events","understanding of natural disasters","supervised machine learning","hazard - related themes","annotation categories"],"Method":["Annotation Procedures","annotation scheme","annotation scheme"]},{"ID":"pezanowski-mitra-2020-recognition","title":"Recognition of Implicit Geographic Movement in Text","abstract":"Analyzing the geographic movement of humans, animals, and other phenomena is a growing field of research. This research has benefited urban planning, logistics, animal migration understanding, and much more. Typically, the movement is captured as precise geographic coordinates and time stamps with Global Positioning Systems (GPS). Although some research uses computational techniques to take advantage of implicit movement in descriptions of route directions, hiking paths, and historical exploration routes, innovation would accelerate with a large and diverse corpus. We created a corpus of sentences labeled as describing geographic movement or not and including the type of entity moving. Creating this corpus proved difficult without any comparable corpora to start with, high human labeling costs, and since movement can at times be interpreted differently. To overcome these challenges, we developed an iterative process employing hand labeling, crowd voting for confirmation, and machine learning to predict more labels. By merging advances in word embeddings with traditional machine learning models and model ensembling, prediction accuracy is at an acceptable level to produce a large silver-standard corpus despite the small gold-standard corpus training set. Our corpus will likely benefit computational processing of geography in text and spatial cognition, in addition to detection of movement.","year":2020,"title_abstract":"Recognition of Implicit Geographic Movement in Text Analyzing the geographic movement of humans, animals, and other phenomena is a growing field of research. This research has benefited urban planning, logistics, animal migration understanding, and much more. Typically, the movement is captured as precise geographic coordinates and time stamps with Global Positioning Systems (GPS). Although some research uses computational techniques to take advantage of implicit movement in descriptions of route directions, hiking paths, and historical exploration routes, innovation would accelerate with a large and diverse corpus. We created a corpus of sentences labeled as describing geographic movement or not and including the type of entity moving. Creating this corpus proved difficult without any comparable corpora to start with, high human labeling costs, and since movement can at times be interpreted differently. To overcome these challenges, we developed an iterative process employing hand labeling, crowd voting for confirmation, and machine learning to predict more labels. By merging advances in word embeddings with traditional machine learning models and model ensembling, prediction accuracy is at an acceptable level to produce a large silver-standard corpus despite the small gold-standard corpus training set. Our corpus will likely benefit computational processing of geography in text and spatial cognition, in addition to detection of movement.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2404853702,"Goal":"Sustainable Cities and Communities","Task":["Recognition of Implicit Geographic Movement","Analyzing the geographic movement","urban planning","logistics","animal migration understanding","movement","hand labeling","confirmation","word embeddings","computational processing of geography in text and spatial cognition","detection of movement"],"Method":["Global Positioning Systems","computational techniques","iterative process","crowd voting","machine learning","machine learning models","model ensembling"]},{"ID":"brooke-2019-condescending","title":"{``}Condescending, Rude, Assholes{''}: Framing gender and hostility on {S}tack {O}verflow","abstract":"The disciplines of Gender Studies and Data Science are incompatible. This is conventional wisdom, supported by how many computational studies simplify gender into an immutable binary categorization that appears crude to the critical social researcher. I argue that the characterization of gender norms is context specific and may prove valuable in constructing useful models. I show how gender can be framed in computational studies as a stylized repetition of acts mediated by a social structure, and not a possessed biological category. By conducting a review of existing work, I show how gender should be explored in multiplicity in computational research through clustering techniques, and layout how this is being achieved in a study in progress on gender hostility on Stack Overflow.","year":2019,"title_abstract":"{``}Condescending, Rude, Assholes{''}: Framing gender and hostility on {S}tack {O}verflow The disciplines of Gender Studies and Data Science are incompatible. This is conventional wisdom, supported by how many computational studies simplify gender into an immutable binary categorization that appears crude to the critical social researcher. I argue that the characterization of gender norms is context specific and may prove valuable in constructing useful models. I show how gender can be framed in computational studies as a stylized repetition of acts mediated by a social structure, and not a possessed biological category. By conducting a review of existing work, I show how gender should be explored in multiplicity in computational research through clustering techniques, and layout how this is being achieved in a study in progress on gender hostility on Stack Overflow.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2403369397,"Goal":"Gender Equality","Task":["Gender Studies","Data Science","social researcher","characterization of gender norms","computational studies","computational research","gender hostility"],"Method":["binary categorization","clustering techniques"]},{"ID":"gonen-webster-2020-automatically","title":"Automatically Identifying Gender Issues in Machine Translation using Perturbations","abstract":"The successful application of neural methods to machine translation has realized huge quality advances for the community. With these improvements, many have noted outstanding challenges, including the modeling and treatment of gendered language. While previous studies have identified issues using synthetic examples, we develop a novel technique to mine examples from real world data to explore challenges for deployed systems. We use our method to compile an evaluation benchmark spanning examples for four languages from three language families, which we publicly release to facilitate research. The examples in our benchmark expose where model representations are gendered, and the unintended consequences these gendered representations can have in downstream application.","year":2020,"title_abstract":"Automatically Identifying Gender Issues in Machine Translation using Perturbations The successful application of neural methods to machine translation has realized huge quality advances for the community. With these improvements, many have noted outstanding challenges, including the modeling and treatment of gendered language. While previous studies have identified issues using synthetic examples, we develop a novel technique to mine examples from real world data to explore challenges for deployed systems. We use our method to compile an evaluation benchmark spanning examples for four languages from three language families, which we publicly release to facilitate research. The examples in our benchmark expose where model representations are gendered, and the unintended consequences these gendered representations can have in downstream application.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2400621027,"Goal":"Gender Equality","Task":["Automatically Identifying Gender Issues","Machine Translation","machine translation","modeling and treatment of gendered language","downstream application"],"Method":["neural methods","model representations","gendered representations"]},{"ID":"przybyla-shardlow-2022-using","title":"Using {NLP} to quantify the environmental cost and diversity benefits of in-person {NLP} conferences","abstract":"The environmental costs of research are progressively important to the NLP community and their associated challenges are increasingly debated. In this work, we analyse the carbon cost (measured as CO2-equivalent) associated with journeys made by researchers attending in-person NLP conferences. We obtain the necessary data by text-mining all publications from the ACL anthology available at the time of the study (n=60,572) and extracting information about an author{'}s affiliation, including their address. This allows us to estimate the corresponding carbon cost and compare it to previously known values for training large models. Further, we look at the benefits of in-person conferences by demonstrating that they can increase participation diversity by encouraging attendance from the region surrounding the host country. We show how the trade-off between carbon cost and diversity of an event depends on its location and type. Our aim is to foster further discussion on the best way to address the joint issue of emissions and diversity in the future.","year":2022,"title_abstract":"Using {NLP} to quantify the environmental cost and diversity benefits of in-person {NLP} conferences The environmental costs of research are progressively important to the NLP community and their associated challenges are increasingly debated. In this work, we analyse the carbon cost (measured as CO2-equivalent) associated with journeys made by researchers attending in-person NLP conferences. We obtain the necessary data by text-mining all publications from the ACL anthology available at the time of the study (n=60,572) and extracting information about an author{'}s affiliation, including their address. This allows us to estimate the corresponding carbon cost and compare it to previously known values for training large models. Further, we look at the benefits of in-person conferences by demonstrating that they can increase participation diversity by encouraging attendance from the region surrounding the host country. We show how the trade-off between carbon cost and diversity of an event depends on its location and type. Our aim is to foster further discussion on the best way to address the joint issue of emissions and diversity in the future.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2395798117,"Goal":"Sustainable Cities and Communities","Task":["environmental costs of research","NLP community"],"Method":["{NLP}","NLP","large models"]},{"ID":"hall-maudslay-etal-2019-name","title":"It{'}s All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution","abstract":"This paper treats gender bias latent in word embeddings. Previous mitigation attempts rely on the operationalisation of gender bias as a projection over a linear subspace. An alternative approach is Counterfactual Data Augmentation (CDA), in which a corpus is duplicated and augmented to remove bias, e.g. by swapping all inherently-gendered words in the copy. We perform an empirical comparison of these approaches on the English Gigaword and Wikipedia, and find that whilst both successfully reduce direct bias and perform well in tasks which quantify embedding quality, CDA variants outperform projection-based methods at the task of drawing non-biased gender analogies by an average of 19{\\%} across both corpora. We propose two improvements to CDA: Counterfactual Data Substitution (CDS), a variant of CDA in which potentially biased text is randomly substituted to avoid duplication, and the Names Intervention, a novel name-pairing technique that vastly increases the number of words being treated. CDA\/S with the Names Intervention is the only approach which is able to mitigate indirect gender bias: following debiasing, previously biased words are significantly less clustered according to gender (cluster purity is reduced by 49{\\%}), thus improving on the state-of-the-art for bias mitigation.","year":2019,"title_abstract":"It{'}s All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution This paper treats gender bias latent in word embeddings. Previous mitigation attempts rely on the operationalisation of gender bias as a projection over a linear subspace. An alternative approach is Counterfactual Data Augmentation (CDA), in which a corpus is duplicated and augmented to remove bias, e.g. by swapping all inherently-gendered words in the copy. We perform an empirical comparison of these approaches on the English Gigaword and Wikipedia, and find that whilst both successfully reduce direct bias and perform well in tasks which quantify embedding quality, CDA variants outperform projection-based methods at the task of drawing non-biased gender analogies by an average of 19{\\%} across both corpora. We propose two improvements to CDA: Counterfactual Data Substitution (CDS), a variant of CDA in which potentially biased text is randomly substituted to avoid duplication, and the Names Intervention, a novel name-pairing technique that vastly increases the number of words being treated. CDA\/S with the Names Intervention is the only approach which is able to mitigate indirect gender bias: following debiasing, previously biased words are significantly less clustered according to gender (cluster purity is reduced by 49{\\%}), thus improving on the state-of-the-art for bias mitigation.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2394879609,"Goal":"Gender Equality","Task":["Mitigating Gender Bias","Name - Based Counterfactual Data Substitution","word embeddings","Counterfactual Data Augmentation","bias mitigation"],"Method":["projection","linear subspace","CDA variants","projection - based methods","CDA","Counterfactual Data Substitution","CDA","Names Intervention","name - pairing technique","CDA\/S","Names Intervention","debiasing"]},{"ID":"abburi-etal-2020-semi","title":"Semi-supervised Multi-task Learning for Multi-label Fine-grained Sexism Classification","abstract":"Sexism, a form of oppression based on one{'}s sex, manifests itself in numerous ways and causes enormous suffering. In view of the growing number of experiences of sexism reported online, categorizing these recollections automatically can assist the fight against sexism, as it can facilitate effective analyses by gender studies researchers and government officials involved in policy making. In this paper, we investigate the fine-grained, multi-label classification of accounts (reports) of sexism. To the best of our knowledge, we work with considerably more categories of sexism than any published work through our 23-class problem formulation. Moreover, we propose a multi-task approach for fine-grained multi-label sexism classification that leverages several supporting tasks without incurring any manual labeling cost. Unlabeled accounts of sexism are utilized through unsupervised learning to help construct our multi-task setup. We also devise objective functions that exploit label correlations in the training data explicitly. Multiple proposed methods outperform the state-of-the-art for multi-label sexism classification on a recently released dataset across five standard metrics.","year":2020,"title_abstract":"Semi-supervised Multi-task Learning for Multi-label Fine-grained Sexism Classification Sexism, a form of oppression based on one{'}s sex, manifests itself in numerous ways and causes enormous suffering. In view of the growing number of experiences of sexism reported online, categorizing these recollections automatically can assist the fight against sexism, as it can facilitate effective analyses by gender studies researchers and government officials involved in policy making. In this paper, we investigate the fine-grained, multi-label classification of accounts (reports) of sexism. To the best of our knowledge, we work with considerably more categories of sexism than any published work through our 23-class problem formulation. Moreover, we propose a multi-task approach for fine-grained multi-label sexism classification that leverages several supporting tasks without incurring any manual labeling cost. Unlabeled accounts of sexism are utilized through unsupervised learning to help construct our multi-task setup. We also devise objective functions that exploit label correlations in the training data explicitly. Multiple proposed methods outperform the state-of-the-art for multi-label sexism classification on a recently released dataset across five standard metrics.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2394657582,"Goal":"Gender Equality","Task":["Semi - supervised Multi - task Learning","Multi - label Fine - grained Sexism Classification","Sexism","sexism","gender studies researchers","policy making","fine - grained","multi - label classification of accounts (reports) of sexism","fine - grained multi - label sexism classification","multi - task setup","multi - label sexism classification"],"Method":["23 - class problem formulation","multi - task approach","unsupervised learning","objective functions"]},{"ID":"dima-etal-2012-repository","title":"A Repository for the Sustainable Management of Research Data","abstract":"This paper presents the system architecture as well as the underlying workflow of the Extensible Repository System of Digital Objects (ERDO) which has been developed for the sustainable archiving of language resources within the T{\\\"u}bingen CLARIN-D project. In contrast to other approaches focusing on archiving experts, the described workflow can be used by researchers without required knowledge in the field of long-term storage for transferring data from their local file systems into a persistent repository.","year":2012,"title_abstract":"A Repository for the Sustainable Management of Research Data This paper presents the system architecture as well as the underlying workflow of the Extensible Repository System of Digital Objects (ERDO) which has been developed for the sustainable archiving of language resources within the T{\\\"u}bingen CLARIN-D project. In contrast to other approaches focusing on archiving experts, the described workflow can be used by researchers without required knowledge in the field of long-term storage for transferring data from their local file systems into a persistent repository.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.2392907739,"Goal":"Life Below Water","Task":["Sustainable Management of Research Data","sustainable archiving of language resources","archiving experts","long - term storage","transferring data"],"Method":["Extensible Repository System of Digital Objects","local file systems","persistent repository"]},{"ID":"ji-etal-2010-annotating","title":"Annotating Event Chains for Carbon Sequestration Literature","abstract":"In this paper we present a project of annotating event chains for an important scientific domain \u2015 carbon sequestration. This domain aims to reduce carbon emissions and has been identified by the U.S. National Academy of Engineering (NAE) as a grand challenge problem for the 21st century. Given a collection of scientific literature, we identify a set of centroid experiments; and then link and order the observations and events centered around these experiments on temporal or causal chains. We describe the fundamental challenges on annotations and our general solutions to address them. We expect that our annotation efforts will produce significant advances in inter-operability through new information extraction techniques and permit scientists to build knowledge that will provide better understanding of important scientific challenges in this domain, share and re-use of diverse data sets and experimental results in a more efficient manner. In addition, the annotations of metadata and ontology for these literature will provide important support for data lifecycle activities.","year":2010,"title_abstract":"Annotating Event Chains for Carbon Sequestration Literature In this paper we present a project of annotating event chains for an important scientific domain \u2015 carbon sequestration. This domain aims to reduce carbon emissions and has been identified by the U.S. National Academy of Engineering (NAE) as a grand challenge problem for the 21st century. Given a collection of scientific literature, we identify a set of centroid experiments; and then link and order the observations and events centered around these experiments on temporal or causal chains. We describe the fundamental challenges on annotations and our general solutions to address them. We expect that our annotation efforts will produce significant advances in inter-operability through new information extraction techniques and permit scientists to build knowledge that will provide better understanding of important scientific challenges in this domain, share and re-use of diverse data sets and experimental results in a more efficient manner. In addition, the annotations of metadata and ontology for these literature will provide important support for data lifecycle activities.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2392637581,"Goal":"Climate Action","Task":["Annotating Event Chains","Carbon Sequestration Literature","annotating event chains","carbon sequestration","annotations","annotation","data lifecycle activities"],"Method":["information extraction techniques"]},{"ID":"jha-2010-tdil","title":"The {TDIL} Program and the {I}ndian Langauge Corpora Intitiative ({ILCI})","abstract":"India is considered a linguistic ocean with 4 language families and 22 scheduled national languages, and 100 un-scheduled languages reported by the 2001 census. This puts tremendous pressures on the Indian government to not only have comprehensive language policies, but also to create resources for their maintenance and development. In the age of information technology, there is a greater need to have a fine balance between allocation of resources to each language keeping in view the political compulsions, electoral potential of a linguistic community and other issues. In this connection, the government of India through various ministries and a think tank consisting of eminent linguistics and policy makers has done a commendable job despite the obvious roadblocks. This paper describes the Indian government\u0092s policies towards language development and maintenance in the age of technology through the Ministry of HRD through its various agencies and the Ministry of Communications {\\&} Information Technology (MCIT) through its dedicated program called TDIL (Technology Development for Indian Languages). The paper also describes some of the recent activities of the TDIL in general and in particular, an innovative corpora project called ILCI - Indian Languages Corpora Initiative.","year":2010,"title_abstract":"The {TDIL} Program and the {I}ndian Langauge Corpora Intitiative ({ILCI}) India is considered a linguistic ocean with 4 language families and 22 scheduled national languages, and 100 un-scheduled languages reported by the 2001 census. This puts tremendous pressures on the Indian government to not only have comprehensive language policies, but also to create resources for their maintenance and development. In the age of information technology, there is a greater need to have a fine balance between allocation of resources to each language keeping in view the political compulsions, electoral potential of a linguistic community and other issues. In this connection, the government of India through various ministries and a think tank consisting of eminent linguistics and policy makers has done a commendable job despite the obvious roadblocks. This paper describes the Indian government\u0092s policies towards language development and maintenance in the age of technology through the Ministry of HRD through its various agencies and the Ministry of Communications {\\&} Information Technology (MCIT) through its dedicated program called TDIL (Technology Development for Indian Languages). The paper also describes some of the recent activities of the TDIL in general and in particular, an innovative corpora project called ILCI - Indian Languages Corpora Initiative.","social_need":"Industry, Innovation and Infrastrucure Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation","cosine_similarity":0.2389282286,"Goal":"Industry, Innovation and Infrastrucure","Task":["maintenance and development","information technology","language development","maintenance","ILCI"],"Method":["{TDIL} Program","TDIL","TDIL"]},{"ID":"roux-2016-south","title":"{S}outh {A}frican National Centre for Digital Language Resources","abstract":"This presentation introduces the imminent establishment of a new language resource infrastructure focusing on languages spoken in Southern Africa, with an eventual aim to become a hub for digital language resources within Sub-Saharan Africa. The Constitution of South Africa makes provision for 11 official languages all with equal status. The current language Resource Management Agency will be merged with the new Centre, which will have a wider focus than that of data acquisition, management and distribution. The Centre will entertain two main programs: Digitisation and Digital Humanities. The digitisation program will focus on the systematic digitisation of relevant text, speech and multi-modal data across the official languages. Relevancy will be determined by a Scientific Advisory Board. This will take place on a continuous basis through specified projects allocated to national members of the Centre, as well as through open-calls aimed at the academic as well as local communities. The digital resources will be managed and distributed through a dedicated web-based portal. The development of the Digital Humanities program will entail extensive academic support for projects implementing digital language based data. The Centre will function as an enabling research infrastructure primarily supported by national government and hosted by the North-West University.","year":2016,"title_abstract":"{S}outh {A}frican National Centre for Digital Language Resources This presentation introduces the imminent establishment of a new language resource infrastructure focusing on languages spoken in Southern Africa, with an eventual aim to become a hub for digital language resources within Sub-Saharan Africa. The Constitution of South Africa makes provision for 11 official languages all with equal status. The current language Resource Management Agency will be merged with the new Centre, which will have a wider focus than that of data acquisition, management and distribution. The Centre will entertain two main programs: Digitisation and Digital Humanities. The digitisation program will focus on the systematic digitisation of relevant text, speech and multi-modal data across the official languages. Relevancy will be determined by a Scientific Advisory Board. This will take place on a continuous basis through specified projects allocated to national members of the Centre, as well as through open-calls aimed at the academic as well as local communities. The digital resources will be managed and distributed through a dedicated web-based portal. The development of the Digital Humanities program will entail extensive academic support for projects implementing digital language based data. The Centre will function as an enabling research infrastructure primarily supported by national government and hosted by the North-West University.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.2385077477,"Goal":"Peace, Justice and Strong Institutions","Task":["Digital Language Resources","language resource infrastructure","data acquisition","management","distribution","Digitisation","Digital Humanities","digitisation program","Digital Humanities program"],"Method":["language Resource Management Agency"]},{"ID":"gupta-etal-2021-summarizing","title":"Summarizing Behavioral Change Goals from {SMS} Exchanges to Support Health Coaches","abstract":"Regular physical activity is associated with a reduced risk of chronic diseases such as type 2 diabetes and improved mental well-being. Yet, more than half of the US population is insufficiently active. Health coaching has been successful in promoting healthy behaviors. In this paper, we present our work towards assisting health coaches by extracting the physical activity goal the user and coach negotiate via text messages. We show that information captured by dialogue acts can help to improve the goal extraction results. We employ both traditional and transformer-based machine learning models for dialogue acts prediction and find them statistically indistinguishable in performance on our health coaching dataset. Moreover, we discuss the feedback provided by the health coaches when evaluating the correctness of the extracted goal summaries. This work is a step towards building a virtual assistant health coach to promote a healthy lifestyle.","year":2021,"title_abstract":"Summarizing Behavioral Change Goals from {SMS} Exchanges to Support Health Coaches Regular physical activity is associated with a reduced risk of chronic diseases such as type 2 diabetes and improved mental well-being. Yet, more than half of the US population is insufficiently active. Health coaching has been successful in promoting healthy behaviors. In this paper, we present our work towards assisting health coaches by extracting the physical activity goal the user and coach negotiate via text messages. We show that information captured by dialogue acts can help to improve the goal extraction results. We employ both traditional and transformer-based machine learning models for dialogue acts prediction and find them statistically indistinguishable in performance on our health coaching dataset. Moreover, we discuss the feedback provided by the health coaches when evaluating the correctness of the extracted goal summaries. This work is a step towards building a virtual assistant health coach to promote a healthy lifestyle.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.2385038733,"Goal":"Good Health and Well-Being","Task":["Summarizing Behavioral Change Goals","Regular physical activity","Health coaching","healthy behaviors","health coaches","goal extraction","dialogue acts prediction","virtual assistant health coach"],"Method":["{SMS} Exchanges","transformer - based machine learning models"]},{"ID":"liang-etal-2020-monolingual","title":"Monolingual and Multilingual Reduction of Gender Bias in Contextualized Representations","abstract":"Pretrained language models (PLMs) learn stereotypes held by humans and reflected in text from their training corpora, including gender bias. When PLMs are used for downstream tasks such as picking candidates for a job, people{'}s lives can be negatively affected by these learned stereotypes. Prior work usually identifies a linear gender subspace and removes gender information by eliminating the subspace. Following this line of work, we propose to use DensRay, an analytical method for obtaining interpretable dense subspaces. We show that DensRay performs on-par with prior approaches, but provide arguments that it is more robust and provide indications that it preserves language model performance better. By applying DensRay to attention heads and layers of BERT we show that gender information is spread across all attention heads and most of the layers. Also we show that DensRay can obtain gender bias scores on both token and sentence levels. Finally, we demonstrate that we can remove bias multilingually, e.g., from Chinese, using only English training data.","year":2020,"title_abstract":"Monolingual and Multilingual Reduction of Gender Bias in Contextualized Representations Pretrained language models (PLMs) learn stereotypes held by humans and reflected in text from their training corpora, including gender bias. When PLMs are used for downstream tasks such as picking candidates for a job, people{'}s lives can be negatively affected by these learned stereotypes. Prior work usually identifies a linear gender subspace and removes gender information by eliminating the subspace. Following this line of work, we propose to use DensRay, an analytical method for obtaining interpretable dense subspaces. We show that DensRay performs on-par with prior approaches, but provide arguments that it is more robust and provide indications that it preserves language model performance better. By applying DensRay to attention heads and layers of BERT we show that gender information is spread across all attention heads and most of the layers. Also we show that DensRay can obtain gender bias scores on both token and sentence levels. Finally, we demonstrate that we can remove bias multilingually, e.g., from Chinese, using only English training data.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2381974906,"Goal":"Gender Equality","Task":["Monolingual and Multilingual Reduction of Gender Bias","downstream tasks","interpretable dense subspaces"],"Method":["Contextualized Representations","Pretrained language models","PLMs","DensRay","analytical method","DensRay","language model","DensRay","DensRay"]},{"ID":"klavans-2012-government","title":"Government Catalog of Language Resources ({GCLR})","abstract":"The purpose of this presentation is to discuss recent efforts within the government to address issues of evaluation and return on investment. Pressure to demonstrate value has increased with the growing amount of foreign language information available, with the variety of languages needing to be exploited, and with the increasing gaps between numbers of language-enabled people and the amount of work to be done. This pressure is only growing as budgets shrink, and as global development grows. Over the past year, the ODNI has led an effort to pull together different government stakeholders to determine some baseline standards for determining Return on Investment via task-based evaluation. Stakeholder consensus on major HLT tasks has involved examination of the different approaches to determining return on investment and how it relates use of HLT in the workflow. In addition to reporting on the goals and progress of this group, we will present future directions and invite community input.","year":2012,"title_abstract":"Government Catalog of Language Resources ({GCLR}) The purpose of this presentation is to discuss recent efforts within the government to address issues of evaluation and return on investment. Pressure to demonstrate value has increased with the growing amount of foreign language information available, with the variety of languages needing to be exploited, and with the increasing gaps between numbers of language-enabled people and the amount of work to be done. This pressure is only growing as budgets shrink, and as global development grows. Over the past year, the ODNI has led an effort to pull together different government stakeholders to determine some baseline standards for determining Return on Investment via task-based evaluation. Stakeholder consensus on major HLT tasks has involved examination of the different approaches to determining return on investment and how it relates use of HLT in the workflow. In addition to reporting on the goals and progress of this group, we will present future directions and invite community input.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.2378748208,"Goal":"Partnership for the Goals","Task":["evaluation","return on investment","determining Return on Investment","task - based evaluation","HLT tasks","return on investment"],"Method":["HLT"]},{"ID":"du-etal-2019-exploring","title":"Exploring Human Gender Stereotypes with Word Association Test","abstract":"Word embeddings have been widely used to study gender stereotypes in texts. One key problem regarding existing bias scores is to evaluate their validities: do they really reflect true bias levels? For a small set of words (e.g. occupations), we can rely on human annotations or external data. However, for most words, evaluating the correctness of them is still an open problem. In this work, we utilize word association test, which contains rich types of word connections annotated by human participants, to explore how gender stereotypes spread within our minds. Specifically, we use random walk on word association graph to derive bias scores for a large amount of words. Experiments show that these bias scores correlate well with bias in the real world. More importantly, comparing with word-embedding-based bias scores, it provides a different perspective on gender stereotypes in words.","year":2019,"title_abstract":"Exploring Human Gender Stereotypes with Word Association Test Word embeddings have been widely used to study gender stereotypes in texts. One key problem regarding existing bias scores is to evaluate their validities: do they really reflect true bias levels? For a small set of words (e.g. occupations), we can rely on human annotations or external data. However, for most words, evaluating the correctness of them is still an open problem. In this work, we utilize word association test, which contains rich types of word connections annotated by human participants, to explore how gender stereotypes spread within our minds. Specifically, we use random walk on word association graph to derive bias scores for a large amount of words. Experiments show that these bias scores correlate well with bias in the real world. More importantly, comparing with word-embedding-based bias scores, it provides a different perspective on gender stereotypes in words.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2378594279,"Goal":"Gender Equality","Task":["Human Gender Stereotypes","Word embeddings"],"Method":["Word Association Test","word association test","random walk","word association graph","word - embedding - based bias scores"]},{"ID":"alam-etal-2021-fighting-covid","title":"Fighting the {COVID}-19 Infodemic: Modeling the Perspective of Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the Society","abstract":"With the emergence of the COVID-19 pandemic, the political and the medical aspects of disinformation merged as the problem got elevated to a whole new level to become the first global infodemic. Fighting this infodemic has been declared one of the most important focus areas of the World Health Organization, with dangers ranging from promoting fake cures, rumors, and conspiracy theories to spreading xenophobia and panic. Addressing the issue requires solving a number of challenging problems such as identifying messages containing claims, determining their check-worthiness and factuality, and their potential to do harm as well as the nature of that harm, to mention just a few. To address this gap, we release a large dataset of 16K manually annotated tweets for fine-grained disinformation analysis that (i) focuses on COVID-19, (ii) combines the perspectives and the interests of journalists, fact-checkers, social media platforms, policy makers, and society, and (iii) covers Arabic, Bulgarian, Dutch, and English. Finally, we show strong evaluation results using pretrained Transformers, thus confirming the practical utility of the dataset in monolingual vs. multilingual, and single task vs. multitask settings.","year":2021,"title_abstract":"Fighting the {COVID}-19 Infodemic: Modeling the Perspective of Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the Society With the emergence of the COVID-19 pandemic, the political and the medical aspects of disinformation merged as the problem got elevated to a whole new level to become the first global infodemic. Fighting this infodemic has been declared one of the most important focus areas of the World Health Organization, with dangers ranging from promoting fake cures, rumors, and conspiracy theories to spreading xenophobia and panic. Addressing the issue requires solving a number of challenging problems such as identifying messages containing claims, determining their check-worthiness and factuality, and their potential to do harm as well as the nature of that harm, to mention just a few. To address this gap, we release a large dataset of 16K manually annotated tweets for fine-grained disinformation analysis that (i) focuses on COVID-19, (ii) combines the perspectives and the interests of journalists, fact-checkers, social media platforms, policy makers, and society, and (iii) covers Arabic, Bulgarian, Dutch, and English. Finally, we show strong evaluation results using pretrained Transformers, thus confirming the practical utility of the dataset in monolingual vs. multilingual, and single task vs. multitask settings.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2371764183,"Goal":"Climate Action","Task":["fine - grained disinformation analysis","multilingual","multitask settings"],"Method":["pretrained Transformers"]},{"ID":"kurita-etal-2019-measuring","title":"Measuring Bias in Contextualized Word Representations","abstract":"Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks. Since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. In this study, we (1) propose a template-based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of Gender Pronoun Resolution. Although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.","year":2019,"title_abstract":"Measuring Bias in Contextualized Word Representations Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks. Since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. In this study, we (1) propose a template-based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of Gender Pronoun Resolution. Although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2364665121,"Goal":"Gender Equality","Task":["Measuring Bias","Contextualized Word Representations","NLP tasks","capturing social biases","gender bias","downstream task","Gender Pronoun Resolution","gender bias","multiclass settings"],"Method":["Contextual word embeddings","BERT","template - based method","BERT;","cosine based method;"]},{"ID":"stambolieva-etal-2017-language","title":"Language Technologies in Teaching Bugarian at Primary and Secondary School Level: the {NBU} Platform of Language Teaching ({PLT})","abstract":"The NBU Language Teaching Platform (PLT) was initially designed for teaching foreign languages for specific purposes; at a second stage, some of its functionalities were extended to answer the needs of teaching general foreign language. New functionalities have now been created for the purpose of providing e-support for Bulgarian language and literature teaching at primary and secondary school level. The article presents the general structure of the platform and the functionalities specifically developed to match the standards and expected results set by the Ministry of Education. The E-platform integrates: 1\/ an environment for creating, organizing and maintaining electronic text archives, for extracting text corpora and aligning corpora; 2\/ a linguistic database; 3\/ a concordancer; 4\/ a set of modules for the generation and editing of practice exercises for each text or corpus; 5\/ functionalities for export from the platform and import to other educational platforms. For Moodle, modules were created for test generation, performance assessment and feedback. The PLT allows centralized presentation of abundant teaching content, control of the educational process, fast and reliable feedback on performance.","year":2017,"title_abstract":"Language Technologies in Teaching Bugarian at Primary and Secondary School Level: the {NBU} Platform of Language Teaching ({PLT}) The NBU Language Teaching Platform (PLT) was initially designed for teaching foreign languages for specific purposes; at a second stage, some of its functionalities were extended to answer the needs of teaching general foreign language. New functionalities have now been created for the purpose of providing e-support for Bulgarian language and literature teaching at primary and secondary school level. The article presents the general structure of the platform and the functionalities specifically developed to match the standards and expected results set by the Ministry of Education. The E-platform integrates: 1\/ an environment for creating, organizing and maintaining electronic text archives, for extracting text corpora and aligning corpora; 2\/ a linguistic database; 3\/ a concordancer; 4\/ a set of modules for the generation and editing of practice exercises for each text or corpus; 5\/ functionalities for export from the platform and import to other educational platforms. For Moodle, modules were created for test generation, performance assessment and feedback. The PLT allows centralized presentation of abundant teaching content, control of the educational process, fast and reliable feedback on performance.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.2363364398,"Goal":"Quality Education","Task":["Language Teaching","e - support","Bulgarian language and literature teaching","organizing and maintaining electronic text archives","extracting text corpora","aligning corpora;","generation and editing of practice exercises","Moodle","test generation","performance assessment","feedback","educational process"],"Method":["Language Technologies","NBU Language Teaching Platform","E - platform","educational platforms","PLT"]},{"ID":"addawood-etal-2020-tracking","title":"Tracking And Understanding Public Reaction During {COVID}-19: {S}audi {A}rabia As A Use Case","abstract":"The coronavirus disease of 2019 (COVID-19) has a huge impact on economies and societies around the world. While governments are taking extreme measures to reduce the spread of the virus, people are getting affected by these new measures. With restrictions like lockdown and social distancing, it became important to understand the emotional response of the public towards the pandemic. In this paper, we study the reaction of Saudi Arabia citizens towards the pandemic. We utilize a collection of Arabic tweets that were sent during 2020, primarily through hashtags that were originated from Saudi Arabia. Our results showed that people had kept a positive reaction towards the pandemic. This positive reaction was at its highest at the beginning of the COVID-19 crisis and started to decline as time passes. Overall, the results showed that people were so supportive of each other through this pandemic. This research can help researchers and policymakers in understanding the emotional effect of a pandemic on societies.","year":2020,"title_abstract":"Tracking And Understanding Public Reaction During {COVID}-19: {S}audi {A}rabia As A Use Case The coronavirus disease of 2019 (COVID-19) has a huge impact on economies and societies around the world. While governments are taking extreme measures to reduce the spread of the virus, people are getting affected by these new measures. With restrictions like lockdown and social distancing, it became important to understand the emotional response of the public towards the pandemic. In this paper, we study the reaction of Saudi Arabia citizens towards the pandemic. We utilize a collection of Arabic tweets that were sent during 2020, primarily through hashtags that were originated from Saudi Arabia. Our results showed that people had kept a positive reaction towards the pandemic. This positive reaction was at its highest at the beginning of the COVID-19 crisis and started to decline as time passes. Overall, the results showed that people were so supportive of each other through this pandemic. This research can help researchers and policymakers in understanding the emotional effect of a pandemic on societies.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2363147885,"Goal":"Climate Action","Task":["Tracking And Understanding Public Reaction","coronavirus disease"],"Method":["hashtags"]},{"ID":"park-etal-2018-reducing","title":"Reducing Gender Bias in Abusive Language Detection","abstract":"Abusive language detection models tend to have a problem of being biased toward identity words of a certain group of people because of imbalanced training datasets. For example, {``}You are a good woman{''} was considered {``}sexist{''} when trained on an existing dataset. Such model bias is an obstacle for models to be robust enough for practical use. In this work, we measure them on models trained with different datasets, while analyzing the effect of different pre-trained word embeddings and model architectures. We also experiment with three mitigation methods: (1) debiased word embeddings, (2) gender swap data augmentation, and (3) fine-tuning with a larger corpus. These methods can effectively reduce model bias by 90-98{\\%} and can be extended to correct model bias in other scenarios.","year":2018,"title_abstract":"Reducing Gender Bias in Abusive Language Detection Abusive language detection models tend to have a problem of being biased toward identity words of a certain group of people because of imbalanced training datasets. For example, {``}You are a good woman{''} was considered {``}sexist{''} when trained on an existing dataset. Such model bias is an obstacle for models to be robust enough for practical use. In this work, we measure them on models trained with different datasets, while analyzing the effect of different pre-trained word embeddings and model architectures. We also experiment with three mitigation methods: (1) debiased word embeddings, (2) gender swap data augmentation, and (3) fine-tuning with a larger corpus. These methods can effectively reduce model bias by 90-98{\\%} and can be extended to correct model bias in other scenarios.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2361650169,"Goal":"Gender Equality","Task":["Reducing Gender Bias","Abusive Language Detection","fine - tuning"],"Method":["Abusive language detection models","word embeddings","model architectures","mitigation methods","gender swap data augmentation"]},{"ID":"bhaskaran-bhallamudi-2019-good","title":"Good Secretaries, Bad Truck Drivers? Occupational Gender Stereotypes in Sentiment Analysis","abstract":"In this work, we investigate the presence of occupational gender stereotypes in sentiment analysis models. Such a task has implications in reducing implicit biases in these models, which are being applied to an increasingly wide variety of downstream tasks. We release a new gender-balanced dataset of 800 sentences pertaining to specific professions and propose a methodology for using it as a test bench to evaluate sentiment analysis models. We evaluate the presence of occupational gender stereotypes in 3 different models using our approach, and explore their relationship with societal perceptions of occupations.","year":2019,"title_abstract":"Good Secretaries, Bad Truck Drivers? Occupational Gender Stereotypes in Sentiment Analysis In this work, we investigate the presence of occupational gender stereotypes in sentiment analysis models. Such a task has implications in reducing implicit biases in these models, which are being applied to an increasingly wide variety of downstream tasks. We release a new gender-balanced dataset of 800 sentences pertaining to specific professions and propose a methodology for using it as a test bench to evaluate sentiment analysis models. We evaluate the presence of occupational gender stereotypes in 3 different models using our approach, and explore their relationship with societal perceptions of occupations.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2360832691,"Goal":"Gender Equality","Task":["Sentiment Analysis","downstream tasks"],"Method":["sentiment analysis models","sentiment analysis models"]},{"ID":"suarez-figueroa-gomez-perez-2008-towards","title":"Towards a Glossary of Activities in the Ontology Engineering Field","abstract":"The Semantic Web of the future will be characterized by using a very large number of ontologies embedded in ontology networks. It is important to provide strong methodological support for collaborative and context-sensitive development of networks of ontologies. This methodological support includes the identification and definition of which activities should be carried out when ontology networks are collaboratively built. In this paper we present the consensus reaching process followed within the NeOn consortium for the identification and definition of the activities involved in the ontology network development process. The consensus reaching process here presented produces as a result the NeOn Glossary of Activities. This work was conceived due to the lack of standardization in the Ontology Engineering terminology, which clearly contrasts with the Software Engineering field. Our future aim is to standardize the NeOn Glossary of Activities.","year":2008,"title_abstract":"Towards a Glossary of Activities in the Ontology Engineering Field The Semantic Web of the future will be characterized by using a very large number of ontologies embedded in ontology networks. It is important to provide strong methodological support for collaborative and context-sensitive development of networks of ontologies. This methodological support includes the identification and definition of which activities should be carried out when ontology networks are collaboratively built. In this paper we present the consensus reaching process followed within the NeOn consortium for the identification and definition of the activities involved in the ontology network development process. The consensus reaching process here presented produces as a result the NeOn Glossary of Activities. This work was conceived due to the lack of standardization in the Ontology Engineering terminology, which clearly contrasts with the Software Engineering field. Our future aim is to standardize the NeOn Glossary of Activities.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.2357085347,"Goal":"Partnership for the Goals","Task":["Glossary of Activities","Ontology Engineering Field","Semantic Web","collaborative and context - sensitive development of networks of ontologies","ontology network development process","consensus reaching process","Ontology Engineering terminology","Software Engineering field"],"Method":["ontology networks","ontology networks","consensus reaching process","NeOn consortium"]},{"ID":"jha-mamidi-2017-compliment","title":"When does a compliment become sexist? Analysis and classification of ambivalent sexism using twitter data","abstract":"Sexism is prevalent in today{'}s society, both offline and online, and poses a credible threat to social equality with respect to gender. According to ambivalent sexism theory (Glick and Fiske, 1996), it comes in two forms: Hostile and Benevolent. While hostile sexism is characterized by an explicitly negative attitude, benevolent sexism is more subtle. Previous works on computationally detecting sexism present online are restricted to identifying the hostile form. Our objective is to investigate the less pronounced form of sexism demonstrated online. We achieve this by creating and analyzing a dataset of tweets that exhibit benevolent sexism. By using Support Vector Machines (SVM), sequence-to-sequence models and FastText classifier, we classify tweets into {`}Hostile{'}, {`}Benevolent{'} or {`}Others{'} class depending on the kind of sexism they exhibit. We have been able to achieve an F1-score of 87.22{\\%} using FastText classifier. Our work helps analyze and understand the much prevalent ambivalent sexism in social media.","year":2017,"title_abstract":"When does a compliment become sexist? Analysis and classification of ambivalent sexism using twitter data Sexism is prevalent in today{'}s society, both offline and online, and poses a credible threat to social equality with respect to gender. According to ambivalent sexism theory (Glick and Fiske, 1996), it comes in two forms: Hostile and Benevolent. While hostile sexism is characterized by an explicitly negative attitude, benevolent sexism is more subtle. Previous works on computationally detecting sexism present online are restricted to identifying the hostile form. Our objective is to investigate the less pronounced form of sexism demonstrated online. We achieve this by creating and analyzing a dataset of tweets that exhibit benevolent sexism. By using Support Vector Machines (SVM), sequence-to-sequence models and FastText classifier, we classify tweets into {`}Hostile{'}, {`}Benevolent{'} or {`}Others{'} class depending on the kind of sexism they exhibit. We have been able to achieve an F1-score of 87.22{\\%} using FastText classifier. Our work helps analyze and understand the much prevalent ambivalent sexism in social media.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2356741428,"Goal":"Gender Equality","Task":["sexist? Analysis","classification of ambivalent sexism","detecting sexism"],"Method":["ambivalent sexism theory","Support Vector Machines","sequence - to - sequence models","FastText classifier","FastText classifier"]},{"ID":"nissim-etal-2020-fair","title":"Fair Is Better than Sensational: Man Is to Doctor as Woman Is to Doctor","abstract":"Analogies such as man is to king as woman is to X are often used to illustrate the amazing power of word embeddings. Concurrently, they have also been used to expose how strongly human biases are encoded in vector spaces trained on natural language, with examples like man is to computer programmer as woman is to homemaker. Recent work has shown that analogies are in fact not an accurate diagnostic for bias, but this does not mean that they are not used anymore, or that their legacy is fading. Instead of focusing on the intrinsic problems of the analogy task as a bias detection tool, we discuss a series of issues involving implementation as well as subjective choices that might have yielded a distorted picture of bias in word embeddings. We stand by the truth that human biases are present in word embeddings, and, of course, the need to address them. But analogies are not an accurate tool to do so, and the way they have been most often used has exacerbated some possibly non-existing biases and perhaps hidden others. Because they are still widely popular, and some of them have become classics within and outside the NLP community, we deem it important to provide a series of clarifications that should put well-known, and potentially new analogies, into the right perspective.","year":2020,"title_abstract":"Fair Is Better than Sensational: Man Is to Doctor as Woman Is to Doctor Analogies such as man is to king as woman is to X are often used to illustrate the amazing power of word embeddings. Concurrently, they have also been used to expose how strongly human biases are encoded in vector spaces trained on natural language, with examples like man is to computer programmer as woman is to homemaker. Recent work has shown that analogies are in fact not an accurate diagnostic for bias, but this does not mean that they are not used anymore, or that their legacy is fading. Instead of focusing on the intrinsic problems of the analogy task as a bias detection tool, we discuss a series of issues involving implementation as well as subjective choices that might have yielded a distorted picture of bias in word embeddings. We stand by the truth that human biases are present in word embeddings, and, of course, the need to address them. But analogies are not an accurate tool to do so, and the way they have been most often used has exacerbated some possibly non-existing biases and perhaps hidden others. Because they are still widely popular, and some of them have become classics within and outside the NLP community, we deem it important to provide a series of clarifications that should put well-known, and potentially new analogies, into the right perspective.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2346937954,"Goal":"Gender Equality","Task":["bias","analogy task","implementation","NLP community"],"Method":["word embeddings","vector spaces","bias detection tool"]},{"ID":"kluck-huckstorf-2008-european","title":"The {E}uropean Thesaurus on International Relations and Area Studies - a Multilingual Resource for Indexing, Retrieval, and Translation","abstract":"The multilingual European Thesaurus on International Relations and Area Studies (European Thesaurus) is a special subject thesaurus for the field of international affairs. It is intended for use in libraries and documentation centres of academic institutions and international organizations. The European Thesaurus was established in a collaborative project involving a number of leading European research institutes on international politics. It integrates the controlled terminologies of several existing thesauri. The European Thesaurus comprises about 8,200 terms and proper names from the 24 subject areas covered by the thesaurus. Because of its multilinguality, the European Thesaurus can not only be used for indexing, retrieval and terminological reference, but serves also as a translation tool for the languages represented. The establishment of cross-concordances to related thesauri extends the range of application of the European Thesaurus even further. They enable the treatment of semantic heterogeneity within subject gateways. The European Thesaurus is available both in a seven-lingual print-version as well as in an eight-lingual online-version. To reflect the changes in terminology the European Thesau-rus is regularly being amended and modified. Further languages are going to be included.","year":2008,"title_abstract":"The {E}uropean Thesaurus on International Relations and Area Studies - a Multilingual Resource for Indexing, Retrieval, and Translation The multilingual European Thesaurus on International Relations and Area Studies (European Thesaurus) is a special subject thesaurus for the field of international affairs. It is intended for use in libraries and documentation centres of academic institutions and international organizations. The European Thesaurus was established in a collaborative project involving a number of leading European research institutes on international politics. It integrates the controlled terminologies of several existing thesauri. The European Thesaurus comprises about 8,200 terms and proper names from the 24 subject areas covered by the thesaurus. Because of its multilinguality, the European Thesaurus can not only be used for indexing, retrieval and terminological reference, but serves also as a translation tool for the languages represented. The establishment of cross-concordances to related thesauri extends the range of application of the European Thesaurus even further. They enable the treatment of semantic heterogeneity within subject gateways. The European Thesaurus is available both in a seven-lingual print-version as well as in an eight-lingual online-version. To reflect the changes in terminology the European Thesau-rus is regularly being amended and modified. Further languages are going to be included.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.2341911197,"Goal":"Peace, Justice and Strong Institutions","Task":["International Relations and Area Studies","Indexing","Retrieval","Translation","multilingual European Thesaurus","International Relations and Area Studies","international affairs","libraries and documentation centres of academic institutions","indexing","retrieval","terminological reference"],"Method":["translation tool"]},{"ID":"friedman-etal-2019-relating","title":"Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis","abstract":"Modern models for common NLP tasks often employ machine learning techniques and train on journalistic, social media, or other culturally-derived text. These have recently been scrutinized for racial and gender biases, rooting from inherent bias in their training text. These biases are often sub-optimal and recent work poses methods to rectify them; however, these biases may shed light on actual racial or gender gaps in the culture(s) that produced the training text, thereby helping us understand cultural context through big data. This paper presents an approach for quantifying gender bias in word embeddings, and then using them to characterize statistical gender gaps in education, politics, economics, and health. We validate these metrics on 2018 Twitter data spanning 51 U.S. regions and 99 countries. We correlate state and country word embedding biases with 18 international and 5 U.S.-based statistical gender gaps, characterizing regularities and predictive strength.","year":2019,"title_abstract":"Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis Modern models for common NLP tasks often employ machine learning techniques and train on journalistic, social media, or other culturally-derived text. These have recently been scrutinized for racial and gender biases, rooting from inherent bias in their training text. These biases are often sub-optimal and recent work poses methods to rectify them; however, these biases may shed light on actual racial or gender gaps in the culture(s) that produced the training text, thereby helping us understand cultural context through big data. This paper presents an approach for quantifying gender bias in word embeddings, and then using them to characterize statistical gender gaps in education, politics, economics, and health. We validate these metrics on 2018 Twitter data spanning 51 U.S. regions and 99 countries. We correlate state and country word embedding biases with 18 international and 5 U.S.-based statistical gender gaps, characterizing regularities and predictive strength.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2339416444,"Goal":"Gender Equality","Task":["Word Embedding Gender Biases","Gender Gaps","NLP tasks","quantifying gender bias","word embeddings","statistical gender gaps","education","economics","health"],"Method":["Cross - Cultural Analysis","machine learning techniques"]},{"ID":"de-vassimon-manela-etal-2021-stereotype","title":"Stereotype and Skew: Quantifying Gender Bias in Pre-trained and Fine-tuned Language Models","abstract":"This paper proposes two intuitive metrics, skew and stereotype, that quantify and analyse the gender bias present in contextual language models when tackling the WinoBias pronoun resolution task. We find evidence that gender stereotype correlates approximately negatively with gender skew in out-of-the-box models, suggesting that there is a trade-off between these two forms of bias. We investigate two methods to mitigate bias. The first approach is an online method which is effective at removing skew at the expense of stereotype. The second, inspired by previous work on ELMo, involves the fine-tuning of BERT using an augmented gender-balanced dataset. We show that this reduces both skew and stereotype relative to its unaugmented fine-tuned counterpart. However, we find that existing gender bias benchmarks do not fully probe professional bias as pronoun resolution may be obfuscated by cross-correlations from other manifestations of gender prejudice.","year":2021,"title_abstract":"Stereotype and Skew: Quantifying Gender Bias in Pre-trained and Fine-tuned Language Models This paper proposes two intuitive metrics, skew and stereotype, that quantify and analyse the gender bias present in contextual language models when tackling the WinoBias pronoun resolution task. We find evidence that gender stereotype correlates approximately negatively with gender skew in out-of-the-box models, suggesting that there is a trade-off between these two forms of bias. We investigate two methods to mitigate bias. The first approach is an online method which is effective at removing skew at the expense of stereotype. The second, inspired by previous work on ELMo, involves the fine-tuning of BERT using an augmented gender-balanced dataset. We show that this reduces both skew and stereotype relative to its unaugmented fine-tuned counterpart. However, we find that existing gender bias benchmarks do not fully probe professional bias as pronoun resolution may be obfuscated by cross-correlations from other manifestations of gender prejudice.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2338724434,"Goal":"Gender Equality","Task":["Quantifying Gender Bias","WinoBias pronoun resolution task","skew","fine - tuning of BERT","pronoun resolution"],"Method":["Fine - tuned Language Models","contextual language models","out - of - the - box models","online method","ELMo","unaugmented fine - tuned counterpart"]},{"ID":"jin-etal-2021-mining-cause","title":"Mining the Cause of Political Decision-Making from Social Media: A Case Study of {COVID}-19 Policies across the {US} States","abstract":"Mining the causes of political decision-making is an active research area in the field of political science. In the past, most studies have focused on long-term policies that are collected over several decades of time, and have primarily relied on surveys as the main source of predictors. However, the recent COVID-19 pandemic has given rise to a new political phenomenon, where political decision-making consists of frequent short-term decisions, all on the same controlled topic{---}the pandemic. In this paper, we focus on the question of how public opinion influences policy decisions, while controlling for confounders such as COVID-19 case increases or unemployment rates. Using a dataset consisting of Twitter data from the 50 US states, we classify the sentiments toward governors of each state, and conduct controlled studies and comparisons. Based on the compiled samples of sentiments, policies, and confounders, we conduct causal inference to discover trends in political decision-making across different states.","year":2021,"title_abstract":"Mining the Cause of Political Decision-Making from Social Media: A Case Study of {COVID}-19 Policies across the {US} States Mining the causes of political decision-making is an active research area in the field of political science. In the past, most studies have focused on long-term policies that are collected over several decades of time, and have primarily relied on surveys as the main source of predictors. However, the recent COVID-19 pandemic has given rise to a new political phenomenon, where political decision-making consists of frequent short-term decisions, all on the same controlled topic{---}the pandemic. In this paper, we focus on the question of how public opinion influences policy decisions, while controlling for confounders such as COVID-19 case increases or unemployment rates. Using a dataset consisting of Twitter data from the 50 US states, we classify the sentiments toward governors of each state, and conduct controlled studies and comparisons. Based on the compiled samples of sentiments, policies, and confounders, we conduct causal inference to discover trends in political decision-making across different states.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2337893248,"Goal":"Climate Action","Task":["Mining the Cause of Political Decision - Making","political decision - making","political science","political decision - making","policy decisions","political decision - making"],"Method":["causal inference"]},{"ID":"ciora-etal-2021-examining","title":"Examining Covert Gender Bias: A Case Study in {T}urkish and {E}nglish Machine Translation Models","abstract":"As Machine Translation (MT) has become increasingly more powerful, accessible, and widespread, the potential for the perpetuation of bias has grown alongside its advances. While overt indicators of bias have been studied in machine translation, we argue that covert biases expose a problem that is further entrenched. Through the use of the gender-neutral language Turkish and the gendered language English, we examine cases of both overt and covert gender bias in MT models. Specifically, we introduce a method to investigate asymmetrical gender markings. We also assess bias in the attribution of personhood and examine occupational and personality stereotypes through overt bias indicators in MT models. Our work explores a deeper layer of bias in MT models and demonstrates the continued need for language-specific, interdisciplinary methodology in MT model development.","year":2021,"title_abstract":"Examining Covert Gender Bias: A Case Study in {T}urkish and {E}nglish Machine Translation Models As Machine Translation (MT) has become increasingly more powerful, accessible, and widespread, the potential for the perpetuation of bias has grown alongside its advances. While overt indicators of bias have been studied in machine translation, we argue that covert biases expose a problem that is further entrenched. Through the use of the gender-neutral language Turkish and the gendered language English, we examine cases of both overt and covert gender bias in MT models. Specifically, we introduce a method to investigate asymmetrical gender markings. We also assess bias in the attribution of personhood and examine occupational and personality stereotypes through overt bias indicators in MT models. Our work explores a deeper layer of bias in MT models and demonstrates the continued need for language-specific, interdisciplinary methodology in MT model development.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2334867418,"Goal":"Gender Equality","Task":["Covert Gender Bias","Machine Translation","bias","machine translation","overt and covert gender bias","MT","attribution of personhood","MT models","MT"],"Method":["{E}nglish Machine Translation Models","MT models"]},{"ID":"mehta-etal-2020-learnings","title":"Learnings from Technological Interventions in a Low Resource Language: A Case-Study on {G}ondi","abstract":"The primary obstacle to developing technologies for low-resource languages is the lack of usable data. In this paper, we report the adaption and deployment of 4 technology-driven methods of data collection for Gondi, a low-resource vulnerable language spoken by around 2.3 million tribal people in south and central India. In the process of data collection, we also help in its revival by expanding access to information in Gondi through the creation of linguistic resources that can be used by the community, such as a dictionary, children{'}s stories, an app with Gondi content from multiple sources and an Interactive Voice Response (IVR) based mass awareness platform. At the end of these interventions, we collected a little less than 12,000 translated words and\/or sentences and identified more than 650 community members whose help can be solicited for future translation efforts. The larger goal of the project is collecting enough data in Gondi to build and deploy viable language technologies like machine translation and speech to text systems that can help take the language onto the internet.","year":2020,"title_abstract":"Learnings from Technological Interventions in a Low Resource Language: A Case-Study on {G}ondi The primary obstacle to developing technologies for low-resource languages is the lack of usable data. In this paper, we report the adaption and deployment of 4 technology-driven methods of data collection for Gondi, a low-resource vulnerable language spoken by around 2.3 million tribal people in south and central India. In the process of data collection, we also help in its revival by expanding access to information in Gondi through the creation of linguistic resources that can be used by the community, such as a dictionary, children{'}s stories, an app with Gondi content from multiple sources and an Interactive Voice Response (IVR) based mass awareness platform. At the end of these interventions, we collected a little less than 12,000 translated words and\/or sentences and identified more than 650 community members whose help can be solicited for future translation efforts. The larger goal of the project is collecting enough data in Gondi to build and deploy viable language technologies like machine translation and speech to text systems that can help take the language onto the internet.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.233102113,"Goal":"Sustainable Cities and Communities","Task":["Technological Interventions","data collection","Gondi","data collection","translation efforts","machine translation","speech to text systems"],"Method":["{G}ondi","technology - driven methods","Interactive Voice Response","mass awareness platform","language technologies"]},{"ID":"ciobanu-etal-2006-using","title":"Using Richly Annotated Trilingual Language Resources for Acquiring Reading Skills in a Foreign Language","abstract":"In an age when demand for innovative and motivating language teaching methodologies is at a very high level, TREAT - the Trilingual REAding Tutor - combines the most advanced natural language processing (NLP) techniques with the latest second and third language acquisition (SLA\/TLA) research in an intuitive and user-friendly environment that has been proven to help adult learners (native speakers of L1) acquire reading skills in an unknown L3 which is related to (cognate with) an L2 they know to some extent. This corpus-based methodology relies on existing linguistic resources, as well as materials that are easy to assemble, and can be adapted to support other pairs of L2-L3 related languages, as well. A small evaluation study conducted at the Leeds University Centre for Translation Studies indicates that, when using TREAT, learners feel more motivated to study an unknown L3, acquire significant linguistic knowledge of both the L3 and L2 rapidly, and increase their performance when translating from L3 into L1.","year":2006,"title_abstract":"Using Richly Annotated Trilingual Language Resources for Acquiring Reading Skills in a Foreign Language In an age when demand for innovative and motivating language teaching methodologies is at a very high level, TREAT - the Trilingual REAding Tutor - combines the most advanced natural language processing (NLP) techniques with the latest second and third language acquisition (SLA\/TLA) research in an intuitive and user-friendly environment that has been proven to help adult learners (native speakers of L1) acquire reading skills in an unknown L3 which is related to (cognate with) an L2 they know to some extent. This corpus-based methodology relies on existing linguistic resources, as well as materials that are easy to assemble, and can be adapted to support other pairs of L2-L3 related languages, as well. A small evaluation study conducted at the Leeds University Centre for Translation Studies indicates that, when using TREAT, learners feel more motivated to study an unknown L3, acquire significant linguistic knowledge of both the L3 and L2 rapidly, and increase their performance when translating from L3 into L1.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.2325565666,"Goal":"Quality Education","Task":["Reading Skills","language acquisition","Translation Studies"],"Method":["language teaching methodologies","TREAT","Trilingual REAding Tutor","natural language processing","corpus - based methodology","TREAT"]},{"ID":"cao-etal-2020-towards","title":"Towards Accurate and Reliable Energy Measurement of {NLP} Models","abstract":"Accurate and reliable measurement of energy consumption is critical for making well-informed design choices when choosing and training large scale NLP models. In this work, we show that existing software-based energy estimations are not accurate because they do not take into account hardware differences and how resource utilization affects energy consumption. We conduct energy measurement experiments with four different models for a question answering task. We quantify the error of existing software-based energy estimations by using a hardware power meter that provides highly accurate energy measurements. Our key takeaway is the need for a more accurate energy estimation model that takes into account hardware variabilities and the non-linear relationship between resource utilization and energy consumption. We release the code and data at https:\/\/github.com\/csarron\/sustainlp2020-energy.","year":2020,"title_abstract":"Towards Accurate and Reliable Energy Measurement of {NLP} Models Accurate and reliable measurement of energy consumption is critical for making well-informed design choices when choosing and training large scale NLP models. In this work, we show that existing software-based energy estimations are not accurate because they do not take into account hardware differences and how resource utilization affects energy consumption. We conduct energy measurement experiments with four different models for a question answering task. We quantify the error of existing software-based energy estimations by using a hardware power meter that provides highly accurate energy measurements. Our key takeaway is the need for a more accurate energy estimation model that takes into account hardware variabilities and the non-linear relationship between resource utilization and energy consumption. We release the code and data at https:\/\/github.com\/csarron\/sustainlp2020-energy.","social_need":"Affordable and Clean Energy Ensure access to affordable, reliable, sustainable and modern energy for all","cosine_similarity":0.2323961556,"Goal":"Affordable and Clean Energy","Task":["measurement of energy consumption","well - informed design choices","large scale NLP models","energy measurement","question answering task"],"Method":["software - based energy estimations","software - based energy estimations","hardware power meter","energy estimation model"]},{"ID":"chakravarthi-2020-hopeedi","title":"{H}ope{EDI}: A Multilingual Hope Speech Detection Dataset for Equality, Diversity, and Inclusion","abstract":"Over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. However, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. Therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. Until now, most studies have focused on solving this problem of negativity in the English language, though the problem is much more than just harmful content. Furthermore, it is multilingual as well. Thus, we have constructed a Hope Speech dataset for Equality, Diversity and Inclusion (HopeEDI) containing user-generated comments from the social media platform YouTube with 28,451, 20,198 and 10,705 comments in English, Tamil and Malayalam, respectively, manually labelled as containing hope speech or not. To our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. We determined that the inter-annotator agreement of our dataset using Krippendorff{'}s alpha. Further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and F1-score. The dataset is publicly available for the research community. We hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.","year":2020,"title_abstract":"{H}ope{EDI}: A Multilingual Hope Speech Detection Dataset for Equality, Diversity, and Inclusion Over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. However, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. Therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. Until now, most studies have focused on solving this problem of negativity in the English language, though the problem is much more than just harmful content. Furthermore, it is multilingual as well. Thus, we have constructed a Hope Speech dataset for Equality, Diversity and Inclusion (HopeEDI) containing user-generated comments from the social media platform YouTube with 28,451, 20,198 and 10,705 comments in English, Tamil and Malayalam, respectively, manually labelled as containing hope speech or not. To our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. We determined that the inter-annotator agreement of our dataset using Krippendorff{'}s alpha. Further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and F1-score. The dataset is publicly available for the research community. We hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2319331914,"Goal":"Gender Equality","Task":["Equality","Inclusion","inclusion","multilingual setting"],"Method":["positive reinforcement approach","Krippendorff{'}s alpha"]},{"ID":"di-donato-etal-2020-social","title":"Social Sciences and Humanities Pathway Towards the {E}uropean Open Science Cloud","abstract":"The paper presents a journey, which starts from various social sciences and humanities (SSH) Research Infrastructures in Europe and arrives at the comprehensive {``}ecosystem of infrastructures{''}, namely the European Open Science Cloud (EOSC). We will highlight how the SSH Open Science infrastructures contribute to the goal of establishing the EOSC. First, through the example of OPERAS, the European Research Infrastructure for Open Scholarly Communication in the SSH, to see how its services are conceived to be part of the EOSC and to address the communities{'} needs. The next two sections highlight collaboration practices between partners in Europe to build the SSH component of the EOSC and a SSH discovery platform, as a service of OPERAS and the EOSC. The last two sections will focus on an implementation network dedicated to SSH data fairification.","year":2020,"title_abstract":"Social Sciences and Humanities Pathway Towards the {E}uropean Open Science Cloud The paper presents a journey, which starts from various social sciences and humanities (SSH) Research Infrastructures in Europe and arrives at the comprehensive {``}ecosystem of infrastructures{''}, namely the European Open Science Cloud (EOSC). We will highlight how the SSH Open Science infrastructures contribute to the goal of establishing the EOSC. First, through the example of OPERAS, the European Research Infrastructure for Open Scholarly Communication in the SSH, to see how its services are conceived to be part of the EOSC and to address the communities{'} needs. The next two sections highlight collaboration practices between partners in Europe to build the SSH component of the EOSC and a SSH discovery platform, as a service of OPERAS and the EOSC. The last two sections will focus on an implementation network dedicated to SSH data fairification.","social_need":"Industry, Innovation and Infrastrucure Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation","cosine_similarity":0.2316279113,"Goal":"Industry, Innovation and Infrastrucure","Task":["Open Scholarly Communication","SSH","SSH data fairification"],"Method":["Social Sciences and Humanities Pathway","social sciences","European Open Science Cloud","SSH","EOSC","OPERAS","EOSC","SSH","EOSC","SSH discovery platform","OPERAS","EOSC"]},{"ID":"loon-etal-2020-explaining","title":"Explaining the Trump Gap in Social Distancing Using {COVID} Discourse","abstract":"Our ability to limit the future spread of COVID-19 will in part depend on our understanding of the psychological and sociological processes that lead people to follow or reject coronavirus health behaviors. We argue that the virus has taken on heterogeneous meanings in communities across the United States and that these disparate meanings shaped communities{'} response to the virus during the early, vital stages of the outbreak in the U.S. Using word embeddings, we demonstrate that counties where residents socially distanced less on average (as measured by residential mobility) more semantically associated the virus in their COVID discourse with concepts of fraud, the political left, and more benign illnesses like the flu. We also show that the different meanings the virus took on in different communities explains a substantial fraction of what we call the {``}{''}Trump Gap{''}, or the empirical tendency for more Trump-supporting counties to socially distance less. This work demonstrates that community-level processes of meaning-making in part determined behavioral responses to the COVID-19 pandemic and that these processes can be measured unobtrusively using Twitter.","year":2020,"title_abstract":"Explaining the Trump Gap in Social Distancing Using {COVID} Discourse Our ability to limit the future spread of COVID-19 will in part depend on our understanding of the psychological and sociological processes that lead people to follow or reject coronavirus health behaviors. We argue that the virus has taken on heterogeneous meanings in communities across the United States and that these disparate meanings shaped communities{'} response to the virus during the early, vital stages of the outbreak in the U.S. Using word embeddings, we demonstrate that counties where residents socially distanced less on average (as measured by residential mobility) more semantically associated the virus in their COVID discourse with concepts of fraud, the political left, and more benign illnesses like the flu. We also show that the different meanings the virus took on in different communities explains a substantial fraction of what we call the {``}{''}Trump Gap{''}, or the empirical tendency for more Trump-supporting counties to socially distance less. This work demonstrates that community-level processes of meaning-making in part determined behavioral responses to the COVID-19 pandemic and that these processes can be measured unobtrusively using Twitter.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2311490476,"Goal":"Climate Action","Task":["Trump Gap","Social Distancing","COVID - 19","psychological and sociological processes","meaning - making","COVID - 19"],"Method":["word embeddings"]},{"ID":"xu-yang-2019-look","title":"Look Again at the Syntax: Relational Graph Convolutional Network for Gendered Ambiguous Pronoun Resolution","abstract":"Gender bias has been found in existing coreference resolvers. In order to eliminate gender bias, a gender-balanced dataset Gendered Ambiguous Pronouns (GAP) has been released and the best baseline model achieves only 66.9{\\%} F1. Bidirectional Encoder Representations from Transformers (BERT) has broken several NLP task records and can be used on GAP dataset. However, fine-tune BERT on a specific task is computationally expensive. In this paper, we propose an end-to-end resolver by combining pre-trained BERT with Relational Graph Convolutional Network (R-GCN). R-GCN is used for digesting structural syntactic information and learning better task-specific embeddings. Empirical results demonstrate that, under explicit syntactic supervision and without the need to fine tune BERT, R-GCN{'}s embeddings outperform the original BERT embeddings on the coreference task. Our work significantly improves the snippet-context baseline F1 score on GAP dataset from 66.9{\\%} to 80.3{\\%}. We participated in the Gender Bias for Natural Language Processing 2019 shared task, and our codes are available online.","year":2019,"title_abstract":"Look Again at the Syntax: Relational Graph Convolutional Network for Gendered Ambiguous Pronoun Resolution Gender bias has been found in existing coreference resolvers. In order to eliminate gender bias, a gender-balanced dataset Gendered Ambiguous Pronouns (GAP) has been released and the best baseline model achieves only 66.9{\\%} F1. Bidirectional Encoder Representations from Transformers (BERT) has broken several NLP task records and can be used on GAP dataset. However, fine-tune BERT on a specific task is computationally expensive. In this paper, we propose an end-to-end resolver by combining pre-trained BERT with Relational Graph Convolutional Network (R-GCN). R-GCN is used for digesting structural syntactic information and learning better task-specific embeddings. Empirical results demonstrate that, under explicit syntactic supervision and without the need to fine tune BERT, R-GCN{'}s embeddings outperform the original BERT embeddings on the coreference task. Our work significantly improves the snippet-context baseline F1 score on GAP dataset from 66.9{\\%} to 80.3{\\%}. We participated in the Gender Bias for Natural Language Processing 2019 shared task, and our codes are available online.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2310794145,"Goal":"Gender Equality","Task":["Gendered Ambiguous Pronoun Resolution Gender bias","coreference resolvers","NLP","task - specific embeddings","coreference task","Gender Bias","Natural Language Processing"],"Method":["Relational Graph Convolutional Network","Bidirectional Encoder Representations","Transformers","BERT","end - to - end resolver","BERT","Relational Graph Convolutional Network","- GCN)","R - GCN","BERT","R - GCN{'}s embeddings","BERT"]},{"ID":"cao-daume-iii-2021-toward","title":"Toward Gender-Inclusive Coreference Resolution: An Analysis of Gender and Bias Throughout the Machine Learning Lifecycle*","abstract":"Abstract Correctly resolving textual mentions of people fundamentally entails making inferences about those people. Such inferences raise the risk of systematic biases in coreference resolution systems, including biases that can harm binary and non-binary trans and cis stakeholders. To better understand such biases, we foreground nuanced conceptualizations of gender from sociology and sociolinguistics, and investigate where in the machine learning pipeline such biases can enter a coreference resolution system. We inspect many existing data sets for trans-exclusionary biases, and develop two new data sets for interrogating bias in both crowd annotations and in existing coreference resolution systems. Through these studies, conducted on English text, we confirm that without acknowledging and building systems that recognize the complexity of gender, we will build systems that fail for: quality of service, stereotyping, and over- or under-representation, especially for binary and non-binary trans users.","year":2021,"title_abstract":"Toward Gender-Inclusive Coreference Resolution: An Analysis of Gender and Bias Throughout the Machine Learning Lifecycle* Abstract Correctly resolving textual mentions of people fundamentally entails making inferences about those people. Such inferences raise the risk of systematic biases in coreference resolution systems, including biases that can harm binary and non-binary trans and cis stakeholders. To better understand such biases, we foreground nuanced conceptualizations of gender from sociology and sociolinguistics, and investigate where in the machine learning pipeline such biases can enter a coreference resolution system. We inspect many existing data sets for trans-exclusionary biases, and develop two new data sets for interrogating bias in both crowd annotations and in existing coreference resolution systems. Through these studies, conducted on English text, we confirm that without acknowledging and building systems that recognize the complexity of gender, we will build systems that fail for: quality of service, stereotyping, and over- or under-representation, especially for binary and non-binary trans users.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2307037264,"Goal":"Gender Equality","Task":["Gender - Inclusive Coreference Resolution","Gender and Bias","Machine Learning Lifecycle*","resolving textual mentions of people","coreference resolution systems","trans - exclusionary biases","interrogating bias","over - or under - representation","binary and non - binary trans users"],"Method":["machine learning pipeline","coreference resolution system","coreference resolution systems"]},{"ID":"zhao-etal-2020-gender","title":"Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer","abstract":"Multilingual representations embed words from many languages into a single semantic space such that words with similar meanings are close to each other regardless of the language. These embeddings have been widely used in various settings, such as cross-lingual transfer, where a natural language processing (NLP) model trained on one language is deployed to another language. While the cross-lingual transfer techniques are powerful, they carry gender bias from the source to target languages. In this paper, we study gender bias in multilingual embeddings and how it affects transfer learning for NLP applications. We create a multilingual dataset for bias analysis and propose several ways for quantifying bias in multilingual representations from both the intrinsic and extrinsic perspectives. Experimental results show that the magnitude of bias in the multilingual representations changes differently when we align the embeddings to different target spaces and that the alignment direction can also have an influence on the bias in transfer learning. We further provide recommendations for using the multilingual word representations for downstream tasks.","year":2020,"title_abstract":"Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer Multilingual representations embed words from many languages into a single semantic space such that words with similar meanings are close to each other regardless of the language. These embeddings have been widely used in various settings, such as cross-lingual transfer, where a natural language processing (NLP) model trained on one language is deployed to another language. While the cross-lingual transfer techniques are powerful, they carry gender bias from the source to target languages. In this paper, we study gender bias in multilingual embeddings and how it affects transfer learning for NLP applications. We create a multilingual dataset for bias analysis and propose several ways for quantifying bias in multilingual representations from both the intrinsic and extrinsic perspectives. Experimental results show that the magnitude of bias in the multilingual representations changes differently when we align the embeddings to different target spaces and that the alignment direction can also have an influence on the bias in transfer learning. We further provide recommendations for using the multilingual word representations for downstream tasks.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2304923683,"Goal":"Gender Equality","Task":["Gender Bias","Multilingual Embeddings","Cross - Lingual Transfer","cross - lingual transfer","multilingual embeddings","NLP","bias analysis","bias","multilingual representations","transfer learning","downstream tasks"],"Method":["Multilingual representations","natural language processing (NLP) model","cross - lingual transfer techniques","transfer learning","multilingual representations","multilingual word representations"]},{"ID":"schwartz-2022-primum","title":"{P}rimum {N}on {N}ocere: {B}efore working with {I}ndigenous data, the {ACL} must confront ongoing colonialism","abstract":"In this paper, we challenge the ACL community to reckon with historical and ongoing colonialism by adopting a set of ethical obligations and best practices drawn from the Indigenous studies literature. While the vast majority of NLP research focuses on a very small number of very high resource languages (English, Chinese, etc), some work has begun to engage with Indigenous languages. No research involving Indigenous language data can be considered ethical without first acknowledging that Indigenous languages are not merely very low resource languages. The toxic legacy of colonialism permeates every aspect of interaction between Indigenous communities and outside researchers. To this end, we propose that the ACL draft and adopt an ethical framework for NLP researchers and computational linguists wishing to engage in research involving Indigenous languages.","year":2022,"title_abstract":"{P}rimum {N}on {N}ocere: {B}efore working with {I}ndigenous data, the {ACL} must confront ongoing colonialism In this paper, we challenge the ACL community to reckon with historical and ongoing colonialism by adopting a set of ethical obligations and best practices drawn from the Indigenous studies literature. While the vast majority of NLP research focuses on a very small number of very high resource languages (English, Chinese, etc), some work has begun to engage with Indigenous languages. No research involving Indigenous language data can be considered ethical without first acknowledging that Indigenous languages are not merely very low resource languages. The toxic legacy of colonialism permeates every aspect of interaction between Indigenous communities and outside researchers. To this end, we propose that the ACL draft and adopt an ethical framework for NLP researchers and computational linguists wishing to engage in research involving Indigenous languages.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.2300679088,"Goal":"Peace, Justice and Strong Institutions","Task":["NLP research","NLP","computational linguists"],"Method":["{ACL}","ethical framework"]},{"ID":"romberg-conrad-2021-citizen","title":"Citizen Involvement in Urban Planning - How Can Municipalities Be Supported in Evaluating Public Participation Processes for Mobility Transitions?","abstract":"Public participation processes allow citizens to engage in municipal decision-making processes by expressing their opinions on specific issues. Municipalities often only have limited resources to analyze a possibly large amount of textual contributions that need to be evaluated in a timely and detailed manner. Automated support for the evaluation is therefore essential, e.g. to analyze arguments. In this paper, we address (A) the identification of argumentative discourse units and (B) their classification as major position or premise in German public participation processes. The objective of our work is to make argument mining viable for use in municipalities. We compare different argument mining approaches and develop a generic model that can successfully detect argument structures in different datasets of mobility-related urban planning. We introduce a new data corpus comprising five public participation processes. In our evaluation, we achieve high macro F1 scores (0.76 - 0.80 for the identification of argumentative units; 0.86 - 0.93 for their classification) on all datasets. Additionally, we improve previous results for the classification of argumentative units on a similar German online participation dataset.","year":2021,"title_abstract":"Citizen Involvement in Urban Planning - How Can Municipalities Be Supported in Evaluating Public Participation Processes for Mobility Transitions? Public participation processes allow citizens to engage in municipal decision-making processes by expressing their opinions on specific issues. Municipalities often only have limited resources to analyze a possibly large amount of textual contributions that need to be evaluated in a timely and detailed manner. Automated support for the evaluation is therefore essential, e.g. to analyze arguments. In this paper, we address (A) the identification of argumentative discourse units and (B) their classification as major position or premise in German public participation processes. The objective of our work is to make argument mining viable for use in municipalities. We compare different argument mining approaches and develop a generic model that can successfully detect argument structures in different datasets of mobility-related urban planning. We introduce a new data corpus comprising five public participation processes. In our evaluation, we achieve high macro F1 scores (0.76 - 0.80 for the identification of argumentative units; 0.86 - 0.93 for their classification) on all datasets. Additionally, we improve previous results for the classification of argumentative units on a similar German online participation dataset.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2297293842,"Goal":"Sustainable Cities and Communities","Task":["Citizen Involvement","Urban Planning","Evaluating Public Participation Processes","Mobility Transitions? Public participation processes","municipal decision - making processes","evaluation","identification of argumentative discourse units","classification","German public participation processes","argument mining","mobility - related urban planning","public participation processes","identification of argumentative units;","classification)","classification of argumentative units"],"Method":["argument mining approaches"]},{"ID":"willis-etal-2010-xml","title":"From {XML} to {XML}: The Why and How of Making the Biodiversity Literature Accessible to Researchers","abstract":"We present the ABLE document collection, which consists of a set of annotated volumes of the Bulletin of the British Museum (Natural History). These were developed during our ongoing work on automating the markup of scanned copies of the biodiversity literature. Such automation is required if historic literature is to be used to inform contemporary issues in biodiversity research. We consider an enhanced TEI XML markup language, which is used as an intermediate stage in translating from the initial XML obtained from Optical Character Recognition to taXMLit, the target annotation schema. The intermediate representation allows additional information from external sources such as a taxonomic thesaurus to be incorporated before the final translation into taXMLit. We give an overview of the project workflow in automating the markup process, and consider what extensions to existing markup schema will be required to best support working taxonomists. Finally, we discuss some of the particular issues which were encountered in converting between different XML formats.","year":2010,"title_abstract":"From {XML} to {XML}: The Why and How of Making the Biodiversity Literature Accessible to Researchers We present the ABLE document collection, which consists of a set of annotated volumes of the Bulletin of the British Museum (Natural History). These were developed during our ongoing work on automating the markup of scanned copies of the biodiversity literature. Such automation is required if historic literature is to be used to inform contemporary issues in biodiversity research. We consider an enhanced TEI XML markup language, which is used as an intermediate stage in translating from the initial XML obtained from Optical Character Recognition to taXMLit, the target annotation schema. The intermediate representation allows additional information from external sources such as a taxonomic thesaurus to be incorporated before the final translation into taXMLit. We give an overview of the project workflow in automating the markup process, and consider what extensions to existing markup schema will be required to best support working taxonomists. Finally, we discuss some of the particular issues which were encountered in converting between different XML formats.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.2293127477,"Goal":"Life on Land","Task":["markup of scanned copies of the biodiversity literature","biodiversity research","translation","taXMLit","markup process"],"Method":["TEI XML markup language","Optical Character Recognition","taXMLit","annotation schema","intermediate representation","markup schema"]},{"ID":"li-etal-2017-nlp","title":"An {NLP} Analysis of Exaggerated Claims in Science News","abstract":"The discrepancy between science and media has been affecting the effectiveness of science communication. Original findings from science publications may be distorted with altered claim strength when reported to the public, causing misinformation spread. This study conducts an NLP analysis of exaggerated claims in science news, and then constructed prediction models for identifying claim strength levels in science reporting. The results demonstrate different writing styles journal articles and news\/press releases use for reporting scientific findings. Preliminary prediction models reached promising result with room for further improvement.","year":2017,"title_abstract":"An {NLP} Analysis of Exaggerated Claims in Science News The discrepancy between science and media has been affecting the effectiveness of science communication. Original findings from science publications may be distorted with altered claim strength when reported to the public, causing misinformation spread. This study conducts an NLP analysis of exaggerated claims in science news, and then constructed prediction models for identifying claim strength levels in science reporting. The results demonstrate different writing styles journal articles and news\/press releases use for reporting scientific findings. Preliminary prediction models reached promising result with room for further improvement.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2286632657,"Goal":"Climate Action","Task":["science communication","NLP analysis of exaggerated claims","science reporting","reporting scientific findings"],"Method":["{NLP} Analysis","prediction models","prediction models"]},{"ID":"suktarachan-etal-2008-workbench","title":"Workbench with Authoring Tools for Collaborative Multi-lingual Ontological Knowledge Construction and Maintenance","abstract":"An ontological knowledge management system requires dynamic and encapsulating operation in order to share knowledge among communities. The key to success of knowledge sharing in the field of agriculture is using and sharing agreed terminologies such as ontological knowledge especially in multiple languages. This paper proposes a workbench with three authoring tools for collaborative multilingual ontological knowledge construction and maintenance, in order to add value and support communities in the field of food and agriculture. The framework consists of the multilingual ontological knowledge construction and maintenance workbench platform, which composes of ontological knowledge management and user management, and three ontological knowledge authoring tools. The authoring tools used are two ontology extraction tools, ATOM and KULEX, and one ontology integration tool.","year":2008,"title_abstract":"Workbench with Authoring Tools for Collaborative Multi-lingual Ontological Knowledge Construction and Maintenance An ontological knowledge management system requires dynamic and encapsulating operation in order to share knowledge among communities. The key to success of knowledge sharing in the field of agriculture is using and sharing agreed terminologies such as ontological knowledge especially in multiple languages. This paper proposes a workbench with three authoring tools for collaborative multilingual ontological knowledge construction and maintenance, in order to add value and support communities in the field of food and agriculture. The framework consists of the multilingual ontological knowledge construction and maintenance workbench platform, which composes of ontological knowledge management and user management, and three ontological knowledge authoring tools. The authoring tools used are two ontology extraction tools, ATOM and KULEX, and one ontology integration tool.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.2273469865,"Goal":"Partnership for the Goals","Task":["Collaborative Multi - lingual Ontological Knowledge Construction and Maintenance","dynamic and encapsulating operation","knowledge sharing","agriculture","collaborative multilingual ontological knowledge construction and maintenance","food","agriculture","multilingual ontological knowledge construction and maintenance workbench platform","ontological knowledge management","user management"],"Method":["Authoring Tools","ontological knowledge management system","authoring tools","ontological knowledge authoring tools","authoring tools","ontology extraction tools","ATOM","KULEX","ontology integration tool"]},{"ID":"baker-gillis-2021-sexism","title":"Sexism in the Judiciary: The Importance of Bias Definition in {NLP} and In Our Courts","abstract":"We analyze 6.7 million case law documents to determine the presence of gender bias within our judicial system. We find that current bias detection methods in NLP are insufficient to determine gender bias in our case law database and propose an alternative approach. We show that existing algorithms{'} inconsistent results are consequences of prior research{'}s inconsistent definitions of biases themselves. Bias detection algorithms rely on groups of words to represent bias (e.g., {`}salary,{'} {`}job,{'} and {`}boss{'} to represent employment as a potentially biased theme against women in text). However, the methods to build these groups of words have several weaknesses, primarily that the word lists are based on the researchers{'} own intuitions. We suggest two new methods of automating the creation of word lists to represent biases. We find that our methods outperform current NLP bias detection methods. Our research improves the capabilities of NLP technology to detect bias and highlights gender biases present in influential case law. In order to test our NLP bias detection method{'}s performance, we regress our results of bias in case law against U.S census data of women{'}s participation in the workforce in the last 100 years.","year":2021,"title_abstract":"Sexism in the Judiciary: The Importance of Bias Definition in {NLP} and In Our Courts We analyze 6.7 million case law documents to determine the presence of gender bias within our judicial system. We find that current bias detection methods in NLP are insufficient to determine gender bias in our case law database and propose an alternative approach. We show that existing algorithms{'} inconsistent results are consequences of prior research{'}s inconsistent definitions of biases themselves. Bias detection algorithms rely on groups of words to represent bias (e.g., {`}salary,{'} {`}job,{'} and {`}boss{'} to represent employment as a potentially biased theme against women in text). However, the methods to build these groups of words have several weaknesses, primarily that the word lists are based on the researchers{'} own intuitions. We suggest two new methods of automating the creation of word lists to represent biases. We find that our methods outperform current NLP bias detection methods. Our research improves the capabilities of NLP technology to detect bias and highlights gender biases present in influential case law. In order to test our NLP bias detection method{'}s performance, we regress our results of bias in case law against U.S census data of women{'}s participation in the workforce in the last 100 years.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2273422182,"Goal":"Gender Equality","Task":["Bias Definition","creation of word lists","detect bias","bias","case law"],"Method":["judicial system","bias detection methods","NLP","Bias detection algorithms","NLP bias detection methods","NLP technology","NLP"]},{"ID":"nakov-etal-2021-second","title":"A Second Pandemic? Analysis of Fake News about {COVID}-19 Vaccines in {Q}atar","abstract":"While COVID-19 vaccines are finally becoming widely available, a second pandemic that revolves around the circulation of anti-vaxxer {``}fake news{''} may hinder efforts to recover from the first one. With this in mind, we performed an extensive analysis of Arabic and English tweets about COVID-19 vaccines, with focus on messages originating from Qatar. We found that Arabic tweets contain a lot of false information and rumors, while English tweets are mostly factual. However, English tweets are much more propagandistic than Arabic ones. In terms of propaganda techniques, about half of the Arabic tweets express doubt, and 1\/5 use loaded language, while English tweets are abundant in loaded language, exaggeration, fear, name-calling, doubt, and flag-waving. Finally, in terms of framing, Arabic tweets adopt a health and safety perspective, while in English economic concerns dominate.","year":2021,"title_abstract":"A Second Pandemic? Analysis of Fake News about {COVID}-19 Vaccines in {Q}atar While COVID-19 vaccines are finally becoming widely available, a second pandemic that revolves around the circulation of anti-vaxxer {``}fake news{''} may hinder efforts to recover from the first one. With this in mind, we performed an extensive analysis of Arabic and English tweets about COVID-19 vaccines, with focus on messages originating from Qatar. We found that Arabic tweets contain a lot of false information and rumors, while English tweets are mostly factual. However, English tweets are much more propagandistic than Arabic ones. In terms of propaganda techniques, about half of the Arabic tweets express doubt, and 1\/5 use loaded language, while English tweets are abundant in loaded language, exaggeration, fear, name-calling, doubt, and flag-waving. Finally, in terms of framing, Arabic tweets adopt a health and safety perspective, while in English economic concerns dominate.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2271981686,"Goal":"Climate Action","Task":["framing"],"Method":["propaganda techniques"]},{"ID":"rechkemmer-etal-2020-small","title":"Small Town or Metropolis? Analyzing the Relationship between Population Size and Language","abstract":"The variance in language used by different cultures has been a topic of study for researchers in linguistics and psychology, but often times, language is compared across multiple countries in order to show a difference in culture. As a geographically large country that is diverse in population in terms of the background and experiences of its citizens, the U.S. also contains cultural differences within its own borders. Using a set of over 2 million posts from distinct Twitter users around the country dating back as far as 2014, we ask the following question: is there a difference in how Americans express themselves online depending on whether they reside in an urban or rural area? We categorize Twitter users as either urban or rural and identify ideas and language that are more commonly expressed in tweets written by one population over the other. We take this further by analyzing how the language from specific cities of the U.S. compares to the language of other cities and by training predictive models to predict whether a user is from an urban or rural area. We publicly release the tweet and user IDs that can be used to reconstruct the dataset for future studies in this direction.","year":2020,"title_abstract":"Small Town or Metropolis? Analyzing the Relationship between Population Size and Language The variance in language used by different cultures has been a topic of study for researchers in linguistics and psychology, but often times, language is compared across multiple countries in order to show a difference in culture. As a geographically large country that is diverse in population in terms of the background and experiences of its citizens, the U.S. also contains cultural differences within its own borders. Using a set of over 2 million posts from distinct Twitter users around the country dating back as far as 2014, we ask the following question: is there a difference in how Americans express themselves online depending on whether they reside in an urban or rural area? We categorize Twitter users as either urban or rural and identify ideas and language that are more commonly expressed in tweets written by one population over the other. We take this further by analyzing how the language from specific cities of the U.S. compares to the language of other cities and by training predictive models to predict whether a user is from an urban or rural area. We publicly release the tweet and user IDs that can be used to reconstruct the dataset for future studies in this direction.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.226840958,"Goal":"Reduced Inequalities","Task":["linguistics","psychology"],"Method":["predictive models"]},{"ID":"bhattacharya-etal-2020-developing","title":"Developing a Multilingual Annotated Corpus of Misogyny and Aggression","abstract":"In this paper, we discuss the development of a multilingual annotated corpus of misogyny and aggression in Indian English, Hindi, and Indian Bangla as part of a project on studying and automatically identifying misogyny and communalism on social media (the ComMA Project). The dataset is collected from comments on YouTube videos and currently contains a total of over 20,000 comments. The comments are annotated at two levels - aggression (overtly aggressive, covertly aggressive, and non-aggressive) and misogyny (gendered and non-gendered). We describe the process of data collection, the tagset used for annotation, and issues and challenges faced during the process of annotation. Finally, we discuss the results of the baseline experiments conducted to develop a classifier for misogyny in the three languages.","year":2020,"title_abstract":"Developing a Multilingual Annotated Corpus of Misogyny and Aggression In this paper, we discuss the development of a multilingual annotated corpus of misogyny and aggression in Indian English, Hindi, and Indian Bangla as part of a project on studying and automatically identifying misogyny and communalism on social media (the ComMA Project). The dataset is collected from comments on YouTube videos and currently contains a total of over 20,000 comments. The comments are annotated at two levels - aggression (overtly aggressive, covertly aggressive, and non-aggressive) and misogyny (gendered and non-gendered). We describe the process of data collection, the tagset used for annotation, and issues and challenges faced during the process of annotation. Finally, we discuss the results of the baseline experiments conducted to develop a classifier for misogyny in the three languages.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2267411053,"Goal":"Gender Equality","Task":["Multilingual Annotated Corpus of Misogyny and Aggression","automatically identifying misogyny and communalism","ComMA Project)","data collection","annotation","annotation","misogyny"],"Method":["classifier"]},{"ID":"bao-qiao-2019-transfer","title":"Transfer Learning from Pre-trained {BERT} for Pronoun Resolution","abstract":"The paper describes the submission of the team {``}We used bert!{''} to the shared task Gendered Pronoun Resolution (Pair pronouns to their correct entities). Our final submission model based on the fine-tuned BERT (Bidirectional Encoder Representations from Transformers) ranks 14th among 838 teams with a multi-class logarithmic loss of 0.208. In this work, contribution of transfer learning technique to pronoun resolution systems is investigated and the gender bias contained in classification models is evaluated.","year":2019,"title_abstract":"Transfer Learning from Pre-trained {BERT} for Pronoun Resolution The paper describes the submission of the team {``}We used bert!{''} to the shared task Gendered Pronoun Resolution (Pair pronouns to their correct entities). Our final submission model based on the fine-tuned BERT (Bidirectional Encoder Representations from Transformers) ranks 14th among 838 teams with a multi-class logarithmic loss of 0.208. In this work, contribution of transfer learning technique to pronoun resolution systems is investigated and the gender bias contained in classification models is evaluated.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2267258465,"Goal":"Gender Equality","Task":["Pronoun Resolution","shared task Gendered Pronoun Resolution","pronoun resolution"],"Method":["Transfer Learning","Pre - trained {BERT}","bert!{''}","fine - tuned BERT (Bidirectional Encoder Representations","Transformers)","transfer learning technique","classification models"]},{"ID":"chavez-mulsa-spanakis-2020-evaluating","title":"Evaluating Bias In {D}utch Word Embeddings","abstract":"Recent research in Natural Language Processing has revealed that word embeddings can encode social biases present in the training data which can affect minorities in real world applications. This paper explores the gender bias implicit in Dutch embeddings while investigating whether English language based approaches can also be used in Dutch. We implement the Word Embeddings Association Test (WEAT), Clustering and Sentence Embeddings Association Test (SEAT) methods to quantify the gender bias in Dutch word embeddings, then we proceed to reduce the bias with Hard-Debias and Sent-Debias mitigation methods and finally we evaluate the performance of the debiased embeddings in downstream tasks. The results suggest that, among others, gender bias is present in traditional and contextualized Dutch word embeddings. We highlight how techniques used to measure and reduce bias created for English can be used in Dutch embeddings by adequately translating the data and taking into account the unique characteristics of the language. Furthermore, we analyze the effect of the debiasing techniques on downstream tasks which show a negligible impact on traditional embeddings and a 2{\\%} decrease in performance in contextualized embeddings. Finally, we release the translated Dutch datasets to the public along with the traditional embeddings with mitigated bias.","year":2020,"title_abstract":"Evaluating Bias In {D}utch Word Embeddings Recent research in Natural Language Processing has revealed that word embeddings can encode social biases present in the training data which can affect minorities in real world applications. This paper explores the gender bias implicit in Dutch embeddings while investigating whether English language based approaches can also be used in Dutch. We implement the Word Embeddings Association Test (WEAT), Clustering and Sentence Embeddings Association Test (SEAT) methods to quantify the gender bias in Dutch word embeddings, then we proceed to reduce the bias with Hard-Debias and Sent-Debias mitigation methods and finally we evaluate the performance of the debiased embeddings in downstream tasks. The results suggest that, among others, gender bias is present in traditional and contextualized Dutch word embeddings. We highlight how techniques used to measure and reduce bias created for English can be used in Dutch embeddings by adequately translating the data and taking into account the unique characteristics of the language. Furthermore, we analyze the effect of the debiasing techniques on downstream tasks which show a negligible impact on traditional embeddings and a 2{\\%} decrease in performance in contextualized embeddings. Finally, we release the translated Dutch datasets to the public along with the traditional embeddings with mitigated bias.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2266740948,"Goal":"Gender Equality","Task":["Evaluating Bias","{D}utch Word Embeddings","Natural Language Processing","minorities","real world applications","downstream tasks","Dutch embeddings","downstream tasks"],"Method":["English language based approaches","Word Embeddings Association Test","Clustering and Sentence Embeddings Association Test (SEAT) methods","Hard - Debias and Sent - Debias mitigation methods","debiased embeddings","debiasing techniques","contextualized embeddings"]},{"ID":"moneglia-varvara-2020-annotation","title":"The Annotation of Thematic Structure and Alternations face to the Semantic Variation of Action Verbs. Current Trends in the {IMAGACT} Ontology","abstract":"We present some issues in the development of the semantic annotation of IMAGACT, a multimodal and multilingual ontology of actions. The resource is structured on action concepts that are meant to be cognitive entities and to which a linguistic caption is attached. For each of these concepts, we annotate the minimal thematic structure of the caption and the possible argument alternations allowed. We present some insights on this process with regards to the notion of thematic structure and the relationship between action concepts and linguistic expressions. From the empirical evidence provided by the annotation, we discuss on the very nature of thematic structure, arguing that it is neither a property of the verb itself nor a property of action concepts. We further show what is the relation between thematic structure and 1- the semantic variation of action verbs; 2- the lexical variation of action concepts.","year":2020,"title_abstract":"The Annotation of Thematic Structure and Alternations face to the Semantic Variation of Action Verbs. Current Trends in the {IMAGACT} Ontology We present some issues in the development of the semantic annotation of IMAGACT, a multimodal and multilingual ontology of actions. The resource is structured on action concepts that are meant to be cognitive entities and to which a linguistic caption is attached. For each of these concepts, we annotate the minimal thematic structure of the caption and the possible argument alternations allowed. We present some insights on this process with regards to the notion of thematic structure and the relationship between action concepts and linguistic expressions. From the empirical evidence provided by the annotation, we discuss on the very nature of thematic structure, arguing that it is neither a property of the verb itself nor a property of action concepts. We further show what is the relation between thematic structure and 1- the semantic variation of action verbs; 2- the lexical variation of action concepts.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2265156657,"Goal":"Climate Action","Task":["Annotation of Thematic Structure","Semantic Variation of Action Verbs","semantic annotation of IMAGACT","multimodal and multilingual ontology of actions","annotation","lexical variation of action concepts"],"Method":["{IMAGACT} Ontology"]},{"ID":"fiumara-etal-2020-languagearc","title":"{L}anguage{ARC}: Developing Language Resources Through Citizen Linguistics","abstract":"This paper introduces the citizen science platform, LanguageARC, developed within the NIEUW (Novel Incentives and Workflows) project supported by the National Science Foundation under Grant No. 1730377. LanguageARC is a community-oriented online platform bringing together researchers and {``}citizen linguists{''} with the shared goal of contributing to linguistic research and language technology development. Like other Citizen Science platforms and projects, LanguageARC harnesses the power and efforts of volunteers who are motivated by the incentives of contributing to science, learning and discovery, and belonging to a community dedicated to social improvement. Citizen linguists contribute language data and judgments by participating in research tasks such as classifying regional accents from audio clips, recording audio of picture descriptions and answering personality questionnaires to create baseline data for NLP research into autism and neurodegenerative conditions. Researchers can create projects on Language ARC without any coding or HTML required using our Project Builder Toolkit.","year":2020,"title_abstract":"{L}anguage{ARC}: Developing Language Resources Through Citizen Linguistics This paper introduces the citizen science platform, LanguageARC, developed within the NIEUW (Novel Incentives and Workflows) project supported by the National Science Foundation under Grant No. 1730377. LanguageARC is a community-oriented online platform bringing together researchers and {``}citizen linguists{''} with the shared goal of contributing to linguistic research and language technology development. Like other Citizen Science platforms and projects, LanguageARC harnesses the power and efforts of volunteers who are motivated by the incentives of contributing to science, learning and discovery, and belonging to a community dedicated to social improvement. Citizen linguists contribute language data and judgments by participating in research tasks such as classifying regional accents from audio clips, recording audio of picture descriptions and answering personality questionnaires to create baseline data for NLP research into autism and neurodegenerative conditions. Researchers can create projects on Language ARC without any coding or HTML required using our Project Builder Toolkit.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2264770418,"Goal":"Sustainable Cities and Communities","Task":["Language Resources","linguistic research","language technology development","science","discovery","social improvement","research tasks","NLP","autism and neurodegenerative conditions","Language ARC"],"Method":["Citizen Linguistics","citizen science platform","LanguageARC","LanguageARC","community - oriented online platform","Citizen Science platforms","LanguageARC","Project Builder Toolkit"]},{"ID":"liu-etal-2022-always","title":"Not always about you: Prioritizing community needs when developing endangered language technology","abstract":"Languages are classified as low-resource when they lack the quantity of data necessary for training statistical and machine learning tools and models. Causes of resource scarcity vary but can include poor access to technology for developing these resources, a relatively small population of speakers, or a lack of urgency for collecting such resources in bilingual populations where the second language is high-resource. As a result, the languages described as low-resource in the literature are as different as Finnish on the one hand, with millions of speakers using it in every imaginable domain, and Seneca, with only a small-handful of fluent speakers using the language primarily in a restricted domain. While issues stemming from the lack of resources necessary to train models unite this disparate group of languages, many other issues cut across the divide between widely-spoken low-resource languages and endangered languages. In this position paper, we discuss the unique technological, cultural, practical, and ethical challenges that researchers and indigenous speech community members face when working together to develop language technology to support endangered language documentation and revitalization. We report the perspectives of language teachers, Master Speakers and elders from indigenous communities, as well as the point of view of academics. We describe an ongoing fruitful collaboration and make recommendations for future partnerships between academic researchers and language community stakeholders.","year":2022,"title_abstract":"Not always about you: Prioritizing community needs when developing endangered language technology Languages are classified as low-resource when they lack the quantity of data necessary for training statistical and machine learning tools and models. Causes of resource scarcity vary but can include poor access to technology for developing these resources, a relatively small population of speakers, or a lack of urgency for collecting such resources in bilingual populations where the second language is high-resource. As a result, the languages described as low-resource in the literature are as different as Finnish on the one hand, with millions of speakers using it in every imaginable domain, and Seneca, with only a small-handful of fluent speakers using the language primarily in a restricted domain. While issues stemming from the lack of resources necessary to train models unite this disparate group of languages, many other issues cut across the divide between widely-spoken low-resource languages and endangered languages. In this position paper, we discuss the unique technological, cultural, practical, and ethical challenges that researchers and indigenous speech community members face when working together to develop language technology to support endangered language documentation and revitalization. We report the perspectives of language teachers, Master Speakers and elders from indigenous communities, as well as the point of view of academics. We describe an ongoing fruitful collaboration and make recommendations for future partnerships between academic researchers and language community stakeholders.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.226342231,"Goal":"Sustainable Cities and Communities","Task":["endangered language technology","endangered language documentation","revitalization","language community stakeholders"],"Method":["statistical and machine learning tools","language technology"]},{"ID":"dinan-etal-2020-multi","title":"Multi-Dimensional Gender Bias Classification","abstract":"Machine learning models are trained to find patterns in data. NLP models can inadvertently learn socially undesirable patterns when training on gender biased text. In this work, we propose a novel, general framework that decomposes gender bias in text along several pragmatic and semantic dimensions: bias from the gender of the person being spoken about, bias from the gender of the person being spoken to, and bias from the gender of the speaker. Using this fine-grained framework, we automatically annotate eight large scale datasets with gender information. In addition, we collect a new, crowdsourced evaluation benchmark. Distinguishing between gender bias along multiple dimensions enables us to train better and more fine-grained gender bias classifiers. We show our classifiers are valuable for a variety of applications, like controlling for gender bias in generative models, detecting gender bias in arbitrary text, and classifying text as offensive based on its genderedness.","year":2020,"title_abstract":"Multi-Dimensional Gender Bias Classification Machine learning models are trained to find patterns in data. NLP models can inadvertently learn socially undesirable patterns when training on gender biased text. In this work, we propose a novel, general framework that decomposes gender bias in text along several pragmatic and semantic dimensions: bias from the gender of the person being spoken about, bias from the gender of the person being spoken to, and bias from the gender of the speaker. Using this fine-grained framework, we automatically annotate eight large scale datasets with gender information. In addition, we collect a new, crowdsourced evaluation benchmark. Distinguishing between gender bias along multiple dimensions enables us to train better and more fine-grained gender bias classifiers. We show our classifiers are valuable for a variety of applications, like controlling for gender bias in generative models, detecting gender bias in arbitrary text, and classifying text as offensive based on its genderedness.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2262783945,"Goal":"Gender Equality","Task":["Multi - Dimensional Gender Bias Classification","gender bias","detecting gender bias"],"Method":["Machine learning models","NLP models","fine - grained framework","gender bias classifiers","classifiers","generative models"]},{"ID":"moneglia-etal-2012-imagact","title":"The {IMAGACT} Cross-linguistic Ontology of Action. A new infrastructure for natural language disambiguation","abstract":"Action verbs, which are highly frequent in speech, cause disambiguation problems that are relevant to Language Technologies. This is a consequence of the peculiar way each natural language categorizes Action i.e. it is a consequence of semantic factors. Action verbs are frequently \u0093general\u0094, since they extend productively to actions belonging to different ontological types. Moreover, each language categorizes action in its own way and therefore the cross-linguistic reference to everyday activities is puzzling. This paper briefly sketches the IMAGACT project, which aims at setting up a cross-linguistic Ontology of Action for grounding disambiguation tasks in this crucial area of the lexicon. The project derives information on the actual variation of action verbs in English and Italian from spontaneous speech corpora, where references to action are high in frequency. Crucially it makes use of the universal language of images to identify action types, avoiding the underdeterminacy of semantic definitions. Action concept entries are implemented as prototypic scenes; this will make it easier to extend the Ontology to other languages.","year":2012,"title_abstract":"The {IMAGACT} Cross-linguistic Ontology of Action. A new infrastructure for natural language disambiguation Action verbs, which are highly frequent in speech, cause disambiguation problems that are relevant to Language Technologies. This is a consequence of the peculiar way each natural language categorizes Action i.e. it is a consequence of semantic factors. Action verbs are frequently \u0093general\u0094, since they extend productively to actions belonging to different ontological types. Moreover, each language categorizes action in its own way and therefore the cross-linguistic reference to everyday activities is puzzling. This paper briefly sketches the IMAGACT project, which aims at setting up a cross-linguistic Ontology of Action for grounding disambiguation tasks in this crucial area of the lexicon. The project derives information on the actual variation of action verbs in English and Italian from spontaneous speech corpora, where references to action are high in frequency. Crucially it makes use of the universal language of images to identify action types, avoiding the underdeterminacy of semantic definitions. Action concept entries are implemented as prototypic scenes; this will make it easier to extend the Ontology to other languages.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2261398733,"Goal":"Climate Action","Task":["natural language disambiguation Action verbs","disambiguation problems","Language Technologies","IMAGACT project","cross - linguistic Ontology of Action","disambiguation tasks"],"Method":["{IMAGACT} Cross - linguistic Ontology of Action"]},{"ID":"de-meulder-2021-good","title":"Is {``}good enough{''} good enough? Ethical and responsible development of sign language technologies","abstract":"This paper identifies some common and specific pitfalls in the development of sign language technologies targeted at deaf communities, with a specific focus on signing avatars. It makes the call to urgently interrogate some of the ideologies behind those technologies, including issues of ethical and responsible development. The paper addresses four separate and interlinked issues: ideologies about deaf people and mediated communication, bias in data sets and learning, user feedback, and applications of the technologies. The paper ends with several take away points for both technology developers and deaf NGOs. Technology developers should give more consideration to diversifying their team and working interdisciplinary, and be mindful of the biases that inevitably creep into data sets. There should also be a consideration of the technologies{'} end users. Sign language interpreters are not the end users nor should they be seen as the benchmark for language use. Technology developers and deaf NGOs can engage in a dialogue about how to prioritize application domains and prioritize within application domains. Finally, deaf NGOs policy statements will need to take a longer view, and use avatars to think of a significantly better system compared to what sign language interpreting services can provide.","year":2021,"title_abstract":"Is {``}good enough{''} good enough? Ethical and responsible development of sign language technologies This paper identifies some common and specific pitfalls in the development of sign language technologies targeted at deaf communities, with a specific focus on signing avatars. It makes the call to urgently interrogate some of the ideologies behind those technologies, including issues of ethical and responsible development. The paper addresses four separate and interlinked issues: ideologies about deaf people and mediated communication, bias in data sets and learning, user feedback, and applications of the technologies. The paper ends with several take away points for both technology developers and deaf NGOs. Technology developers should give more consideration to diversifying their team and working interdisciplinary, and be mindful of the biases that inevitably creep into data sets. There should also be a consideration of the technologies{'} end users. Sign language interpreters are not the end users nor should they be seen as the benchmark for language use. Technology developers and deaf NGOs can engage in a dialogue about how to prioritize application domains and prioritize within application domains. Finally, deaf NGOs policy statements will need to take a longer view, and use avatars to think of a significantly better system compared to what sign language interpreting services can provide.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2255317867,"Goal":"Sustainable Cities and Communities","Task":["sign language technologies","sign language technologies","ethical and responsible development","language use"],"Method":["technology developers","Sign language interpreters","Technology developers","sign language interpreting services"]},{"ID":"kumar-etal-2020-nurse","title":"Nurse is Closer to Woman than Surgeon? Mitigating Gender-Biased Proximities in Word Embeddings","abstract":"Word embeddings are the standard model for semantic and syntactic representations of words. Unfortunately, these models have been shown to exhibit undesirable word associations resulting from gender, racial, and religious biases. Existing post-processing methods for debiasing word embeddings are unable to mitigate gender bias hidden in the spatial arrangement of word vectors. In this paper, we propose RAN-Debias, a novel gender debiasing methodology that not only eliminates the bias present in a word vector but also alters the spatial distribution of its neighboring vectors, achieving a bias-free setting while maintaining minimal semantic offset. We also propose a new bias evaluation metric, Gender-based Illicit Proximity Estimate (GIPE), which measures the extent of undue proximity in word vectors resulting from the presence of gender-based predilections. Experiments based on a suite of evaluation metrics show that RAN-Debias significantly outperforms the state-of-the-art in reducing proximity bias (GIPE) by at least 42.02{\\%}. It also reduces direct bias, adding minimal semantic disturbance, and achieves the best performance in a downstream application task (coreference resolution).","year":2020,"title_abstract":"Nurse is Closer to Woman than Surgeon? Mitigating Gender-Biased Proximities in Word Embeddings Word embeddings are the standard model for semantic and syntactic representations of words. Unfortunately, these models have been shown to exhibit undesirable word associations resulting from gender, racial, and religious biases. Existing post-processing methods for debiasing word embeddings are unable to mitigate gender bias hidden in the spatial arrangement of word vectors. In this paper, we propose RAN-Debias, a novel gender debiasing methodology that not only eliminates the bias present in a word vector but also alters the spatial distribution of its neighboring vectors, achieving a bias-free setting while maintaining minimal semantic offset. We also propose a new bias evaluation metric, Gender-based Illicit Proximity Estimate (GIPE), which measures the extent of undue proximity in word vectors resulting from the presence of gender-based predilections. Experiments based on a suite of evaluation metrics show that RAN-Debias significantly outperforms the state-of-the-art in reducing proximity bias (GIPE) by at least 42.02{\\%}. It also reduces direct bias, adding minimal semantic disturbance, and achieves the best performance in a downstream application task (coreference resolution).","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2255123705,"Goal":"Gender Equality","Task":["Word Embeddings","semantic and syntactic representations of words","debiasing word embeddings","bias - free setting","resolution)"],"Method":["Word embeddings","post - processing methods","RAN - Debias","gender debiasing methodology","Gender - based Illicit Proximity Estimate","RAN - Debias"]},{"ID":"gabriel-etal-2022-misinfo","title":"Misinfo Reaction Frames: Reasoning about Readers{'} Reactions to News Headlines","abstract":"Even to a simple and short news headline, readers react in a multitude of ways: cognitively (e.g. inferring the writer{'}s intent), emotionally (e.g. feeling distrust), and behaviorally (e.g. sharing the news with their friends). Such reactions are instantaneous and yet complex, as they rely on factors that go beyond interpreting factual content of news.We propose Misinfo Reaction Frames (MRF), a pragmatic formalism for modeling how readers might react to a news headline. In contrast to categorical schema, our free-text dimensions provide a more nuanced way of understanding intent beyond being benign or malicious. We also introduce a Misinfo Reaction Frames corpus, a crowdsourced dataset of reactions to over 25k news headlines focusing on global crises: the Covid-19 pandemic, climate change, and cancer. Empirical results confirm that it is indeed possible for neural models to predict the prominent patterns of readers{'} reactions to previously unseen news headlines. Additionally, our user study shows that displaying machine-generated MRF implications alongside news headlines to readers can increase their trust in real news while decreasing their trust in misinformation. Our work demonstrates the feasibility and importance of pragmatic inferences on news headlines to help enhance AI-guided misinformation detection and mitigation.","year":2022,"title_abstract":"Misinfo Reaction Frames: Reasoning about Readers{'} Reactions to News Headlines Even to a simple and short news headline, readers react in a multitude of ways: cognitively (e.g. inferring the writer{'}s intent), emotionally (e.g. feeling distrust), and behaviorally (e.g. sharing the news with their friends). Such reactions are instantaneous and yet complex, as they rely on factors that go beyond interpreting factual content of news.We propose Misinfo Reaction Frames (MRF), a pragmatic formalism for modeling how readers might react to a news headline. In contrast to categorical schema, our free-text dimensions provide a more nuanced way of understanding intent beyond being benign or malicious. We also introduce a Misinfo Reaction Frames corpus, a crowdsourced dataset of reactions to over 25k news headlines focusing on global crises: the Covid-19 pandemic, climate change, and cancer. Empirical results confirm that it is indeed possible for neural models to predict the prominent patterns of readers{'} reactions to previously unseen news headlines. Additionally, our user study shows that displaying machine-generated MRF implications alongside news headlines to readers can increase their trust in real news while decreasing their trust in misinformation. Our work demonstrates the feasibility and importance of pragmatic inferences on news headlines to help enhance AI-guided misinformation detection and mitigation.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2248910069,"Goal":"Climate Action","Task":["Reasoning about Readers{'} Reactions","AI - guided misinformation detection","mitigation"],"Method":["Misinfo Reaction Frames","Misinfo Reaction Frames","pragmatic formalism","categorical schema","neural models","pragmatic inferences"]},{"ID":"talat-etal-2022-reap","title":"You reap what you sow: On the Challenges of Bias Evaluation Under Multilingual Settings","abstract":"Evaluating bias, fairness, and social impact in monolingual language models is a difficult task. This challenge is further compounded when language modeling occurs in a multilingual context. Considering the implication of evaluation biases for large multilingual language models, we situate the discussion of bias evaluation within a wider context of social scientific research with computational work.We highlight three dimensions of developing multilingual bias evaluation frameworks: (1) increasing transparency through documentation, (2) expanding targets of bias beyond gender, and (3) addressing cultural differences that exist between languages.We further discuss the power dynamics and consequences of training large language models and recommend that researchers remain cognizant of the ramifications of developing such technologies.","year":2022,"title_abstract":"You reap what you sow: On the Challenges of Bias Evaluation Under Multilingual Settings Evaluating bias, fairness, and social impact in monolingual language models is a difficult task. This challenge is further compounded when language modeling occurs in a multilingual context. Considering the implication of evaluation biases for large multilingual language models, we situate the discussion of bias evaluation within a wider context of social scientific research with computational work.We highlight three dimensions of developing multilingual bias evaluation frameworks: (1) increasing transparency through documentation, (2) expanding targets of bias beyond gender, and (3) addressing cultural differences that exist between languages.We further discuss the power dynamics and consequences of training large language models and recommend that researchers remain cognizant of the ramifications of developing such technologies.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2247381806,"Goal":"Gender Equality","Task":["Bias Evaluation","Evaluating bias","fairness","social impact","monolingual language models","large multilingual language models","bias evaluation","social scientific research","computational work"],"Method":["language modeling","multilingual bias evaluation frameworks","large language models"]},{"ID":"zamora-fernandez-etal-2020-poio","title":"Poio Text Prediction: Lessons on the Development and Sustainability of {LT}s for Endangered Languages","abstract":"2019, the International Year of Indigenous Languages (IYIL), marked a crucial milestone for a diverse community united by a strong sense of urgency. In this presentation, we evaluate the impact of IYIL{'}s outcomes in the development of LTs for endangered languages. We give a brief description of the field of Language Documentation, whose experts have led the research and data collection efforts surrounding endangered languages for the past 30 years. We introduce the work of the Interdisciplinary Centre for Social and Language Documentation and we look at Poio as an example of an LT developed specifically with speakers of endangered languages in mind. This example illustrates how the deeper systemic causes of language endangerment are reflected in the development of LTs. Additionally, we share some of the strategic decisions that have led the development of this project. Finally, we advocate the importance of bridging the divide between research and activism, pushing for the inclusion of threatened languages in the world of LTs, and doing so in close collaboration with the speaker community.","year":2020,"title_abstract":"Poio Text Prediction: Lessons on the Development and Sustainability of {LT}s for Endangered Languages 2019, the International Year of Indigenous Languages (IYIL), marked a crucial milestone for a diverse community united by a strong sense of urgency. In this presentation, we evaluate the impact of IYIL{'}s outcomes in the development of LTs for endangered languages. We give a brief description of the field of Language Documentation, whose experts have led the research and data collection efforts surrounding endangered languages for the past 30 years. We introduce the work of the Interdisciplinary Centre for Social and Language Documentation and we look at Poio as an example of an LT developed specifically with speakers of endangered languages in mind. This example illustrates how the deeper systemic causes of language endangerment are reflected in the development of LTs. Additionally, we share some of the strategic decisions that have led the development of this project. Finally, we advocate the importance of bridging the divide between research and activism, pushing for the inclusion of threatened languages in the world of LTs, and doing so in close collaboration with the speaker community.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2243008763,"Goal":"Sustainable Cities and Communities","Task":["Poio Text Prediction","LTs","Language Documentation","Social and Language Documentation","Poio","LTs","LTs"],"Method":["{LT}s","LT"]},{"ID":"fan-gardent-2022-generating","title":"Generating Biographies on {W}ikipedia: The Impact of Gender Bias on the Retrieval-Based Generation of Women Biographies","abstract":"Generating factual, long-form text such as Wikipedia articles raises three key challenges: how to gather relevant evidence, how to structure information into well-formed text, and how to ensure that the generated text is factually correct. We address these by developing a model for English text that uses a retrieval mechanism to identify relevant supporting information on the web and a cache-based pre-trained encoder-decoder to generate long-form biographies section by section, including citation information. To assess the impact of available web evidence on the output text, we compare the performance of our approach when generating biographies about women (for which less information is available on the web) vs. biographies generally. To this end, we curate a dataset of 1,500 biographies about women. We analyze our generated text to understand how differences in available web evidence data affect generation. We evaluate the factuality, fluency, and quality of the generated texts using automatic metrics and human evaluation. We hope that these techniques can be used as a starting point for human writers, to aid in reducing the complexity inherent in the creation of long-form, factual text.","year":2022,"title_abstract":"Generating Biographies on {W}ikipedia: The Impact of Gender Bias on the Retrieval-Based Generation of Women Biographies Generating factual, long-form text such as Wikipedia articles raises three key challenges: how to gather relevant evidence, how to structure information into well-formed text, and how to ensure that the generated text is factually correct. We address these by developing a model for English text that uses a retrieval mechanism to identify relevant supporting information on the web and a cache-based pre-trained encoder-decoder to generate long-form biographies section by section, including citation information. To assess the impact of available web evidence on the output text, we compare the performance of our approach when generating biographies about women (for which less information is available on the web) vs. biographies generally. To this end, we curate a dataset of 1,500 biographies about women. We analyze our generated text to understand how differences in available web evidence data affect generation. We evaluate the factuality, fluency, and quality of the generated texts using automatic metrics and human evaluation. We hope that these techniques can be used as a starting point for human writers, to aid in reducing the complexity inherent in the creation of long-form, factual text.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2242042124,"Goal":"Gender Equality","Task":["Generating Biographies","Retrieval - Based Generation of Women Biographies","long - form biographies","generation","human writers","long - form , factual text"],"Method":["retrieval mechanism","encoder - decoder"]},{"ID":"lucas-etal-2022-detecting","title":"Detecting False Claims in Low-Resource Regions: A Case Study of Caribbean Islands","abstract":"The COVID-19 pandemic has created threats to global health control. Misinformation circulated on social media and news outlets has undermined public trust towards Government and health agencies. This problem is further exacerbated in developing countries or low-resource regions, where the news is not equipped with abundant English fact-checking information. In this paper, we make the first attempt to detect COVID-19 misinformation (in English, Spanish, and Haitian French) populated in the Caribbean regions, using the fact-checked claims in the US (in English). We started by collecting a dataset of Caribbean real {\\&} fake claims. Then we trained several classification and language models on COVID-19 in the high-resource language regions and transferred the knowledge to the Caribbean claim dataset. The experimental results of this paper reveal the limitations of current fake claim detection in low-resource regions and encourage further research on multi-lingual detection.","year":2022,"title_abstract":"Detecting False Claims in Low-Resource Regions: A Case Study of Caribbean Islands The COVID-19 pandemic has created threats to global health control. Misinformation circulated on social media and news outlets has undermined public trust towards Government and health agencies. This problem is further exacerbated in developing countries or low-resource regions, where the news is not equipped with abundant English fact-checking information. In this paper, we make the first attempt to detect COVID-19 misinformation (in English, Spanish, and Haitian French) populated in the Caribbean regions, using the fact-checked claims in the US (in English). We started by collecting a dataset of Caribbean real {\\&} fake claims. Then we trained several classification and language models on COVID-19 in the high-resource language regions and transferred the knowledge to the Caribbean claim dataset. The experimental results of this paper reveal the limitations of current fake claim detection in low-resource regions and encourage further research on multi-lingual detection.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2239016593,"Goal":"Climate Action","Task":["Detecting False Claims in Low - Resource Regions","COVID - 19","global health control","COVID - 19","COVID - 19","fake claim detection","multi - lingual detection"],"Method":["classification and language models"]},{"ID":"chesney-etal-2017-incongruent","title":"Incongruent Headlines: Yet Another Way to Mislead Your Readers","abstract":"This paper discusses the problem of incongruent headlines: those which do not accurately represent the information contained in the article with which they occur. We emphasise that this phenomenon should be considered separately from recognised problematic headline types such as clickbait and sensationalism, arguing that existing natural language processing (NLP) methods applied to these related concepts are not appropriate for the automatic detection of headline incongruence, as an analysis beyond stylistic traits is necessary. We therefore suggest a number of alternative methodologies that may be appropriate to the task at hand as a foundation for future work in this area. In addition, we provide an analysis of existing data sets which are related to this work, and motivate the need for a novel data set in this domain.","year":2017,"title_abstract":"Incongruent Headlines: Yet Another Way to Mislead Your Readers This paper discusses the problem of incongruent headlines: those which do not accurately represent the information contained in the article with which they occur. We emphasise that this phenomenon should be considered separately from recognised problematic headline types such as clickbait and sensationalism, arguing that existing natural language processing (NLP) methods applied to these related concepts are not appropriate for the automatic detection of headline incongruence, as an analysis beyond stylistic traits is necessary. We therefore suggest a number of alternative methodologies that may be appropriate to the task at hand as a foundation for future work in this area. In addition, we provide an analysis of existing data sets which are related to this work, and motivate the need for a novel data set in this domain.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2225899994,"Goal":"Climate Action","Task":["automatic detection of headline incongruence"],"Method":["natural language processing"]},{"ID":"dev-etal-2021-oscar","title":"{OSC}a{R}: Orthogonal Subspace Correction and Rectification of Biases in Word Embeddings","abstract":"Language representations are known to carry stereotypical biases and, as a result, lead to biased predictions in downstream tasks. While existing methods are effective at mitigating biases by linear projection, such methods are too aggressive: they not only remove bias, but also erase valuable information from word embeddings. We develop new measures for evaluating specific information retention that demonstrate the tradeoff between bias removal and information retention. To address this challenge, we propose OSCaR (Orthogonal Subspace Correction and Rectification), a bias-mitigating method that focuses on disentangling biased associations between concepts instead of removing concepts wholesale. Our experiments on gender biases show that OSCaR is a well-balanced approach that ensures that semantic information is retained in the embeddings and bias is also effectively mitigated.","year":2021,"title_abstract":"{OSC}a{R}: Orthogonal Subspace Correction and Rectification of Biases in Word Embeddings Language representations are known to carry stereotypical biases and, as a result, lead to biased predictions in downstream tasks. While existing methods are effective at mitigating biases by linear projection, such methods are too aggressive: they not only remove bias, but also erase valuable information from word embeddings. We develop new measures for evaluating specific information retention that demonstrate the tradeoff between bias removal and information retention. To address this challenge, we propose OSCaR (Orthogonal Subspace Correction and Rectification), a bias-mitigating method that focuses on disentangling biased associations between concepts instead of removing concepts wholesale. Our experiments on gender biases show that OSCaR is a well-balanced approach that ensures that semantic information is retained in the embeddings and bias is also effectively mitigated.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2225418687,"Goal":"Gender Equality","Task":["Rectification of Biases","downstream tasks","information retention","bias removal","information retention","gender biases"],"Method":["Orthogonal Subspace Correction","Word Embeddings","Language representations","linear projection","OSCaR (Orthogonal Subspace Correction","Rectification)","bias - mitigating method","OSCaR"]},{"ID":"el-haj-etal-2020-financial","title":"The Financial Narrative Summarisation Shared Task ({FNS} 2020)","abstract":"This paper presents the results and findings of the Financial Narrative Summarisation shared task (FNS 2020) on summarising UK annual reports. The shared task was organised as part of the 1st Financial Narrative Processing and Financial Narrative Summarisation Workshop (FNP-FNS 2020). The shared task included one main task which is the use of either abstractive or extractive summarisation methodologies and techniques to automatically summarise UK financial annual reports. FNS summarisation shared task is the first to target financial annual reports. The data for the shared task was created and collected from publicly available UK annual reports published by firms listed on the London Stock Exchange (LSE). A total number of 24 systems from 9 different teams participated in the shared task. In addition we had 2 baseline summarisers and additional 2 topline summarisers to help evaluate and compare against the results of the participants.","year":2020,"title_abstract":"The Financial Narrative Summarisation Shared Task ({FNS} 2020) This paper presents the results and findings of the Financial Narrative Summarisation shared task (FNS 2020) on summarising UK annual reports. The shared task was organised as part of the 1st Financial Narrative Processing and Financial Narrative Summarisation Workshop (FNP-FNS 2020). The shared task included one main task which is the use of either abstractive or extractive summarisation methodologies and techniques to automatically summarise UK financial annual reports. FNS summarisation shared task is the first to target financial annual reports. The data for the shared task was created and collected from publicly available UK annual reports published by firms listed on the London Stock Exchange (LSE). A total number of 24 systems from 9 different teams participated in the shared task. In addition we had 2 baseline summarisers and additional 2 topline summarisers to help evaluate and compare against the results of the participants.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.2218966335,"Goal":"Partnership for the Goals","Task":["Financial Narrative Summarisation Shared Task","Financial Narrative Summarisation shared task","summarising","Financial Narrative Processing and Financial Narrative Summarisation Workshop","FNS summarisation shared task","summarisers","topline summarisers"],"Method":["abstractive or extractive summarisation methodologies"]},{"ID":"asakura-etal-2016-disaster","title":"Disaster Analysis using User-Generated Weather Report","abstract":"Information extraction from user-generated text has gained much attention with the growth of the Web.Disaster analysis using information from social media provides valuable, real-time, geolocation information for helping people caught up these in disasters. However, it is not convenient to analyze texts posted on social media because disaster keywords match any texts that contain words. For collecting posts about a disaster from social media, we need to develop a classifier to filter posts irrelevant to disasters. Moreover, because of the nature of social media, we can take advantage of posts that come with GPS information. However, a post does not always refer to an event occurring at the place where it has been posted. Therefore, we propose a new task of classifying whether a flood disaster occurred, in addition to predicting the geolocation of events from user-generated text. We report the annotation of the flood disaster corpus and develop a classifier to demonstrate the use of this corpus for disaster analysis.","year":2016,"title_abstract":"Disaster Analysis using User-Generated Weather Report Information extraction from user-generated text has gained much attention with the growth of the Web.Disaster analysis using information from social media provides valuable, real-time, geolocation information for helping people caught up these in disasters. However, it is not convenient to analyze texts posted on social media because disaster keywords match any texts that contain words. For collecting posts about a disaster from social media, we need to develop a classifier to filter posts irrelevant to disasters. Moreover, because of the nature of social media, we can take advantage of posts that come with GPS information. However, a post does not always refer to an event occurring at the place where it has been posted. Therefore, we propose a new task of classifying whether a flood disaster occurred, in addition to predicting the geolocation of events from user-generated text. We report the annotation of the flood disaster corpus and develop a classifier to demonstrate the use of this corpus for disaster analysis.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2212643623,"Goal":"Sustainable Cities and Communities","Task":["Disaster Analysis","User - Generated Weather Report","Information extraction","Disaster analysis","flood disaster","geolocation of events","disaster analysis"],"Method":["classifier","classifier"]},{"ID":"kaneko-bollegala-2021-dictionary","title":"Dictionary-based Debiasing of Pre-trained Word Embeddings","abstract":"Word embeddings trained on large corpora have shown to encode high levels of unfair discriminatory gender, racial, religious and ethnic biases. In contrast, human-written dictionaries describe the meanings of words in a concise, objective and an unbiased manner. We propose a method for debiasing pre-trained word embeddings using dictionaries, without requiring access to the original training resources or any knowledge regarding the word embedding algorithms used. Unlike prior work, our proposed method does not require the types of biases to be pre-defined in the form of word lists, and learns the constraints that must be satisfied by unbiased word embeddings automatically from dictionary definitions of the words. Specifically, we learn an encoder to generate a debiased version of an input word embedding such that it (a) retains the semantics of the pre-trained word embedding, (b) agrees with the unbiased definition of the word according to the dictionary, and (c) remains orthogonal to the vector space spanned by any biased basis vectors in the pre-trained word embedding space. Experimental results on standard benchmark datasets show that the proposed method can accurately remove unfair biases encoded in pre-trained word embeddings, while preserving useful semantics.","year":2021,"title_abstract":"Dictionary-based Debiasing of Pre-trained Word Embeddings Word embeddings trained on large corpora have shown to encode high levels of unfair discriminatory gender, racial, religious and ethnic biases. In contrast, human-written dictionaries describe the meanings of words in a concise, objective and an unbiased manner. We propose a method for debiasing pre-trained word embeddings using dictionaries, without requiring access to the original training resources or any knowledge regarding the word embedding algorithms used. Unlike prior work, our proposed method does not require the types of biases to be pre-defined in the form of word lists, and learns the constraints that must be satisfied by unbiased word embeddings automatically from dictionary definitions of the words. Specifically, we learn an encoder to generate a debiased version of an input word embedding such that it (a) retains the semantics of the pre-trained word embedding, (b) agrees with the unbiased definition of the word according to the dictionary, and (c) remains orthogonal to the vector space spanned by any biased basis vectors in the pre-trained word embedding space. Experimental results on standard benchmark datasets show that the proposed method can accurately remove unfair biases encoded in pre-trained word embeddings, while preserving useful semantics.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2208624184,"Goal":"Gender Equality","Task":["debiasing pre - trained word embeddings"],"Method":["Dictionary - based Debiasing of Pre - trained Word Embeddings","Word embeddings","word embedding algorithms","encoder","debiased version"]},{"ID":"mulki-ghanem-2021-mi","title":"Let-Mi: An {A}rabic {L}evantine {T}witter Dataset for Misogynistic Language","abstract":"Online misogyny has become an increasing worry for Arab women who experience gender-based online abuse on a daily basis. Misogyny automatic detection systems can assist in the prohibition of anti-women Arabic toxic content. Developing such systems is hindered by the lack of the Arabic misogyny benchmark datasets. In this paper, we introduce an Arabic Levantine Twitter dataset for Misogynistic language (LeT-Mi) to be the first benchmark dataset for Arabic misogyny. We further provide a detailed review of the dataset creation and annotation phases. The consistency of the annotations for the proposed dataset was emphasized through inter-rater agreement evaluation measures. Moreover, Let-Mi was used as an evaluation dataset through binary\/multi-\/target classification tasks conducted by several state-of-the-art machine learning systems along with Multi-Task Learning (MTL) configuration. The obtained results indicated that the performances achieved by the used systems are consistent with state-of-the-art results for languages other than Arabic, while employing MTL improved the performance of the misogyny\/target classification tasks.","year":2021,"title_abstract":"Let-Mi: An {A}rabic {L}evantine {T}witter Dataset for Misogynistic Language Online misogyny has become an increasing worry for Arab women who experience gender-based online abuse on a daily basis. Misogyny automatic detection systems can assist in the prohibition of anti-women Arabic toxic content. Developing such systems is hindered by the lack of the Arabic misogyny benchmark datasets. In this paper, we introduce an Arabic Levantine Twitter dataset for Misogynistic language (LeT-Mi) to be the first benchmark dataset for Arabic misogyny. We further provide a detailed review of the dataset creation and annotation phases. The consistency of the annotations for the proposed dataset was emphasized through inter-rater agreement evaluation measures. Moreover, Let-Mi was used as an evaluation dataset through binary\/multi-\/target classification tasks conducted by several state-of-the-art machine learning systems along with Multi-Task Learning (MTL) configuration. The obtained results indicated that the performances achieved by the used systems are consistent with state-of-the-art results for languages other than Arabic, while employing MTL improved the performance of the misogyny\/target classification tasks.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.220813483,"Goal":"Gender Equality","Task":["misogyny","prohibition of anti - women Arabic toxic content","Arabic misogyny","dataset creation","annotation phases","binary\/multi - \/target classification tasks","misogyny\/target classification tasks"],"Method":["Misogyny automatic detection systems","machine learning systems","Multi - Task Learning","MTL"]},{"ID":"hiware-etal-2020-narmada","title":"{NARMADA}: Need and Available Resource Managing Assistant for Disasters and Adversities","abstract":"Although a lot of research has been done on utilising Online Social Media during disasters, there exists no system for a specific task that is critical in a post-disaster scenario {--} identifying resource-needs and resource-availabilities in the disaster-affected region, coupled with their subsequent matching. To this end, we present NARMADA, a semi-automated platform which leverages the crowd-sourced information from social media posts for assisting post-disaster relief coordination efforts. The system employs Natural Language Processing and Information Retrieval techniques for identifying resource-needs and resource-availabilities from microblogs, extracting resources from the posts, and also matching the needs to suitable availabilities. The system is thus capable of facilitating the judicious management of resources during post-disaster relief operations.","year":2020,"title_abstract":"{NARMADA}: Need and Available Resource Managing Assistant for Disasters and Adversities Although a lot of research has been done on utilising Online Social Media during disasters, there exists no system for a specific task that is critical in a post-disaster scenario {--} identifying resource-needs and resource-availabilities in the disaster-affected region, coupled with their subsequent matching. To this end, we present NARMADA, a semi-automated platform which leverages the crowd-sourced information from social media posts for assisting post-disaster relief coordination efforts. The system employs Natural Language Processing and Information Retrieval techniques for identifying resource-needs and resource-availabilities from microblogs, extracting resources from the posts, and also matching the needs to suitable availabilities. The system is thus capable of facilitating the judicious management of resources during post-disaster relief operations.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2207440287,"Goal":"Sustainable Cities and Communities","Task":["post - disaster scenario","matching","post - disaster relief coordination efforts","identifying resource - needs","judicious management of resources","post - disaster relief operations"],"Method":["Resource Managing Assistant","NARMADA","semi - automated platform","Natural Language Processing","Information Retrieval techniques"]},{"ID":"mapelli-etal-2012-elra","title":"{ELRA} in the heart of a cooperative {HLT} world","abstract":"This paper aims at giving an overview of ELRA\u0092s recent activities. The first part elaborates on ELRA\u0092s means of boosting the sharing Language Resources (LRs) within the HLT community through its catalogues, LRE-Map initiative, as well as its work towards the integration of its LRs within the META-SHARE open infrastructure. The second part shows how ELRA helps in the development and evaluation of HLT, in particular through its numerous participations to collaborative projects for the production of resources and platforms to facilitate their production and exploitation. A third part focuses on ELRA\u0092s work for clearing IPR issues in a HLT-oriented context, one of its latest initiative being its involvement in a Fair Research Act proposal to promote the easy access to LRs to the widest community. Finally, the last part elaborates on recent actions for disseminating information and promoting cooperation in the field, e.g. an the Language Library being launched at LREC2012 and the creation of an International Standard LR Number, a LR unique identifier to enable the accurate identification of LRs. Among the other messages ELRA will be conveying the attendees are the announcement of a set of freely available resources, the establishment of a LR and Evaluation forum, etc.","year":2012,"title_abstract":"{ELRA} in the heart of a cooperative {HLT} world This paper aims at giving an overview of ELRA\u0092s recent activities. The first part elaborates on ELRA\u0092s means of boosting the sharing Language Resources (LRs) within the HLT community through its catalogues, LRE-Map initiative, as well as its work towards the integration of its LRs within the META-SHARE open infrastructure. The second part shows how ELRA helps in the development and evaluation of HLT, in particular through its numerous participations to collaborative projects for the production of resources and platforms to facilitate their production and exploitation. A third part focuses on ELRA\u0092s work for clearing IPR issues in a HLT-oriented context, one of its latest initiative being its involvement in a Fair Research Act proposal to promote the easy access to LRs to the widest community. Finally, the last part elaborates on recent actions for disseminating information and promoting cooperation in the field, e.g. an the Language Library being launched at LREC2012 and the creation of an International Standard LR Number, a LR unique identifier to enable the accurate identification of LRs. Among the other messages ELRA will be conveying the attendees are the announcement of a set of freely available resources, the establishment of a LR and Evaluation forum, etc.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.2206013501,"Goal":"Partnership for the Goals","Task":["sharing Language Resources","evaluation","HLT","production of resources","production and exploitation","clearing IPR issues","HLT","accurate identification of LRs","LR and Evaluation forum"],"Method":["ELRA\u0092s","ELRA\u0092s","HLT","catalogues","LRE - Map","LRs","ELRA","ELRA\u0092s","Language Library","LR","LR unique identifier","ELRA"]},{"ID":"bannour-etal-2021-evaluating","title":"Evaluating the carbon footprint of {NLP} methods: a survey and analysis of existing tools","abstract":"Modern Natural Language Processing (NLP) makes intensive use of deep learning methods because of the accuracy they offer for a variety of applications. Due to the significant environmental impact of deep learning, cost-benefit analysis including carbon footprint as well as accuracy measures has been suggested to better document the use of NLP methods for research or deployment. In this paper, we review the tools that are available to measure energy use and CO2 emissions of NLP methods. We describe the scope of the measures provided and compare the use of six tools (carbon tracker, experiment impact tracker, green algorithms, ML CO2 impact, energy usage and cumulator) on named entity recognition experiments performed on different computational set-ups (local server vs. computing facility). Based on these findings, we propose actionable recommendations to accurately measure the environmental impact of NLP experiments.","year":2021,"title_abstract":"Evaluating the carbon footprint of {NLP} methods: a survey and analysis of existing tools Modern Natural Language Processing (NLP) makes intensive use of deep learning methods because of the accuracy they offer for a variety of applications. Due to the significant environmental impact of deep learning, cost-benefit analysis including carbon footprint as well as accuracy measures has been suggested to better document the use of NLP methods for research or deployment. In this paper, we review the tools that are available to measure energy use and CO2 emissions of NLP methods. We describe the scope of the measures provided and compare the use of six tools (carbon tracker, experiment impact tracker, green algorithms, ML CO2 impact, energy usage and cumulator) on named entity recognition experiments performed on different computational set-ups (local server vs. computing facility). Based on these findings, we propose actionable recommendations to accurately measure the environmental impact of NLP experiments.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2203800678,"Goal":"Climate Action","Task":["named entity recognition experiments"],"Method":["Natural Language Processing","deep learning methods","deep learning","NLP methods","NLP methods","experiment impact tracker","green algorithms","cumulator)","NLP experiments"]},{"ID":"fornaciari-hovy-2019-identifying","title":"Identifying Linguistic Areas for Geolocation","abstract":"Geolocating social media posts relies on the assumption that language carries sufficient geographic information. However, locations are usually given as continuous latitude\/longitude tuples, so we first need to define discrete geographic regions that can serve as labels. Most studies use some form of clustering to discretize the continuous coordinates (Han et al., 2016). However, the resulting regions do not always correspond to existing linguistic areas. Consequently, accuracy at 100 miles tends to be good, but degrades for finer-grained distinctions, when different linguistic regions get lumped together. We describe a new algorithm, Point-to-City (P2C), an iterative k-d tree-based method for clustering geographic coordinates and associating them with towns. We create three sets of labels at different levels of granularity, and compare performance of a state-of-the-art geolocation model trained and tested with P2C labels to one with regular k-d tree labels. Even though P2C results in substantially more labels than the baseline, model accuracy increases significantly over using traditional labels at the fine-grained level, while staying comparable at 100 miles. The results suggest that identifying meaningful linguistic areas is crucial for improving geolocation at a fine-grained level.","year":2019,"title_abstract":"Identifying Linguistic Areas for Geolocation Geolocating social media posts relies on the assumption that language carries sufficient geographic information. However, locations are usually given as continuous latitude\/longitude tuples, so we first need to define discrete geographic regions that can serve as labels. Most studies use some form of clustering to discretize the continuous coordinates (Han et al., 2016). However, the resulting regions do not always correspond to existing linguistic areas. Consequently, accuracy at 100 miles tends to be good, but degrades for finer-grained distinctions, when different linguistic regions get lumped together. We describe a new algorithm, Point-to-City (P2C), an iterative k-d tree-based method for clustering geographic coordinates and associating them with towns. We create three sets of labels at different levels of granularity, and compare performance of a state-of-the-art geolocation model trained and tested with P2C labels to one with regular k-d tree labels. Even though P2C results in substantially more labels than the baseline, model accuracy increases significantly over using traditional labels at the fine-grained level, while staying comparable at 100 miles. The results suggest that identifying meaningful linguistic areas is crucial for improving geolocation at a fine-grained level.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2201368511,"Goal":"Sustainable Cities and Communities","Task":["Identifying Linguistic Areas","Geolocation Geolocating social media posts","clustering geographic coordinates","geolocation"],"Method":["clustering","Point - to - City","iterative k - d tree - based method","geolocation model","regular k - d tree","P2C"]},{"ID":"dhalleweyn-etal-2006-dutch","title":"The {D}utch-{F}lemish {HLT} Programme {STEVIN}: Essential Speech and Language Technology Resources","abstract":"In 2004 a consortium of ministries and organizations in the Netherlands and Flanders launched the comprehensive Dutch-Flemish HLT programme STEVIN (a Dutch acronym for \u0093Essential Speech and Language Technology Resources\u0094). To guarantee its Dutch-Flemish character, this large-scale programme is carried out under the auspices of the intergovernmental Dutch Language Union (NTU). The aim of STEVIN is to contribute to the further progress of HLT for the Dutch language, by raising awareness of HLT results, stimulating the demand of HLT products, promoting strategic research in HLT, and developing HLT resources that are essential and are known to be missing. Furthermore, a structure was set up for the management, maintenance and distribution of HLT resources. The STEVIN programme, which will run from 2004 to 2009, resulted from HLT activities in the Dutch language area, which were reported on at previous LREC conferences (2000, 2002, 2004). In this paper we will explain how different activities are combined in one comprehensive programme. We will show how cooperation can successfully be realized between different parties (language and speech technology, Flanders and the Netherlands, academia, industry and policy institutions) so as to achieve one common goal: progress in HLT.","year":2006,"title_abstract":"The {D}utch-{F}lemish {HLT} Programme {STEVIN}: Essential Speech and Language Technology Resources In 2004 a consortium of ministries and organizations in the Netherlands and Flanders launched the comprehensive Dutch-Flemish HLT programme STEVIN (a Dutch acronym for \u0093Essential Speech and Language Technology Resources\u0094). To guarantee its Dutch-Flemish character, this large-scale programme is carried out under the auspices of the intergovernmental Dutch Language Union (NTU). The aim of STEVIN is to contribute to the further progress of HLT for the Dutch language, by raising awareness of HLT results, stimulating the demand of HLT products, promoting strategic research in HLT, and developing HLT resources that are essential and are known to be missing. Furthermore, a structure was set up for the management, maintenance and distribution of HLT resources. The STEVIN programme, which will run from 2004 to 2009, resulted from HLT activities in the Dutch language area, which were reported on at previous LREC conferences (2000, 2002, 2004). In this paper we will explain how different activities are combined in one comprehensive programme. We will show how cooperation can successfully be realized between different parties (language and speech technology, Flanders and the Netherlands, academia, industry and policy institutions) so as to achieve one common goal: progress in HLT.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.2200920433,"Goal":"Partnership for the Goals","Task":["Speech","HLT products","strategic research","HLT","HLT resources","management","maintenance and distribution of HLT resources","speech technology","HLT"],"Method":["HLT","STEVIN","HLT","STEVIN programme","HLT"]},{"ID":"wang-iyyer-2019-casting","title":"{C}asting {L}ight on {I}nvisible {C}ities: {C}omputationally {E}ngaging with {L}iterary {C}riticism","abstract":"Literary critics often attempt to uncover meaning in a single work of literature through careful reading and analysis. Applying natural language processing methods to aid in such literary analyses remains a challenge in digital humanities. While most previous work focuses on {``}distant reading{''} by algorithmically discovering high-level patterns from large collections of literary works, here we sharpen the focus of our methods to a single literary theory about Italo Calvino{'}s postmodern novel \\textit{Invisible Cities}, which consists of 55 short descriptions of imaginary cities. Calvino has provided a classification of these cities into eleven thematic groups, but literary scholars disagree as to how trustworthy his categorization is. Due to the unique structure of this novel, we can computationally weigh in on this debate: we leverage pretrained contextualized representations to embed each city{'}s description and use unsupervised methods to cluster these embeddings. Additionally, we compare results of our computational approach to similarity judgments generated by human readers. Our work is a first step towards incorporating natural language processing into literary criticism.","year":2019,"title_abstract":"{C}asting {L}ight on {I}nvisible {C}ities: {C}omputationally {E}ngaging with {L}iterary {C}riticism Literary critics often attempt to uncover meaning in a single work of literature through careful reading and analysis. Applying natural language processing methods to aid in such literary analyses remains a challenge in digital humanities. While most previous work focuses on {``}distant reading{''} by algorithmically discovering high-level patterns from large collections of literary works, here we sharpen the focus of our methods to a single literary theory about Italo Calvino{'}s postmodern novel \\textit{Invisible Cities}, which consists of 55 short descriptions of imaginary cities. Calvino has provided a classification of these cities into eleven thematic groups, but literary scholars disagree as to how trustworthy his categorization is. Due to the unique structure of this novel, we can computationally weigh in on this debate: we leverage pretrained contextualized representations to embed each city{'}s description and use unsupervised methods to cluster these embeddings. Additionally, we compare results of our computational approach to similarity judgments generated by human readers. Our work is a first step towards incorporating natural language processing into literary criticism.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2200918049,"Goal":"Sustainable Cities and Communities","Task":["analysis","literary analyses","digital humanities","literary theory","similarity judgments","natural language processing","literary criticism"],"Method":["Literary critics","natural language processing methods","pretrained contextualized representations","unsupervised methods","computational approach"]},{"ID":"gorog-2014-taus","title":"{TAUS} post-editing course","abstract":"While there is a massive adoption of MT post-editing as a new service in the global translation industry, a common reference to skills and best practices to do this work well has been missing. TAUS took up the challenge to provide a course that would integrate with the DQF tools and the post-editing best practices developed by TAUS members in the previous years and offers both theory and practice to develop post-editing skills. The contribution of language service providers who are involved in MT and post-editing on a daily basis allowed TAUS to deliver fast on this industry need. This online course addresses the challenges for linguists and translators deciding to work on post-editing assignments and is aimed at those who want to learn the best practices and skills to become more efficient and proficient in the activity of post-editing.","year":2014,"title_abstract":"{TAUS} post-editing course While there is a massive adoption of MT post-editing as a new service in the global translation industry, a common reference to skills and best practices to do this work well has been missing. TAUS took up the challenge to provide a course that would integrate with the DQF tools and the post-editing best practices developed by TAUS members in the previous years and offers both theory and practice to develop post-editing skills. The contribution of language service providers who are involved in MT and post-editing on a daily basis allowed TAUS to deliver fast on this industry need. This online course addresses the challenges for linguists and translators deciding to work on post-editing assignments and is aimed at those who want to learn the best practices and skills to become more efficient and proficient in the activity of post-editing.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.2198739797,"Goal":"Quality Education","Task":["post - editing course","MT post - editing","global translation industry","post - editing skills","language service providers","MT","post - editing","linguists","translators","post - editing assignments","post - editing"],"Method":["TAUS","DQF tools","post - editing"]},{"ID":"prost-etal-2019-debiasing","title":"Debiasing Embeddings for Reduced Gender Bias in Text Classification","abstract":"(Bolukbasi et al., 2016) demonstrated that pretrained word embeddings can inherit gender bias from the data they were trained on. We investigate how this bias affects downstream classification tasks, using the case study of occupation classification (De-Arteaga et al., 2019). We show that traditional techniques for debiasing embeddings can actually worsen the bias of the downstream classifier by providing a less noisy channel for communicating gender information. With a relatively minor adjustment, however, we show how these same techniques can be used to simultaneously reduce bias and maintain high classification accuracy.","year":2019,"title_abstract":"Debiasing Embeddings for Reduced Gender Bias in Text Classification (Bolukbasi et al., 2016) demonstrated that pretrained word embeddings can inherit gender bias from the data they were trained on. We investigate how this bias affects downstream classification tasks, using the case study of occupation classification (De-Arteaga et al., 2019). We show that traditional techniques for debiasing embeddings can actually worsen the bias of the downstream classifier by providing a less noisy channel for communicating gender information. With a relatively minor adjustment, however, we show how these same techniques can be used to simultaneously reduce bias and maintain high classification accuracy.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2197731435,"Goal":"Gender Equality","Task":["Reduced Gender Bias","Text Classification","downstream classification tasks","occupation classification","debiasing embeddings"],"Method":["Debiasing Embeddings","downstream classifier"]},{"ID":"amir-etal-2021-impact","title":"On the Impact of Random Seeds on the Fairness of Clinical Classifiers","abstract":"Recent work has shown that fine-tuning large networks is surprisingly sensitive to changes in random seed(s). We explore the implications of this phenomenon for model fairness across demographic groups in clinical prediction tasks over electronic health records (EHR) in MIMIC-III {---}{---} the standard dataset in clinical NLP research. Apparent subgroup performance varies substantially for seeds that yield similar overall performance, although there is no evidence of a trade-off between overall and subgroup performance. However, we also find that the small sample sizes inherent to looking at intersections of minority groups and somewhat rare conditions limit our ability to accurately estimate disparities. Further, we find that jointly optimizing for high overall performance and low disparities does not yield statistically significant improvements. Our results suggest that fairness work using MIMIC-III should carefully account for variations in apparent differences that may arise from stochasticity and small sample sizes.","year":2021,"title_abstract":"On the Impact of Random Seeds on the Fairness of Clinical Classifiers Recent work has shown that fine-tuning large networks is surprisingly sensitive to changes in random seed(s). We explore the implications of this phenomenon for model fairness across demographic groups in clinical prediction tasks over electronic health records (EHR) in MIMIC-III {---}{---} the standard dataset in clinical NLP research. Apparent subgroup performance varies substantially for seeds that yield similar overall performance, although there is no evidence of a trade-off between overall and subgroup performance. However, we also find that the small sample sizes inherent to looking at intersections of minority groups and somewhat rare conditions limit our ability to accurately estimate disparities. Further, we find that jointly optimizing for high overall performance and low disparities does not yield statistically significant improvements. Our results suggest that fairness work using MIMIC-III should carefully account for variations in apparent differences that may arise from stochasticity and small sample sizes.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.2195727527,"Goal":"Reduced Inequalities","Task":["fine - tuning large networks","clinical prediction tasks","fairness"],"Method":["Clinical Classifiers","MIMIC - III"]},{"ID":"jacovi-goldberg-2021-aligning","title":"Aligning Faithful Interpretations with their Social Attribution","abstract":"Abstract We find that the requirement of model interpretations to be faithful is vague and incomplete. With interpretation by textual highlights as a case study, we present several failure cases. Borrowing concepts from social science, we identify that the problem is a misalignment between the causal chain of decisions (causal attribution) and the attribution of human behavior to the interpretation (social attribution). We reformulate faithfulness as an accurate attribution of causality to the model, and introduce the concept of aligned faithfulness: faithful causal chains that are aligned with their expected social behavior. The two steps of causal attribution and social attribution together complete the process of explaining behavior. With this formalization, we characterize various failures of misaligned faithful highlight interpretations, and propose an alternative causal chain to remedy the issues. Finally, we implement highlight explanations of the proposed causal format using contrastive explanations.","year":2021,"title_abstract":"Aligning Faithful Interpretations with their Social Attribution Abstract We find that the requirement of model interpretations to be faithful is vague and incomplete. With interpretation by textual highlights as a case study, we present several failure cases. Borrowing concepts from social science, we identify that the problem is a misalignment between the causal chain of decisions (causal attribution) and the attribution of human behavior to the interpretation (social attribution). We reformulate faithfulness as an accurate attribution of causality to the model, and introduce the concept of aligned faithfulness: faithful causal chains that are aligned with their expected social behavior. The two steps of causal attribution and social attribution together complete the process of explaining behavior. With this formalization, we characterize various failures of misaligned faithful highlight interpretations, and propose an alternative causal chain to remedy the issues. Finally, we implement highlight explanations of the proposed causal format using contrastive explanations.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2192943245,"Goal":"Climate Action","Task":["interpretation","social science","attribution)","explaining behavior"],"Method":["Aligning Faithful Interpretations","causal attribution","social attribution","causal chain","highlight explanations","causal format","contrastive explanations"]},{"ID":"fadhil-aburaed-2019-ollobot","title":"{O}llo{B}ot - Towards A Text-Based {A}rabic Health Conversational Agent: Evaluation and Results","abstract":"We introduce OlloBot, an Arabic conversational agent that assists physicians and supports patients with the care process. It doesn{'}t replace the physicians, instead provides health tracking and support and assists physicians with the care delivery through a conversation medium. The current model comprises healthy diet, physical activity, mental health, in addition to food logging. Not only OlloBot tracks user daily food, it also offers useful tips for healthier living. We will discuss the design, development and testing of OlloBot, and highlight the findings and limitations arose from the testing.","year":2019,"title_abstract":"{O}llo{B}ot - Towards A Text-Based {A}rabic Health Conversational Agent: Evaluation and Results We introduce OlloBot, an Arabic conversational agent that assists physicians and supports patients with the care process. It doesn{'}t replace the physicians, instead provides health tracking and support and assists physicians with the care delivery through a conversation medium. The current model comprises healthy diet, physical activity, mental health, in addition to food logging. Not only OlloBot tracks user daily food, it also offers useful tips for healthier living. We will discuss the design, development and testing of OlloBot, and highlight the findings and limitations arose from the testing.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.2190417945,"Goal":"Good Health and Well-Being","Task":["care process","health tracking and support","care delivery","food logging","OlloBot"],"Method":["Text - Based {A}rabic Health Conversational Agent","OlloBot","Arabic conversational agent","conversation medium","OlloBot"]},{"ID":"hamalainen-honkela-2019-co","title":"Co-Operation as an Asymmetric Form of Human-Computer Creativity. Case: Peace Machine","abstract":"This theoretical paper identifies a need for a definition of asymmetric co-creativity where creativity is expected from the computational agent but not from the human user. Our co-operative creativity framework takes into account that the computational agent has a message to convey in a co-operative fashion, which introduces a trade-off on how creative the computer can be. The requirements of co-operation are identified from an interdisciplinary point of view. We divide co-operative creativity in message creativity, contextual creativity and communicative creativity. Finally these notions are applied in the context of the Peace Machine system concept.","year":2019,"title_abstract":"Co-Operation as an Asymmetric Form of Human-Computer Creativity. Case: Peace Machine This theoretical paper identifies a need for a definition of asymmetric co-creativity where creativity is expected from the computational agent but not from the human user. Our co-operative creativity framework takes into account that the computational agent has a message to convey in a co-operative fashion, which introduces a trade-off on how creative the computer can be. The requirements of co-operation are identified from an interdisciplinary point of view. We divide co-operative creativity in message creativity, contextual creativity and communicative creativity. Finally these notions are applied in the context of the Peace Machine system concept.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.2189719081,"Goal":"Peace, Justice and Strong Institutions","Task":["Peace Machine","asymmetric co - creativity","co - operation","co - operative creativity","message creativity","contextual creativity","communicative creativity","Peace Machine system"],"Method":["Co - Operation","Asymmetric Form of Human - Computer Creativity","computational agent","co - operative creativity framework","computational agent"]},{"ID":"ch-wang-jurgens-2021-using","title":"Using Sociolinguistic Variables to Reveal Changing Attitudes Towards Sexuality and Gender","abstract":"Individuals signal aspects of their identity and beliefs through linguistic choices. Studying these choices in aggregate allows us to examine large-scale attitude shifts within a population. Here, we develop computational methods to study word choice within a sociolinguistic lexical variable{---}alternate words used to express the same concept{---}in order to test for change in the United States towards sexuality and gender. We examine two variables: i) referents to significant others, such as the word {``}partner{''} and ii) referents to an indefinite person, both of which could optionally be marked with gender. The linguistic choices in each variable allow us to study increased rates of acceptances of gay marriage and gender equality, respectively. In longitudinal analyses across Twitter and Reddit over 87M messages, we demonstrate that attitudes are changing but that these changes are driven by specific demographics within the United States. Further, in a quasi-causal analysis, we show that passages of Marriage Equality Acts in different states are drivers of linguistic change.","year":2021,"title_abstract":"Using Sociolinguistic Variables to Reveal Changing Attitudes Towards Sexuality and Gender Individuals signal aspects of their identity and beliefs through linguistic choices. Studying these choices in aggregate allows us to examine large-scale attitude shifts within a population. Here, we develop computational methods to study word choice within a sociolinguistic lexical variable{---}alternate words used to express the same concept{---}in order to test for change in the United States towards sexuality and gender. We examine two variables: i) referents to significant others, such as the word {``}partner{''} and ii) referents to an indefinite person, both of which could optionally be marked with gender. The linguistic choices in each variable allow us to study increased rates of acceptances of gay marriage and gender equality, respectively. In longitudinal analyses across Twitter and Reddit over 87M messages, we demonstrate that attitudes are changing but that these changes are driven by specific demographics within the United States. Further, in a quasi-causal analysis, we show that passages of Marriage Equality Acts in different states are drivers of linguistic change.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2186449766,"Goal":"Gender Equality","Task":["longitudinal analyses","linguistic change"],"Method":["computational methods","quasi - causal analysis"]},{"ID":"wang-etal-2022-measuring","title":"Measuring and Mitigating Name Biases in Neural Machine Translation","abstract":"Neural Machine Translation (NMT) systems exhibit problematic biases, such as stereotypical gender bias in the translation of occupation terms into languages with grammatical gender. In this paper we describe a new source of bias prevalent in NMT systems, relating to translations of sentences containing person names. To correctly translate such sentences, a NMT system needs to determine the gender of the name. We show that leading systems are particularly poor at this task, especially for female given names. This bias is deeper than given name gender: we show that the translation of terms with ambiguous sentiment can also be affected by person names, and the same holds true for proper nouns denoting race. To mitigate these biases we propose a simple but effective data augmentation method based on randomly switching entities during translation, which effectively eliminates the problem without any effect on translation quality.","year":2022,"title_abstract":"Measuring and Mitigating Name Biases in Neural Machine Translation Neural Machine Translation (NMT) systems exhibit problematic biases, such as stereotypical gender bias in the translation of occupation terms into languages with grammatical gender. In this paper we describe a new source of bias prevalent in NMT systems, relating to translations of sentences containing person names. To correctly translate such sentences, a NMT system needs to determine the gender of the name. We show that leading systems are particularly poor at this task, especially for female given names. This bias is deeper than given name gender: we show that the translation of terms with ambiguous sentiment can also be affected by person names, and the same holds true for proper nouns denoting race. To mitigate these biases we propose a simple but effective data augmentation method based on randomly switching entities during translation, which effectively eliminates the problem without any effect on translation quality.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2185528576,"Goal":"Gender Equality","Task":["Measuring and Mitigating Name Biases","Neural Machine Translation","Neural Machine Translation","translation","NMT","translations","NMT","translation","translation"],"Method":["data augmentation method"]},{"ID":"henia-etal-2021-icompass","title":"i{C}ompass at {NLP}4{IF}-2021{--}Fighting the {COVID}-19 Infodemic","abstract":"This paper provides a detailed overview of the system and its outcomes, which were produced as part of the NLP4IF Shared Task on Fighting the COVID-19 Infodemic at NAACL 2021. This task is accomplished using a variety of techniques. We used state-of-the-art contextualized text representation models that were fine-tuned for the downstream task in hand. ARBERT, MARBERT,AraBERT, Arabic ALBERT and BERT-base-arabic were used. According to the results, BERT-base-arabic had the highest 0.784 F1 score on the test set.","year":2021,"title_abstract":"i{C}ompass at {NLP}4{IF}-2021{--}Fighting the {COVID}-19 Infodemic This paper provides a detailed overview of the system and its outcomes, which were produced as part of the NLP4IF Shared Task on Fighting the COVID-19 Infodemic at NAACL 2021. This task is accomplished using a variety of techniques. We used state-of-the-art contextualized text representation models that were fine-tuned for the downstream task in hand. ARBERT, MARBERT,AraBERT, Arabic ALBERT and BERT-base-arabic were used. According to the results, BERT-base-arabic had the highest 0.784 F1 score on the test set.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2183180004,"Goal":"Climate Action","Task":["NLP4IF Shared Task","downstream task"],"Method":["contextualized text representation models","BERT"]},{"ID":"moorthy-etal-2018-nike","title":"Is {N}ike female? Exploring the role of sound symbolism in predicting brand name gender","abstract":"Are brand names such as Nike female or male? Previous research suggests that the sound of a person{'}s first name is associated with the person{'}s gender, but no research has tried to use this knowledge to assess the gender of brand names. We present a simple computational approach that uses sound symbolism to address this open issue. Consistent with previous research, a model trained on various linguistic features of name endings predicts human gender with high accuracy. Applying this model to a data set of over a thousand commercially-traded brands in 17 product categories, our results reveal an overall bias toward male names, cutting across both male-oriented product categories as well as female-oriented categories. In addition, we find variation within categories, suggesting that firms might be seeking to imbue their brands with differentiating characteristics as part of their competitive strategy.","year":2018,"title_abstract":"Is {N}ike female? Exploring the role of sound symbolism in predicting brand name gender Are brand names such as Nike female or male? Previous research suggests that the sound of a person{'}s first name is associated with the person{'}s gender, but no research has tried to use this knowledge to assess the gender of brand names. We present a simple computational approach that uses sound symbolism to address this open issue. Consistent with previous research, a model trained on various linguistic features of name endings predicts human gender with high accuracy. Applying this model to a data set of over a thousand commercially-traded brands in 17 product categories, our results reveal an overall bias toward male names, cutting across both male-oriented product categories as well as female-oriented categories. In addition, we find variation within categories, suggesting that firms might be seeking to imbue their brands with differentiating characteristics as part of their competitive strategy.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2180441618,"Goal":"Gender Equality","Task":["predicting brand name gender"],"Method":["computational approach","sound symbolism"]},{"ID":"ciosici-etal-2021-perhaps","title":"Perhaps {PTLM}s Should Go to School {--} A Task to Assess Open Book and Closed Book {QA}","abstract":"Our goal is to deliver a new task and leaderboard to stimulate research on question answering and pre-trained language models (PTLMs) to understand a significant instructional document, e.g., an introductory college textbook or a manual. PTLMs have shown great success in many question-answering tasks, given significant supervised training, but much less so in zero-shot settings. We propose a new task that includes two college-level introductory texts in the social sciences (American Government 2e) and humanities (U.S. History), hundreds of true\/false statements based on review questions written by the textbook authors, validation\/development tests based on the first eight chapters of the textbooks, blind tests based on the remaining textbook chapters, and baseline results given state-of-the-art PTLMs. Since the questions are balanced, random performance should be {\\textasciitilde}50{\\%}. T5, fine-tuned with BoolQ achieves the same performance, suggesting that the textbook{'}s content is not pre-represented in the PTLM. Taking the exam closed book, but having read the textbook (i.e., adding the textbook to T5{'}s pre-training), yields at best minor improvement (56{\\%}), suggesting that the PTLM may not have {``}understood{''} the textbook (or perhaps misunderstood the questions). Performance is better ({\\textasciitilde}60{\\%}) when the exam is taken open-book (i.e., allowing the machine to automatically retrieve a paragraph and use it to answer the question).","year":2021,"title_abstract":"Perhaps {PTLM}s Should Go to School {--} A Task to Assess Open Book and Closed Book {QA} Our goal is to deliver a new task and leaderboard to stimulate research on question answering and pre-trained language models (PTLMs) to understand a significant instructional document, e.g., an introductory college textbook or a manual. PTLMs have shown great success in many question-answering tasks, given significant supervised training, but much less so in zero-shot settings. We propose a new task that includes two college-level introductory texts in the social sciences (American Government 2e) and humanities (U.S. History), hundreds of true\/false statements based on review questions written by the textbook authors, validation\/development tests based on the first eight chapters of the textbooks, blind tests based on the remaining textbook chapters, and baseline results given state-of-the-art PTLMs. Since the questions are balanced, random performance should be {\\textasciitilde}50{\\%}. T5, fine-tuned with BoolQ achieves the same performance, suggesting that the textbook{'}s content is not pre-represented in the PTLM. Taking the exam closed book, but having read the textbook (i.e., adding the textbook to T5{'}s pre-training), yields at best minor improvement (56{\\%}), suggesting that the PTLM may not have {``}understood{''} the textbook (or perhaps misunderstood the questions). Performance is better ({\\textasciitilde}60{\\%}) when the exam is taken open-book (i.e., allowing the machine to automatically retrieve a paragraph and use it to answer the question).","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.21787709,"Goal":"Quality Education","Task":["question answering","question - answering tasks","supervised training","zero - shot settings"],"Method":["language models","PTLMs","PTLMs","BoolQ","PTLM","PTLM"]},{"ID":"jiao-luo-2021-gender","title":"Gender Bias Hidden Behind {C}hinese Word Embeddings: The Case of {C}hinese Adjectives","abstract":"Gender bias in word embeddings gradually becomes a vivid research field in recent years. Most studies in this field aim at measurement and debiasing methods with English as the target language. This paper investigates gender bias in static word embeddings from a unique perspective, Chinese adjectives. By training word representations with different models, the gender bias behind the vectors of adjectives is assessed. Through a comparison between the produced results and a human scored data set, we demonstrate how gender bias encoded in word embeddings differentiates from people{'}s attitudes.","year":2021,"title_abstract":"Gender Bias Hidden Behind {C}hinese Word Embeddings: The Case of {C}hinese Adjectives Gender bias in word embeddings gradually becomes a vivid research field in recent years. Most studies in this field aim at measurement and debiasing methods with English as the target language. This paper investigates gender bias in static word embeddings from a unique perspective, Chinese adjectives. By training word representations with different models, the gender bias behind the vectors of adjectives is assessed. Through a comparison between the produced results and a human scored data set, we demonstrate how gender bias encoded in word embeddings differentiates from people{'}s attitudes.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.21737881,"Goal":"Gender Equality","Task":["word embeddings","static word embeddings"],"Method":["Word Embeddings","debiasing methods","word representations"]},{"ID":"blasi-etal-2022-systematic","title":"Systematic Inequalities in Language Technology Performance across the World{'}s Languages","abstract":"Natural language processing (NLP) systems have become a central technology in communication, education, medicine, artificial intelligence, and many other domains of research and development. While the performance of NLP methods has grown enormously over the last decade, this progress has been restricted to a minuscule subset of the world{'}s $\\approx$6,500 languages. We introduce a framework for estimating the global utility of language technologies as revealed in a comprehensive snapshot of recent publications in NLP. Our analyses involve the field at large, but also more in-depth studies on both user-facing technologies (machine translation, language understanding, question answering, text-to-speech synthesis) as well as foundational NLP tasks (dependency parsing, morphological inflection). In the process, we (1) quantify disparities in the current state of NLP research, (2) explore some of its associated societal and academic factors, and (3) produce tailored recommendations for evidence-based policy making aimed at promoting more global and equitable language technologies. Data and code to reproduce the findings discussed in this paper areavailable on GitHub (https:\/\/github.com\/neubig\/globalutility).","year":2022,"title_abstract":"Systematic Inequalities in Language Technology Performance across the World{'}s Languages Natural language processing (NLP) systems have become a central technology in communication, education, medicine, artificial intelligence, and many other domains of research and development. While the performance of NLP methods has grown enormously over the last decade, this progress has been restricted to a minuscule subset of the world{'}s $\\approx$6,500 languages. We introduce a framework for estimating the global utility of language technologies as revealed in a comprehensive snapshot of recent publications in NLP. Our analyses involve the field at large, but also more in-depth studies on both user-facing technologies (machine translation, language understanding, question answering, text-to-speech synthesis) as well as foundational NLP tasks (dependency parsing, morphological inflection). In the process, we (1) quantify disparities in the current state of NLP research, (2) explore some of its associated societal and academic factors, and (3) produce tailored recommendations for evidence-based policy making aimed at promoting more global and equitable language technologies. Data and code to reproduce the findings discussed in this paper areavailable on GitHub (https:\/\/github.com\/neubig\/globalutility).","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.2172393352,"Goal":"Reduced Inequalities","Task":["Language Technology","Natural language processing","communication","education","medicine","artificial intelligence","research and development","language technologies","NLP","translation","language understanding","question answering","text - to - speech synthesis)","foundational NLP tasks","parsing","morphological inflection)","NLP","evidence - based policy making","global and equitable language technologies"],"Method":["NLP methods","user - facing technologies"]},{"ID":"xu-etal-2022-faoi","title":"Faoi Gheasa an adaptive game for {I}rish language learning","abstract":"In this paper, we present a game with a purpose (GWAP) (Von Ahn 2006). The aim of the game is to promote language learning and {`}noticing{'} (Skehan, 2013). The game has been designed for Irish, but the framework could be used for other languages. Irish is a minority language which means that L2 learners have limited opportunities for exposure to the language, and additionally, there are also limited (digital) learning resources available. This research incorporates game development, language pedagogy and ICALL language materials development. This paper will focus on the language materials development as this is a bottleneck in the teaching and learning of minority and endangered languages.","year":2022,"title_abstract":"Faoi Gheasa an adaptive game for {I}rish language learning In this paper, we present a game with a purpose (GWAP) (Von Ahn 2006). The aim of the game is to promote language learning and {`}noticing{'} (Skehan, 2013). The game has been designed for Irish, but the framework could be used for other languages. Irish is a minority language which means that L2 learners have limited opportunities for exposure to the language, and additionally, there are also limited (digital) learning resources available. This research incorporates game development, language pedagogy and ICALL language materials development. This paper will focus on the language materials development as this is a bottleneck in the teaching and learning of minority and endangered languages.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.2171123922,"Goal":"Quality Education","Task":["{I}rish language learning","language learning","game development","language pedagogy","ICALL language materials development","language materials development","teaching and learning of minority and endangered languages"],"Method":["adaptive game"]},{"ID":"zamani-etal-2018-residualized","title":"Residualized Factor Adaptation for Community Social Media Prediction Tasks","abstract":"Predictive models over social media language have shown promise in capturing community outcomes, but approaches thus far largely neglect the socio-demographic context (e.g. age, education rates, race) of the community from which the language originates. For example, it may be inaccurate to assume people in Mobile, Alabama, where the population is relatively older, will use words the same way as those from San Francisco, where the median age is younger with a higher rate of college education. In this paper, we present residualized factor adaptation, a novel approach to community prediction tasks which both (a) effectively integrates community attributes, as well as (b) adapts linguistic features to community attributes (factors). We use eleven demographic and socioeconomic attributes, and evaluate our approach over five different community-level predictive tasks, spanning health (heart disease mortality, percent fair\/poor health), psychology (life satisfaction), and economics (percent housing price increase, foreclosure rate). Our evaluation shows that residualized factor adaptation significantly improves 4 out of 5 community-level outcome predictions over prior state-of-the-art for incorporating socio-demographic contexts.","year":2018,"title_abstract":"Residualized Factor Adaptation for Community Social Media Prediction Tasks Predictive models over social media language have shown promise in capturing community outcomes, but approaches thus far largely neglect the socio-demographic context (e.g. age, education rates, race) of the community from which the language originates. For example, it may be inaccurate to assume people in Mobile, Alabama, where the population is relatively older, will use words the same way as those from San Francisco, where the median age is younger with a higher rate of college education. In this paper, we present residualized factor adaptation, a novel approach to community prediction tasks which both (a) effectively integrates community attributes, as well as (b) adapts linguistic features to community attributes (factors). We use eleven demographic and socioeconomic attributes, and evaluate our approach over five different community-level predictive tasks, spanning health (heart disease mortality, percent fair\/poor health), psychology (life satisfaction), and economics (percent housing price increase, foreclosure rate). Our evaluation shows that residualized factor adaptation significantly improves 4 out of 5 community-level outcome predictions over prior state-of-the-art for incorporating socio-demographic contexts.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2170589268,"Goal":"Sustainable Cities and Communities","Task":["Community Social Media Prediction Tasks","community outcomes","community prediction tasks","community - level predictive tasks","health","economics","community - level outcome predictions","socio - demographic contexts"],"Method":["Residualized Factor Adaptation","Predictive models","residualized factor adaptation","residualized factor adaptation"]},{"ID":"zhong-etal-2021-qmsum","title":"{QMS}um: A New Benchmark for Query-based Multi-domain Meeting Summarization","abstract":"Meetings are a key component of human collaboration. As increasing numbers of meetings are recorded and transcribed, meeting summaries have become essential to remind those who may or may not have attended the meetings about the key decisions made and the tasks to be completed. However, it is hard to create a single short summary that covers all the content of a long meeting involving multiple people and topics. In order to satisfy the needs of different types of users, we define a new query-based multi-domain meeting summarization task, where models have to select and summarize relevant spans of meetings in response to a query, and we introduce QMSum, a new benchmark for this task. QMSum consists of 1,808 query-summary pairs over 232 meetings in multiple domains. Besides, we investigate a locate-then-summarize method and evaluate a set of strong summarization baselines on the task. Experimental results and manual analysis reveal that QMSum presents significant challenges in long meeting summarization for future research. Dataset is available at \\url{https:\/\/github.com\/Yale-LILY\/QMSum}.","year":2021,"title_abstract":"{QMS}um: A New Benchmark for Query-based Multi-domain Meeting Summarization Meetings are a key component of human collaboration. As increasing numbers of meetings are recorded and transcribed, meeting summaries have become essential to remind those who may or may not have attended the meetings about the key decisions made and the tasks to be completed. However, it is hard to create a single short summary that covers all the content of a long meeting involving multiple people and topics. In order to satisfy the needs of different types of users, we define a new query-based multi-domain meeting summarization task, where models have to select and summarize relevant spans of meetings in response to a query, and we introduce QMSum, a new benchmark for this task. QMSum consists of 1,808 query-summary pairs over 232 meetings in multiple domains. Besides, we investigate a locate-then-summarize method and evaluate a set of strong summarization baselines on the task. Experimental results and manual analysis reveal that QMSum presents significant challenges in long meeting summarization for future research. Dataset is available at \\url{https:\/\/github.com\/Yale-LILY\/QMSum}.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.2170063853,"Goal":"Partnership for the Goals","Task":["Query - based Multi - domain Meeting Summarization Meetings","human collaboration","query - based multi - domain meeting summarization task","summarization","long meeting summarization"],"Method":["QMSum","QMSum","locate - then - summarize method","QMSum"]},{"ID":"ni-chiarain-etal-2022-using","title":"Using Speech and {NLP} Resources to build an i{CALL} platform for a minority language, the story of An Sc{\\'e}ala{\\'\\i}, the {I}rish experience to date","abstract":"This paper describes how emerging linguistic resources and technologies can be used to build a language learning platform for Irish, an endangered language. This platform, An Sc{\\'e}ala{\\'\\i}, harvests learner corpora - a vital resource both to study the stages of learners{'} language acquisition and to guide future platform development. A technical description of the platform is provided, including details of how different speech technologies and linguistic resources are fused to provide a holistic learner experience. The active continuous participation of the community, and platform evaluations by learners and teachers, are discussed.","year":2022,"title_abstract":"Using Speech and {NLP} Resources to build an i{CALL} platform for a minority language, the story of An Sc{\\'e}ala{\\'\\i}, the {I}rish experience to date This paper describes how emerging linguistic resources and technologies can be used to build a language learning platform for Irish, an endangered language. This platform, An Sc{\\'e}ala{\\'\\i}, harvests learner corpora - a vital resource both to study the stages of learners{'} language acquisition and to guide future platform development. A technical description of the platform is provided, including details of how different speech technologies and linguistic resources are fused to provide a holistic learner experience. The active continuous participation of the community, and platform evaluations by learners and teachers, are discussed.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2167122066,"Goal":"Sustainable Cities and Communities","Task":["learners{'} language acquisition"],"Method":["i{CALL} platform","Sc{\\'e}ala{\\'\\i}","language learning platform","Sc{\\'e}ala{\\'\\i}","speech technologies"]},{"ID":"wevers-2019-using","title":"Using Word Embeddings to Examine Gender Bias in {D}utch Newspapers, 1950-1990","abstract":"Contemporary debates on filter bubbles and polarization in public and social media raise the question to what extent news media of the past exhibited biases. This paper specifically examines bias related to gender in six Dutch national newspapers between 1950 and 1990. We measure bias related to gender by comparing local changes in word embedding models trained on newspapers with divergent ideological backgrounds. We demonstrate clear differences in gender bias and changes within and between newspapers over time. In relation to themes such as sexuality and leisure, we see the bias moving toward women, whereas, generally, the bias shifts in the direction of men, despite growing female employment number and feminist movements. Even though Dutch society became less stratified ideologically (depillarization), we found an increasing divergence in gender bias between religious and social-democratic on the one hand and liberal newspapers on the other. Methodologically, this paper illustrates how word embeddings can be used to examine historical language change. Future work will investigate how fine-tuning deep contextualized embedding models, such as ELMO, might be used for similar tasks with greater contextual information.","year":2019,"title_abstract":"Using Word Embeddings to Examine Gender Bias in {D}utch Newspapers, 1950-1990 Contemporary debates on filter bubbles and polarization in public and social media raise the question to what extent news media of the past exhibited biases. This paper specifically examines bias related to gender in six Dutch national newspapers between 1950 and 1990. We measure bias related to gender by comparing local changes in word embedding models trained on newspapers with divergent ideological backgrounds. We demonstrate clear differences in gender bias and changes within and between newspapers over time. In relation to themes such as sexuality and leisure, we see the bias moving toward women, whereas, generally, the bias shifts in the direction of men, despite growing female employment number and feminist movements. Even though Dutch society became less stratified ideologically (depillarization), we found an increasing divergence in gender bias between religious and social-democratic on the one hand and liberal newspapers on the other. Methodologically, this paper illustrates how word embeddings can be used to examine historical language change. Future work will investigate how fine-tuning deep contextualized embedding models, such as ELMO, might be used for similar tasks with greater contextual information.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2167001069,"Goal":"Gender Equality","Task":["Gender Bias"],"Method":["Word Embeddings","word embedding models","word embeddings","deep contextualized embedding models","ELMO"]},{"ID":"schnoebelen-2017-goal","title":"Goal-Oriented Design for Ethical Machine Learning and {NLP}","abstract":"The argument made in this paper is that to act ethically in machine learning and NLP requires focusing on goals. NLP projects are often classificatory systems that deal with human subjects, which means that goals from people affected by the systems should be included. The paper takes as its core example a model that detects criminality, showing the problems of training data, categories, and outcomes. The paper is oriented to the kinds of critiques on power and the reproduction of inequality that are found in social theory, but it also includes concrete suggestions on how to put goal-oriented design into practice.","year":2017,"title_abstract":"Goal-Oriented Design for Ethical Machine Learning and {NLP} The argument made in this paper is that to act ethically in machine learning and NLP requires focusing on goals. NLP projects are often classificatory systems that deal with human subjects, which means that goals from people affected by the systems should be included. The paper takes as its core example a model that detects criminality, showing the problems of training data, categories, and outcomes. The paper is oriented to the kinds of critiques on power and the reproduction of inequality that are found in social theory, but it also includes concrete suggestions on how to put goal-oriented design into practice.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2160693109,"Goal":"Gender Equality","Task":["Ethical Machine Learning","machine learning","NLP","NLP projects"],"Method":["Goal - Oriented Design","classificatory systems","social theory","goal - oriented design"]},{"ID":"glandt-etal-2021-stance","title":"Stance Detection in {COVID}-19 Tweets","abstract":"The prevalence of the COVID-19 pandemic in day-to-day life has yielded large amounts of stance detection data on social media sites, as users turn to social media to share their views regarding various issues related to the pandemic, e.g. stay at home mandates and wearing face masks when out in public. We set out to make use of this data by collecting the stance expressed by Twitter users, with respect to topics revolving around the pandemic. We annotate a new stance detection dataset, called COVID-19-Stance. Using this newly annotated dataset, we train several established stance detection models to ascertain a baseline performance for this specific task. To further improve the performance, we employ self-training and domain adaptation approaches to take advantage of large amounts of unlabeled data and existing stance detection datasets. The dataset, code, and other resources are available on GitHub.","year":2021,"title_abstract":"Stance Detection in {COVID}-19 Tweets The prevalence of the COVID-19 pandemic in day-to-day life has yielded large amounts of stance detection data on social media sites, as users turn to social media to share their views regarding various issues related to the pandemic, e.g. stay at home mandates and wearing face masks when out in public. We set out to make use of this data by collecting the stance expressed by Twitter users, with respect to topics revolving around the pandemic. We annotate a new stance detection dataset, called COVID-19-Stance. Using this newly annotated dataset, we train several established stance detection models to ascertain a baseline performance for this specific task. To further improve the performance, we employ self-training and domain adaptation approaches to take advantage of large amounts of unlabeled data and existing stance detection datasets. The dataset, code, and other resources are available on GitHub.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2156172246,"Goal":"Climate Action","Task":["Stance Detection","stance detection","stance detection"],"Method":["stance detection models","self - training","domain adaptation approaches"]},{"ID":"rosa-teixeira-etal-2020-damata","title":"{D}a{M}ata: A Robot-Journalist Covering the {B}razilian {A}mazon Deforestation","abstract":"This demo paper introduces DaMata, a robot-journalist covering deforestation in the Brazilian Amazon. The robot-journalist is based on a pipeline architecture of Natural Language Generation, which yields multilingual daily and monthly reports based on the public data provided by DETER, a real-time deforestation satellite monitor developed and maintained by the Brazilian National Institute for Space Research (INPE). DaMata automatically generates reports in Brazilian Portuguese and English and publishes them on the Twitter platform. Corpus and code are publicly available.","year":2020,"title_abstract":"{D}a{M}ata: A Robot-Journalist Covering the {B}razilian {A}mazon Deforestation This demo paper introduces DaMata, a robot-journalist covering deforestation in the Brazilian Amazon. The robot-journalist is based on a pipeline architecture of Natural Language Generation, which yields multilingual daily and monthly reports based on the public data provided by DETER, a real-time deforestation satellite monitor developed and maintained by the Brazilian National Institute for Space Research (INPE). DaMata automatically generates reports in Brazilian Portuguese and English and publishes them on the Twitter platform. Corpus and code are publicly available.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.2154282033,"Goal":"Life on Land","Task":["deforestation","Natural Language Generation"],"Method":["Robot - Journalist","DaMata","robot - journalist","robot - journalist","pipeline architecture","DaMata","Twitter platform"]},{"ID":"garimella-etal-2019-womens","title":"Women{'}s Syntactic Resilience and Men{'}s Grammatical Luck: Gender-Bias in Part-of-Speech Tagging and Dependency Parsing","abstract":"Several linguistic studies have shown the prevalence of various lexical and grammatical patterns in texts authored by a person of a particular gender, but models for part-of-speech tagging and dependency parsing have still not adapted to account for these differences. To address this, we annotate the Wall Street Journal part of the Penn Treebank with the gender information of the articles{'} authors, and build taggers and parsers trained on this data that show performance differences in text written by men and women. Further analyses reveal numerous part-of-speech tags and syntactic relations whose prediction performances benefit from the prevalence of a specific gender in the training data. The results underscore the importance of accounting for gendered differences in syntactic tasks, and outline future venues for developing more accurate taggers and parsers. We release our data to the research community.","year":2019,"title_abstract":"Women{'}s Syntactic Resilience and Men{'}s Grammatical Luck: Gender-Bias in Part-of-Speech Tagging and Dependency Parsing Several linguistic studies have shown the prevalence of various lexical and grammatical patterns in texts authored by a person of a particular gender, but models for part-of-speech tagging and dependency parsing have still not adapted to account for these differences. To address this, we annotate the Wall Street Journal part of the Penn Treebank with the gender information of the articles{'} authors, and build taggers and parsers trained on this data that show performance differences in text written by men and women. Further analyses reveal numerous part-of-speech tags and syntactic relations whose prediction performances benefit from the prevalence of a specific gender in the training data. The results underscore the importance of accounting for gendered differences in syntactic tasks, and outline future venues for developing more accurate taggers and parsers. We release our data to the research community.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2152847946,"Goal":"Gender Equality","Task":["Part - of - Speech Tagging","Dependency Parsing","part - of - speech tagging","dependency parsing","prediction","syntactic tasks"],"Method":["taggers","parsers","taggers","parsers"]},{"ID":"shah-etal-2019-content","title":"Content Customization for Micro Learning using Human Augmented {AI} Techniques","abstract":"Visual content has been proven to be effective for micro-learning compared to other media. In this paper, we discuss leveraging this observation in our efforts to build audio-visual content for young learners{'} vocabulary learning. We attempt to tackle two major issues in the process of traditional visual curation tasks. Generic learning videos do not necessarily satisfy the unique context of a learner and\/or an educator, and hence may not result in maximal learning outcomes. Also, manual video curation by educators is a highly labor-intensive process. To this end, we present a customizable micro-learning audio-visual content curation tool that is designed to reduce the human (educator) effort in creating just-in-time learning videos from a textual description (learning script). This provides educators with control of the content while preparing the learning scripts, and in turn can also be customized to capture the desired learning objectives and outcomes. As a use case, we automatically generate learning videos with British National Corpus{'} (BNC) frequently spoken vocabulary words and evaluate them with experts. They positively recommended the generated learning videos with an average rating of 4.25 on a Likert scale of 5 points. The inter-annotator agreement between the experts for the video quality was substantial (Fleiss Kappa=0.62) with an overall agreement of 81{\\%}.","year":2019,"title_abstract":"Content Customization for Micro Learning using Human Augmented {AI} Techniques Visual content has been proven to be effective for micro-learning compared to other media. In this paper, we discuss leveraging this observation in our efforts to build audio-visual content for young learners{'} vocabulary learning. We attempt to tackle two major issues in the process of traditional visual curation tasks. Generic learning videos do not necessarily satisfy the unique context of a learner and\/or an educator, and hence may not result in maximal learning outcomes. Also, manual video curation by educators is a highly labor-intensive process. To this end, we present a customizable micro-learning audio-visual content curation tool that is designed to reduce the human (educator) effort in creating just-in-time learning videos from a textual description (learning script). This provides educators with control of the content while preparing the learning scripts, and in turn can also be customized to capture the desired learning objectives and outcomes. As a use case, we automatically generate learning videos with British National Corpus{'} (BNC) frequently spoken vocabulary words and evaluate them with experts. They positively recommended the generated learning videos with an average rating of 4.25 on a Likert scale of 5 points. The inter-annotator agreement between the experts for the video quality was substantial (Fleiss Kappa=0.62) with an overall agreement of 81{\\%}.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.2151051164,"Goal":"Quality Education","Task":["Micro Learning","micro - learning","audio - visual content","young learners{'} vocabulary learning","visual curation tasks","manual video curation","just - in - time learning videos"],"Method":["Content Customization","Human Augmented {AI} Techniques","Visual content","micro - learning audio - visual content curation tool"]},{"ID":"loukina-etal-2019-many","title":"The many dimensions of algorithmic fairness in educational applications","abstract":"The issues of algorithmic fairness and bias have recently featured prominently in many publications highlighting the fact that training the algorithms for maximum performance may often result in predictions that are biased against various groups. Educational applications based on NLP and speech processing technologies often combine multiple complex machine learning algorithms and are thus vulnerable to the same sources of bias as other machine learning systems. Yet such systems can have high impact on people{'}s lives especially when deployed as part of high-stakes tests. In this paper we discuss different definitions of fairness and possible ways to apply them to educational applications. We then use simulated and real data to consider how test-takers{'} native language backgrounds can affect their automated scores on an English language proficiency assessment. We illustrate that total fairness may not be achievable and that different definitions of fairness may require different solutions.","year":2019,"title_abstract":"The many dimensions of algorithmic fairness in educational applications The issues of algorithmic fairness and bias have recently featured prominently in many publications highlighting the fact that training the algorithms for maximum performance may often result in predictions that are biased against various groups. Educational applications based on NLP and speech processing technologies often combine multiple complex machine learning algorithms and are thus vulnerable to the same sources of bias as other machine learning systems. Yet such systems can have high impact on people{'}s lives especially when deployed as part of high-stakes tests. In this paper we discuss different definitions of fairness and possible ways to apply them to educational applications. We then use simulated and real data to consider how test-takers{'} native language backgrounds can affect their automated scores on an English language proficiency assessment. We illustrate that total fairness may not be achievable and that different definitions of fairness may require different solutions.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.2149883807,"Goal":"Quality Education","Task":["algorithmic fairness","educational applications","Educational applications","educational applications","English language proficiency assessment"],"Method":["NLP and speech processing technologies","machine learning algorithms","machine learning systems"]},{"ID":"gupta-etal-2022-mitigating","title":"Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal","abstract":"Language models excel at generating coherent text, and model compression techniques such as knowledge distillation have enabled their use in resource-constrained settings. However, these models can be biased in multiple ways, including the unfounded association of male and female genders with gender-neutral professions. Therefore, knowledge distillation without any fairness constraints may preserve or exaggerate the teacher model{'}s biases onto the distilled model. To this end, we present a novel approach to mitigate gender disparity in text generation by learning a fair model during knowledge distillation. We propose two modifications to the base knowledge distillation based on counterfactual role reversal{---}modifying teacher probabilities and augmenting the training set. We evaluate gender polarity across professions in open-ended text generated from the resulting distilled and finetuned GPT{--}2 models and demonstrate a substantial reduction in gender disparity with only a minor compromise in utility. Finally, we observe that language models that reduce gender polarity in language generation do not improve embedding fairness or downstream classification fairness.","year":2022,"title_abstract":"Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal Language models excel at generating coherent text, and model compression techniques such as knowledge distillation have enabled their use in resource-constrained settings. However, these models can be biased in multiple ways, including the unfounded association of male and female genders with gender-neutral professions. Therefore, knowledge distillation without any fairness constraints may preserve or exaggerate the teacher model{'}s biases onto the distilled model. To this end, we present a novel approach to mitigate gender disparity in text generation by learning a fair model during knowledge distillation. We propose two modifications to the base knowledge distillation based on counterfactual role reversal{---}modifying teacher probabilities and augmenting the training set. We evaluate gender polarity across professions in open-ended text generated from the resulting distilled and finetuned GPT{--}2 models and demonstrate a substantial reduction in gender disparity with only a minor compromise in utility. Finally, we observe that language models that reduce gender polarity in language generation do not improve embedding fairness or downstream classification fairness.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2149679512,"Goal":"Gender Equality","Task":["Mitigating Gender Bias","resource - constrained settings","gender disparity","text generation","language generation","downstream classification fairness"],"Method":["Distilled Language Models","Counterfactual Role Reversal","Language models","model compression techniques","knowledge distillation","knowledge distillation","teacher model{'}s","distilled model","fair model","knowledge distillation","knowledge distillation","counterfactual role reversal{","distilled and finetuned GPT{ - - }2 models","language models"]},{"ID":"tierney-volfovsky-2021-sensitivity","title":"Sensitivity Analysis for Causal Mediation through Text: an Application to Political Polarization","abstract":"We introduce a procedure to examine a text-as-mediator problem from a novel randomized experiment that studied the effect of conversations on political polarization. In this randomized experiment, Americans from the Democratic and Republican parties were either randomly paired with one-another to have an anonymous conversation about politics or alternatively not assigned to a conversation {---} change in political polarization over time was measured for all participants. This paper analyzes the text of the conversations to identify potential mediators of depolarization and is faced with a unique challenge, necessitated by the primary research hypothesis, that individuals in the control condition do not have conversations and so lack observed text data. We highlight the importance of using domain knowledge to perform dimension reduction on the text data, and describe a procedure to characterize indirect effects via text when the text is only observed in one arm of the experiment.","year":2021,"title_abstract":"Sensitivity Analysis for Causal Mediation through Text: an Application to Political Polarization We introduce a procedure to examine a text-as-mediator problem from a novel randomized experiment that studied the effect of conversations on political polarization. In this randomized experiment, Americans from the Democratic and Republican parties were either randomly paired with one-another to have an anonymous conversation about politics or alternatively not assigned to a conversation {---} change in political polarization over time was measured for all participants. This paper analyzes the text of the conversations to identify potential mediators of depolarization and is faced with a unique challenge, necessitated by the primary research hypothesis, that individuals in the control condition do not have conversations and so lack observed text data. We highlight the importance of using domain knowledge to perform dimension reduction on the text data, and describe a procedure to characterize indirect effects via text when the text is only observed in one arm of the experiment.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.2147739977,"Goal":"Reduced Inequalities","Task":["Causal Mediation","Political Polarization","text - as - mediator problem","political polarization","dimension reduction"],"Method":["Sensitivity Analysis"]},{"ID":"griesel-etal-2019-thinking","title":"Thinking globally, acting locally {--} Progress in the {A}frican {W}ordnet Project","abstract":"The African Wordnet Project (AWN) includes all nine indigenous South African languages, namely isiZulu, isiXhosa, Setswana, Sesotho sa Leboa, Tshivenda, Siswati, Sesotho, isiNdebele and Xitsonga. The AWN currently includes 61 000 synsets as well as definitions and usage examples for a large part of the synsets. The project recently received extended funding from the South African Centre for Digital Language Resources (SADiLaR) and aims to update all aspects of the current resource, including the seed list used for new development, software tools used and mapping the AWN to the latest version of PWN 3.1. As with any resource development project, it is essential to also include phases of focused quality assurance and updating of the basis on which the resource is built. The African languages remain under-resourced. This paper describes progress made in the development of the AWN as well as recent technical improvements.","year":2019,"title_abstract":"Thinking globally, acting locally {--} Progress in the {A}frican {W}ordnet Project The African Wordnet Project (AWN) includes all nine indigenous South African languages, namely isiZulu, isiXhosa, Setswana, Sesotho sa Leboa, Tshivenda, Siswati, Sesotho, isiNdebele and Xitsonga. The AWN currently includes 61 000 synsets as well as definitions and usage examples for a large part of the synsets. The project recently received extended funding from the South African Centre for Digital Language Resources (SADiLaR) and aims to update all aspects of the current resource, including the seed list used for new development, software tools used and mapping the AWN to the latest version of PWN 3.1. As with any resource development project, it is essential to also include phases of focused quality assurance and updating of the basis on which the resource is built. The African languages remain under-resourced. This paper describes progress made in the development of the AWN as well as recent technical improvements.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.2146992981,"Goal":"Partnership for the Goals","Task":["resource development project"],"Method":["AWN","software tools","AWN","PWN 3","AWN"]},{"ID":"chiril-etal-2020-said","title":"He said {``}who{'}s gonna take care of your children when you are at {ACL}?{''}: Reported Sexist Acts are Not Sexist","abstract":"In a context of offensive content mediation on social media now regulated by European laws, it is important not only to be able to automatically detect sexist content but also to identify if a message with a sexist content is really sexist or is a story of sexism experienced by a woman. We propose: (1) a new characterization of sexist content inspired by speech acts theory and discourse analysis studies, (2) the first French dataset annotated for sexism detection, and (3) a set of deep learning experiments trained on top of a combination of several tweet{'}s vectorial representations (word embeddings, linguistic features, and various generalization strategies). Our results are encouraging and constitute a first step towards offensive content moderation.","year":2020,"title_abstract":"He said {``}who{'}s gonna take care of your children when you are at {ACL}?{''}: Reported Sexist Acts are Not Sexist In a context of offensive content mediation on social media now regulated by European laws, it is important not only to be able to automatically detect sexist content but also to identify if a message with a sexist content is really sexist or is a story of sexism experienced by a woman. We propose: (1) a new characterization of sexist content inspired by speech acts theory and discourse analysis studies, (2) the first French dataset annotated for sexism detection, and (3) a set of deep learning experiments trained on top of a combination of several tweet{'}s vectorial representations (word embeddings, linguistic features, and various generalization strategies). Our results are encouraging and constitute a first step towards offensive content moderation.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.213950038,"Goal":"Gender Equality","Task":["offensive content mediation","characterization of sexist content","discourse analysis studies","sexism detection","offensive content moderation"],"Method":["speech acts theory","deep learning experiments","tweet{'}s vectorial representations","generalization strategies)"]},{"ID":"iserman-etal-2018-approach","title":"An Approach to the {CLP}sych 2018 Shared Task Using Top-Down Text Representation and Simple Bottom-Up Model Selection","abstract":"The Computational Linguistics and Clinical Psychology (CLPsych) 2018 Shared Task asked teams to predict cross-sectional indices of anxiety and distress, and longitudinal indices of psychological distress from a subsample of the National Child Development Study, started in the United Kingdom in 1958. Teams aimed to predict mental health outcomes from essays written by 11-year-olds about what they believed their lives would be like at age 25. In the hopes of producing results that could be easily disseminated and applied, we used largely theory-based dictionaries to process the texts, and a simple data-driven approach to model selection. This approach yielded only modest results in terms of out-of-sample accuracy, but most of the category-level findings are interpretable and consistent with existing literature on psychological distress, anxiety, and depression.","year":2018,"title_abstract":"An Approach to the {CLP}sych 2018 Shared Task Using Top-Down Text Representation and Simple Bottom-Up Model Selection The Computational Linguistics and Clinical Psychology (CLPsych) 2018 Shared Task asked teams to predict cross-sectional indices of anxiety and distress, and longitudinal indices of psychological distress from a subsample of the National Child Development Study, started in the United Kingdom in 1958. Teams aimed to predict mental health outcomes from essays written by 11-year-olds about what they believed their lives would be like at age 25. In the hopes of producing results that could be easily disseminated and applied, we used largely theory-based dictionaries to process the texts, and a simple data-driven approach to model selection. This approach yielded only modest results in terms of out-of-sample accuracy, but most of the category-level findings are interpretable and consistent with existing literature on psychological distress, anxiety, and depression.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.2136712074,"Goal":"Good Health and Well-Being","Task":["Computational Linguistics and Clinical Psychology","psychological distress"],"Method":["Top - Down Text Representation","Bottom - Up Model Selection","theory - based dictionaries","data - driven approach","model selection"]},{"ID":"hussein-etal-2021-damascusteam","title":"{D}amascus{T}eam at {NLP}4{IF}2021: Fighting the {A}rabic {COVID}-19 Infodemic on {T}witter Using {A}ra{BERT}","abstract":"The objective of this work was the introduction of an effective approach based on the AraBERT language model for fighting Tweets COVID-19 Infodemic. It was arranged in the form of a two-step pipeline, where the first step involved a series of pre-processing procedures to transform Twitter jargon, including emojis and emoticons, into plain text, and the second step exploited a version of AraBERT, which was pre-trained on plain text, to fine-tune and classify the tweets with respect to their Label. The use of language models pre-trained on plain texts rather than on tweets was motivated by the necessity to address two critical issues shown by the scientific literature, namely (1) pre-trained language models are widely available in many languages, avoiding the time-consuming and resource-intensive model training directly on tweets from scratch, allowing to focus only on their fine-tuning; (2) available plain text corpora are larger than tweet-only ones, allowing for better performance.","year":2021,"title_abstract":"{D}amascus{T}eam at {NLP}4{IF}2021: Fighting the {A}rabic {COVID}-19 Infodemic on {T}witter Using {A}ra{BERT} The objective of this work was the introduction of an effective approach based on the AraBERT language model for fighting Tweets COVID-19 Infodemic. It was arranged in the form of a two-step pipeline, where the first step involved a series of pre-processing procedures to transform Twitter jargon, including emojis and emoticons, into plain text, and the second step exploited a version of AraBERT, which was pre-trained on plain text, to fine-tune and classify the tweets with respect to their Label. The use of language models pre-trained on plain texts rather than on tweets was motivated by the necessity to address two critical issues shown by the scientific literature, namely (1) pre-trained language models are widely available in many languages, avoiding the time-consuming and resource-intensive model training directly on tweets from scratch, allowing to focus only on their fine-tuning; (2) available plain text corpora are larger than tweet-only ones, allowing for better performance.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2132703066,"Goal":"Climate Action","Task":["fine - tuning;"],"Method":["AraBERT language model","pre - processing procedures","AraBERT","language models","language models","model training"]},{"ID":"cao-daume-iii-2020-toward","title":"Toward Gender-Inclusive Coreference Resolution","abstract":"Correctly resolving textual mentions of people fundamentally entails making inferences about those people. Such inferences raise the risk of systemic biases in coreference resolution systems, including biases that can harm binary and non-binary trans and cis stakeholders. To better understand such biases, we foreground nuanced conceptualizations of gender from sociology and sociolinguistics, and develop two new datasets for interrogating bias in crowd annotations and in existing coreference resolution systems. Through these studies, conducted on English text, we confirm that without acknowledging and building systems that recognize the complexity of gender, we build systems that lead to many potential harms.","year":2020,"title_abstract":"Toward Gender-Inclusive Coreference Resolution Correctly resolving textual mentions of people fundamentally entails making inferences about those people. Such inferences raise the risk of systemic biases in coreference resolution systems, including biases that can harm binary and non-binary trans and cis stakeholders. To better understand such biases, we foreground nuanced conceptualizations of gender from sociology and sociolinguistics, and develop two new datasets for interrogating bias in crowd annotations and in existing coreference resolution systems. Through these studies, conducted on English text, we confirm that without acknowledging and building systems that recognize the complexity of gender, we build systems that lead to many potential harms.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2132381797,"Goal":"Gender Equality","Task":["Gender - Inclusive Coreference Resolution","resolving textual mentions of people","coreference resolution","interrogating bias","crowd annotations"],"Method":["sociology","coreference resolution systems"]},{"ID":"czarnowska-etal-2021-quantifying","title":"Quantifying Social Biases in {NLP}: A Generalization and Empirical Comparison of Extrinsic Fairness Metrics","abstract":"Abstract Measuring bias is key for better understanding and addressing unfairness in NLP\/ML models. This is often done via fairness metrics, which quantify the differences in a model{'}s behaviour across a range of demographic groups. In this work, we shed more light on the differences and similarities between the fairness metrics used in NLP. First, we unify a broad range of existing metrics under three generalized fairness metrics, revealing the connections between them. Next, we carry out an extensive empirical comparison of existing metrics and demonstrate that the observed differences in bias measurement can be systematically explained via differences in parameter choices for our generalized metrics.","year":2021,"title_abstract":"Quantifying Social Biases in {NLP}: A Generalization and Empirical Comparison of Extrinsic Fairness Metrics Abstract Measuring bias is key for better understanding and addressing unfairness in NLP\/ML models. This is often done via fairness metrics, which quantify the differences in a model{'}s behaviour across a range of demographic groups. In this work, we shed more light on the differences and similarities between the fairness metrics used in NLP. First, we unify a broad range of existing metrics under three generalized fairness metrics, revealing the connections between them. Next, we carry out an extensive empirical comparison of existing metrics and demonstrate that the observed differences in bias measurement can be systematically explained via differences in parameter choices for our generalized metrics.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.2131333351,"Goal":"Reduced Inequalities","Task":["Quantifying Social Biases","Measuring bias","unfairness","NLP\/ML","bias measurement"],"Method":["{NLP}","Generalization","NLP"]},{"ID":"fujita-etal-2017-consistent","title":"Consistent Classification of Translation Revisions: A Case Study of {E}nglish-{J}apanese Student Translations","abstract":"Consistency is a crucial requirement in text annotation. It is especially important in educational applications, as lack of consistency directly affects learners{'} motivation and learning performance. This paper presents a quality assessment scheme for English-to-Japanese translations produced by learner translators at university. We constructed a revision typology and a decision tree manually through an application of the OntoNotes method, i.e., an iteration of assessing learners{'} translations and hypothesizing the conditions for consistent decision making, as well as re-organizing the typology. Intrinsic evaluation of the created scheme confirmed its potential contribution to the consistent classification of identified erroneous text spans, achieving visibly higher Cohen{'}s kappa values, up to 0.831, than previous work. This paper also describes an application of our scheme to an English-to-Japanese translation exercise course for undergraduate students at a university in Japan.","year":2017,"title_abstract":"Consistent Classification of Translation Revisions: A Case Study of {E}nglish-{J}apanese Student Translations Consistency is a crucial requirement in text annotation. It is especially important in educational applications, as lack of consistency directly affects learners{'} motivation and learning performance. This paper presents a quality assessment scheme for English-to-Japanese translations produced by learner translators at university. We constructed a revision typology and a decision tree manually through an application of the OntoNotes method, i.e., an iteration of assessing learners{'} translations and hypothesizing the conditions for consistent decision making, as well as re-organizing the typology. Intrinsic evaluation of the created scheme confirmed its potential contribution to the consistent classification of identified erroneous text spans, achieving visibly higher Cohen{'}s kappa values, up to 0.831, than previous work. This paper also describes an application of our scheme to an English-to-Japanese translation exercise course for undergraduate students at a university in Japan.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.2130102217,"Goal":"Quality Education","Task":["Consistent Classification of Translation Revisions","text annotation","educational applications","learning","English - to - Japanese translations","consistent decision making","classification","English - to - Japanese translation exercise course"],"Method":["quality assessment scheme","revision typology","decision tree","OntoNotes method"]},{"ID":"pociello-etal-2008-wnterm","title":"{WNTERM}: Enriching the {MCR} with a Terminological Dictionary","abstract":"In this paper we describe the methodology and the first steps for the creation of WNTERM (from WordNet and Terminology), a specialized lexicon produced from the merger of the EuroWordNet-based Multilingual Central Repository (MCR) and the Basic Encyclopaedic Dictionary of Science and Technology (BDST). As an example, the ecology domain has been used. The final result is a multilingual (Basque and English) light-weight domain ontology, including taxonomic and other semantic relations among its concepts, which is tightly connected to other wordnets.","year":2008,"title_abstract":"{WNTERM}: Enriching the {MCR} with a Terminological Dictionary In this paper we describe the methodology and the first steps for the creation of WNTERM (from WordNet and Terminology), a specialized lexicon produced from the merger of the EuroWordNet-based Multilingual Central Repository (MCR) and the Basic Encyclopaedic Dictionary of Science and Technology (BDST). As an example, the ecology domain has been used. The final result is a multilingual (Basque and English) light-weight domain ontology, including taxonomic and other semantic relations among its concepts, which is tightly connected to other wordnets.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.2129024863,"Goal":"Life Below Water","Task":["Encyclopaedic Dictionary of Science and Technology"],"Method":["WNTERM"]},{"ID":"welch-etal-2020-expressive","title":"Expressive Interviewing: A Conversational System for Coping with {COVID}-19","abstract":"The ongoing COVID-19 pandemic has raised concerns for many regarding personal and public health implications, financial security and economic stability. Alongside many other unprecedented challenges, there are increasing concerns over social isolation and mental health. We introduce Expressive Interviewing {--} an interview-style conversational system that draws on ideas from motivational interviewing and expressive writing. Expressive Interviewing seeks to encourage users to express their thoughts and feelings through writing by asking them questions about how COVID-19 has impacted their lives. We present relevant aspects of the system{'}s design and implementation as well as quantitative and qualitative analyses of user interactions with the system. In addition, we conduct a comparative evaluation with a general purpose dialogue system for mental health that shows our system potential in helping users to cope with COVID-19 issues.","year":2020,"title_abstract":"Expressive Interviewing: A Conversational System for Coping with {COVID}-19 The ongoing COVID-19 pandemic has raised concerns for many regarding personal and public health implications, financial security and economic stability. Alongside many other unprecedented challenges, there are increasing concerns over social isolation and mental health. We introduce Expressive Interviewing {--} an interview-style conversational system that draws on ideas from motivational interviewing and expressive writing. Expressive Interviewing seeks to encourage users to express their thoughts and feelings through writing by asking them questions about how COVID-19 has impacted their lives. We present relevant aspects of the system{'}s design and implementation as well as quantitative and qualitative analyses of user interactions with the system. In addition, we conduct a comparative evaluation with a general purpose dialogue system for mental health that shows our system potential in helping users to cope with COVID-19 issues.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.212765485,"Goal":"Good Health and Well-Being","Task":["COVID - 19","personal and public health implications","financial security","motivational interviewing","expressive writing","Expressive Interviewing","mental health"],"Method":["Expressive Interviewing","Conversational System","Expressive Interviewing","interview - style conversational system","general purpose dialogue system"]},{"ID":"pruthi-etal-2020-learning","title":"Learning to Deceive with Attention-Based Explanations","abstract":"Attention mechanisms are ubiquitous components in neural architectures applied to natural language processing. In addition to yielding gains in predictive accuracy, attention weights are often claimed to confer interpretability, purportedly useful both for providing insights to practitioners and for explaining why a model makes its decisions to stakeholders. We call the latter use of attention mechanisms into question by demonstrating a simple method for training models to produce deceptive attention masks. Our method diminishes the total weight assigned to designated impermissible tokens, even when the models can be shown to nevertheless rely on these features to drive predictions. Across multiple models and tasks, our approach manipulates attention weights while paying surprisingly little cost in accuracy. Through a human study, we show that our manipulated attention-based explanations deceive people into thinking that predictions from a model biased against gender minorities do not rely on the gender. Consequently, our results cast doubt on attention{'}s reliability as a tool for auditing algorithms in the context of fairness and accountability.","year":2020,"title_abstract":"Learning to Deceive with Attention-Based Explanations Attention mechanisms are ubiquitous components in neural architectures applied to natural language processing. In addition to yielding gains in predictive accuracy, attention weights are often claimed to confer interpretability, purportedly useful both for providing insights to practitioners and for explaining why a model makes its decisions to stakeholders. We call the latter use of attention mechanisms into question by demonstrating a simple method for training models to produce deceptive attention masks. Our method diminishes the total weight assigned to designated impermissible tokens, even when the models can be shown to nevertheless rely on these features to drive predictions. Across multiple models and tasks, our approach manipulates attention weights while paying surprisingly little cost in accuracy. Through a human study, we show that our manipulated attention-based explanations deceive people into thinking that predictions from a model biased against gender minorities do not rely on the gender. Consequently, our results cast doubt on attention{'}s reliability as a tool for auditing algorithms in the context of fairness and accountability.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2124666572,"Goal":"Gender Equality","Task":["natural language processing","fairness","accountability"],"Method":["Attention - Based Explanations","Attention mechanisms","neural architectures","attention mechanisms","attention - based explanations","auditing algorithms"]},{"ID":"scerri-etal-2010-classifying","title":"Classifying Action Items for Semantic Email","abstract":"Email can be considered as a virtual working environment in which users are constantly struggling to manage the vast amount of exchanged data. Although most of this data belongs to well-defined workflows, these are implicit and largely unsupported by existing email clients. Semanta provides this support by enabling Semantic Email \u2015 email enhanced with machine-processable metadata about specific types of email Action Items (e.g. Task Assignment, Meeting Proposal). In the larger picture, these items form part of ad-hoc workflows (e.g. Task Delegation, Meeting Scheduling). Semanta is faced with a knowledge-acquisition bottleneck, as users cannot be expected to annotate each action item, and their automatic recognition proves difficult. This paper focuses on applying computationally treatable aspects of speech act theory for the classification of email action items. A rule-based classification model is employed, based on the presence or form of a number of linguistic features. The technology\u0092s evaluation suggests that whereas full automation is not feasible, the results are good enough to be presented as suggestions for the user to review. In addition the rule-based system will bootstrap a machine learning system that is currently in development, to generate the initial training sets which are then improved through the user\u0092s reviewing.","year":2010,"title_abstract":"Classifying Action Items for Semantic Email Email can be considered as a virtual working environment in which users are constantly struggling to manage the vast amount of exchanged data. Although most of this data belongs to well-defined workflows, these are implicit and largely unsupported by existing email clients. Semanta provides this support by enabling Semantic Email \u2015 email enhanced with machine-processable metadata about specific types of email Action Items (e.g. Task Assignment, Meeting Proposal). In the larger picture, these items form part of ad-hoc workflows (e.g. Task Delegation, Meeting Scheduling). Semanta is faced with a knowledge-acquisition bottleneck, as users cannot be expected to annotate each action item, and their automatic recognition proves difficult. This paper focuses on applying computationally treatable aspects of speech act theory for the classification of email action items. A rule-based classification model is employed, based on the presence or form of a number of linguistic features. The technology\u0092s evaluation suggests that whereas full automation is not feasible, the results are good enough to be presented as suggestions for the user to review. In addition the rule-based system will bootstrap a machine learning system that is currently in development, to generate the initial training sets which are then improved through the user\u0092s reviewing.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2123062015,"Goal":"Climate Action","Task":["Classifying Action Items","Semantic Email Email","virtual working environment","Semantic Email","Task Assignment","Meeting Proposal)","Task Delegation","Meeting Scheduling)","knowledge - acquisition bottleneck","automatic recognition","classification of email action items","full automation"],"Method":["Semanta","Semanta","speech act theory","rule - based classification model","rule - based system","machine learning system"]},{"ID":"bartl-etal-2020-unmasking","title":"Unmasking Contextual Stereotypes: Measuring and Mitigating {BERT}{'}s Gender Bias","abstract":"Contextualized word embeddings have been replacing standard embeddings as the representational knowledge source of choice in NLP systems. Since a variety of biases have previously been found in standard word embeddings, it is crucial to assess biases encoded in their replacements as well. Focusing on BERT (Devlin et al., 2018), we measure gender bias by studying associations between gender-denoting target words and names of professions in English and German, comparing the findings with real-world workforce statistics. We mitigate bias by fine-tuning BERT on the GAP corpus (Webster et al., 2018), after applying Counterfactual Data Substitution (CDS) (Maudslay et al., 2019). We show that our method of measuring bias is appropriate for languages such as English, but not for languages with a rich morphology and gender-marking, such as German. Our results highlight the importance of investigating bias and mitigation techniques cross-linguistically,especially in view of the current emphasis on large-scale, multilingual language models.","year":2020,"title_abstract":"Unmasking Contextual Stereotypes: Measuring and Mitigating {BERT}{'}s Gender Bias Contextualized word embeddings have been replacing standard embeddings as the representational knowledge source of choice in NLP systems. Since a variety of biases have previously been found in standard word embeddings, it is crucial to assess biases encoded in their replacements as well. Focusing on BERT (Devlin et al., 2018), we measure gender bias by studying associations between gender-denoting target words and names of professions in English and German, comparing the findings with real-world workforce statistics. We mitigate bias by fine-tuning BERT on the GAP corpus (Webster et al., 2018), after applying Counterfactual Data Substitution (CDS) (Maudslay et al., 2019). We show that our method of measuring bias is appropriate for languages such as English, but not for languages with a rich morphology and gender-marking, such as German. Our results highlight the importance of investigating bias and mitigation techniques cross-linguistically,especially in view of the current emphasis on large-scale, multilingual language models.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2122702897,"Goal":"Gender Equality","Task":["Measuring and Mitigating {BERT}{'}s Gender Bias","NLP","gender bias","measuring bias","large - scale , multilingual language models"],"Method":["Unmasking Contextual Stereotypes","Contextualized word embeddings","Counterfactual Data Substitution","bias and mitigation techniques"]},{"ID":"gordeev-lykova-2020-bert","title":"{BERT} of all trades, master of some","abstract":"This paper describes our results for TRAC 2020 competition held together with the conference LREC 2020. Our team name was Ms8qQxMbnjJMgYcw. The competition consisted of 2 subtasks in 3 languages (Bengali, English and Hindi) where the participants{'} task was to classify aggression in short texts from social media and decide whether it is gendered or not. We used a single BERT-based system with two outputs for all tasks simultaneously. Our model placed first in English and second in Bengali gendered text classification competition tasks with 0.87 and 0.93 in F1-score respectively.","year":2020,"title_abstract":"{BERT} of all trades, master of some This paper describes our results for TRAC 2020 competition held together with the conference LREC 2020. Our team name was Ms8qQxMbnjJMgYcw. The competition consisted of 2 subtasks in 3 languages (Bengali, English and Hindi) where the participants{'} task was to classify aggression in short texts from social media and decide whether it is gendered or not. We used a single BERT-based system with two outputs for all tasks simultaneously. Our model placed first in English and second in Bengali gendered text classification competition tasks with 0.87 and 0.93 in F1-score respectively.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2121873051,"Goal":"Gender Equality","Task":["TRAC 2020 competition","LREC","Bengali gendered text classification competition tasks"],"Method":["BERT - based system"]},{"ID":"chiril-etal-2020-annotated","title":"An Annotated Corpus for Sexism Detection in {F}rench Tweets","abstract":"Social media networks have become a space where users are free to relate their opinions and sentiments which may lead to a large spreading of hatred or abusive messages which have to be moderated. This paper presents the first French corpus annotated for sexism detection composed of about 12,000 tweets. In a context of offensive content mediation on social media now regulated by European laws, we think that it is important to be able to detect automatically not only sexist content but also to identify if a message with a sexist content is really sexist (i.e. addressed to a woman or describing a woman or women in general) or is a story of sexism experienced by a woman. This point is the novelty of our annotation scheme. We also propose some preliminary results for sexism detection obtained with a deep learning approach. Our experiments show encouraging results.","year":2020,"title_abstract":"An Annotated Corpus for Sexism Detection in {F}rench Tweets Social media networks have become a space where users are free to relate their opinions and sentiments which may lead to a large spreading of hatred or abusive messages which have to be moderated. This paper presents the first French corpus annotated for sexism detection composed of about 12,000 tweets. In a context of offensive content mediation on social media now regulated by European laws, we think that it is important to be able to detect automatically not only sexist content but also to identify if a message with a sexist content is really sexist (i.e. addressed to a woman or describing a woman or women in general) or is a story of sexism experienced by a woman. This point is the novelty of our annotation scheme. We also propose some preliminary results for sexism detection obtained with a deep learning approach. Our experiments show encouraging results.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.212169379,"Goal":"Gender Equality","Task":["Sexism Detection","sexism detection","offensive content mediation","sexism detection"],"Method":["annotation scheme","deep learning approach"]},{"ID":"vaidyanathan-etal-2022-glimpse","title":"A glimpse of assistive technology in daily life","abstract":"Robitaille (2010) wrote {`}if all technology companies have accessibility in their mind then people with disabilities won{'}t be left behind.{'} Current technology has come a long way from where it stood decades ago; however, researchers and manufacturers often do not include people with disabilities in the design process and tend to accommodate them after the fact. In this paper we share feedback from four assistive technology users who rely on one or more assistive technology devices in their everyday lives. We believe end users should be part of the design process and that by bringing together experts and users, we can bridge the research\/practice gap.","year":2022,"title_abstract":"A glimpse of assistive technology in daily life Robitaille (2010) wrote {`}if all technology companies have accessibility in their mind then people with disabilities won{'}t be left behind.{'} Current technology has come a long way from where it stood decades ago; however, researchers and manufacturers often do not include people with disabilities in the design process and tend to accommodate them after the fact. In this paper we share feedback from four assistive technology users who rely on one or more assistive technology devices in their everyday lives. We believe end users should be part of the design process and that by bringing together experts and users, we can bridge the research\/practice gap.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2121122479,"Goal":"Sustainable Cities and Communities","Task":["design process","design process"],"Method":["assistive technology"]},{"ID":"wadden-etal-2020-fact","title":"Fact or Fiction: Verifying Scientific Claims","abstract":"We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. To study this task, we construct SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. We develop baseline models for SciFact, and demonstrate that simple domain adaptation techniques substantially improve performance compared to models trained on Wikipedia or political news. We show that our system is able to verify claims related to COVID-19 by identifying evidence from the CORD-19 corpus. Our experiments indicate that SciFact will provide a challenging testbed for the development of new systems designed to retrieve and reason over corpora containing specialized domain knowledge. Data and code for this new task are publicly available at https:\/\/github.com\/allenai\/scifact. A leaderboard and COVID-19 fact-checking demo are available at https:\/\/scifact.apps.allenai.org.","year":2020,"title_abstract":"Fact or Fiction: Verifying Scientific Claims We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. To study this task, we construct SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. We develop baseline models for SciFact, and demonstrate that simple domain adaptation techniques substantially improve performance compared to models trained on Wikipedia or political news. We show that our system is able to verify claims related to COVID-19 by identifying evidence from the CORD-19 corpus. Our experiments indicate that SciFact will provide a challenging testbed for the development of new systems designed to retrieve and reason over corpora containing specialized domain knowledge. Data and code for this new task are publicly available at https:\/\/github.com\/allenai\/scifact. A leaderboard and COVID-19 fact-checking demo are available at https:\/\/scifact.apps.allenai.org.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2121038884,"Goal":"Climate Action","Task":["Verifying Scientific Claims","scientific claim verification","SciFact"],"Method":["domain adaptation techniques","leaderboard"]},{"ID":"li-etal-2022-covid","title":"{COVID}-19 Claim Radar: A Structured Claim Extraction and Tracking System","abstract":"To tackle the challenge of accurate and timely communication regarding the COVID-19 pandemic, we present a COVID-19 Claim Radar to automatically extract supporting and refuting claims on a daily basis. We provide a comprehensive structured view of claims, including rich claim attributes (such as claimers and claimer affiliations) and associated knowledge elements as claim semantics (such as events, relations and entities), enabling users to explore equivalent, refuting, or supporting claims with structural evidence, such as shared claimers, similar centroid events and arguments. In order to consolidate claim structures at the corpus-level, we leverage Wikidata as the hub to merge coreferential knowledge elements. The system automatically provides users a comprehensive exposure to COVID-19 related claims, their importance, and their interconnections. The system is publicly available at GitHub and DockerHub, with complete documentation.","year":2022,"title_abstract":"{COVID}-19 Claim Radar: A Structured Claim Extraction and Tracking System To tackle the challenge of accurate and timely communication regarding the COVID-19 pandemic, we present a COVID-19 Claim Radar to automatically extract supporting and refuting claims on a daily basis. We provide a comprehensive structured view of claims, including rich claim attributes (such as claimers and claimer affiliations) and associated knowledge elements as claim semantics (such as events, relations and entities), enabling users to explore equivalent, refuting, or supporting claims with structural evidence, such as shared claimers, similar centroid events and arguments. In order to consolidate claim structures at the corpus-level, we leverage Wikidata as the hub to merge coreferential knowledge elements. The system automatically provides users a comprehensive exposure to COVID-19 related claims, their importance, and their interconnections. The system is publicly available at GitHub and DockerHub, with complete documentation.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2117348313,"Goal":"Climate Action","Task":["accurate and timely communication","COVID - 19","supporting and refuting claims"],"Method":["Claim Radar","Structured Claim Extraction and Tracking System","COVID - 19 Claim Radar","Wikidata"]},{"ID":"van-den-berg-etal-2019-president","title":"Not My President: How Names and Titles Frame Political Figures","abstract":"Naming and titling have been discussed in sociolinguistics as markers of status or solidarity. However, these functions have not been studied on a larger scale or for social media data. We collect a corpus of tweets mentioning presidents of six G20 countries by various naming forms. We show that naming variation relates to stance towards the president in a way that is suggestive of a framing effect mediated by respectfulness. This confirms sociolinguistic theory of naming and titling as markers of status.","year":2019,"title_abstract":"Not My President: How Names and Titles Frame Political Figures Naming and titling have been discussed in sociolinguistics as markers of status or solidarity. However, these functions have not been studied on a larger scale or for social media data. We collect a corpus of tweets mentioning presidents of six G20 countries by various naming forms. We show that naming variation relates to stance towards the president in a way that is suggestive of a framing effect mediated by respectfulness. This confirms sociolinguistic theory of naming and titling as markers of status.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.2114227116,"Goal":"Peace, Justice and Strong Institutions","Task":["titling","markers of status"],"Method":["sociolinguistic theory of naming"]},{"ID":"zaporojets-etal-2018-predicting","title":"Predicting Psychological Health from Childhood Essays. The {UG}ent-{IDL}ab {CLP}sych 2018 Shared Task System.","abstract":"This paper describes the IDLab system submitted to Task A of the CLPsych 2018 shared task. The goal of this task is predicting psychological health of children based on language used in hand-written essays and socio-demographic control variables. Our entry uses word- and character-based features as well as lexicon-based features and features derived from the essays such as the quality of the language. We apply linear models, gradient boosting as well as neural-network based regressors (feed-forward, CNNs and RNNs) to predict scores. We then make ensembles of our best performing models using a weighted average.","year":2018,"title_abstract":"Predicting Psychological Health from Childhood Essays. The {UG}ent-{IDL}ab {CLP}sych 2018 Shared Task System. This paper describes the IDLab system submitted to Task A of the CLPsych 2018 shared task. The goal of this task is predicting psychological health of children based on language used in hand-written essays and socio-demographic control variables. Our entry uses word- and character-based features as well as lexicon-based features and features derived from the essays such as the quality of the language. We apply linear models, gradient boosting as well as neural-network based regressors (feed-forward, CNNs and RNNs) to predict scores. We then make ensembles of our best performing models using a weighted average.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.2112931311,"Goal":"Good Health and Well-Being","Task":["Predicting Psychological Health","predicting psychological health of children"],"Method":["IDLab system","linear models","gradient boosting","neural - network based regressors","- forward","CNNs","RNNs)","weighted average"]},{"ID":"kordoni-etal-2016-enhancing","title":"Enhancing Access to Online Education: Quality Machine Translation of {MOOC} Content","abstract":"The present work is an overview of the TraMOOC (Translation for Massive Open Online Courses) research and innovation project, a machine translation approach for online educational content. More specifically, videolectures, assignments, and MOOC forum text is automatically translated from English into eleven European and BRIC languages. Unlike previous approaches to machine translation, the output quality in TraMOOC relies on a multimodal evaluation schema that involves crowdsourcing, error type markup, an error taxonomy for translation model comparison, and implicit evaluation via text mining, i.e. entity recognition and its performance comparison between the source and the translated text, and sentiment analysis on the students{'} forum posts. Finally, the evaluation output will result in more and better quality in-domain parallel data that will be fed back to the translation engine for higher quality output. The translation service will be incorporated into the Iversity MOOC platform and into the VideoLectures.net digital library portal.","year":2016,"title_abstract":"Enhancing Access to Online Education: Quality Machine Translation of {MOOC} Content The present work is an overview of the TraMOOC (Translation for Massive Open Online Courses) research and innovation project, a machine translation approach for online educational content. More specifically, videolectures, assignments, and MOOC forum text is automatically translated from English into eleven European and BRIC languages. Unlike previous approaches to machine translation, the output quality in TraMOOC relies on a multimodal evaluation schema that involves crowdsourcing, error type markup, an error taxonomy for translation model comparison, and implicit evaluation via text mining, i.e. entity recognition and its performance comparison between the source and the translated text, and sentiment analysis on the students{'} forum posts. Finally, the evaluation output will result in more and better quality in-domain parallel data that will be fed back to the translation engine for higher quality output. The translation service will be incorporated into the Iversity MOOC platform and into the VideoLectures.net digital library portal.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.2110291272,"Goal":"Quality Education","Task":["Online Education","Machine Translation","{MOOC} Content","Massive Open Online Courses) research and innovation project","online educational content","machine translation","translation model comparison","implicit evaluation","text mining","entity recognition"],"Method":["TraMOOC","machine translation approach","TraMOOC","multimodal evaluation schema","error type markup","error taxonomy","sentiment analysis","translation engine","translation service","Iversity MOOC platform"]},{"ID":"blessing-schutze-2010-fine","title":"Fine-Grained Geographical Relation Extraction from {W}ikipedia","abstract":"In this paper, we present work on enhancing the basic data resource of a context-aware system. Electronic text offers a wealth of information about geospatial data and can be used to improve the completeness and accuracy of geospatial resources (e.g., gazetteers). First, we introduce a supervised approach to extracting geographical relations on a fine-grained level. Second, we present a novel way of using Wikipedia as a corpus based on self-annotation. A self-annotation is an automatically created high-quality annotation that can be used for training and evaluation. Wikipedia contains two types of different context: (i) unstructured text and (ii) structured data: templates (e.g., infoboxes about cities), lists and tables. We use the structured data to annotate the unstructured text. Finally, the extracted fine-grained relations are used to complete gazetteer data. The precision and recall scores of more than 97 percent confirm that a statistical IE pipeline can be used to improve the data quality of community-based resources.","year":2010,"title_abstract":"Fine-Grained Geographical Relation Extraction from {W}ikipedia In this paper, we present work on enhancing the basic data resource of a context-aware system. Electronic text offers a wealth of information about geospatial data and can be used to improve the completeness and accuracy of geospatial resources (e.g., gazetteers). First, we introduce a supervised approach to extracting geographical relations on a fine-grained level. Second, we present a novel way of using Wikipedia as a corpus based on self-annotation. A self-annotation is an automatically created high-quality annotation that can be used for training and evaluation. Wikipedia contains two types of different context: (i) unstructured text and (ii) structured data: templates (e.g., infoboxes about cities), lists and tables. We use the structured data to annotate the unstructured text. Finally, the extracted fine-grained relations are used to complete gazetteer data. The precision and recall scores of more than 97 percent confirm that a statistical IE pipeline can be used to improve the data quality of community-based resources.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2109514177,"Goal":"Sustainable Cities and Communities","Task":["Fine - Grained Geographical Relation Extraction","extracting geographical relations","self - annotation","self - annotation","evaluation","community - based resources"],"Method":["context - aware system","supervised approach","statistical IE pipeline"]},{"ID":"zhang-etal-2022-nlp","title":"How can {NLP} Help Revitalize Endangered Languages? A Case Study and Roadmap for the {C}herokee Language","abstract":"More than 43{\\%} of the languages spoken in the world are endangered, and language loss currently occurs at an accelerated rate because of globalization and neocolonialism. Saving and revitalizing endangered languages has become very important for maintaining the cultural diversity on our planet. In this work, we focus on discussing how NLP can help revitalize endangered languages. We first suggest three principles that may help NLP practitioners to foster mutual understanding and collaboration with language communities, and we discuss three ways in which NLP can potentially assist in language education. We then take Cherokee, a severely-endangered Native American language, as a case study. After reviewing the language{'}s history, linguistic features, and existing resources, we (in collaboration with Cherokee community members) arrive at a few meaningful ways NLP practitioners can collaborate with community partners. We suggest two approaches to enrich the Cherokee language{'}s resources with machine-in-the-loop processing, and discuss several NLP tools that people from the Cherokee community have shown interest in. We hope that our work serves not only to inform the NLP community about Cherokee, but also to provide inspiration for future work on endangered languages in general.","year":2022,"title_abstract":"How can {NLP} Help Revitalize Endangered Languages? A Case Study and Roadmap for the {C}herokee Language More than 43{\\%} of the languages spoken in the world are endangered, and language loss currently occurs at an accelerated rate because of globalization and neocolonialism. Saving and revitalizing endangered languages has become very important for maintaining the cultural diversity on our planet. In this work, we focus on discussing how NLP can help revitalize endangered languages. We first suggest three principles that may help NLP practitioners to foster mutual understanding and collaboration with language communities, and we discuss three ways in which NLP can potentially assist in language education. We then take Cherokee, a severely-endangered Native American language, as a case study. After reviewing the language{'}s history, linguistic features, and existing resources, we (in collaboration with Cherokee community members) arrive at a few meaningful ways NLP practitioners can collaborate with community partners. We suggest two approaches to enrich the Cherokee language{'}s resources with machine-in-the-loop processing, and discuss several NLP tools that people from the Cherokee community have shown interest in. We hope that our work serves not only to inform the NLP community about Cherokee, but also to provide inspiration for future work on endangered languages in general.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.2107652128,"Goal":"Life on Land","Task":["Revitalize Endangered Languages?","language loss","Saving and revitalizing endangered languages","NLP","language education","machine - in - the - loop processing","NLP community","endangered languages"],"Method":["NLP","NLP","NLP practitioners","NLP tools"]},{"ID":"lange-ljunglof-2018-mulle","title":"{MULLE}: A grammar-based {L}atin language learning tool to supplement the classroom setting","abstract":"MULLE is a tool for language learning that focuses on teaching Latin as a foreign language. It is aimed for easy integration into the traditional classroom setting and syllabus, which makes it distinct from other language learning tools that provide standalone learning experience. It uses grammar-based lessons and embraces methods of gamification to improve the learner motivation. The main type of exercise provided by our application is to practice translation, but it is also possible to shift the focus to vocabulary or morphology training.","year":2018,"title_abstract":"{MULLE}: A grammar-based {L}atin language learning tool to supplement the classroom setting MULLE is a tool for language learning that focuses on teaching Latin as a foreign language. It is aimed for easy integration into the traditional classroom setting and syllabus, which makes it distinct from other language learning tools that provide standalone learning experience. It uses grammar-based lessons and embraces methods of gamification to improve the learner motivation. The main type of exercise provided by our application is to practice translation, but it is also possible to shift the focus to vocabulary or morphology training.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.2104709893,"Goal":"Quality Education","Task":["language learning","gamification","translation","morphology training"],"Method":["grammar - based {L}atin language learning tool","MULLE","language learning tools","grammar - based lessons"]},{"ID":"skeppstedt-etal-2018-stance","title":"Stance-Taking in Topics Extracted from Vaccine-Related Tweets and Discussion Forum Posts","abstract":"The occurrence of stance-taking towards vaccination was measured in documents extracted by topic modelling from two different corpora, one discussion forum corpus and one tweet corpus. For some of the topics extracted, their most closely associated documents contained a proportion of vaccine stance-taking texts that exceeded the corpus average by a large margin. These extracted document sets would, therefore, form a useful resource in a process for computer-assisted analysis of argumentation on the subject of vaccination.","year":2018,"title_abstract":"Stance-Taking in Topics Extracted from Vaccine-Related Tweets and Discussion Forum Posts The occurrence of stance-taking towards vaccination was measured in documents extracted by topic modelling from two different corpora, one discussion forum corpus and one tweet corpus. For some of the topics extracted, their most closely associated documents contained a proportion of vaccine stance-taking texts that exceeded the corpus average by a large margin. These extracted document sets would, therefore, form a useful resource in a process for computer-assisted analysis of argumentation on the subject of vaccination.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2103094161,"Goal":"Climate Action","Task":["Stance - Taking","computer - assisted analysis of argumentation","subject of vaccination"],"Method":["topic modelling"]},{"ID":"reimerink-etal-2010-ecolexicon","title":"{E}co{L}exicon: An Environmental {TKB}","abstract":"EcoLexicon, a multilingual knowledge resource on the environment, provides an internally coherent information system covering a wide range of specialized linguistic and conceptual needs. Data in our terminological knowledge base (TKB) are primarily hosted in a relational database which is now linked to an ontology in order to apply reasoning techniques and enhance user queries. The advantages of ontological reasoning can only be obtained if conceptual description is based on systematic criteria and a wide inventory of non-hierarchical relations, which confer dynamism to knowledge representation. Thus, our research has mainly focused on conceptual modelling and providing a user-friendly multimodal interface. The dynamic interface, which combines conceptual (networks and definitions), linguistic (contexts, concordances) and graphical information offers users the freedom to surf it according to their needs. Furthermore, dynamism is also present at the representational level. Contextual constraints have been applied to reconceptualise versatile concepts that cause a great deal of information overload.","year":2010,"title_abstract":"{E}co{L}exicon: An Environmental {TKB} EcoLexicon, a multilingual knowledge resource on the environment, provides an internally coherent information system covering a wide range of specialized linguistic and conceptual needs. Data in our terminological knowledge base (TKB) are primarily hosted in a relational database which is now linked to an ontology in order to apply reasoning techniques and enhance user queries. The advantages of ontological reasoning can only be obtained if conceptual description is based on systematic criteria and a wide inventory of non-hierarchical relations, which confer dynamism to knowledge representation. Thus, our research has mainly focused on conceptual modelling and providing a user-friendly multimodal interface. The dynamic interface, which combines conceptual (networks and definitions), linguistic (contexts, concordances) and graphical information offers users the freedom to surf it according to their needs. Furthermore, dynamism is also present at the representational level. Contextual constraints have been applied to reconceptualise versatile concepts that cause a great deal of information overload.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.2100784183,"Goal":"Life on Land","Task":["knowledge representation","conceptual modelling"],"Method":["information system","terminological knowledge base","reasoning techniques","ontological reasoning","conceptual description","dynamic interface"]},{"ID":"ils-etal-2021-changes","title":"Changes in {E}uropean Solidarity Before and During {COVID}-19: Evidence from a Large Crowd- and Expert-Annotated {T}witter Dataset","abstract":"We introduce the well-established social scientific concept of social solidarity and its contestation, anti-solidarity, as a new problem setting to supervised machine learning in NLP to assess how European solidarity discourses changed before and after the COVID-19 outbreak was declared a global pandemic. To this end, we annotate 2.3k English and German tweets for (anti-)solidarity expressions, utilizing multiple human annotators and two annotation approaches (experts vs. crowds). We use these annotations to train a BERT model with multiple data augmentation strategies. Our augmented BERT model that combines both expert and crowd annotations outperforms the baseline BERT classifier trained with expert annotations only by over 25 points, from 58{\\%} macro-F1 to almost 85{\\%}. We use this high-quality model to automatically label over 270k tweets between September 2019 and December 2020. We then assess the automatically labeled data for how statements related to European (anti-)solidarity discourses developed over time and in relation to one another, before and during the COVID-19 crisis. Our results show that solidarity became increasingly salient and contested during the crisis. While the number of solidarity tweets remained on a higher level and dominated the discourse in the scrutinized time frame, anti-solidarity tweets initially spiked, then decreased to (almost) pre-COVID-19 values before rising to a stable higher level until the end of 2020.","year":2021,"title_abstract":"Changes in {E}uropean Solidarity Before and During {COVID}-19: Evidence from a Large Crowd- and Expert-Annotated {T}witter Dataset We introduce the well-established social scientific concept of social solidarity and its contestation, anti-solidarity, as a new problem setting to supervised machine learning in NLP to assess how European solidarity discourses changed before and after the COVID-19 outbreak was declared a global pandemic. To this end, we annotate 2.3k English and German tweets for (anti-)solidarity expressions, utilizing multiple human annotators and two annotation approaches (experts vs. crowds). We use these annotations to train a BERT model with multiple data augmentation strategies. Our augmented BERT model that combines both expert and crowd annotations outperforms the baseline BERT classifier trained with expert annotations only by over 25 points, from 58{\\%} macro-F1 to almost 85{\\%}. We use this high-quality model to automatically label over 270k tweets between September 2019 and December 2020. We then assess the automatically labeled data for how statements related to European (anti-)solidarity discourses developed over time and in relation to one another, before and during the COVID-19 crisis. Our results show that solidarity became increasingly salient and contested during the crisis. While the number of solidarity tweets remained on a higher level and dominated the discourse in the scrutinized time frame, anti-solidarity tweets initially spiked, then decreased to (almost) pre-COVID-19 values before rising to a stable higher level until the end of 2020.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2099365294,"Goal":"Climate Action","Task":["supervised machine learning","NLP"],"Method":["human annotators","annotation approaches","BERT","data augmentation strategies","BERT","BERT"]},{"ID":"torisawa-2016-disaana","title":"{DISAANA} and {D}-{SUMM}: Large-scale Real Time {NLP} Systems for Analyzing Disaster Related Reports in Tweets","abstract":"This talk presents two NLP systems that were developed for helping disaster victims and rescue workers in the aftermath of large-scale disasters. DISAANA provides answers to questions such as {``}What is in short supply in Tokyo?{''} and displays locations related to each answer on a map. D-SUMM automatically summarizes a large number of disaster related reports concerning a specified area and helps rescue workers to understand disaster situations from a macro perspective. Both systems are publicly available as Web services. In the aftermath of the 2016 Kumamoto Earthquake (M7.0), the Japanese government actually used DISAANA to analyze the situation.","year":2016,"title_abstract":"{DISAANA} and {D}-{SUMM}: Large-scale Real Time {NLP} Systems for Analyzing Disaster Related Reports in Tweets This talk presents two NLP systems that were developed for helping disaster victims and rescue workers in the aftermath of large-scale disasters. DISAANA provides answers to questions such as {``}What is in short supply in Tokyo?{''} and displays locations related to each answer on a map. D-SUMM automatically summarizes a large number of disaster related reports concerning a specified area and helps rescue workers to understand disaster situations from a macro perspective. Both systems are publicly available as Web services. In the aftermath of the 2016 Kumamoto Earthquake (M7.0), the Japanese government actually used DISAANA to analyze the situation.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2098970711,"Goal":"Sustainable Cities and Communities","Task":["Analyzing Disaster Related Reports","disaster victims","rescue workers"],"Method":["Large - scale Real Time {NLP} Systems","NLP systems","DISAANA","D - SUMM","DISAANA"]},{"ID":"elahi-monachesi-2012-examination","title":"An Examination of Cross-Cultural Similarities and Differences from Social Media Data with respect to Language Use","abstract":"We present a methodology for analyzing cross-cultural similarities and differences using language as a medium, love as domain, social media as a data source and 'Terms' and 'Topics' as cultural features. We discuss the techniques necessary for the creation of the social data corpus from which emotion terms have been extracted using NLP techniques. Topics of love discussion were then extracted from the corpus by means of Latent Dirichlet Allocation (LDA). Finally, on the basis of these features, a cross-cultural comparison was carried out. For the purpose of cross-cultural analysis, the experimental focus was on comparing data from a culture from the East (India) with a culture from the West (United States of America). Similarities and differences between these cultures have been analyzed with respect to the usage of emotions, their intensities and the topics used during love discussion in social media.","year":2012,"title_abstract":"An Examination of Cross-Cultural Similarities and Differences from Social Media Data with respect to Language Use We present a methodology for analyzing cross-cultural similarities and differences using language as a medium, love as domain, social media as a data source and 'Terms' and 'Topics' as cultural features. We discuss the techniques necessary for the creation of the social data corpus from which emotion terms have been extracted using NLP techniques. Topics of love discussion were then extracted from the corpus by means of Latent Dirichlet Allocation (LDA). Finally, on the basis of these features, a cross-cultural comparison was carried out. For the purpose of cross-cultural analysis, the experimental focus was on comparing data from a culture from the East (India) with a culture from the West (United States of America). Similarities and differences between these cultures have been analyzed with respect to the usage of emotions, their intensities and the topics used during love discussion in social media.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.2097990811,"Goal":"Reduced Inequalities","Task":["cross - cultural similarities","cross - cultural comparison","cross - cultural analysis","love discussion"],"Method":["NLP techniques","Latent Dirichlet Allocation"]},{"ID":"odijk-2010-clarin","title":"The {CLARIN}-{NL} Project","abstract":"In this paper I present the CLARIN-NL project, the Dutch national project that aims to play a central role in the European CLARIN infrastructure, not only for the preparatory phase, but also for the implementation and exploitation phases. I argue that the way the CLARIN-NL project has been set-up can serve as an excellent example for other national CLARIN projects, for the following reasons: (1) it is a mix between a programme and a project; (2) it offers opportunities to seriously test standards and protocols currently proposed by CLARIN, thus providing evidence-based requirements and desiderata for the CLARIN infrastructure and ensuring compatibility of CLARIN with national data and tools; (3) it brings the intended users (humanities researchers) and the technology providers (infrastructure specialists and language and speech technology researchers) together in concrete cooperation projects, with a central role for the user\u0092s research questions,, thus ensuring that the infrastructure will provide functionality that is needed by its intended users.","year":2010,"title_abstract":"The {CLARIN}-{NL} Project In this paper I present the CLARIN-NL project, the Dutch national project that aims to play a central role in the European CLARIN infrastructure, not only for the preparatory phase, but also for the implementation and exploitation phases. I argue that the way the CLARIN-NL project has been set-up can serve as an excellent example for other national CLARIN projects, for the following reasons: (1) it is a mix between a programme and a project; (2) it offers opportunities to seriously test standards and protocols currently proposed by CLARIN, thus providing evidence-based requirements and desiderata for the CLARIN infrastructure and ensuring compatibility of CLARIN with national data and tools; (3) it brings the intended users (humanities researchers) and the technology providers (infrastructure specialists and language and speech technology researchers) together in concrete cooperation projects, with a central role for the user\u0092s research questions,, thus ensuring that the infrastructure will provide functionality that is needed by its intended users.","social_need":"Industry, Innovation and Infrastrucure Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation","cosine_similarity":0.2094117701,"Goal":"Industry, Innovation and Infrastrucure","Task":["CLARIN infrastructure","implementation and exploitation phases","CLARIN projects"],"Method":["CLARIN - NL","CLARIN","CLARIN infrastructure","CLARIN"]},{"ID":"min-zhao-2019-measure","title":"Measure Country-Level Socio-Economic Indicators with Streaming News: An Empirical Study","abstract":"Socio-economic conditions are difficult to measure. For example, the U.S. Bureau of Labor Statistics needs to conduct large-scale household surveys regularly to track the unemployment rate, an indicator widely used by economists and policymakers. We argue that events reported in streaming news can be used as {``}micro-sensors{''} for measuring socio-economic conditions. Similar to collecting surveys and then counting answers, it is possible to measure a socio-economic indicator by counting related events. In this paper, we propose Event-Centric Indicator Measure (ECIM), a novel approach to measure socio-economic indicators with events. We empirically demonstrate strong correlation between ECIM values to several representative indicators in socio-economic research.","year":2019,"title_abstract":"Measure Country-Level Socio-Economic Indicators with Streaming News: An Empirical Study Socio-economic conditions are difficult to measure. For example, the U.S. Bureau of Labor Statistics needs to conduct large-scale household surveys regularly to track the unemployment rate, an indicator widely used by economists and policymakers. We argue that events reported in streaming news can be used as {``}micro-sensors{''} for measuring socio-economic conditions. Similar to collecting surveys and then counting answers, it is possible to measure a socio-economic indicator by counting related events. In this paper, we propose Event-Centric Indicator Measure (ECIM), a novel approach to measure socio-economic indicators with events. We empirically demonstrate strong correlation between ECIM values to several representative indicators in socio-economic research.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.209337011,"Goal":"Reduced Inequalities","Task":["Socio - economic conditions","measuring socio - economic conditions","socio - economic indicator","socio - economic indicators","socio - economic research"],"Method":["Event - Centric Indicator Measure"]},{"ID":"stowe-etal-2018-improving","title":"Improving Classification of {T}witter Behavior During Hurricane Events","abstract":"A large amount of social media data is generated during natural disasters, and identifying the relevant portions of this data is critical for researchers attempting to understand human behavior, the effects of information sources, and preparatory actions undertaken during these events. In order to classify human behavior during hazard events, we employ machine learning for two tasks: identifying hurricane related tweets and classifying user evacuation behavior during hurricanes. We show that feature-based and deep learning methods provide different benefits for tweet classification, and ensemble-based methods using linguistic, temporal, and geospatial features can effectively classify user behavior.","year":2018,"title_abstract":"Improving Classification of {T}witter Behavior During Hurricane Events A large amount of social media data is generated during natural disasters, and identifying the relevant portions of this data is critical for researchers attempting to understand human behavior, the effects of information sources, and preparatory actions undertaken during these events. In order to classify human behavior during hazard events, we employ machine learning for two tasks: identifying hurricane related tweets and classifying user evacuation behavior during hurricanes. We show that feature-based and deep learning methods provide different benefits for tweet classification, and ensemble-based methods using linguistic, temporal, and geospatial features can effectively classify user behavior.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2092916369,"Goal":"Climate Action","Task":["Classification of {T}witter Behavior","Hurricane Events","identifying hurricane related tweets","classifying user evacuation behavior","tweet classification"],"Method":["machine learning","feature - based and deep learning methods","ensemble - based methods"]},{"ID":"sun-peng-2021-men","title":"Men Are Elected, Women Are Married: Events Gender Bias on {W}ikipedia","abstract":"Human activities can be seen as sequences of events, which are crucial to understanding societies. Disproportional event distribution for different demographic groups can manifest and amplify social stereotypes, and potentially jeopardize the ability of members in some groups to pursue certain goals. In this paper, we present the first event-centric study of gender biases in a Wikipedia corpus. To facilitate the study, we curate a corpus of career and personal life descriptions with demographic information consisting of 7,854 fragments from 10,412 celebrities. Then we detect events with a state-of-the-art event detection model, calibrate the results using strategically generated templates, and extract events that have asymmetric associations with genders. Our study discovers that the Wikipedia pages tend to intermingle personal life events with professional events for females but not for males, which calls for the awareness of the Wikipedia community to formalize guidelines and train the editors to mind the implicit biases that contributors carry. Our work also lays the foundation for future works on quantifying and discovering event biases at the corpus level.","year":2021,"title_abstract":"Men Are Elected, Women Are Married: Events Gender Bias on {W}ikipedia Human activities can be seen as sequences of events, which are crucial to understanding societies. Disproportional event distribution for different demographic groups can manifest and amplify social stereotypes, and potentially jeopardize the ability of members in some groups to pursue certain goals. In this paper, we present the first event-centric study of gender biases in a Wikipedia corpus. To facilitate the study, we curate a corpus of career and personal life descriptions with demographic information consisting of 7,854 fragments from 10,412 celebrities. Then we detect events with a state-of-the-art event detection model, calibrate the results using strategically generated templates, and extract events that have asymmetric associations with genders. Our study discovers that the Wikipedia pages tend to intermingle personal life events with professional events for females but not for males, which calls for the awareness of the Wikipedia community to formalize guidelines and train the editors to mind the implicit biases that contributors carry. Our work also lays the foundation for future works on quantifying and discovering event biases at the corpus level.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2092909068,"Goal":"Gender Equality","Task":["event - centric study of gender biases","event biases"],"Method":["event detection model"]},{"ID":"iwakura-etal-2018-detecting","title":"Detecting Heavy Rain Disaster from Social and Physical Sensor","abstract":"We present our system that assists to detect heavy rain disaster, which is being used in real world in Japan. Our system selects tweets about heavy rain disaster with a document classifier. Then, the locations mentioned in the selected tweets are estimated by a location estimator. Finally, combined the selected tweets with amount of rainfall given by physical sensors and a statistical analysis, our system provides users with visualized results for detecting heavy rain disaster.","year":2018,"title_abstract":"Detecting Heavy Rain Disaster from Social and Physical Sensor We present our system that assists to detect heavy rain disaster, which is being used in real world in Japan. Our system selects tweets about heavy rain disaster with a document classifier. Then, the locations mentioned in the selected tweets are estimated by a location estimator. Finally, combined the selected tweets with amount of rainfall given by physical sensors and a statistical analysis, our system provides users with visualized results for detecting heavy rain disaster.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2092356384,"Goal":"Sustainable Cities and Communities","Task":["Detecting Heavy Rain Disaster","heavy rain disaster","detecting heavy rain disaster"],"Method":["Social and Physical Sensor","document classifier","location estimator","statistical analysis"]},{"ID":"ethayarajh-2020-classifier","title":"Is Your Classifier Actually Biased? Measuring Fairness under Uncertainty with Bernstein Bounds","abstract":"Most NLP datasets are not annotated with protected attributes such as gender, making it difficult to measure classification bias using standard measures of fairness (e.g., equal opportunity). However, manually annotating a large dataset with a protected attribute is slow and expensive. Instead of annotating all the examples, can we annotate a subset of them and use that sample to estimate the bias? While it is possible to do so, the smaller this annotated sample is, the less certain we are that the estimate is close to the true bias. In this work, we propose using Bernstein bounds to represent this uncertainty about the bias estimate as a confidence interval. We provide empirical evidence that a 95{\\%} confidence interval derived this way consistently bounds the true bias. In quantifying this uncertainty, our method, which we call Bernstein-bounded unfairness, helps prevent classifiers from being deemed biased or unbiased when there is insufficient evidence to make either claim. Our findings suggest that the datasets currently used to measure specific biases are too small to conclusively identify bias except in the most egregious cases. For example, consider a co-reference resolution system that is 5{\\%} more accurate on gender-stereotypical sentences {--} to claim it is biased with 95{\\%} confidence, we need a bias-specific dataset that is 3.8 times larger than WinoBias, the largest available.","year":2020,"title_abstract":"Is Your Classifier Actually Biased? Measuring Fairness under Uncertainty with Bernstein Bounds Most NLP datasets are not annotated with protected attributes such as gender, making it difficult to measure classification bias using standard measures of fairness (e.g., equal opportunity). However, manually annotating a large dataset with a protected attribute is slow and expensive. Instead of annotating all the examples, can we annotate a subset of them and use that sample to estimate the bias? While it is possible to do so, the smaller this annotated sample is, the less certain we are that the estimate is close to the true bias. In this work, we propose using Bernstein bounds to represent this uncertainty about the bias estimate as a confidence interval. We provide empirical evidence that a 95{\\%} confidence interval derived this way consistently bounds the true bias. In quantifying this uncertainty, our method, which we call Bernstein-bounded unfairness, helps prevent classifiers from being deemed biased or unbiased when there is insufficient evidence to make either claim. Our findings suggest that the datasets currently used to measure specific biases are too small to conclusively identify bias except in the most egregious cases. For example, consider a co-reference resolution system that is 5{\\%} more accurate on gender-stereotypical sentences {--} to claim it is biased with 95{\\%} confidence, we need a bias-specific dataset that is 3.8 times larger than WinoBias, the largest available.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.2091871351,"Goal":"Reduced Inequalities","Task":["Measuring Fairness","NLP"],"Method":["Bernstein Bounds","Bernstein bounds","Bernstein - bounded unfairness","classifiers","co - reference resolution system"]},{"ID":"du-etal-2022-understanding","title":"Understanding Gender Bias in Knowledge Base Embeddings","abstract":"Knowledge base (KB) embeddings have been shown to contain gender biases. In this paper, we study two questions regarding these biases: how to quantify them, and how to trace their origins in KB? Specifically, first, we develop two novel bias measures respectively for a group of person entities and an individual person entity. Evidence of their validity is observed by comparison with real-world census data. Second, we use the influence function to inspect the contribution of each triple in KB to the overall group bias. To exemplify the potential applications of our study, we also present two strategies (by adding and removing KB triples) to mitigate gender biases in KB embeddings.","year":2022,"title_abstract":"Understanding Gender Bias in Knowledge Base Embeddings Knowledge base (KB) embeddings have been shown to contain gender biases. In this paper, we study two questions regarding these biases: how to quantify them, and how to trace their origins in KB? Specifically, first, we develop two novel bias measures respectively for a group of person entities and an individual person entity. Evidence of their validity is observed by comparison with real-world census data. Second, we use the influence function to inspect the contribution of each triple in KB to the overall group bias. To exemplify the potential applications of our study, we also present two strategies (by adding and removing KB triples) to mitigate gender biases in KB embeddings.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2090719938,"Goal":"Gender Equality","Task":["Understanding Gender Bias","Knowledge Base Embeddings"],"Method":["KB?","influence function","KB","KB","KB"]},{"ID":"broeder-etal-2012-standardizing","title":"Standardizing a Component Metadata Infrastructure","abstract":"This paper describes the status of the standardization efforts of a Component Metadata approach for describing Language Resources with metadata. Different linguistic and Language {\\&} Technology communities as CLARIN, META-SHARE and NaLiDa use this component approach and see its standardization of as a matter for cooperation that has the possibility to create a large interoperable domain of joint metadata. Starting with an overview of the component metadata approach together with the related semantic interoperability tools and services as the ISOcat data category registry and the relation registry we explain the standardization plan and efforts for component metadata within ISO TC37\/SC4. Finally, we present information about uptake and plans of the use of component metadata within the three mentioned linguistic and L{\\&}T communities.","year":2012,"title_abstract":"Standardizing a Component Metadata Infrastructure This paper describes the status of the standardization efforts of a Component Metadata approach for describing Language Resources with metadata. Different linguistic and Language {\\&} Technology communities as CLARIN, META-SHARE and NaLiDa use this component approach and see its standardization of as a matter for cooperation that has the possibility to create a large interoperable domain of joint metadata. Starting with an overview of the component metadata approach together with the related semantic interoperability tools and services as the ISOcat data category registry and the relation registry we explain the standardization plan and efforts for component metadata within ISO TC37\/SC4. Finally, we present information about uptake and plans of the use of component metadata within the three mentioned linguistic and L{\\&}T communities.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.2090617567,"Goal":"Partnership for the Goals","Task":["component metadata"],"Method":["Component Metadata Infrastructure","Component Metadata approach","component approach","component metadata approach","semantic interoperability tools","ISOcat data category registry","relation registry"]},{"ID":"dobrov-loukachevitch-2006-development","title":"Development of Linguistic Ontology on Natural Sciences and Technology","abstract":"The paper describes the main principles of development and current state of Linguistic Ontology on Natural Sciences and Technology intended for information-retrieval tasks. In the development of the ontology we combined three different methodologies: development of information-retrieval thesauri, development of wordnets, formal ontology research. Combination of these methodologies allows us to develop large ontologies for broad domains.","year":2006,"title_abstract":"Development of Linguistic Ontology on Natural Sciences and Technology The paper describes the main principles of development and current state of Linguistic Ontology on Natural Sciences and Technology intended for information-retrieval tasks. In the development of the ontology we combined three different methodologies: development of information-retrieval thesauri, development of wordnets, formal ontology research. Combination of these methodologies allows us to develop large ontologies for broad domains.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.2090388536,"Goal":"Life Below Water","Task":["Linguistic Ontology","Linguistic Ontology","information - retrieval tasks","information - retrieval thesauri"],"Method":["wordnets","formal ontology research"]},{"ID":"ananya-etal-2019-genderquant","title":"{G}ender{Q}uant: Quantifying Mention-Level Genderedness","abstract":"Language is gendered if the context surrounding a mention is suggestive of a particular binary gender for that mention. Detecting the different ways in which language is gendered is an important task since gendered language can bias NLP models (such as for coreference resolution). This task is challenging since genderedness is often expressed in subtle ways. Existing approaches need considerable annotation efforts for each language, domain, and author, and often require handcrafted lexicons and features. Additionally, these approaches do not provide a quantifiable measure of how gendered the text is, nor are they applicable at the fine-grained mention level. In this paper, we use existing NLP pipelines to automatically annotate gender of mentions in the text. On corpora labeled using this method, we train a supervised classifier to predict the gender of any mention from its context and evaluate it on unseen text. The model confidence for a mention{'}s gender can be used as a proxy to indicate the level of genderedness of the context. We test this gendered language detector on movie summaries, movie reviews, news articles, and fiction novels, achieving an AUC-ROC of up to 0.71, and observe that the model predictions agree with human judgments collected for this task. We also provide examples of detected gendered sentences from aforementioned domains.","year":2019,"title_abstract":"{G}ender{Q}uant: Quantifying Mention-Level Genderedness Language is gendered if the context surrounding a mention is suggestive of a particular binary gender for that mention. Detecting the different ways in which language is gendered is an important task since gendered language can bias NLP models (such as for coreference resolution). This task is challenging since genderedness is often expressed in subtle ways. Existing approaches need considerable annotation efforts for each language, domain, and author, and often require handcrafted lexicons and features. Additionally, these approaches do not provide a quantifiable measure of how gendered the text is, nor are they applicable at the fine-grained mention level. In this paper, we use existing NLP pipelines to automatically annotate gender of mentions in the text. On corpora labeled using this method, we train a supervised classifier to predict the gender of any mention from its context and evaluate it on unseen text. The model confidence for a mention{'}s gender can be used as a proxy to indicate the level of genderedness of the context. We test this gendered language detector on movie summaries, movie reviews, news articles, and fiction novels, achieving an AUC-ROC of up to 0.71, and observe that the model predictions agree with human judgments collected for this task. We also provide examples of detected gendered sentences from aforementioned domains.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2089967579,"Goal":"Gender Equality","Task":["Quantifying Mention - Level Genderedness Language","coreference resolution)"],"Method":["NLP models","NLP pipelines","supervised classifier","gendered language detector"]},{"ID":"papanikolaou-papageorgiou-2020-protest","title":"Protest Event Analysis: A Longitudinal Analysis for {G}reece","abstract":"The advent of Big Data has shifted social science research towards computational methods. The volume of data that is nowadays available has brought a radical change in traditional approaches due to the cost and effort needed for processing. Knowledge extraction from heterogeneous and ample data is not an easy task to tackle. Thus, interdisciplinary approaches are necessary, combining experts of both social and computer science. This paper aims to present a work in the context of protest analysis, which falls into the scope of Computational Social Science. More specifically, the contribution of this work is to describe a Computational Social Science methodology for Event Analysis. The presented methodology is generic in the sense that it can be applied in every event typology and moreover, it is innovative and suitable for interdisciplinary tasks as it incorporates the human-in-the-loop. Additionally, a case study is presented concerning Protest Analysis in Greece over the last two decades. The conceptual foundation lies mainly upon claims analysis, and newspaper data were used in order to map, document and discuss protests in Greece in a longitudinal perspective.","year":2020,"title_abstract":"Protest Event Analysis: A Longitudinal Analysis for {G}reece The advent of Big Data has shifted social science research towards computational methods. The volume of data that is nowadays available has brought a radical change in traditional approaches due to the cost and effort needed for processing. Knowledge extraction from heterogeneous and ample data is not an easy task to tackle. Thus, interdisciplinary approaches are necessary, combining experts of both social and computer science. This paper aims to present a work in the context of protest analysis, which falls into the scope of Computational Social Science. More specifically, the contribution of this work is to describe a Computational Social Science methodology for Event Analysis. The presented methodology is generic in the sense that it can be applied in every event typology and moreover, it is innovative and suitable for interdisciplinary tasks as it incorporates the human-in-the-loop. Additionally, a case study is presented concerning Protest Analysis in Greece over the last two decades. The conceptual foundation lies mainly upon claims analysis, and newspaper data were used in order to map, document and discuss protests in Greece in a longitudinal perspective.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2089880258,"Goal":"Sustainable Cities and Communities","Task":["Protest Event Analysis","social science research","Knowledge extraction","social and computer science","protest analysis","Computational Social Science","Event Analysis","event typology","interdisciplinary tasks","Protest Analysis","claims analysis"],"Method":["Longitudinal Analysis","{G}reece","computational methods","interdisciplinary approaches","Computational Social Science methodology"]},{"ID":"schroder-etal-2021-supporting","title":"Supporting Land Reuse of Former Open Pit Mining Sites using Text Classification and Active Learning","abstract":"Open pit mines left many regions worldwide inhospitable or uninhabitable. Many sites are left behind in a hazardous or contaminated state, show remnants of waste, or have other restrictions imposed upon them, e.g., for the protection of human or nature. Such information has to be permanently managed in order to reuse those areas in the future. In this work we present and evaluate an automated workflow for supporting the post-mining management of former lignite open pit mines in the eastern part of Germany, where prior to any planned land reuse, aforementioned information has to be acquired to ensure the safety and validity of such an endeavor. Usually, this information is found in expert reports, either in the form of paper documents, or in the best case as digitized unstructured text{---}all of them in German language. However, due to the size and complexity of these documents, any inquiry is tedious and time-consuming, thereby slowing down or even obstructing the reuse of related areas. Since no training data is available, we employ active learning in order to perform multi-label sentence classification for two categories of restrictions and seven categories of topics. The final system integrates optical character recognition (OCR), active-learning-based text classification, and geographic information system visualization in order to effectively extract, query, and visualize this information for any area of interest. Active learning and text classification results are twofold: Whereas the restriction categories were reasonably accurate ({\\textgreater}0.85 F1), the seven topic-oriented categories seemed to be complex even for human annotators and achieved mediocre evaluation scores ({\\textless}0.70 F1).","year":2021,"title_abstract":"Supporting Land Reuse of Former Open Pit Mining Sites using Text Classification and Active Learning Open pit mines left many regions worldwide inhospitable or uninhabitable. Many sites are left behind in a hazardous or contaminated state, show remnants of waste, or have other restrictions imposed upon them, e.g., for the protection of human or nature. Such information has to be permanently managed in order to reuse those areas in the future. In this work we present and evaluate an automated workflow for supporting the post-mining management of former lignite open pit mines in the eastern part of Germany, where prior to any planned land reuse, aforementioned information has to be acquired to ensure the safety and validity of such an endeavor. Usually, this information is found in expert reports, either in the form of paper documents, or in the best case as digitized unstructured text{---}all of them in German language. However, due to the size and complexity of these documents, any inquiry is tedious and time-consuming, thereby slowing down or even obstructing the reuse of related areas. Since no training data is available, we employ active learning in order to perform multi-label sentence classification for two categories of restrictions and seven categories of topics. The final system integrates optical character recognition (OCR), active-learning-based text classification, and geographic information system visualization in order to effectively extract, query, and visualize this information for any area of interest. Active learning and text classification results are twofold: Whereas the restriction categories were reasonably accurate ({\\textgreater}0.85 F1), the seven topic-oriented categories seemed to be complex even for human annotators and achieved mediocre evaluation scores ({\\textless}0.70 F1).","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.2085266709,"Goal":"Life on Land","Task":["Land Reuse of Former Open Pit Mining Sites","Open pit mines","post - mining management of former lignite open pit mines","land reuse","multi - label sentence classification","text classification"],"Method":["Text Classification","Active Learning","automated workflow","active learning","optical character recognition","active - learning - based text classification","geographic information system visualization","Active learning"]},{"ID":"zhao-etal-2018-gender","title":"Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods","abstract":"In this paper, we introduce a new benchmark for co-reference resolution focused on gender bias, WinoBias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing datasets.","year":2018,"title_abstract":"Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods In this paper, we introduce a new benchmark for co-reference resolution focused on gender bias, WinoBias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing datasets.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2082894593,"Goal":"Gender Equality","Task":["Gender Bias","Coreference Resolution","co - reference resolution"],"Method":["Debiasing Methods","WinoBias","rule - based","feature - rich","neural coreference system","data - augmentation approach","word - embedding debiasing techniques","WinoBias"]},{"ID":"tessarollo-rademaker-2020-inclusion","title":"Inclusion of Lithological terms (rocks and minerals) in The Open {W}ordnet for {E}nglish","abstract":"We extend the Open WordNet for English (OWN-EN) with rock-related and other lithological terms using the authoritative source of GBA{'}s Thesaurus. Our aim is to improve WordNet to better function within Oil {\\&} Gas domain, particularly geoscience texts. We use a three step approach: a proof of concept-level extension of WordNet, a major extension on which we evaluate the impact with positive results and a full extension encompassing all GBA{'}s lithological terms. We also build a mapping to GBA which also links to several other resources: WikiData, British Geological Survey, Inspire, GeoSciML and DBpedia.","year":2020,"title_abstract":"Inclusion of Lithological terms (rocks and minerals) in The Open {W}ordnet for {E}nglish We extend the Open WordNet for English (OWN-EN) with rock-related and other lithological terms using the authoritative source of GBA{'}s Thesaurus. Our aim is to improve WordNet to better function within Oil {\\&} Gas domain, particularly geoscience texts. We use a three step approach: a proof of concept-level extension of WordNet, a major extension on which we evaluate the impact with positive results and a full extension encompassing all GBA{'}s lithological terms. We also build a mapping to GBA which also links to several other resources: WikiData, British Geological Survey, Inspire, GeoSciML and DBpedia.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.2082662433,"Goal":"Life Below Water","Task":["Oil {\\&} Gas domain"],"Method":["GBA{'}s Thesaurus","WordNet","WordNet","GBA"]},{"ID":"xu-etal-2020-cognitively","title":"A Cognitively Motivated Approach to Spatial Information Extraction","abstract":"Automatic extraction of spatial information from natural language can boost human-centered applications that rely on spatial dynamics. The field of cognitive linguistics has provided theories and cognitive models to address this task. Yet, existing solutions tend to focus on specific word classes, subject areas, or machine learning techniques that cannot provide cognitively plausible explanations for their decisions. We propose an automated spatial semantic analysis (ASSA) framework building on grammar and cognitive linguistic theories to identify spatial entities and relations, bringing together methods of spatial information extraction and cognitive frameworks on spatial language. The proposed rule-based and explainable approach contributes constructions and preposition schemas and outperforms previous solutions on the CLEF-2017 standard dataset.","year":2020,"title_abstract":"A Cognitively Motivated Approach to Spatial Information Extraction Automatic extraction of spatial information from natural language can boost human-centered applications that rely on spatial dynamics. The field of cognitive linguistics has provided theories and cognitive models to address this task. Yet, existing solutions tend to focus on specific word classes, subject areas, or machine learning techniques that cannot provide cognitively plausible explanations for their decisions. We propose an automated spatial semantic analysis (ASSA) framework building on grammar and cognitive linguistic theories to identify spatial entities and relations, bringing together methods of spatial information extraction and cognitive frameworks on spatial language. The proposed rule-based and explainable approach contributes constructions and preposition schemas and outperforms previous solutions on the CLEF-2017 standard dataset.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2081113607,"Goal":"Sustainable Cities and Communities","Task":["Spatial Information Extraction","Automatic extraction of spatial information","human - centered applications","cognitive linguistics","spatial information extraction","spatial language"],"Method":["Cognitively Motivated Approach","cognitive models","machine learning techniques","automated spatial semantic analysis","grammar","cognitive linguistic theories","cognitive frameworks","rule - based and explainable approach"]},{"ID":"mccurdy-etal-2020-conditioning","title":"Conditioning, but on Which Distribution? Grammatical Gender in {G}erman Plural Inflection","abstract":"Grammatical gender is a consistent and informative cue to the plural class of German nouns. We find that neural encoder-decoder models learn to rely on this cue to predict plural class, but adult speakers are relatively insensitive to it. This suggests that the neural models are not an effective cognitive model of German plural formation.","year":2020,"title_abstract":"Conditioning, but on Which Distribution? Grammatical Gender in {G}erman Plural Inflection Grammatical gender is a consistent and informative cue to the plural class of German nouns. We find that neural encoder-decoder models learn to rely on this cue to predict plural class, but adult speakers are relatively insensitive to it. This suggests that the neural models are not an effective cognitive model of German plural formation.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2080323398,"Goal":"Gender Equality","Task":["German plural formation"],"Method":["encoder - decoder models","neural models","cognitive model"]},{"ID":"rogers-2021-changing","title":"Changing the World by Changing the Data","abstract":"NLP community is currently investing a lot more research and resources into development of deep learning models than training data. While we have made a lot of progress, it is now clear that our models learn all kinds of spurious patterns, social biases, and annotation artifacts. Algorithmic solutions have so far had limited success. An alternative that is being actively discussed is more careful design of datasets so as to deliver specific signals. This position paper maps out the arguments for and against data curation, and argues that fundamentally the point is moot: curation already is and will be happening, and it is changing the world. The question is only how much thought we want to invest into that process.","year":2021,"title_abstract":"Changing the World by Changing the Data NLP community is currently investing a lot more research and resources into development of deep learning models than training data. While we have made a lot of progress, it is now clear that our models learn all kinds of spurious patterns, social biases, and annotation artifacts. Algorithmic solutions have so far had limited success. An alternative that is being actively discussed is more careful design of datasets so as to deliver specific signals. This position paper maps out the arguments for and against data curation, and argues that fundamentally the point is moot: curation already is and will be happening, and it is changing the world. The question is only how much thought we want to invest into that process.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2080046833,"Goal":"Climate Action","Task":["NLP","data curation","curation"],"Method":["deep learning models","Algorithmic solutions"]},{"ID":"maegaard-etal-2008-medar","title":"{MEDAR}: Collaboration between {E}uropean and Mediterranean {A}rabic Partners to Support the Development of Language Technology for {A}rabic","abstract":"After the successful completion of the NEMLAR project 2003-2005, a new opportunity for a project was opened by the European Commission, and a group of largely the same partners is now executing the MEDAR project. MEDAR will be updating the surveys and BLARK for Arabic already made, and will then focus on machine translation (and other tools for translation) and information retrieval with a focus on language resources, tools and evaluation for these applications. A very important part of the MEDAR project is to reinforce and extend the NEMLAR network and to create a cooperation roadmap for Human Language Technologies for Arabic. It is expected that the cooperation roadmap will attract wide attention from other parties and that it can help create a larger platform for collaborative projects. Finally, the project will focus on dissemination of knowledge about existing resources and tools, as well as actors and activities; this will happen through newsletter, website and an international conference which will follow up on the Cairo conference of 2004. Dissemination to user communities will also be important, e.g. through participation in translators? conferences. The goal of these activities is to create a stronger and lasting collaboration between EU countries and Arabic speaking countries.","year":2008,"title_abstract":"{MEDAR}: Collaboration between {E}uropean and Mediterranean {A}rabic Partners to Support the Development of Language Technology for {A}rabic After the successful completion of the NEMLAR project 2003-2005, a new opportunity for a project was opened by the European Commission, and a group of largely the same partners is now executing the MEDAR project. MEDAR will be updating the surveys and BLARK for Arabic already made, and will then focus on machine translation (and other tools for translation) and information retrieval with a focus on language resources, tools and evaluation for these applications. A very important part of the MEDAR project is to reinforce and extend the NEMLAR network and to create a cooperation roadmap for Human Language Technologies for Arabic. It is expected that the cooperation roadmap will attract wide attention from other parties and that it can help create a larger platform for collaborative projects. Finally, the project will focus on dissemination of knowledge about existing resources and tools, as well as actors and activities; this will happen through newsletter, website and an international conference which will follow up on the Cairo conference of 2004. Dissemination to user communities will also be important, e.g. through participation in translators? conferences. The goal of these activities is to create a stronger and lasting collaboration between EU countries and Arabic speaking countries.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.2079940587,"Goal":"Partnership for the Goals","Task":["Language Technology","machine translation","translation)","information retrieval","evaluation","MEDAR project","NEMLAR network","Human Language Technologies","Arabic","Dissemination"],"Method":["MEDAR"]},{"ID":"min-etal-2019-towards","title":"Towards Machine Reading for Interventions from Humanitarian-Assistance Program Literature","abstract":"Solving long-lasting problems such as food insecurity requires a comprehensive understanding of interventions applied by governments and international humanitarian assistance organizations, and their results and consequences. Towards achieving this grand goal, a crucial first step is to extract past interventions and when and where they have been applied, from hundreds of thousands of reports automatically. In this paper, we developed a corpus annotated with interventions to foster research, and developed an information extraction system for extracting interventions and their location and time from text. We demonstrate early, very encouraging results on extracting interventions.","year":2019,"title_abstract":"Towards Machine Reading for Interventions from Humanitarian-Assistance Program Literature Solving long-lasting problems such as food insecurity requires a comprehensive understanding of interventions applied by governments and international humanitarian assistance organizations, and their results and consequences. Towards achieving this grand goal, a crucial first step is to extract past interventions and when and where they have been applied, from hundreds of thousands of reports automatically. In this paper, we developed a corpus annotated with interventions to foster research, and developed an information extraction system for extracting interventions and their location and time from text. We demonstrate early, very encouraging results on extracting interventions.","social_need":"No Hunger End hunger, achieve food security and improved nutrition and promote sustainable agriculture","cosine_similarity":0.2077790201,"Goal":"No Hunger","Task":["Machine Reading","Humanitarian - Assistance Program Literature","long - lasting problems","extracting interventions","extracting interventions"],"Method":["information extraction system"]},{"ID":"gundapu-mamidi-2021-autobots","title":"Autobots@{LT}-{EDI}-{EACL}2021: One World, One Family: Hope Speech Detection with {BERT} Transformer Model","abstract":"The rapid rise of online social networks like YouTube, Facebook, Twitter allows people to express their views more widely online. However, at the same time, it can lead to an increase in conflict and hatred among consumers in the form of freedom of speech. Therefore, it is essential to take a positive strengthening method to research on encouraging, positive, helping, and supportive social media content. In this paper, we describe a Transformer-based BERT model for Hope speech detection for equality, diversity, and inclusion, submitted for LT-EDI-2021 Task 2. Our model achieves a weighted averaged f1-score of 0.93 on the test set.","year":2021,"title_abstract":"Autobots@{LT}-{EDI}-{EACL}2021: One World, One Family: Hope Speech Detection with {BERT} Transformer Model The rapid rise of online social networks like YouTube, Facebook, Twitter allows people to express their views more widely online. However, at the same time, it can lead to an increase in conflict and hatred among consumers in the form of freedom of speech. Therefore, it is essential to take a positive strengthening method to research on encouraging, positive, helping, and supportive social media content. In this paper, we describe a Transformer-based BERT model for Hope speech detection for equality, diversity, and inclusion, submitted for LT-EDI-2021 Task 2. Our model achieves a weighted averaged f1-score of 0.93 on the test set.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2077426612,"Goal":"Gender Equality","Task":["Hope Speech Detection","Hope speech detection","equality","LT - EDI - 2021"],"Method":["{BERT} Transformer Model","positive strengthening method","Transformer - based BERT model"]},{"ID":"radford-etal-2018-adult","title":"Can adult mental health be predicted by childhood future-self narratives? Insights from the {CLP}sych 2018 Shared Task","abstract":"The CLPsych 2018 Shared Task B explores how childhood essays can predict psychological distress throughout the author{'}s life. Our main aim was to build tools to help our psychologists understand the data, propose features and interpret predictions. We submitted two linear regression models: ModelA uses simple demographic and word-count features, while ModelB uses linguistic, entity, typographic, expert-gazetteer, and readability features. Our models perform best at younger prediction ages, with our best unofficial score at 23 of 0.426 disattenuated Pearson correlation. This task is challenging and although predictive performance is limited, we propose that tight integration of expertise across computational linguistics and clinical psychology is a productive direction.","year":2018,"title_abstract":"Can adult mental health be predicted by childhood future-self narratives? Insights from the {CLP}sych 2018 Shared Task The CLPsych 2018 Shared Task B explores how childhood essays can predict psychological distress throughout the author{'}s life. Our main aim was to build tools to help our psychologists understand the data, propose features and interpret predictions. We submitted two linear regression models: ModelA uses simple demographic and word-count features, while ModelB uses linguistic, entity, typographic, expert-gazetteer, and readability features. Our models perform best at younger prediction ages, with our best unofficial score at 23 of 0.426 disattenuated Pearson correlation. This task is challenging and although predictive performance is limited, we propose that tight integration of expertise across computational linguistics and clinical psychology is a productive direction.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.2077265829,"Goal":"Good Health and Well-Being","Task":["adult mental health","computational linguistics","clinical psychology"],"Method":["linear regression models"]},{"ID":"madonsela-etal-2016-african","title":"{A}frican {W}ord{N}et: A Viable Tool for Sense Discrimination in the Indigenous {A}frican Languages of {S}outh {A}frica","abstract":"In promoting a multilingual South Africa, the government is encouraging people to speak more than one language. In order to comply with this initiative, people choose to learn the languages which they do not speak as home language. The African languages are mostly chosen because they are spoken by the majority of the country{'}s population. Most words in these languages have many possible senses. This phenomenon tends to pose problems to people who want to learn these languages. This article argues that the African WordNet may the best tool to address the problem of sense discrimination. The focus of the argument will be on the primary sense of the word {`}hand{'}, which is part of the body, as lexicalized in three indigenous languages spoken in South Africa, namely, Tshiven\u1e13a, Sesotho sa Leboa and isiZulu. A brief historical background of the African WordNet will be provided, followed by the definition of the word {`}hand{'} in the three languages and the analysis of the word in context. Lastly, the primary sense of the word {`}hand{'} across the three languages will be discussed.","year":2016,"title_abstract":"{A}frican {W}ord{N}et: A Viable Tool for Sense Discrimination in the Indigenous {A}frican Languages of {S}outh {A}frica In promoting a multilingual South Africa, the government is encouraging people to speak more than one language. In order to comply with this initiative, people choose to learn the languages which they do not speak as home language. The African languages are mostly chosen because they are spoken by the majority of the country{'}s population. Most words in these languages have many possible senses. This phenomenon tends to pose problems to people who want to learn these languages. This article argues that the African WordNet may the best tool to address the problem of sense discrimination. The focus of the argument will be on the primary sense of the word {`}hand{'}, which is part of the body, as lexicalized in three indigenous languages spoken in South Africa, namely, Tshiven\u1e13a, Sesotho sa Leboa and isiZulu. A brief historical background of the African WordNet will be provided, followed by the definition of the word {`}hand{'} in the three languages and the analysis of the word in context. Lastly, the primary sense of the word {`}hand{'} across the three languages will be discussed.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.2075139135,"Goal":"Peace, Justice and Strong Institutions","Task":["Sense Discrimination","sense discrimination"],"Method":["African WordNet"]},{"ID":"arranz-etal-2008-guide","title":"A Guide for the Production of Reusable Language Resources","abstract":"The project described in this paper is funded by the French Ministry of Research. It aims at providing producers of Language Resources, and HLT players in general, with a guide which offers technical, legal and strategic recommendations\/guidelines for the reuse of their Language Resources. The guide is dedicated in particular to academic laboratories which produce Language Resources and may benefit from further advice to start development, but also to any HLT player who wishes to follow the best practices in this field. The guidelines focus on different steps of a Language Resource\u0092s life, i.e. specifications, production, validation, distribution, and maintenance. This paper gives a brief overview of the guide, and describes a) technical formats, standards and best practices which correspond to the current state of the art, for different types of resources, whether written or spoken, at different steps of the production line, b) legal issues and models\/templates which can be used for the dissemination of Language Resources as widely as possible, c) strategic issues, by offering a dissemination plan which takes into account all types of constraints faced by HLT community players.","year":2008,"title_abstract":"A Guide for the Production of Reusable Language Resources The project described in this paper is funded by the French Ministry of Research. It aims at providing producers of Language Resources, and HLT players in general, with a guide which offers technical, legal and strategic recommendations\/guidelines for the reuse of their Language Resources. The guide is dedicated in particular to academic laboratories which produce Language Resources and may benefit from further advice to start development, but also to any HLT player who wishes to follow the best practices in this field. The guidelines focus on different steps of a Language Resource\u0092s life, i.e. specifications, production, validation, distribution, and maintenance. This paper gives a brief overview of the guide, and describes a) technical formats, standards and best practices which correspond to the current state of the art, for different types of resources, whether written or spoken, at different steps of the production line, b) legal issues and models\/templates which can be used for the dissemination of Language Resources as widely as possible, c) strategic issues, by offering a dissemination plan which takes into account all types of constraints faced by HLT community players.","social_need":"Responsible Consumption and Production Ensure sustainable consumption and production patterns","cosine_similarity":0.2073592544,"Goal":"Responsible Consumption and Production","Task":["Production of Reusable Language Resources","Language Resource\u0092s life","specifications","production","validation","distribution","maintenance","dissemination of Language Resources"],"Method":["HLT","HLT player","dissemination plan"]},{"ID":"smal-etal-2020-language","title":"Language Data Sharing in {E}uropean Public Services {--} Overcoming Obstacles and Creating Sustainable Data Sharing Infrastructures","abstract":"Data is key in training modern language technologies. In this paper, we summarise the findings of the first pan-European study on obstacles to sharing language data across 29 EU Member States and CEF-affiliated countries carried out under the ELRC White Paper action on Sustainable Language Data Sharing to Support Language Equality in Multilingual Europe. Why Language Data Matters. We present the methodology of the study, the obstacles identified and report on recommendations on how to overcome those. The obstacles are classified into (1) lack of appreciation of the value of language data, (2) structural challenges, (3) disposition towards CAT tools and lack of digital skills, (4) inadequate language data management practices, (5) limited access to outsourced translations, and (6) legal concerns. Recommendations are grouped into addressing the European\/national policy level, and the organisational\/institutional level.","year":2020,"title_abstract":"Language Data Sharing in {E}uropean Public Services {--} Overcoming Obstacles and Creating Sustainable Data Sharing Infrastructures Data is key in training modern language technologies. In this paper, we summarise the findings of the first pan-European study on obstacles to sharing language data across 29 EU Member States and CEF-affiliated countries carried out under the ELRC White Paper action on Sustainable Language Data Sharing to Support Language Equality in Multilingual Europe. Why Language Data Matters. We present the methodology of the study, the obstacles identified and report on recommendations on how to overcome those. The obstacles are classified into (1) lack of appreciation of the value of language data, (2) structural challenges, (3) disposition towards CAT tools and lack of digital skills, (4) inadequate language data management practices, (5) limited access to outsourced translations, and (6) legal concerns. Recommendations are grouped into addressing the European\/national policy level, and the organisational\/institutional level.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.2071032673,"Goal":"Partnership for the Goals","Task":["Language Data Sharing","Sustainable Data Sharing Infrastructures Data","Sustainable Language Data Sharing","Language Equality"],"Method":["language technologies","language data management practices","organisational\/institutional level"]},{"ID":"ahmad-etal-2006-ontological","title":"Ontological and Terminological Commitments and the Discourse of Specialist Communities","abstract":"The paper presents a corpus-based study aimed at an analysis of ontological and terminological commitments in the discourse of specialist communities. The analyzed corpus contains the lectures delivered by the Nobel Prize winners in Physics and Economics. The analysis focuses on (a) the collocational use of automatically identified domain-specific terms and (b) a description of meta-discourse in the lectures. Candidate terms are extracted based on the z-score of frequency and weirdness. Compounds comprising these candidate terms are then identified using the ontology representation system Prot{\\'e}g{\\'e}. This method is then replicated to complete analysis by including an investigation of metadiscourse markers signalling how writers project themselves into their work.","year":2006,"title_abstract":"Ontological and Terminological Commitments and the Discourse of Specialist Communities The paper presents a corpus-based study aimed at an analysis of ontological and terminological commitments in the discourse of specialist communities. The analyzed corpus contains the lectures delivered by the Nobel Prize winners in Physics and Economics. The analysis focuses on (a) the collocational use of automatically identified domain-specific terms and (b) a description of meta-discourse in the lectures. Candidate terms are extracted based on the z-score of frequency and weirdness. Compounds comprising these candidate terms are then identified using the ontology representation system Prot{\\'e}g{\\'e}. This method is then replicated to complete analysis by including an investigation of metadiscourse markers signalling how writers project themselves into their work.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.206781283,"Goal":"Sustainable Cities and Communities","Task":["Discourse of Specialist Communities","discourse of specialist communities"],"Method":["ontology representation system Prot{\\'e}g{\\'e}"]},{"ID":"laparra-bethard-2020-dataset","title":"A Dataset and Evaluation Framework for Complex Geographical Description Parsing","abstract":"Much previous work on geoparsing has focused on identifying and resolving individual toponyms in text like Adrano, S.Maria di Licodia or Catania. However, geographical locations occur not only as individual toponyms, but also as compositions of reference geolocations joined and modified by connectives, e.g., {``}. . . between the towns of Adrano and S.Maria di Licodia, 32 kilometres northwest of Catania{''}. Ideally, a geoparser should be able to take such text, and the geographical shapes of the toponyms referenced within it, and parse these into a geographical shape, formed by a set of coordinates, that represents the location described. But creating a dataset for this complex geoparsing task is difficult and, if done manually, would require a huge amount of effort to annotate the geographical shapes of not only the geolocation described but also the reference toponyms. We present an approach that automates most of the process by combining Wikipedia and OpenStreetMap. As a result, we have gathered a collection of 360,187 uncurated complex geolocation descriptions, from which we have manually curated 1,000 examples intended to be used as a test set. To accompany the data, we define a new geoparsing evaluation framework along with a scoring methodology and a set of baselines.","year":2020,"title_abstract":"A Dataset and Evaluation Framework for Complex Geographical Description Parsing Much previous work on geoparsing has focused on identifying and resolving individual toponyms in text like Adrano, S.Maria di Licodia or Catania. However, geographical locations occur not only as individual toponyms, but also as compositions of reference geolocations joined and modified by connectives, e.g., {``}. . . between the towns of Adrano and S.Maria di Licodia, 32 kilometres northwest of Catania{''}. Ideally, a geoparser should be able to take such text, and the geographical shapes of the toponyms referenced within it, and parse these into a geographical shape, formed by a set of coordinates, that represents the location described. But creating a dataset for this complex geoparsing task is difficult and, if done manually, would require a huge amount of effort to annotate the geographical shapes of not only the geolocation described but also the reference toponyms. We present an approach that automates most of the process by combining Wikipedia and OpenStreetMap. As a result, we have gathered a collection of 360,187 uncurated complex geolocation descriptions, from which we have manually curated 1,000 examples intended to be used as a test set. To accompany the data, we define a new geoparsing evaluation framework along with a scoring methodology and a set of baselines.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2067184448,"Goal":"Sustainable Cities and Communities","Task":["Complex Geographical Description Parsing","geoparsing","geoparsing task"],"Method":["Evaluation Framework","geoparser","Wikipedia","OpenStreetMap","geoparsing evaluation framework","scoring methodology"]},{"ID":"verhoeven-etal-2017-gender","title":"Gender Profiling for {S}lovene {T}witter communication: the Influence of Gender Marking, Content and Style","abstract":"We present results of the first gender classification experiments on Slovene text to our knowledge. Inspired by the TwiSty corpus and experiments (Verhoeven et al., 2016), we employed the Janes corpus (Erjavec et al., 2016) and its gender annotations to perform gender classification experiments on Twitter text comparing a token-based and a lemma-based approach. We find that the token-based approach (92.6{\\%} accuracy), containing gender markings related to the author, outperforms the lemma-based approach by about 5{\\%}. Especially in the lemmatized version, we also observe stylistic and content-based differences in writing between men (e.g. more profane language, numerals and beer mentions) and women (e.g. more pronouns, emoticons and character flooding). Many of our findings corroborate previous research on other languages.","year":2017,"title_abstract":"Gender Profiling for {S}lovene {T}witter communication: the Influence of Gender Marking, Content and Style We present results of the first gender classification experiments on Slovene text to our knowledge. Inspired by the TwiSty corpus and experiments (Verhoeven et al., 2016), we employed the Janes corpus (Erjavec et al., 2016) and its gender annotations to perform gender classification experiments on Twitter text comparing a token-based and a lemma-based approach. We find that the token-based approach (92.6{\\%} accuracy), containing gender markings related to the author, outperforms the lemma-based approach by about 5{\\%}. Especially in the lemmatized version, we also observe stylistic and content-based differences in writing between men (e.g. more profane language, numerals and beer mentions) and women (e.g. more pronouns, emoticons and character flooding). Many of our findings corroborate previous research on other languages.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2066050768,"Goal":"Gender Equality","Task":["Gender Profiling","gender classification","gender classification"],"Method":["token - based","lemma - based approach","token - based approach","lemma - based approach","lemmatized version"]},{"ID":"jayasinghe-etal-2016-csiro","title":"{CSIRO} {D}ata61 at the {WNUT} Geo Shared Task","abstract":"In this paper, we describe CSIRO Data61{'}s participation in the Geolocation shared task at the Workshop for Noisy User-generated Text. Our approach was to use ensemble methods to capitalise on four component methods: heuristics based on metadata, a label propagation method, timezone text classifiers, and an information retrieval approach. The ensembles we explored focused on examining the role of language technologies in geolocation prediction and also in examining the use of hard voting and cascading ensemble methods. Based on the accuracy of city-level predictions, our systems were the best performing submissions at this year{'}s shared task. Furthermore, when estimating the latitude and longitude of a user, our median error distance was accurate to within 30 kilometers.","year":2016,"title_abstract":"{CSIRO} {D}ata61 at the {WNUT} Geo Shared Task In this paper, we describe CSIRO Data61{'}s participation in the Geolocation shared task at the Workshop for Noisy User-generated Text. Our approach was to use ensemble methods to capitalise on four component methods: heuristics based on metadata, a label propagation method, timezone text classifiers, and an information retrieval approach. The ensembles we explored focused on examining the role of language technologies in geolocation prediction and also in examining the use of hard voting and cascading ensemble methods. Based on the accuracy of city-level predictions, our systems were the best performing submissions at this year{'}s shared task. Furthermore, when estimating the latitude and longitude of a user, our median error distance was accurate to within 30 kilometers.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2066019177,"Goal":"Sustainable Cities and Communities","Task":["Geo Shared Task","Geolocation shared task","geolocation prediction","city - level predictions"],"Method":["ensemble methods","heuristics","label propagation method","timezone text classifiers","information retrieval approach","language technologies","hard voting","cascading ensemble methods"]},{"ID":"gupta-etal-2020-human","title":"Human-Human Health Coaching via Text Messages: Corpus, Annotation, and Analysis","abstract":"Our goal is to develop and deploy a virtual assistant health coach that can help patients set realistic physical activity goals and live a more active lifestyle. Since there is no publicly shared dataset of health coaching dialogues, the first phase of our research focused on data collection. We hired a certified health coach and 28 patients to collect the first round of human-human health coaching interaction which took place via text messages. This resulted in 2853 messages. The data collection phase was followed by conversation analysis to gain insight into the way information exchange takes place between a health coach and a patient. This was formalized using two annotation schemas: one that focuses on the goals the patient is setting and another that models the higher-level structure of the interactions. In this paper, we discuss these schemas and briefly talk about their application for automatically extracting activity goals and annotating the second round of data, collected with different health coaches and patients. Given the resource-intensive nature of data annotation, successfully annotating a new dataset automatically is key to answer the need for high quality, large datasets.","year":2020,"title_abstract":"Human-Human Health Coaching via Text Messages: Corpus, Annotation, and Analysis Our goal is to develop and deploy a virtual assistant health coach that can help patients set realistic physical activity goals and live a more active lifestyle. Since there is no publicly shared dataset of health coaching dialogues, the first phase of our research focused on data collection. We hired a certified health coach and 28 patients to collect the first round of human-human health coaching interaction which took place via text messages. This resulted in 2853 messages. The data collection phase was followed by conversation analysis to gain insight into the way information exchange takes place between a health coach and a patient. This was formalized using two annotation schemas: one that focuses on the goals the patient is setting and another that models the higher-level structure of the interactions. In this paper, we discuss these schemas and briefly talk about their application for automatically extracting activity goals and annotating the second round of data, collected with different health coaches and patients. Given the resource-intensive nature of data annotation, successfully annotating a new dataset automatically is key to answer the need for high quality, large datasets.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.2062289417,"Goal":"Good Health and Well-Being","Task":["Human - Human Health Coaching","Annotation","virtual assistant health coach","data collection","human - human health coaching interaction","conversation analysis","information exchange","automatically extracting activity goals","annotating","data annotation"],"Method":["annotation schemas"]},{"ID":"mccarthy-etal-2020-measuring","title":"Measuring the Similarity of Grammatical Gender Systems by Comparing Partitions","abstract":"A grammatical gender system divides a lexicon into a small number of relatively fixed grammatical categories. How similar are these gender systems across languages? To quantify the similarity, we define gender systems extensionally, thereby reducing the problem of comparisons between languages{'} gender systems to cluster evaluation. We borrow a rich inventory of statistical tools for cluster evaluation from the field of community detection (Driver and Kroeber, 1932; Cattell, 1945), that enable us to craft novel information theoretic metrics for measuring similarity between gender systems. We first validate our metrics, then use them to measure gender system similarity in 20 languages. We then ask whether our gender system similarities alone are sufficient to reconstruct historical relationships between languages. Towards this end, we make phylogenetic predictions on the popular, but thorny, problem from historical linguistics of inducing a phylogenetic tree over extant Indo-European languages. Of particular interest, languages on the same branch of our phylogenetic tree are notably similar, whereas languages from separate branches are no more similar than chance.","year":2020,"title_abstract":"Measuring the Similarity of Grammatical Gender Systems by Comparing Partitions A grammatical gender system divides a lexicon into a small number of relatively fixed grammatical categories. How similar are these gender systems across languages? To quantify the similarity, we define gender systems extensionally, thereby reducing the problem of comparisons between languages{'} gender systems to cluster evaluation. We borrow a rich inventory of statistical tools for cluster evaluation from the field of community detection (Driver and Kroeber, 1932; Cattell, 1945), that enable us to craft novel information theoretic metrics for measuring similarity between gender systems. We first validate our metrics, then use them to measure gender system similarity in 20 languages. We then ask whether our gender system similarities alone are sufficient to reconstruct historical relationships between languages. Towards this end, we make phylogenetic predictions on the popular, but thorny, problem from historical linguistics of inducing a phylogenetic tree over extant Indo-European languages. Of particular interest, languages on the same branch of our phylogenetic tree are notably similar, whereas languages from separate branches are no more similar than chance.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2062176168,"Goal":"Gender Equality","Task":["Similarity of Grammatical Gender Systems","cluster evaluation","cluster evaluation","community detection","measuring similarity between gender systems","gender system similarity","phylogenetic predictions","historical linguistics"],"Method":["grammatical gender system","gender systems","statistical tools","gender system similarities"]},{"ID":"jokinen-2014-open","title":"Open-domain Interaction and Online Content in the {S}ami Language","abstract":"This paper presents data collection and collaborative community events organised within the project Digital Natives on the North Sami language. The project is one of the collaboration initiatives on endangered Finno-Ugric languages, supported by the larger framework between the Academy of Finland and the Hungarian Academy of Sciences. The goal of the project is to improve digital visibility and viability of the targeted Finno-Ugric languages, as well as to develop language technology tools and resources in order to assist automatic language processing and experimenting with multilingual interactive applications.","year":2014,"title_abstract":"Open-domain Interaction and Online Content in the {S}ami Language This paper presents data collection and collaborative community events organised within the project Digital Natives on the North Sami language. The project is one of the collaboration initiatives on endangered Finno-Ugric languages, supported by the larger framework between the Academy of Finland and the Hungarian Academy of Sciences. The goal of the project is to improve digital visibility and viability of the targeted Finno-Ugric languages, as well as to develop language technology tools and resources in order to assist automatic language processing and experimenting with multilingual interactive applications.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2061518133,"Goal":"Sustainable Cities and Communities","Task":["Open - domain Interaction","Online Content","data collection","automatic language processing","multilingual interactive applications"],"Method":["language technology tools"]},{"ID":"parida-etal-2020-odianlps","title":"{ODIANLP}{'}s Participation in {WAT}2020","abstract":"This paper describes the ODIANLP submission to WAT 2020. We have participated in the English-Hindi Multimodal task and Indic task. We have used the state-of-the-art Transformer model for the translation task and InceptionResNetV2 for the Hindi Image Captioning task. Our submission tops in English-{\\textgreater}Hindi Multimodal task in its track and Odia{\\textless}-{\\textgreater}English translation tasks. Also, our submissions performed well in the Indic Multilingual tasks.","year":2020,"title_abstract":"{ODIANLP}{'}s Participation in {WAT}2020 This paper describes the ODIANLP submission to WAT 2020. We have participated in the English-Hindi Multimodal task and Indic task. We have used the state-of-the-art Transformer model for the translation task and InceptionResNetV2 for the Hindi Image Captioning task. Our submission tops in English-{\\textgreater}Hindi Multimodal task in its track and Odia{\\textless}-{\\textgreater}English translation tasks. Also, our submissions performed well in the Indic Multilingual tasks.","social_need":"Clean Water and Sanitation Ensure availability and sustainable management of water and sanitation for all","cosine_similarity":0.2060169578,"Goal":"Clean Water and Sanitation","Task":["Indic task","translation task","Hindi Image Captioning task","Multimodal task","translation tasks","Indic Multilingual tasks"],"Method":["Transformer model","InceptionResNetV2"]},{"ID":"kumar-etal-2022-soa","title":"{SOA}{\\_}{NLP}@{LT}-{EDI}-{ACL}2022: An Ensemble Model for Hope Speech Detection from {Y}ou{T}ube Comments","abstract":"Language should be accommodating of equality and diversity as a fundamental aspect of communication. The language of internet users has a big impact on peer users all over the world. On virtual platforms such as Facebook, Twitter, and YouTube, people express their opinions in different languages. People respect others{'} accomplishments, pray for their well-being, and cheer them on when they fail. Such motivational remarks are hope speech remarks. Simultaneously, a group of users encourages discrimination against women, people of color, people with disabilities, and other minorities based on gender, race, sexual orientation, and other factors. To recognize hope speech from YouTube comments, the current study offers an ensemble approach that combines a support vector machine, logistic regression, and random forest classifiers. Extensive testing was carried out to discover the best features for the aforementioned classifiers. In the support vector machine and logistic regression classifiers, char-level TF-IDF features were used, whereas in the random forest classifier, word-level features were used. The proposed ensemble model performed significantly well among English, Spanish, Tamil, Malayalam, and Kannada YouTube comments.","year":2022,"title_abstract":"{SOA}{\\_}{NLP}@{LT}-{EDI}-{ACL}2022: An Ensemble Model for Hope Speech Detection from {Y}ou{T}ube Comments Language should be accommodating of equality and diversity as a fundamental aspect of communication. The language of internet users has a big impact on peer users all over the world. On virtual platforms such as Facebook, Twitter, and YouTube, people express their opinions in different languages. People respect others{'} accomplishments, pray for their well-being, and cheer them on when they fail. Such motivational remarks are hope speech remarks. Simultaneously, a group of users encourages discrimination against women, people of color, people with disabilities, and other minorities based on gender, race, sexual orientation, and other factors. To recognize hope speech from YouTube comments, the current study offers an ensemble approach that combines a support vector machine, logistic regression, and random forest classifiers. Extensive testing was carried out to discover the best features for the aforementioned classifiers. In the support vector machine and logistic regression classifiers, char-level TF-IDF features were used, whereas in the random forest classifier, word-level features were used. The proposed ensemble model performed significantly well among English, Spanish, Tamil, Malayalam, and Kannada YouTube comments.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2059219033,"Goal":"Gender Equality","Task":["Hope Speech Detection","communication"],"Method":["Ensemble Model","ensemble approach","support vector machine","logistic regression","random forest classifiers","classifiers","support vector machine","logistic regression classifiers","char - level TF - IDF features","random forest classifier","ensemble model"]},{"ID":"bird-2020-decolonising","title":"Decolonising Speech and Language Technology","abstract":"After generations of exploitation, Indigenous people often respond negatively to the idea that their languages are data ready for the taking. By treating Indigenous knowledge as a commodity, speech and language technologists risk disenfranchising local knowledge authorities, reenacting the causes of language endangerment. Scholars in related fields have responded to calls for decolonisation, and we in the speech and language technology community need to follow suit, and explore what this means for our practices that involve Indigenous languages and the communities who own them. This paper reviews colonising discourses in speech and language technology, and suggests new ways of working with Indigenous communities, and seeks to open a discussion of a postcolonial approach to computational methods for supporting language vitality.","year":2020,"title_abstract":"Decolonising Speech and Language Technology After generations of exploitation, Indigenous people often respond negatively to the idea that their languages are data ready for the taking. By treating Indigenous knowledge as a commodity, speech and language technologists risk disenfranchising local knowledge authorities, reenacting the causes of language endangerment. Scholars in related fields have responded to calls for decolonisation, and we in the speech and language technology community need to follow suit, and explore what this means for our practices that involve Indigenous languages and the communities who own them. This paper reviews colonising discourses in speech and language technology, and suggests new ways of working with Indigenous communities, and seeks to open a discussion of a postcolonial approach to computational methods for supporting language vitality.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2058673203,"Goal":"Sustainable Cities and Communities","Task":["Decolonising Speech and Language Technology","language technologists","language endangerment","speech","language technology community","colonising discourses","speech","language technology","language vitality"],"Method":["postcolonial approach","computational methods"]},{"ID":"sai-2020-siva","title":"Siva at {WNUT}-2020 Task 2: Fine-tuning Transformer Neural Networks for Identification of Informative Covid-19 Tweets","abstract":"Social media witnessed vast amounts of misinformation being circulated every day during the Covid-19 pandemic so much so that the WHO Director-General termed the phenomenon as {``}infodemic.{''} The ill-effects of such misinformation are multifarious. Thus, identifying and eliminating the sources of misinformation becomes very crucial, especially when mass panic can be controlled only through the right information. However, manual identification is arduous, with such large amounts of data being generated every day. This shows the importance of automatic identification of misinformative posts on social media. WNUT-2020 Task 2 aims at building systems for automatic identification of informative tweets. In this paper, I discuss my approach to WNUT-2020 Task 2. I fine-tuned eleven variants of four transformer networks -BERT, RoBERTa, XLM-RoBERTa, ELECTRA, on top of two different preprocessing techniques to reap good results. My top submission achieved an F1-score of 85.3{\\%} in the final evaluation.","year":2020,"title_abstract":"Siva at {WNUT}-2020 Task 2: Fine-tuning Transformer Neural Networks for Identification of Informative Covid-19 Tweets Social media witnessed vast amounts of misinformation being circulated every day during the Covid-19 pandemic so much so that the WHO Director-General termed the phenomenon as {``}infodemic.{''} The ill-effects of such misinformation are multifarious. Thus, identifying and eliminating the sources of misinformation becomes very crucial, especially when mass panic can be controlled only through the right information. However, manual identification is arduous, with such large amounts of data being generated every day. This shows the importance of automatic identification of misinformative posts on social media. WNUT-2020 Task 2 aims at building systems for automatic identification of informative tweets. In this paper, I discuss my approach to WNUT-2020 Task 2. I fine-tuned eleven variants of four transformer networks -BERT, RoBERTa, XLM-RoBERTa, ELECTRA, on top of two different preprocessing techniques to reap good results. My top submission achieved an F1-score of 85.3{\\%} in the final evaluation.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2057270557,"Goal":"Climate Action","Task":["Identification of Informative Covid - 19","mass panic","manual identification","automatic identification of misinformative posts","automatic identification of informative tweets","WNUT - 2020 Task"],"Method":["Transformer Neural Networks","transformer networks","BERT","RoBERTa","XLM - RoBERTa","ELECTRA","preprocessing techniques"]},{"ID":"marcinczuk-etal-2020-pst","title":"{PST} 2.0 {--} Corpus of {P}olish Spatial Texts","abstract":"In the paper, we focus on modeling spatial expressions in texts. We present the guidelines used to annotate the PST 2.0 (Corpus of Polish Spatial Texts) {---} a corpus designed for training and testing the tools for spatial expression recognition. The corpus contains a set of texts gathered from texts collected from travel blogs available under Creative Commons license. We have defined our guidelines based on three existing specifications for English (SpatialML, SpatialRole Labelling from SemEval-2013 Task 3 and ISO-Space1.4 from SpaceEval 2014). We briefly present the existing specifications and discuss what modifications have been made to adapt the guidelines to the characteristics of the Polish language. We also describe the process of data collection and manual annotation, including inter-annotator agreement calculation and corpus statistics. In the end, we present detailed statistics of the PST 2.0 corpus, which include the number of components, relations, expressions, and the most common values of spatial indicators, motion indicators, path indicators, distances, directions, and regions.","year":2020,"title_abstract":"{PST} 2.0 {--} Corpus of {P}olish Spatial Texts In the paper, we focus on modeling spatial expressions in texts. We present the guidelines used to annotate the PST 2.0 (Corpus of Polish Spatial Texts) {---} a corpus designed for training and testing the tools for spatial expression recognition. The corpus contains a set of texts gathered from texts collected from travel blogs available under Creative Commons license. We have defined our guidelines based on three existing specifications for English (SpatialML, SpatialRole Labelling from SemEval-2013 Task 3 and ISO-Space1.4 from SpaceEval 2014). We briefly present the existing specifications and discuss what modifications have been made to adapt the guidelines to the characteristics of the Polish language. We also describe the process of data collection and manual annotation, including inter-annotator agreement calculation and corpus statistics. In the end, we present detailed statistics of the PST 2.0 corpus, which include the number of components, relations, expressions, and the most common values of spatial indicators, motion indicators, path indicators, distances, directions, and regions.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2055885047,"Goal":"Sustainable Cities and Communities","Task":["modeling spatial expressions","spatial expression recognition","SpatialRole Labelling","data collection","manual annotation"],"Method":["PST 2"]},{"ID":"miao-etal-2022-interactive","title":"An Interactive Analysis of User-reported Long {COVID} Symptoms using {T}witter Data","abstract":"With millions of documented recoveries from COVID-19 worldwide, various long-term sequelae have been observed in a large group of survivors. This paper is aimed at systematically analyzing user-generated conversations on Twitter that are related to long-term COVID symptoms for a better understanding of the Long COVID health consequences. Using an interactive information extraction tool built especially for this purpose, we extracted key information from the relevant tweets and analyzed the user-reported Long COVID symptoms with respect to their demographic and geographical characteristics. The results of our analysis are expected to improve the public awareness on long-term COVID-19 sequelae and provide important insights to public health authorities.","year":2022,"title_abstract":"An Interactive Analysis of User-reported Long {COVID} Symptoms using {T}witter Data With millions of documented recoveries from COVID-19 worldwide, various long-term sequelae have been observed in a large group of survivors. This paper is aimed at systematically analyzing user-generated conversations on Twitter that are related to long-term COVID symptoms for a better understanding of the Long COVID health consequences. Using an interactive information extraction tool built especially for this purpose, we extracted key information from the relevant tweets and analyzed the user-reported Long COVID symptoms with respect to their demographic and geographical characteristics. The results of our analysis are expected to improve the public awareness on long-term COVID-19 sequelae and provide important insights to public health authorities.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2053444088,"Goal":"Climate Action","Task":["Long COVID health consequences","COVID - 19"],"Method":["Interactive Analysis","{T}witter Data","interactive information extraction tool"]},{"ID":"chanda-etal-2020-irlab","title":"{IRL}ab@{IITBHU} at {WNUT}-2020 Task 2: Identification of informative {COVID}-19 {E}nglish Tweets using {BERT}","abstract":"This paper reports our submission to the shared Task 2: Identification of informative COVID-19 English tweets at W-NUT 2020. We attempted a few techniques, and we briefly explain here two models that showed promising results in tweet classification tasks: DistilBERT and FastText. DistilBERT achieves a F1 score of 0.7508 on the test set, which is the best of our submissions.","year":2020,"title_abstract":"{IRL}ab@{IITBHU} at {WNUT}-2020 Task 2: Identification of informative {COVID}-19 {E}nglish Tweets using {BERT} This paper reports our submission to the shared Task 2: Identification of informative COVID-19 English tweets at W-NUT 2020. We attempted a few techniques, and we briefly explain here two models that showed promising results in tweet classification tasks: DistilBERT and FastText. DistilBERT achieves a F1 score of 0.7508 on the test set, which is the best of our submissions.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2053027749,"Goal":"Climate Action","Task":["Identification of informative {COVID} - 19","Identification of informative COVID - 19 English tweets","W - NUT","tweet classification tasks"],"Method":["DistilBERT","FastText","DistilBERT"]},{"ID":"gupta-nishu-2020-mapping","title":"Mapping Local News Coverage: Precise location extraction in textual news content using fine-tuned {BERT} based language model","abstract":"Mapping local news coverage from textual content is a challenging problem that requires extracting precise location mentions from news articles. While traditional named entity taggers are able to extract geo-political entities and certain non geo-political entities, they cannot recognize precise location mentions such as addresses, streets and intersections that are required to accurately map the news article. We fine-tune a BERT-based language model for achieving high level of granularity in location extraction. We incorporate the model into an end-to-end tool that further geocodes the extracted locations for the broader objective of mapping news coverage.","year":2020,"title_abstract":"Mapping Local News Coverage: Precise location extraction in textual news content using fine-tuned {BERT} based language model Mapping local news coverage from textual content is a challenging problem that requires extracting precise location mentions from news articles. While traditional named entity taggers are able to extract geo-political entities and certain non geo-political entities, they cannot recognize precise location mentions such as addresses, streets and intersections that are required to accurately map the news article. We fine-tune a BERT-based language model for achieving high level of granularity in location extraction. We incorporate the model into an end-to-end tool that further geocodes the extracted locations for the broader objective of mapping news coverage.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2052165866,"Goal":"Sustainable Cities and Communities","Task":["Mapping Local News Coverage","Precise location extraction","Mapping local news coverage","location extraction","mapping news coverage"],"Method":["fine - tuned {BERT} based language model","named entity taggers","BERT - based language model","end tool"]},{"ID":"gonzalez-etal-2020-type","title":"Type {B} Reflexivization as an Unambiguous Testbed for Multilingual Multi-Task Gender Bias","abstract":"The one-sided focus on English in previous studies of gender bias in NLP misses out on opportunities in other languages: English challenge datasets such as GAP and WinoGender highlight model preferences that are {``}hallucinatory{''}, e.g., disambiguating gender-ambiguous occurrences of {`}doctor{'} as male doctors. We show that for languages with type B reflexivization, e.g., Swedish and Russian, we can construct multi-task challenge datasets for detecting gender bias that lead to unambiguously wrong model predictions: In these languages, the direct translation of {`}the doctor removed his mask{'} is not ambiguous between a coreferential reading and a disjoint reading. Instead, the coreferential reading requires a non-gendered pronoun, and the gendered, possessive pronouns are anti-reflexive. We present a multilingual, multi-task challenge dataset, which spans four languages and four NLP tasks and focuses only on this phenomenon. We find evidence for gender bias across all task-language combinations and correlate model bias with national labor market statistics.","year":2020,"title_abstract":"Type {B} Reflexivization as an Unambiguous Testbed for Multilingual Multi-Task Gender Bias The one-sided focus on English in previous studies of gender bias in NLP misses out on opportunities in other languages: English challenge datasets such as GAP and WinoGender highlight model preferences that are {``}hallucinatory{''}, e.g., disambiguating gender-ambiguous occurrences of {`}doctor{'} as male doctors. We show that for languages with type B reflexivization, e.g., Swedish and Russian, we can construct multi-task challenge datasets for detecting gender bias that lead to unambiguously wrong model predictions: In these languages, the direct translation of {`}the doctor removed his mask{'} is not ambiguous between a coreferential reading and a disjoint reading. Instead, the coreferential reading requires a non-gendered pronoun, and the gendered, possessive pronouns are anti-reflexive. We present a multilingual, multi-task challenge dataset, which spans four languages and four NLP tasks and focuses only on this phenomenon. We find evidence for gender bias across all task-language combinations and correlate model bias with national labor market statistics.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2050942928,"Goal":"Gender Equality","Task":["Multilingual Multi - Task Gender Bias","gender bias","NLP","detecting gender bias","NLP tasks"],"Method":["Type {B} Reflexivization","coreferential reading"]},{"ID":"benotti-blackburn-2021-grounding","title":"Grounding as a Collaborative Process","abstract":"Collaborative grounding is a fundamental aspect of human-human dialog which allows people to negotiate meaning. In this paper we argue that it is missing from current deep learning approaches to dialog. Our central point is that making mistakes and being able to recover from them collaboratively is a key ingredient in grounding meaning. We illustrate the pitfalls of being unable to ground collaboratively, discuss what can be learned from the language acquisition and dialog systems literature, and reflect on how to move forward.","year":2021,"title_abstract":"Grounding as a Collaborative Process Collaborative grounding is a fundamental aspect of human-human dialog which allows people to negotiate meaning. In this paper we argue that it is missing from current deep learning approaches to dialog. Our central point is that making mistakes and being able to recover from them collaboratively is a key ingredient in grounding meaning. We illustrate the pitfalls of being unable to ground collaboratively, discuss what can be learned from the language acquisition and dialog systems literature, and reflect on how to move forward.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.2049908191,"Goal":"Partnership for the Goals","Task":["Grounding","Collaborative grounding","human - human dialog","dialog","grounding meaning","language acquisition","dialog systems literature"],"Method":["Collaborative Process","deep learning approaches"]},{"ID":"vo-lee-2020-facts","title":"Where Are the Facts? Searching for Fact-checked Information to Alleviate the Spread of Fake News","abstract":"Although many fact-checking systems have been developed in academia and industry, fake news is still proliferating on social media. These systems mostly focus on fact-checking but usually neglect online users who are the main drivers of the spread of misinformation. How can we use fact-checked information to improve users{'} consciousness of fake news to which they are exposed? How can we stop users from spreading fake news? To tackle these questions, we propose a novel framework to search for fact-checking articles, which address the content of an original tweet (that may contain misinformation) posted by online users. The search can directly warn fake news posters and online users (e.g. the posters{'} followers) about misinformation, discourage them from spreading fake news, and scale up verified content on social media. Our framework uses both text and images to search for fact-checking articles, and achieves promising results on real-world datasets. Our code and datasets are released at https:\/\/github.com\/nguyenvo09\/EMNLP2020.","year":2020,"title_abstract":"Where Are the Facts? Searching for Fact-checked Information to Alleviate the Spread of Fake News Although many fact-checking systems have been developed in academia and industry, fake news is still proliferating on social media. These systems mostly focus on fact-checking but usually neglect online users who are the main drivers of the spread of misinformation. How can we use fact-checked information to improve users{'} consciousness of fake news to which they are exposed? How can we stop users from spreading fake news? To tackle these questions, we propose a novel framework to search for fact-checking articles, which address the content of an original tweet (that may contain misinformation) posted by online users. The search can directly warn fake news posters and online users (e.g. the posters{'} followers) about misinformation, discourage them from spreading fake news, and scale up verified content on social media. Our framework uses both text and images to search for fact-checking articles, and achieves promising results on real-world datasets. Our code and datasets are released at https:\/\/github.com\/nguyenvo09\/EMNLP2020.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2046079338,"Goal":"Climate Action","Task":["Fact - checked Information","fact - checking","fact - checking articles","fact - checking articles"],"Method":["fact - checking systems"]},{"ID":"rehm-etal-2020-european-language","title":"{E}uropean Language Grid: An Overview","abstract":"With 24 official EU and many additional languages, multilingualism in Europe and an inclusive Digital Single Market can only be enabled through Language Technologies (LTs). European LT business is dominated by hundreds of SMEs and a few large players. Many are world-class, with technologies that outperform the global players. However, European LT business is also fragmented {--} by nation states, languages, verticals and sectors, significantly holding back its impact. The European Language Grid (ELG) project addresses this fragmentation by establishing the ELG as the primary platform for LT in Europe. The ELG is a scalable cloud platform, providing, in an easy-to-integrate way, access to hundreds of commercial and non-commercial LTs for all European languages, including running tools and services as well as data sets and resources. Once fully operational, it will enable the commercial and non-commercial European LT community to deposit and upload their technologies and data sets into the ELG, to deploy them through the grid, and to connect with other resources. The ELG will boost the Multilingual Digital Single Market towards a thriving European LT community, creating new jobs and opportunities. Furthermore, the ELG project organises two open calls for up to 20 pilot projects. It also sets up 32 national competence centres and the European LT Council for outreach and coordination purposes.","year":2020,"title_abstract":"{E}uropean Language Grid: An Overview With 24 official EU and many additional languages, multilingualism in Europe and an inclusive Digital Single Market can only be enabled through Language Technologies (LTs). European LT business is dominated by hundreds of SMEs and a few large players. Many are world-class, with technologies that outperform the global players. However, European LT business is also fragmented {--} by nation states, languages, verticals and sectors, significantly holding back its impact. The European Language Grid (ELG) project addresses this fragmentation by establishing the ELG as the primary platform for LT in Europe. The ELG is a scalable cloud platform, providing, in an easy-to-integrate way, access to hundreds of commercial and non-commercial LTs for all European languages, including running tools and services as well as data sets and resources. Once fully operational, it will enable the commercial and non-commercial European LT community to deposit and upload their technologies and data sets into the ELG, to deploy them through the grid, and to connect with other resources. The ELG will boost the Multilingual Digital Single Market towards a thriving European LT community, creating new jobs and opportunities. Furthermore, the ELG project organises two open calls for up to 20 pilot projects. It also sets up 32 national competence centres and the European LT Council for outreach and coordination purposes.","social_need":"Industry, Innovation and Infrastrucure Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation","cosine_similarity":0.2044789195,"Goal":"Industry, Innovation and Infrastrucure","Task":["LT business","LT business","LT","outreach and coordination purposes"],"Method":["Language Grid","Language Technologies","European Language Grid","ELG","ELG","cloud platform","ELG","ELG","ELG project"]},{"ID":"howcroft-etal-2020-twenty","title":"Twenty Years of Confusion in Human Evaluation: {NLG} Needs Evaluation Sheets and Standardised Definitions","abstract":"Human assessment remains the most trusted form of evaluation in NLG, but highly diverse approaches and a proliferation of different quality criteria used by researchers make it difficult to compare results and draw conclusions across papers, with adverse implications for meta-evaluation and reproducibility. In this paper, we present (i) our dataset of 165 NLG papers with human evaluations, (ii) the annotation scheme we developed to label the papers for different aspects of evaluations, (iii) quantitative analyses of the annotations, and (iv) a set of recommendations for improving standards in evaluation reporting. We use the annotations as a basis for examining information included in evaluation reports, and levels of consistency in approaches, experimental design and terminology, focusing in particular on the 200+ different terms that have been used for evaluated aspects of quality. We conclude that due to a pervasive lack of clarity in reports and extreme diversity in approaches, human evaluation in NLG presents as extremely confused in 2020, and that the field is in urgent need of standard methods and terminology.","year":2020,"title_abstract":"Twenty Years of Confusion in Human Evaluation: {NLG} Needs Evaluation Sheets and Standardised Definitions Human assessment remains the most trusted form of evaluation in NLG, but highly diverse approaches and a proliferation of different quality criteria used by researchers make it difficult to compare results and draw conclusions across papers, with adverse implications for meta-evaluation and reproducibility. In this paper, we present (i) our dataset of 165 NLG papers with human evaluations, (ii) the annotation scheme we developed to label the papers for different aspects of evaluations, (iii) quantitative analyses of the annotations, and (iv) a set of recommendations for improving standards in evaluation reporting. We use the annotations as a basis for examining information included in evaluation reports, and levels of consistency in approaches, experimental design and terminology, focusing in particular on the 200+ different terms that have been used for evaluated aspects of quality. We conclude that due to a pervasive lack of clarity in reports and extreme diversity in approaches, human evaluation in NLG presents as extremely confused in 2020, and that the field is in urgent need of standard methods and terminology.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2041635364,"Goal":"Sustainable Cities and Communities","Task":["Human Evaluation","Human assessment","evaluation","NLG","NLG","evaluations","evaluation reporting","human evaluation","NLG"],"Method":["annotation scheme"]},{"ID":"dinan-etal-2020-queens","title":"Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation","abstract":"Social biases present in data are often directly reflected in the predictions of models trained on that data. We analyze gender bias in dialogue data, and examine how this bias is not only replicated, but is also amplified in subsequent generative chit-chat dialogue models. We measure gender bias in six existing dialogue datasets before selecting the most biased one, the multi-player text-based fantasy adventure dataset LIGHT, as a testbed for bias mitigation techniques. We consider three techniques to mitigate gender bias: counterfactual data augmentation, targeted data collection, and bias controlled training. We show that our proposed techniques mitigate gender bias by balancing the genderedness of generated dialogue utterances, and find that they are particularly effective in combination. We evaluate model performance with a variety of quantitative methods{---}including the quantity of gendered words, a dialogue safety classifier, and human assessments{---}all of which show that our models generate less gendered, but equally engaging chit-chat responses.","year":2020,"title_abstract":"Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation Social biases present in data are often directly reflected in the predictions of models trained on that data. We analyze gender bias in dialogue data, and examine how this bias is not only replicated, but is also amplified in subsequent generative chit-chat dialogue models. We measure gender bias in six existing dialogue datasets before selecting the most biased one, the multi-player text-based fantasy adventure dataset LIGHT, as a testbed for bias mitigation techniques. We consider three techniques to mitigate gender bias: counterfactual data augmentation, targeted data collection, and bias controlled training. We show that our proposed techniques mitigate gender bias by balancing the genderedness of generated dialogue utterances, and find that they are particularly effective in combination. We evaluate model performance with a variety of quantitative methods{---}including the quantity of gendered words, a dialogue safety classifier, and human assessments{---}all of which show that our models generate less gendered, but equally engaging chit-chat responses.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2040826231,"Goal":"Gender Equality","Task":["Gender Bias","Dialogue Generation","gender bias","counterfactual data augmentation","data collection"],"Method":["generative chit - chat","bias mitigation techniques","bias controlled training","quantitative methods{","dialogue safety classifier"]},{"ID":"lopez-2012-managing","title":"Managing Change when Implementing {MT} Systems","abstract":"Managing large scale MT post-editing projects is a challenging endeavor. From securing linguists buy-in to ensuring consistency of the output, it is important to develop a set of specific processes and tools that facilitate this task. Drawing from years of experience in such projects, we will attempt here to describe the challenges associated to the management of such projects and to define best practices.","year":2012,"title_abstract":"Managing Change when Implementing {MT} Systems Managing large scale MT post-editing projects is a challenging endeavor. From securing linguists buy-in to ensuring consistency of the output, it is important to develop a set of specific processes and tools that facilitate this task. Drawing from years of experience in such projects, we will attempt here to describe the challenges associated to the management of such projects and to define best practices.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.2039740384,"Goal":"Partnership for the Goals","Task":["Managing Change","large scale MT post - editing projects"],"Method":["{MT} Systems"]},{"ID":"zariquiey-etal-2022-cld2","title":"{CLD}{\\mbox{$^2$}} Language Documentation Meets Natural Language Processing for Revitalising Endangered Languages","abstract":"Language revitalisation should not be understood as a direct outcome of language documentation, which is mainly focused on the creation of language repositories. Natural language processing (NLP) offers the potential to complement and exploit these repositories through the development of language technologies that may contribute to improving the vitality status of endangered languages. In this paper, we discuss the current state of the interaction between language documentation and computational linguistics, present a diagnosis of how the outputs of recent documentation projects for endangered languages are underutilised for the NLP community, and discuss how the situation could change from both the documentary linguistics and NLP perspectives. All this is introduced as a bridging paradigm dubbed as Computational Language Documentation and Development (CLD{\\mbox{$^2$}}). CLD{\\mbox{$^2$}} calls for (1) the inclusion of NLP-friendly annotated data as a deliverable of future language documentation projects; and (2) the exploitation of language documentation databases by the NLP community to promote the computerization of endangered languages, as one way to contribute to their revitalization.","year":2022,"title_abstract":"{CLD}{\\mbox{$^2$}} Language Documentation Meets Natural Language Processing for Revitalising Endangered Languages Language revitalisation should not be understood as a direct outcome of language documentation, which is mainly focused on the creation of language repositories. Natural language processing (NLP) offers the potential to complement and exploit these repositories through the development of language technologies that may contribute to improving the vitality status of endangered languages. In this paper, we discuss the current state of the interaction between language documentation and computational linguistics, present a diagnosis of how the outputs of recent documentation projects for endangered languages are underutilised for the NLP community, and discuss how the situation could change from both the documentary linguistics and NLP perspectives. All this is introduced as a bridging paradigm dubbed as Computational Language Documentation and Development (CLD{\\mbox{$^2$}}). CLD{\\mbox{$^2$}} calls for (1) the inclusion of NLP-friendly annotated data as a deliverable of future language documentation projects; and (2) the exploitation of language documentation databases by the NLP community to promote the computerization of endangered languages, as one way to contribute to their revitalization.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.2039538622,"Goal":"Life on Land","Task":["Language Documentation","Natural Language Processing","Revitalising Endangered Languages","Language revitalisation","language documentation","language repositories","Natural language processing","language documentation","computational linguistics","NLP community","documentary linguistics","NLP","Computational Language Documentation","Development","NLP","language documentation projects;","NLP","computerization of endangered languages"],"Method":["language technologies","documentation projects","bridging paradigm"]},{"ID":"marinov-etal-2020-topic","title":"Topic and Emotion Development among {D}utch {COVID}-19 {T}witter Communities in the early Pandemic","abstract":"The paper focuses on a large collection of Dutch tweets from the Netherlands to get an insight into the perception and reactions of users during the early months of the COVID-19 pandemic. We focused on five major user communities of users: government and health organizations, news media, politicians, the general public and conspiracy theory supporters, investigating differences among them in topic dominance and the expressions of emotions. Through topic modeling we monitor the evolution of the conversation about COVID-19 among these communities. Our results indicate that the national focus on COVID-19 shifted from the virus itself to its impact on the economy between February and April. Surprisingly, the overall emotional public response appears to be substantially positive and expressing trust, although differences can be observed in specific group of users.","year":2020,"title_abstract":"Topic and Emotion Development among {D}utch {COVID}-19 {T}witter Communities in the early Pandemic The paper focuses on a large collection of Dutch tweets from the Netherlands to get an insight into the perception and reactions of users during the early months of the COVID-19 pandemic. We focused on five major user communities of users: government and health organizations, news media, politicians, the general public and conspiracy theory supporters, investigating differences among them in topic dominance and the expressions of emotions. Through topic modeling we monitor the evolution of the conversation about COVID-19 among these communities. Our results indicate that the national focus on COVID-19 shifted from the virus itself to its impact on the economy between February and April. Surprisingly, the overall emotional public response appears to be substantially positive and expressing trust, although differences can be observed in specific group of users.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2037512809,"Goal":"Climate Action","Task":["Topic and Emotion Development"],"Method":["topic modeling"]},{"ID":"brundage-etal-1997-managing","title":"Managing Distributed {MT} Projects Today {---} A New Challenge","abstract":"The current trend towards globalization means that even the most modern of industries must constantly re-evaluate its strategies and adapt to new technologies. As a long-time supporter of MT technology, SAP has shown that it can make productive use of competitive, commercial MT products along with other CAT products. In making MT work for them, however, SAP has also had to substantially adapt the products that they received from MT companies. The result, after many years, is a full range of peripheral tools and workflow scenarios that support the use of their MT programs.","year":1997,"title_abstract":"Managing Distributed {MT} Projects Today {---} A New Challenge The current trend towards globalization means that even the most modern of industries must constantly re-evaluate its strategies and adapt to new technologies. As a long-time supporter of MT technology, SAP has shown that it can make productive use of competitive, commercial MT products along with other CAT products. In making MT work for them, however, SAP has also had to substantially adapt the products that they received from MT companies. The result, after many years, is a full range of peripheral tools and workflow scenarios that support the use of their MT programs.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.2036828101,"Goal":"Partnership for the Goals","Task":["Distributed {MT} Projects","MT","MT"],"Method":["MT technology","SAP","SAP","workflow scenarios"]},{"ID":"ilieva-kancheva-2019-success","title":"The Success Story of Mitra Translations","abstract":"Technologies and their constant updates and innovative nature drastically and irreversibly transformed this small business into a leading brand on the translation market, along with just few other LSPs integrating translation software solutions. Now, we are constantly following the new developments in software updates and online platforms and we are successfully keeping up with any new trend in the field of translation, localization, transcreation, revision, post-editing, etc. Ultimately, we are positive that proper implementation of technology (with focus on quality, cost and time) and hard work are the stepping stones in the way to become a trusted translation services provider.","year":2019,"title_abstract":"The Success Story of Mitra Translations Technologies and their constant updates and innovative nature drastically and irreversibly transformed this small business into a leading brand on the translation market, along with just few other LSPs integrating translation software solutions. Now, we are constantly following the new developments in software updates and online platforms and we are successfully keeping up with any new trend in the field of translation, localization, transcreation, revision, post-editing, etc. Ultimately, we are positive that proper implementation of technology (with focus on quality, cost and time) and hard work are the stepping stones in the way to become a trusted translation services provider.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.2036223412,"Goal":"Partnership for the Goals","Task":["translation market","software updates","translation","localization","transcreation","revision","post - editing","translation services"],"Method":["translation software solutions"]},{"ID":"mcgregor-lim-2018-affordances","title":"Affordances in Grounded Language Learning","abstract":"We present a novel methodology involving mappings between different modes of semantic representation. We propose distributional semantic models as a mechanism for representing the kind of world knowledge inherent in the system of abstract symbols characteristic of a sophisticated community of language users. Then, motivated by insight from ecological psychology, we describe a model approximating affordances, by which we mean a language learner{'}s direct perception of opportunities for action in an environment. We present a preliminary experiment involving mapping between these two representational modalities, and propose that our methodology can become the basis for a cognitively inspired model of grounded language learning.","year":2018,"title_abstract":"Affordances in Grounded Language Learning We present a novel methodology involving mappings between different modes of semantic representation. We propose distributional semantic models as a mechanism for representing the kind of world knowledge inherent in the system of abstract symbols characteristic of a sophisticated community of language users. Then, motivated by insight from ecological psychology, we describe a model approximating affordances, by which we mean a language learner{'}s direct perception of opportunities for action in an environment. We present a preliminary experiment involving mapping between these two representational modalities, and propose that our methodology can become the basis for a cognitively inspired model of grounded language learning.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.203450799,"Goal":"Life on Land","Task":["Affordances","Grounded Language Learning","ecological psychology","approximating affordances","grounded language learning"],"Method":["semantic representation","distributional semantic models","language learner{'}s","representational modalities","cognitively inspired model"]},{"ID":"nozza-etal-2021-honest","title":"{HONEST}: Measuring Hurtful Sentence Completion in Language Models","abstract":"Language models have revolutionized the field of NLP. However, language models capture and proliferate hurtful stereotypes, especially in text generation. Our results show that 4.3{\\%} of the time, language models complete a sentence with a hurtful word. These cases are not random, but follow language and gender-specific patterns. We propose a score to measure hurtful sentence completions in language models (HONEST). It uses a systematic template- and lexicon-based bias evaluation methodology for six languages. Our findings suggest that these models replicate and amplify deep-seated societal stereotypes about gender roles. Sentence completions refer to sexual promiscuity when the target is female in 9{\\%} of the time, and in 4{\\%} to homosexuality when the target is male. The results raise questions about the use of these models in production settings.","year":2021,"title_abstract":"{HONEST}: Measuring Hurtful Sentence Completion in Language Models Language models have revolutionized the field of NLP. However, language models capture and proliferate hurtful stereotypes, especially in text generation. Our results show that 4.3{\\%} of the time, language models complete a sentence with a hurtful word. These cases are not random, but follow language and gender-specific patterns. We propose a score to measure hurtful sentence completions in language models (HONEST). It uses a systematic template- and lexicon-based bias evaluation methodology for six languages. Our findings suggest that these models replicate and amplify deep-seated societal stereotypes about gender roles. Sentence completions refer to sexual promiscuity when the target is female in 9{\\%} of the time, and in 4{\\%} to homosexuality when the target is male. The results raise questions about the use of these models in production settings.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2033050805,"Goal":"Gender Equality","Task":["Hurtful Sentence Completion","NLP","text generation","hurtful sentence completions","Sentence completions","production settings"],"Method":["Language Models","Language models","language models","language models","language models","template - and lexicon - based bias evaluation methodology"]},{"ID":"mircea-2020-real","title":"Real-time Classification, Geolocation and Interactive Visualization of {COVID}-19 Information Shared on Social Media to Better Understand Global Developments","abstract":"As people communicate on social media during COVID-19, it can be an invaluable source of useful and up-to-date information. However, the large volume and noise-to-signal ratio of social media can make this impractical. We present a prototype dashboard for the real-time classification, geolocation and interactive visualization of COVID-19 tweets that addresses these issues. We also describe a novel L2 classification layer that outperforms linear layers on a dataset of respiratory virus tweets.","year":2020,"title_abstract":"Real-time Classification, Geolocation and Interactive Visualization of {COVID}-19 Information Shared on Social Media to Better Understand Global Developments As people communicate on social media during COVID-19, it can be an invaluable source of useful and up-to-date information. However, the large volume and noise-to-signal ratio of social media can make this impractical. We present a prototype dashboard for the real-time classification, geolocation and interactive visualization of COVID-19 tweets that addresses these issues. We also describe a novel L2 classification layer that outperforms linear layers on a dataset of respiratory virus tweets.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2032264322,"Goal":"Climate Action","Task":["Real - time Classification","Geolocation","Interactive Visualization of {COVID} - 19 Information","real - time classification","geolocation","interactive visualization of COVID - 19"],"Method":["prototype dashboard","L2 classification layer","linear layers"]},{"ID":"gritta-etal-2018-melbourne","title":"Which {M}elbourne? Augmenting Geocoding with Maps","abstract":"The purpose of text geolocation is to associate geographic information contained in a document with a set (or sets) of coordinates, either implicitly by using linguistic features and\/or explicitly by using geographic metadata combined with heuristics. We introduce a geocoder (location mention disambiguator) that achieves state-of-the-art (SOTA) results on three diverse datasets by exploiting the implicit lexical clues. Moreover, we propose a new method for systematic encoding of geographic metadata to generate two distinct views of the same text. To that end, we introduce the Map Vector (MapVec), a sparse representation obtained by plotting prior geographic probabilities, derived from population figures, on a World Map. We then integrate the implicit (language) and explicit (map) features to significantly improve a range of metrics. We also introduce an open-source dataset for geoparsing of news events covering global disease outbreaks and epidemics to help future evaluation in geoparsing.","year":2018,"title_abstract":"Which {M}elbourne? Augmenting Geocoding with Maps The purpose of text geolocation is to associate geographic information contained in a document with a set (or sets) of coordinates, either implicitly by using linguistic features and\/or explicitly by using geographic metadata combined with heuristics. We introduce a geocoder (location mention disambiguator) that achieves state-of-the-art (SOTA) results on three diverse datasets by exploiting the implicit lexical clues. Moreover, we propose a new method for systematic encoding of geographic metadata to generate two distinct views of the same text. To that end, we introduce the Map Vector (MapVec), a sparse representation obtained by plotting prior geographic probabilities, derived from population figures, on a World Map. We then integrate the implicit (language) and explicit (map) features to significantly improve a range of metrics. We also introduce an open-source dataset for geoparsing of news events covering global disease outbreaks and epidemics to help future evaluation in geoparsing.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2032119185,"Goal":"Sustainable Cities and Communities","Task":["text geolocation","systematic encoding of geographic metadata","geoparsing of news events","global disease outbreaks","geoparsing"],"Method":["geocoder (location mention disambiguator)","Map Vector","sparse representation"]},{"ID":"rehm-etal-2020-european","title":"The {E}uropean Language Technology Landscape in 2020: Language-Centric and Human-Centric {AI} for Cross-Cultural Communication in Multilingual {E}urope","abstract":"Multilingualism is a cultural cornerstone of Europe and firmly anchored in the European treaties including full language equality. However, language barriers impacting business, cross-lingual and cross-cultural communication are still omnipresent. Language Technologies (LTs) are a powerful means to break down these barriers. While the last decade has seen various initiatives that created a multitude of approaches and technologies tailored to Europe{'}s specific needs, there is still an immense level of fragmentation. At the same time, AI has become an increasingly important concept in the European Information and Communication Technology area. For a few years now, AI {--} including many opportunities, synergies but also misconceptions {--} has been overshadowing every other topic. We present an overview of the European LT landscape, describing funding programmes, activities, actions and challenges in the different countries with regard to LT, including the current state of play in industry and the LT market. We present a brief overview of the main LT-related activities on the EU level in the last ten years and develop strategic guidance with regard to four key dimensions.","year":2020,"title_abstract":"The {E}uropean Language Technology Landscape in 2020: Language-Centric and Human-Centric {AI} for Cross-Cultural Communication in Multilingual {E}urope Multilingualism is a cultural cornerstone of Europe and firmly anchored in the European treaties including full language equality. However, language barriers impacting business, cross-lingual and cross-cultural communication are still omnipresent. Language Technologies (LTs) are a powerful means to break down these barriers. While the last decade has seen various initiatives that created a multitude of approaches and technologies tailored to Europe{'}s specific needs, there is still an immense level of fragmentation. At the same time, AI has become an increasingly important concept in the European Information and Communication Technology area. For a few years now, AI {--} including many opportunities, synergies but also misconceptions {--} has been overshadowing every other topic. We present an overview of the European LT landscape, describing funding programmes, activities, actions and challenges in the different countries with regard to LT, including the current state of play in industry and the LT market. We present a brief overview of the main LT-related activities on the EU level in the last ten years and develop strategic guidance with regard to four key dimensions.","social_need":"Industry, Innovation and Infrastrucure Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation","cosine_similarity":0.2032006979,"Goal":"Industry, Innovation and Infrastrucure","Task":["Cross - Cultural Communication","Multilingual","Multilingualism","business","cross - lingual and cross - cultural communication","European Information and Communication Technology area","AI","LT","industry","LT market","LT"],"Method":["Language - Centric","Language Technologies","AI","LT"]},{"ID":"madnani-etal-2019-turn","title":"My Turn To Read: An Interleaved {E}-book Reading Tool for Developing and Struggling Readers","abstract":"Literacy is crucial for functioning in modern society. It underpins everything from educational attainment and employment opportunities to health outcomes. We describe My Turn To Read, an app that uses interleaved reading to help developing and struggling readers improve reading skills while reading for meaning and pleasure. We hypothesize that the longer-term impact of the app will be to help users become better, more confident readers with an increased stamina for extended reading. We describe the technology and present preliminary evidence in support of this hypothesis.","year":2019,"title_abstract":"My Turn To Read: An Interleaved {E}-book Reading Tool for Developing and Struggling Readers Literacy is crucial for functioning in modern society. It underpins everything from educational attainment and employment opportunities to health outcomes. We describe My Turn To Read, an app that uses interleaved reading to help developing and struggling readers improve reading skills while reading for meaning and pleasure. We hypothesize that the longer-term impact of the app will be to help users become better, more confident readers with an increased stamina for extended reading. We describe the technology and present preliminary evidence in support of this hypothesis.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.2030753344,"Goal":"Good Health and Well-Being","Task":["Developing and Struggling Readers Literacy","educational attainment","extended reading"],"Method":["Interleaved {E} - book Reading Tool","interleaved reading"]},{"ID":"yu-etal-2021-research","title":"A Research Framework for Understanding Education-Occupation Alignment with {NLP} Techniques","abstract":"Understanding the gaps between job requirements and university curricula is crucial for improving student success and institutional effectiveness in higher education. In this context, natural language processing (NLP) can be leveraged to generate granular insights into where the gaps are and how they change. This paper proposes a three-dimensional research framework that combines NLP techniques with economic and educational research to quantify the alignment between course syllabi and job postings. We elaborate on key technical details of the framework and further discuss its potential positive impacts on practice, including unveiling the inequalities in and long-term consequences of education-occupation alignment to inform policymakers, and fostering information systems to support students, institutions and employers in the school-to-work pipeline.","year":2021,"title_abstract":"A Research Framework for Understanding Education-Occupation Alignment with {NLP} Techniques Understanding the gaps between job requirements and university curricula is crucial for improving student success and institutional effectiveness in higher education. In this context, natural language processing (NLP) can be leveraged to generate granular insights into where the gaps are and how they change. This paper proposes a three-dimensional research framework that combines NLP techniques with economic and educational research to quantify the alignment between course syllabi and job postings. We elaborate on key technical details of the framework and further discuss its potential positive impacts on practice, including unveiling the inequalities in and long-term consequences of education-occupation alignment to inform policymakers, and fostering information systems to support students, institutions and employers in the school-to-work pipeline.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.2030641884,"Goal":"Quality Education","Task":["Education - Occupation Alignment","student success","higher education","natural language processing","economic and educational research","education - occupation alignment","school - to - work pipeline"],"Method":["Research Framework","{NLP} Techniques","three - dimensional research framework","NLP techniques","information systems"]},{"ID":"roy-etal-2021-identifying","title":"Identifying Morality Frames in Political Tweets using Relational Learning","abstract":"Extracting moral sentiment from text is a vital component in understanding public opinion, social movements, and policy decisions. The Moral Foundation Theory identifies five moral foundations, each associated with a positive and negative polarity. However, moral sentiment is often motivated by its targets, which can correspond to individuals or collective entities. In this paper, we introduce morality frames, a representation framework for organizing moral attitudes directed at different entities, and come up with a novel and high-quality annotated dataset of tweets written by US politicians. Then, we propose a relational learning model to predict moral attitudes towards entities and moral foundations jointly. We do qualitative and quantitative evaluations, showing that moral sentiment towards entities differs highly across political ideologies.","year":2021,"title_abstract":"Identifying Morality Frames in Political Tweets using Relational Learning Extracting moral sentiment from text is a vital component in understanding public opinion, social movements, and policy decisions. The Moral Foundation Theory identifies five moral foundations, each associated with a positive and negative polarity. However, moral sentiment is often motivated by its targets, which can correspond to individuals or collective entities. In this paper, we introduce morality frames, a representation framework for organizing moral attitudes directed at different entities, and come up with a novel and high-quality annotated dataset of tweets written by US politicians. Then, we propose a relational learning model to predict moral attitudes towards entities and moral foundations jointly. We do qualitative and quantitative evaluations, showing that moral sentiment towards entities differs highly across political ideologies.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.2029656917,"Goal":"Peace, Justice and Strong Institutions","Task":["Identifying Morality Frames","Extracting moral sentiment","public opinion","social movements","policy decisions","organizing moral attitudes"],"Method":["Relational Learning","Moral Foundation Theory","morality frames","representation framework","relational learning model"]},{"ID":"maronikolakis-etal-2022-listening","title":"Listening to Affected Communities to Define Extreme Speech: Dataset and Experiments","abstract":"Building on current work on multilingual hate speech (e.g., Ousidhoum et al. (2019)) and hate speech reduction (e.g., Sap et al. (2020)), we present XTREMESPEECH, a new hate speech dataset containing 20,297 social media passages from Brazil, Germany, India and Kenya. The key novelty is that we directly involve the affected communities in collecting and annotating the data {--} as opposed to giving companies and governments control over defining and combatting hate speech. This inclusive approach results in datasets more representative of actually occurring online speech and is likely to facilitate the removal of the social media content that marginalized communities view as causing the most harm. Based on XTREMESPEECH, we establish novel tasks with accompanying baselines, provide evidence that cross-country training is generally not feasible due to cultural differences between countries and perform an interpretability analysis of BERT{'}s predictions.","year":2022,"title_abstract":"Listening to Affected Communities to Define Extreme Speech: Dataset and Experiments Building on current work on multilingual hate speech (e.g., Ousidhoum et al. (2019)) and hate speech reduction (e.g., Sap et al. (2020)), we present XTREMESPEECH, a new hate speech dataset containing 20,297 social media passages from Brazil, Germany, India and Kenya. The key novelty is that we directly involve the affected communities in collecting and annotating the data {--} as opposed to giving companies and governments control over defining and combatting hate speech. This inclusive approach results in datasets more representative of actually occurring online speech and is likely to facilitate the removal of the social media content that marginalized communities view as causing the most harm. Based on XTREMESPEECH, we establish novel tasks with accompanying baselines, provide evidence that cross-country training is generally not feasible due to cultural differences between countries and perform an interpretability analysis of BERT{'}s predictions.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.2027826011,"Goal":"Reduced Inequalities","Task":["multilingual hate speech","hate speech reduction","cross - country training","interpretability analysis","BERT{'}s"],"Method":["XTREMESPEECH"]},{"ID":"ohashi-etal-2020-idsou","title":"{IDSOU} at {WNUT}-2020 Task 2: Identification of Informative {COVID}-19 {E}nglish Tweets","abstract":"We introduce the IDSOU submission for the WNUT-2020 task 2: identification of informative COVID-19 English Tweets. Our system is an ensemble of pre-trained language models such as BERT. We ranked 16th in the F1 score.","year":2020,"title_abstract":"{IDSOU} at {WNUT}-2020 Task 2: Identification of Informative {COVID}-19 {E}nglish Tweets We introduce the IDSOU submission for the WNUT-2020 task 2: identification of informative COVID-19 English Tweets. Our system is an ensemble of pre-trained language models such as BERT. We ranked 16th in the F1 score.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2027190626,"Goal":"Climate Action","Task":["Identification of Informative {COVID} - 19","WNUT - 2020 task","identification of informative COVID"],"Method":["language models","BERT"]},{"ID":"giorgi-etal-2018-remarkable","title":"The Remarkable Benefit of User-Level Aggregation for Lexical-based Population-Level Predictions","abstract":"Nowcasting based on social media text promises to provide unobtrusive and near real-time predictions of community-level outcomes. These outcomes are typically regarding people, but the data is often aggregated without regard to users in the Twitter populations of each community. This paper describes a simple yet effective method for building community-level models using Twitter language aggregated by user. Results on four different U.S. county-level tasks, spanning demographic, health, and psychological outcomes show large and consistent improvements in prediction accuracies (e.g. from Pearson r=.73 to .82 for median income prediction or r=.37 to .47 for life satisfaction prediction) over the standard approach of aggregating all tweets. We make our aggregated and anonymized community-level data, derived from 37 billion tweets {--} over 1 billion of which were mapped to counties, available for research.","year":2018,"title_abstract":"The Remarkable Benefit of User-Level Aggregation for Lexical-based Population-Level Predictions Nowcasting based on social media text promises to provide unobtrusive and near real-time predictions of community-level outcomes. These outcomes are typically regarding people, but the data is often aggregated without regard to users in the Twitter populations of each community. This paper describes a simple yet effective method for building community-level models using Twitter language aggregated by user. Results on four different U.S. county-level tasks, spanning demographic, health, and psychological outcomes show large and consistent improvements in prediction accuracies (e.g. from Pearson r=.73 to .82 for median income prediction or r=.37 to .47 for life satisfaction prediction) over the standard approach of aggregating all tweets. We make our aggregated and anonymized community-level data, derived from 37 billion tweets {--} over 1 billion of which were mapped to counties, available for research.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2026002109,"Goal":"Sustainable Cities and Communities","Task":["Lexical - based Population - Level Predictions","real - time predictions of community - level outcomes","county - level tasks","median income prediction","life satisfaction prediction)"],"Method":["User - Level Aggregation","community - level models"]},{"ID":"gawronska-duczak-2000-understanding","title":"Understanding politics by studying weather: a cognitive approach to representation of {P}olish verbs of motion, appearance, and existence","abstract":"The paper deals with the question whether representations of verb semantics formulated on the basis of a lexically and syntactically restricted domain (weather forecasts) can apply to other, less restricted textual domains. An analysis of a group of Polish polysemous verbs of motion, existence and appearance inspired by cognitive semantics, especially the metaphor theory, is presented, and the usefulness of the conceptual representations of the Polish motion\/appearance\/existence verbs for automatic translation of texts belonging to less restricted domains is evaluated and discussed.","year":2000,"title_abstract":"Understanding politics by studying weather: a cognitive approach to representation of {P}olish verbs of motion, appearance, and existence The paper deals with the question whether representations of verb semantics formulated on the basis of a lexically and syntactically restricted domain (weather forecasts) can apply to other, less restricted textual domains. An analysis of a group of Polish polysemous verbs of motion, existence and appearance inspired by cognitive semantics, especially the metaphor theory, is presented, and the usefulness of the conceptual representations of the Polish motion\/appearance\/existence verbs for automatic translation of texts belonging to less restricted domains is evaluated and discussed.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2025505006,"Goal":"Climate Action","Task":["Understanding politics","representation of {P}olish verbs of motion","representations of verb semantics","automatic translation of texts"],"Method":["cognitive approach","cognitive semantics","metaphor theory","conceptual representations"]},{"ID":"finzel-etal-2021-everyday","title":"Everyday Living Artificial Intelligence Hub","abstract":"We present the Everyday Living Artificial Intelligence (AI) Hub, a novel proof-of-concept framework for enhancing human health and wellbeing via a combination of tailored wear-able and Conversational Agent (CA) solutions for non-invasive monitoring of physiological signals, assessment of behaviors through unobtrusive wearable devices, and the provision of personalized interventions to reduce stress and anxiety. We utilize recent advancements and industry standards in the Internet of Things (IoT)and AI technologies to develop this proof-of-concept framework.","year":2021,"title_abstract":"Everyday Living Artificial Intelligence Hub We present the Everyday Living Artificial Intelligence (AI) Hub, a novel proof-of-concept framework for enhancing human health and wellbeing via a combination of tailored wear-able and Conversational Agent (CA) solutions for non-invasive monitoring of physiological signals, assessment of behaviors through unobtrusive wearable devices, and the provision of personalized interventions to reduce stress and anxiety. We utilize recent advancements and industry standards in the Internet of Things (IoT)and AI technologies to develop this proof-of-concept framework.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.202539593,"Goal":"Good Health and Well-Being","Task":["Everyday Living Artificial Intelligence Hub","human health and wellbeing","non - invasive monitoring of physiological signals","assessment of behaviors","personalized interventions"],"Method":["Everyday Living Artificial Intelligence","proof - of - concept framework","tailored wear - able and Conversational Agent (CA) solutions","unobtrusive wearable devices","AI technologies","proof - of - concept framework"]},{"ID":"delgado-etal-2020-safe","title":"The {SAFE}-{T} Corpus: A New Resource for Simulated Public Safety Communications","abstract":"We introduce a new resource, the SAFE-T (Speech Analysis for Emergency Response Technology) Corpus, designed to simulate first-responder communications by inducing high vocal effort and urgent speech with situational background noise in a game-based collection protocol. Linguistic Data Consortium developed the SAFE-T Corpus to support the NIST (National Institute of Standards and Technology) OpenSAT (Speech Analytic Technologies) evaluation series, whose goal is to advance speech analytic technologies including automatic speech recognition, speech activity detection and keyword search in multiple domains including simulated public safety communications data. The corpus comprises over 300 hours of audio from 115 unique speakers engaged in a collaborative problem-solving activity representative of public safety communications in terms of speech content, noise types and noise levels. Portions of the corpus have been used in the OpenSAT 2019 evaluation and the full corpus will be published in the LDC catalog. We describe the design and implementation of the SAFE-T Corpus collection, discuss the approach of capturing spontaneous speech from study participants through game-based speech collection, and report on the collection results including several challenges associated with the collection.","year":2020,"title_abstract":"The {SAFE}-{T} Corpus: A New Resource for Simulated Public Safety Communications We introduce a new resource, the SAFE-T (Speech Analysis for Emergency Response Technology) Corpus, designed to simulate first-responder communications by inducing high vocal effort and urgent speech with situational background noise in a game-based collection protocol. Linguistic Data Consortium developed the SAFE-T Corpus to support the NIST (National Institute of Standards and Technology) OpenSAT (Speech Analytic Technologies) evaluation series, whose goal is to advance speech analytic technologies including automatic speech recognition, speech activity detection and keyword search in multiple domains including simulated public safety communications data. The corpus comprises over 300 hours of audio from 115 unique speakers engaged in a collaborative problem-solving activity representative of public safety communications in terms of speech content, noise types and noise levels. Portions of the corpus have been used in the OpenSAT 2019 evaluation and the full corpus will be published in the LDC catalog. We describe the design and implementation of the SAFE-T Corpus collection, discuss the approach of capturing spontaneous speech from study participants through game-based speech collection, and report on the collection results including several challenges associated with the collection.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2025082111,"Goal":"Sustainable Cities and Communities","Task":["Simulated Public Safety Communications","first - responder communications","automatic speech recognition","speech activity detection","keyword search","collaborative problem - solving activity","public safety communications"],"Method":["speech analytic technologies"]},{"ID":"yan-nakashole-2021-grounded","title":"A Grounded Well-being Conversational Agent with Multiple Interaction Modes: Preliminary Results","abstract":"Technologies for enhancing well-being, healthcare vigilance and monitoring are on the rise. However, despite patient interest, such technologies suffer from low adoption. One hypothesis for this limited adoption is loss of human interaction that is central to doctor-patient encounters. In this paper we seek to address this limitation via a conversational agent that adopts one aspect of in-person doctor-patient interactions: A human avatar to facilitate medical grounded question answering. This is akin to the in-person scenario where the doctor may point to the human body or the patient may point to their own body to express their conditions. Additionally, our agent has multiple interaction modes, that may give more options for the patient to use the agent, not just for medical question answering, but also to engage in conversations about general topics and current events. Both the avatar, and the multiple interaction modes could help improve adherence. We present a high level overview of the design of our agent, Marie Bot Wellbeing. We also report implementation details of our early prototype , and present preliminary results.","year":2021,"title_abstract":"A Grounded Well-being Conversational Agent with Multiple Interaction Modes: Preliminary Results Technologies for enhancing well-being, healthcare vigilance and monitoring are on the rise. However, despite patient interest, such technologies suffer from low adoption. One hypothesis for this limited adoption is loss of human interaction that is central to doctor-patient encounters. In this paper we seek to address this limitation via a conversational agent that adopts one aspect of in-person doctor-patient interactions: A human avatar to facilitate medical grounded question answering. This is akin to the in-person scenario where the doctor may point to the human body or the patient may point to their own body to express their conditions. Additionally, our agent has multiple interaction modes, that may give more options for the patient to use the agent, not just for medical question answering, but also to engage in conversations about general topics and current events. Both the avatar, and the multiple interaction modes could help improve adherence. We present a high level overview of the design of our agent, Marie Bot Wellbeing. We also report implementation details of our early prototype , and present preliminary results.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.2022710443,"Goal":"Good Health and Well-Being","Task":["well - being","healthcare vigilance","monitoring","doctor - patient encounters","in - person doctor - patient interactions","medical grounded question answering","in - person scenario","medical question answering"],"Method":["Grounded Well - being Conversational Agent","conversational agent","Bot"]},{"ID":"balahur-tanev-2016-detecting","title":"Detecting Implicit Expressions of Affect from Text using Semantic Knowledge on Common Concept Properties","abstract":"Emotions are an important part of the human experience. They are responsible for the adaptation and integration in the environment, offering, most of the time together with the cognitive system, the appropriate responses to stimuli in the environment. As such, they are an important component in decision-making processes. In today{'}s society, the avalanche of stimuli present in the environment (physical or virtual) makes people more prone to respond to stronger affective stimuli (i.e., those that are related to their basic needs and motivations \u2015 survival, food, shelter, etc.). In media reporting, this is translated in the use of arguments (factual data) that are known to trigger specific (strong, affective) behavioural reactions from the readers. This paper describes initial efforts to detect such arguments from text, based on the properties of concepts. The final system able to retrieve and label this type of data from the news in traditional and social platforms is intended to be integrated Europe Media Monitor family of applications to detect texts that trigger certain (especially negative) reactions from the public, with consequences on citizen safety and security.","year":2016,"title_abstract":"Detecting Implicit Expressions of Affect from Text using Semantic Knowledge on Common Concept Properties Emotions are an important part of the human experience. They are responsible for the adaptation and integration in the environment, offering, most of the time together with the cognitive system, the appropriate responses to stimuli in the environment. As such, they are an important component in decision-making processes. In today{'}s society, the avalanche of stimuli present in the environment (physical or virtual) makes people more prone to respond to stronger affective stimuli (i.e., those that are related to their basic needs and motivations \u2015 survival, food, shelter, etc.). In media reporting, this is translated in the use of arguments (factual data) that are known to trigger specific (strong, affective) behavioural reactions from the readers. This paper describes initial efforts to detect such arguments from text, based on the properties of concepts. The final system able to retrieve and label this type of data from the news in traditional and social platforms is intended to be integrated Europe Media Monitor family of applications to detect texts that trigger certain (especially negative) reactions from the public, with consequences on citizen safety and security.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2020681947,"Goal":"Climate Action","Task":["Detecting Implicit Expressions of Affect","decision - making processes","media reporting","citizen safety","security"],"Method":["cognitive system","Media Monitor family of applications"]},{"ID":"handschke-etal-2018-corpus","title":"A Corpus of Corporate Annual and Social Responsibility Reports: 280 Million Tokens of Balanced Organizational Writing","abstract":"We introduce JOCo, a novel text corpus for NLP analytics in the field of economics, business and management. This corpus is composed of corporate annual and social responsibility reports of the top 30 US, UK and German companies in the major (DJIA, FTSE 100, DAX), middle-sized (S{\\&}P 500, FTSE 250, MDAX) and technology (NASDAQ, FTSE AIM 100, TECDAX) stock indices, respectively. Altogether, this adds up to 5,000 reports from 270 companies headquartered in three of the world{'}s most important economies. The corpus spans a time frame from 2000 up to 2015 and contains, in total, 282M tokens. We also feature JOCo in a small-scale experiment to demonstrate its potential for NLP-fueled studies in economics, business and management research.","year":2018,"title_abstract":"A Corpus of Corporate Annual and Social Responsibility Reports: 280 Million Tokens of Balanced Organizational Writing We introduce JOCo, a novel text corpus for NLP analytics in the field of economics, business and management. This corpus is composed of corporate annual and social responsibility reports of the top 30 US, UK and German companies in the major (DJIA, FTSE 100, DAX), middle-sized (S{\\&}P 500, FTSE 250, MDAX) and technology (NASDAQ, FTSE AIM 100, TECDAX) stock indices, respectively. Altogether, this adds up to 5,000 reports from 270 companies headquartered in three of the world{'}s most important economies. The corpus spans a time frame from 2000 up to 2015 and contains, in total, 282M tokens. We also feature JOCo in a small-scale experiment to demonstrate its potential for NLP-fueled studies in economics, business and management research.","social_need":"Decent Work and Economic Growth Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all","cosine_similarity":0.2020424455,"Goal":"Decent Work and Economic Growth","Task":["NLP analytics","economics","business","management","economics","business","management"],"Method":["JOCo","JOCo","NLP"]},{"ID":"kiritchenko-mohammad-2018-examining","title":"Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems","abstract":"Automatic machine learning systems can inadvertently accentuate and perpetuate inappropriate human biases. Past work on examining inappropriate biases has largely focused on just individual systems. Further, there is no benchmark dataset for examining inappropriate biases in systems. Here for the first time, we present the Equity Evaluation Corpus (EEC), which consists of 8,640 English sentences carefully chosen to tease out biases towards certain races and genders. We use the dataset to examine 219 automatic sentiment analysis systems that took part in a recent shared task, SemEval-2018 Task 1 {`}Affect in Tweets{'}. We find that several of the systems show statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for one race or one gender. We make the EEC freely available.","year":2018,"title_abstract":"Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems Automatic machine learning systems can inadvertently accentuate and perpetuate inappropriate human biases. Past work on examining inappropriate biases has largely focused on just individual systems. Further, there is no benchmark dataset for examining inappropriate biases in systems. Here for the first time, we present the Equity Evaluation Corpus (EEC), which consists of 8,640 English sentences carefully chosen to tease out biases towards certain races and genders. We use the dataset to examine 219 automatic sentiment analysis systems that took part in a recent shared task, SemEval-2018 Task 1 {`}Affect in Tweets{'}. We find that several of the systems show statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for one race or one gender. We make the EEC freely available.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2019196451,"Goal":"Gender Equality","Task":["Examining Gender and Race Bias","inappropriate biases in systems","automatic sentiment analysis systems"],"Method":["Sentiment Analysis Systems","Automatic machine learning systems"]},{"ID":"sundar-ram-lalitha-devi-2020-handling","title":"Handling Noun-Noun Coreference in {T}amil","abstract":"Natural language understanding by automatic tools is the vital requirement for document processing tools. To achieve it, automatic system has to understand the coherence in the text. Co-reference chains bring coherence to the text. The commonly occurring reference markers which bring cohesiveness are Pronominal, Reflexives, Reciprocals, Distributives, One-anaphors, Noun{--}noun reference. Here in this paper, we deal with noun-noun reference in Tamil. We present the methodology to resolve these noun-noun anaphors and also present the challenges in handling the noun-noun anaphoric relations in Tamil.","year":2020,"title_abstract":"Handling Noun-Noun Coreference in {T}amil Natural language understanding by automatic tools is the vital requirement for document processing tools. To achieve it, automatic system has to understand the coherence in the text. Co-reference chains bring coherence to the text. The commonly occurring reference markers which bring cohesiveness are Pronominal, Reflexives, Reciprocals, Distributives, One-anaphors, Noun{--}noun reference. Here in this paper, we deal with noun-noun reference in Tamil. We present the methodology to resolve these noun-noun anaphors and also present the challenges in handling the noun-noun anaphoric relations in Tamil.","social_need":"Clean Water and Sanitation Ensure availability and sustainable management of water and sanitation for all","cosine_similarity":0.2018969655,"Goal":"Clean Water and Sanitation","Task":["Noun - Noun Coreference","Natural language understanding","document processing tools","noun - noun reference","noun - noun anaphoric relations"],"Method":["automatic tools","automatic system"]},{"ID":"joshi-etal-2019-unsung","title":"Unsung Challenges of Building and Deploying Language Technologies for Low Resource Language Communities","abstract":"In this paper, we examine and analyze the challenges associated with developing and introducing language technologies to low-resource language communities. While doing so we bring to light the successes and failures of past work in this area, challenges being faced in doing so, and what have they achieved. Throughout this paper, we take a problem-facing approach and describe essential factors which the success of such technologies hinges upon. We present the various aspects in a manner which clarify and lay out the different tasks involved, which can aid organizations looking to make an impact in this area. We take the example of Gondi, an extremely-low resource Indian language, to reinforce and complement our discussion.","year":2019,"title_abstract":"Unsung Challenges of Building and Deploying Language Technologies for Low Resource Language Communities In this paper, we examine and analyze the challenges associated with developing and introducing language technologies to low-resource language communities. While doing so we bring to light the successes and failures of past work in this area, challenges being faced in doing so, and what have they achieved. Throughout this paper, we take a problem-facing approach and describe essential factors which the success of such technologies hinges upon. We present the various aspects in a manner which clarify and lay out the different tasks involved, which can aid organizations looking to make an impact in this area. We take the example of Gondi, an extremely-low resource Indian language, to reinforce and complement our discussion.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2018943429,"Goal":"Sustainable Cities and Communities","Task":["Low Resource Language Communities"],"Method":["Language Technologies","language technologies","problem - facing approach"]},{"ID":"lepori-2020-unequal","title":"Unequal Representations: Analyzing Intersectional Biases in Word Embeddings Using Representational Similarity Analysis","abstract":"We present a new approach for detecting human-like social biases in word embeddings using representational similarity analysis. Specifically, we probe contextualized and non-contextualized embeddings for evidence of intersectional biases against Black women. We show that these embeddings represent Black women as simultaneously less feminine than White women, and less Black than Black men. This finding aligns with intersectionality theory, which argues that multiple identity categories (such as race or sex) layer on top of each other in order to create unique modes of discrimination that are not shared by any individual category.","year":2020,"title_abstract":"Unequal Representations: Analyzing Intersectional Biases in Word Embeddings Using Representational Similarity Analysis We present a new approach for detecting human-like social biases in word embeddings using representational similarity analysis. Specifically, we probe contextualized and non-contextualized embeddings for evidence of intersectional biases against Black women. We show that these embeddings represent Black women as simultaneously less feminine than White women, and less Black than Black men. This finding aligns with intersectionality theory, which argues that multiple identity categories (such as race or sex) layer on top of each other in order to create unique modes of discrimination that are not shared by any individual category.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2018563151,"Goal":"Gender Equality","Task":["Analyzing Intersectional Biases","Word Embeddings","detecting human - like social biases","word embeddings"],"Method":["Unequal Representations","Representational Similarity Analysis","representational similarity analysis","contextualized and non - contextualized embeddings","intersectionality theory"]},{"ID":"wolfarth-2017-aligner","title":"Aligner production et normalisation : une premi{\\`e}re approche pour l{'}{\\'e}tude d{'}{\\'e}crits scolaires (To align production and normalization : first approach to study school learner{'}s writings)","abstract":"L{'}{\\'e}mergence des corpus scolaires et la volont{\\'e} d{'}outiller ces corpus sp{\\'e}cifiques font apparaitre de nouvelles probl{\\'e}matiques de recherche pour le traitement automatique des langues (TAL). Nous exposons ici une recherche qui vise le traitement de productions d{'}apprenants en d{\\'e}but d{'}apprentissage de l{'}{\\'e}criture, en vue d{'}une annotation et d{'}une exploitation ult{\\'e}rieure. {\\`A} cette fin, nous proposons d{'}envisager cette {\\'e}tape comme une t{\\^a}che d{'}alignement entre la production de l{'}apprenant et une normalisation produite manuellement. Ce proc{\\'e}d{\\'e} permet d{'}augmenter significativement les scores d{'}identification des formes et lemmes produits et am{\\'e}liore les perspectives d{'}annotation.","year":2017,"title_abstract":"Aligner production et normalisation : une premi{\\`e}re approche pour l{'}{\\'e}tude d{'}{\\'e}crits scolaires (To align production and normalization : first approach to study school learner{'}s writings) L{'}{\\'e}mergence des corpus scolaires et la volont{\\'e} d{'}outiller ces corpus sp{\\'e}cifiques font apparaitre de nouvelles probl{\\'e}matiques de recherche pour le traitement automatique des langues (TAL). Nous exposons ici une recherche qui vise le traitement de productions d{'}apprenants en d{\\'e}but d{'}apprentissage de l{'}{\\'e}criture, en vue d{'}une annotation et d{'}une exploitation ult{\\'e}rieure. {\\`A} cette fin, nous proposons d{'}envisager cette {\\'e}tape comme une t{\\^a}che d{'}alignement entre la production de l{'}apprenant et une normalisation produite manuellement. Ce proc{\\'e}d{\\'e} permet d{'}augmenter significativement les scores d{'}identification des formes et lemmes produits et am{\\'e}liore les perspectives d{'}annotation.","social_need":"Responsible Consumption and Production Ensure sustainable consumption and production patterns","cosine_similarity":0.2016727328,"Goal":"Responsible Consumption and Production","Task":["Aligner production","align production","normalization"],"Method":["normalisation","normalisation"]},{"ID":"tziafas-etal-2021-multilingual","title":"A Multilingual Approach to Identify and Classify Exceptional Measures against {COVID}-19","abstract":"The COVID-19 pandemic has witnessed the implementations of exceptional measures by governments across the world to counteract its impact. This work presents the initial results of an on-going project, EXCEPTIUS, aiming to automatically identify, classify and com- pare exceptional measures against COVID-19 across 32 countries in Europe. To this goal, we created a corpus of legal documents with sentence-level annotations of eight different classes of exceptional measures that are im- plemented across these countries. We evalu- ated multiple multi-label classifiers on a manu- ally annotated corpus at sentence level. The XLM-RoBERTa model achieves highest per- formance on this multilingual multi-label clas- sification task, with a macro-average F1 score of 59.8{\\%}.","year":2021,"title_abstract":"A Multilingual Approach to Identify and Classify Exceptional Measures against {COVID}-19 The COVID-19 pandemic has witnessed the implementations of exceptional measures by governments across the world to counteract its impact. This work presents the initial results of an on-going project, EXCEPTIUS, aiming to automatically identify, classify and com- pare exceptional measures against COVID-19 across 32 countries in Europe. To this goal, we created a corpus of legal documents with sentence-level annotations of eight different classes of exceptional measures that are im- plemented across these countries. We evalu- ated multiple multi-label classifiers on a manu- ally annotated corpus at sentence level. The XLM-RoBERTa model achieves highest per- formance on this multilingual multi-label clas- sification task, with a macro-average F1 score of 59.8{\\%}.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2016623467,"Goal":"Climate Action","Task":["Classify Exceptional Measures","multilingual multi - label clas - sification task"],"Method":["Multilingual Approach","multi - label classifiers"]},{"ID":"vijayakumar-etal-2022-ssn","title":"{SSN}{\\_}{ARMM}@ {LT}-{EDI} -{ACL}2022: Hope Speech Detection for Equality, Diversity, and Inclusion Using {ALBERT} model","abstract":"In recent years social media has become one of the major forums for expressing human views and emotions. With the help of smartphones and high-speed internet, anyone can express their views on Social media. However, this can also lead to the spread of hatred and violence in society. Therefore it is necessary to build a method to find and support helpful social media content. In this paper, we studied Natural Language Processing approach for detecting Hope speech in a given sentence. The task was to classify the sentences into {`}Hope speech{'} and {`}Non-hope speech{'}. The dataset was provided by LT-EDI organizers with text from Youtube comments. Based on the task description, we developed a system using the pre-trained language model BERT to complete this task. Our model achieved 1st rank in the Kannada language with a weighted average F1 score of 0.750, 2nd rank in the Malayalam language with a weighted average F1 score of 0.740, 3rd rank in the Tamil language with a weighted average F1 score of 0.390 and 6th rank in the English language with a weighted average F1 score of 0.880.","year":2022,"title_abstract":"{SSN}{\\_}{ARMM}@ {LT}-{EDI} -{ACL}2022: Hope Speech Detection for Equality, Diversity, and Inclusion Using {ALBERT} model In recent years social media has become one of the major forums for expressing human views and emotions. With the help of smartphones and high-speed internet, anyone can express their views on Social media. However, this can also lead to the spread of hatred and violence in society. Therefore it is necessary to build a method to find and support helpful social media content. In this paper, we studied Natural Language Processing approach for detecting Hope speech in a given sentence. The task was to classify the sentences into {`}Hope speech{'} and {`}Non-hope speech{'}. The dataset was provided by LT-EDI organizers with text from Youtube comments. Based on the task description, we developed a system using the pre-trained language model BERT to complete this task. Our model achieved 1st rank in the Kannada language with a weighted average F1 score of 0.750, 2nd rank in the Malayalam language with a weighted average F1 score of 0.740, 3rd rank in the Tamil language with a weighted average F1 score of 0.390 and 6th rank in the English language with a weighted average F1 score of 0.880.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.201531902,"Goal":"Gender Equality","Task":["Hope Speech Detection","Equality","detecting Hope speech"],"Method":["{ALBERT} model","Natural Language Processing approach","language model"]},{"ID":"johannssen-biemann-2020-social","title":"Social Media Unrest Prediction during the {COVID}-19 Pandemic: Neural Implicit Motive Pattern Recognition as Psychometric Signs of Severe Crises","abstract":"The COVID-19 pandemic has caused international social tension and unrest. Besides the crisis itself, there are growing signs of rising conflict potential of societies around the world. Indicators of global mood changes are hard to detect and direct questionnaires suffer from social desirability biases. However, so-called implicit methods can reveal humans intrinsic desires from e.g. social media texts. We present psychologically validated social unrest predictors and replicate scalable and automated predictions, setting a new state of the art on a recent German shared task dataset. We employ this model to investigate a change of language towards social unrest during the COVID-19 pandemic by comparing established psychological predictors on samples of tweets from spring 2019 with spring 2020. The results show a significant increase of the conflict indicating psychometrics. With this work, we demonstrate the applicability of automated NLP-based approaches to quantitative psychological research.","year":2020,"title_abstract":"Social Media Unrest Prediction during the {COVID}-19 Pandemic: Neural Implicit Motive Pattern Recognition as Psychometric Signs of Severe Crises The COVID-19 pandemic has caused international social tension and unrest. Besides the crisis itself, there are growing signs of rising conflict potential of societies around the world. Indicators of global mood changes are hard to detect and direct questionnaires suffer from social desirability biases. However, so-called implicit methods can reveal humans intrinsic desires from e.g. social media texts. We present psychologically validated social unrest predictors and replicate scalable and automated predictions, setting a new state of the art on a recent German shared task dataset. We employ this model to investigate a change of language towards social unrest during the COVID-19 pandemic by comparing established psychological predictors on samples of tweets from spring 2019 with spring 2020. The results show a significant increase of the conflict indicating psychometrics. With this work, we demonstrate the applicability of automated NLP-based approaches to quantitative psychological research.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2015067339,"Goal":"Climate Action","Task":["Social Media Unrest Prediction","Psychometric Signs of Severe Crises","automated predictions","quantitative psychological research"],"Method":["Neural Implicit Motive Pattern Recognition","implicit methods","social unrest predictors","psychological predictors","automated NLP - based approaches"]},{"ID":"zerva-etal-2021-ist","title":"{IST}-Unbabel 2021 Submission for the Quality Estimation Shared Task","abstract":"We present the joint contribution of IST and Unbabel to the WMT 2021 Shared Task on Quality Estimation. Our team participated on two tasks: Direct Assessment and Post-Editing Effort, encompassing a total of 35 submissions. For all submissions, our efforts focused on training multilingual models on top of OpenKiwi predictor-estimator architecture, using pre-trained multilingual encoders combined with adapters. We further experiment with and uncertainty-related objectives and features as well as training on out-of-domain direct assessment data.","year":2021,"title_abstract":"{IST}-Unbabel 2021 Submission for the Quality Estimation Shared Task We present the joint contribution of IST and Unbabel to the WMT 2021 Shared Task on Quality Estimation. Our team participated on two tasks: Direct Assessment and Post-Editing Effort, encompassing a total of 35 submissions. For all submissions, our efforts focused on training multilingual models on top of OpenKiwi predictor-estimator architecture, using pre-trained multilingual encoders combined with adapters. We further experiment with and uncertainty-related objectives and features as well as training on out-of-domain direct assessment data.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.201472193,"Goal":"Quality Education","Task":["Quality Estimation Shared Task","Quality Estimation","Direct Assessment","Post - Editing Effort"],"Method":["IST","Unbabel","multilingual models","OpenKiwi predictor - estimator architecture","multilingual encoders","adapters"]},{"ID":"sarkar-etal-2020-social","title":"Social Media Attributions in the Context of Water Crisis","abstract":"Attribution of natural disasters\/collective misfortune is a widely-studied political science problem. However, such studies typically rely on surveys, or expert opinions, or external signals such as voting outcomes. In this paper, we explore the viability of using unstructured, noisy social media data to complement traditional surveys through automatically extracting attribution factors. We present a novel prediction task of \\textit{attribution tie detection} of identifying the factors (e.g., poor city planning, exploding population etc.) held responsible for the crisis in a social media document. We focus on the 2019 Chennai water crisis that rapidly escalated into a discussion topic with global importance following alarming water-crisis statistics. On a challenging data set constructed from YouTube comments (72,098 comments posted by 43,859 users on 623 videos relevant to the crisis), we present a neural baseline to identify attribution ties that achieves a reasonable performance (accuracy: 87.34{\\%} on attribution detection and 81.37{\\%} on attribution resolution). We release the first annotated data set of 2,500 comments in this important domain.","year":2020,"title_abstract":"Social Media Attributions in the Context of Water Crisis Attribution of natural disasters\/collective misfortune is a widely-studied political science problem. However, such studies typically rely on surveys, or expert opinions, or external signals such as voting outcomes. In this paper, we explore the viability of using unstructured, noisy social media data to complement traditional surveys through automatically extracting attribution factors. We present a novel prediction task of \\textit{attribution tie detection} of identifying the factors (e.g., poor city planning, exploding population etc.) held responsible for the crisis in a social media document. We focus on the 2019 Chennai water crisis that rapidly escalated into a discussion topic with global importance following alarming water-crisis statistics. On a challenging data set constructed from YouTube comments (72,098 comments posted by 43,859 users on 623 videos relevant to the crisis), we present a neural baseline to identify attribution ties that achieves a reasonable performance (accuracy: 87.34{\\%} on attribution detection and 81.37{\\%} on attribution resolution). We release the first annotated data set of 2,500 comments in this important domain.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.2013043761,"Goal":"Climate Action","Task":["Social Media Attributions","Water Crisis","Attribution of natural disasters\/collective misfortune","political science problem","automatically extracting attribution factors","prediction task","\\textit{attribution tie detection}","attribution ties","attribution detection","attribution resolution)"],"Method":["neural baseline"]},{"ID":"zhao-etal-2017-men","title":"Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints","abstract":"Language is increasingly being used to de-fine rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33{\\%} more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68{\\%} at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5{\\%} and 40.5{\\%} for multilabel classification and visual semantic role labeling, respectively\u3002","year":2017,"title_abstract":"Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints Language is increasingly being used to de-fine rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33{\\%} more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68{\\%} at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5{\\%} and 40.5{\\%} for multilabel classification and visual semantic role labeling, respectively\u3002","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2010675371,"Goal":"Gender Equality","Task":["Gender Bias Amplification","visual recognition problems","multilabel object classification","visual semantic role labeling","activity cooking","collective inference","recognition task","multilabel classification","visual semantic role labeling"],"Method":["Structured prediction models","structured prediction models","Lagrangian relaxation"]},{"ID":"li-etal-2016-semi","title":"Semi-supervised Gender Classification with Joint Textual and Social Modeling","abstract":"In gender classification, labeled data is often limited while unlabeled data is ample. This motivates semi-supervised learning for gender classification to improve the performance by exploring the knowledge in both labeled and unlabeled data. In this paper, we propose a semi-supervised approach to gender classification by leveraging textual features and a specific kind of indirect links among the users which we call {``}same-interest{''} links. Specifically, we propose a factor graph, namely Textual and Social Factor Graph (TSFG), to model both the textual and the {``}same-interest{''} link information. Empirical studies demonstrate the effectiveness of the proposed approach to semi-supervised gender classification.","year":2016,"title_abstract":"Semi-supervised Gender Classification with Joint Textual and Social Modeling In gender classification, labeled data is often limited while unlabeled data is ample. This motivates semi-supervised learning for gender classification to improve the performance by exploring the knowledge in both labeled and unlabeled data. In this paper, we propose a semi-supervised approach to gender classification by leveraging textual features and a specific kind of indirect links among the users which we call {``}same-interest{''} links. Specifically, we propose a factor graph, namely Textual and Social Factor Graph (TSFG), to model both the textual and the {``}same-interest{''} link information. Empirical studies demonstrate the effectiveness of the proposed approach to semi-supervised gender classification.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2009449452,"Goal":"Gender Equality","Task":["Semi - supervised Gender Classification","gender classification","semi - supervised learning","gender classification","gender classification","semi - supervised gender classification"],"Method":["Joint Textual and Social Modeling","semi - supervised approach","factor graph","Textual and Social Factor Graph"]},{"ID":"kiomourtzis-etal-2014-nomad","title":"{NOMAD}: Linguistic Resources and Tools Aimed at Policy Formulation and Validation","abstract":"The NOMAD project (Policy Formulation and Validation through non Moderated Crowd-sourcing) is a project that supports policy making, by providing rich, actionable information related to how citizens perceive different policies. NOMAD automatically analyzes citizen contributions to the informal web (e.g. forums, social networks, blogs, newsgroups and wikis) using a variety of tools. These tools comprise text retrieval, topic classification, argument detection and sentiment analysis, as well as argument summarization. NOMAD provides decision-makers with a full arsenal of solutions starting from describing a domain and a policy to applying content search and acquisition, categorization and visualization. These solutions work in a collaborative manner in the policy-making arena. NOMAD, thus, embeds editing, analysis and visualization technologies into a concrete framework, applicable in a variety of policy-making and decision support settings In this paper we provide an overview of the linguistic tools and resources of NOMAD.","year":2014,"title_abstract":"{NOMAD}: Linguistic Resources and Tools Aimed at Policy Formulation and Validation The NOMAD project (Policy Formulation and Validation through non Moderated Crowd-sourcing) is a project that supports policy making, by providing rich, actionable information related to how citizens perceive different policies. NOMAD automatically analyzes citizen contributions to the informal web (e.g. forums, social networks, blogs, newsgroups and wikis) using a variety of tools. These tools comprise text retrieval, topic classification, argument detection and sentiment analysis, as well as argument summarization. NOMAD provides decision-makers with a full arsenal of solutions starting from describing a domain and a policy to applying content search and acquisition, categorization and visualization. These solutions work in a collaborative manner in the policy-making arena. NOMAD, thus, embeds editing, analysis and visualization technologies into a concrete framework, applicable in a variety of policy-making and decision support settings In this paper we provide an overview of the linguistic tools and resources of NOMAD.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2009267509,"Goal":"Sustainable Cities and Communities","Task":["Policy Formulation","Validation","NOMAD project","Formulation","Validation","non Moderated Crowd - sourcing)","policy making","text retrieval","topic classification","argument detection","sentiment analysis","argument summarization","content search","acquisition","categorization","visualization","policy - making arena","editing","policy - making","decision support settings"],"Method":["Linguistic Resources","NOMAD","NOMAD","NOMAD","analysis and visualization technologies","linguistic tools","NOMAD"]},{"ID":"heinisch-2020-developing","title":"Developing Language Resources with Citizen Linguistics in {A}ustria {--} A Case Study","abstract":"Language resources are a major ingredient for the advancement of language technologies. Citizen linguistics can help to create language resources and annotate language resources, not only for the improvement of language technologies, such as machine translation but also for the advancement of linguistic research. The (language) resources covered in this article are a corpus related to the Question of the Month project strand, which was initially aimed at co-creation in citizen linguistics and a partially annotated database of pictures of written text in different languages found in the public sphere. The number of participants in these project strands differed significantly. Especially those activities that were related to data collection (and analysis) had a significantly higher number of contributions per participant. This especially held true for the activities with (prize) incentives. Nevertheless, the activities of the Question of the Month could reach a higher number of participants, even after the co-creation approach was no longer followed. In addition, the Question of the Month brought research gaps and new knowledge to light and challenged existing paradigms and practices. These are especially important for the advancement of scholarly research. Citizen linguistics can help gather and analyze linguistic data, including language resources, in a short period of time. Thus, it may help increase the access to and availability of language resources.","year":2020,"title_abstract":"Developing Language Resources with Citizen Linguistics in {A}ustria {--} A Case Study Language resources are a major ingredient for the advancement of language technologies. Citizen linguistics can help to create language resources and annotate language resources, not only for the improvement of language technologies, such as machine translation but also for the advancement of linguistic research. The (language) resources covered in this article are a corpus related to the Question of the Month project strand, which was initially aimed at co-creation in citizen linguistics and a partially annotated database of pictures of written text in different languages found in the public sphere. The number of participants in these project strands differed significantly. Especially those activities that were related to data collection (and analysis) had a significantly higher number of contributions per participant. This especially held true for the activities with (prize) incentives. Nevertheless, the activities of the Question of the Month could reach a higher number of participants, even after the co-creation approach was no longer followed. In addition, the Question of the Month brought research gaps and new knowledge to light and challenged existing paradigms and practices. These are especially important for the advancement of scholarly research. Citizen linguistics can help gather and analyze linguistic data, including language resources, in a short period of time. Thus, it may help increase the access to and availability of language resources.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2008972168,"Goal":"Sustainable Cities and Communities","Task":["Language Resources","Language resources","language technologies","Citizen linguistics","language technologies","machine translation","linguistic research","co - creation","citizen linguistics","data collection","analysis)","scholarly research","Citizen linguistics"],"Method":["Citizen Linguistics","co - creation approach"]},{"ID":"kim-etal-2017-concept","title":"Concept Equalization to Guide Correct Training of Neural Machine Translation","abstract":"Neural machine translation decoders are usually conditional language models to sequentially generate words for target sentences. This approach is limited to find the best word composition and requires help of explicit methods as beam search. To help learning correct compositional mechanisms in NMTs, we propose concept equalization using direct mapping distributed representations of source and target sentences. In a translation experiment from English to French, the concept equalization significantly improved translation quality by 3.00 BLEU points compared to a state-of-the-art NMT model.","year":2017,"title_abstract":"Concept Equalization to Guide Correct Training of Neural Machine Translation Neural machine translation decoders are usually conditional language models to sequentially generate words for target sentences. This approach is limited to find the best word composition and requires help of explicit methods as beam search. To help learning correct compositional mechanisms in NMTs, we propose concept equalization using direct mapping distributed representations of source and target sentences. In a translation experiment from English to French, the concept equalization significantly improved translation quality by 3.00 BLEU points compared to a state-of-the-art NMT model.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2008640468,"Goal":"Gender Equality","Task":["Correct Training","Neural Machine Translation","compositional mechanisms","NMTs","translation"],"Method":["Concept Equalization","Neural machine translation decoders","conditional language models","explicit methods","beam search","concept equalization","direct mapping distributed representations","concept equalization","NMT model"]},{"ID":"tu-etal-2020-improving","title":"Improving Joint Training of Inference Networks and Structured Prediction Energy Networks","abstract":"Deep energy-based models are powerful, but pose challenges for learning and inference (Belanger and McCallum, 2016). Tu and Gimpel (2018) developed an efficient framework for energy-based models by training {``}inference networks{''} to approximate structured inference instead of using gradient descent. However, their alternating optimization approach suffers from instabilities during training, requiring additional loss terms and careful hyperparameter tuning. In this paper, we contribute several strategies to stabilize and improve this joint training of energy functions and inference networks for structured prediction. We design a compound objective to jointly train both cost-augmented and test-time inference networks along with the energy function. We propose joint parameterizations for the inference networks that encourage them to capture complementary functionality during learning. We empirically validate our strategies on two sequence labeling tasks, showing easier paths to strong performance than prior work, as well as further improvements with global energy terms.","year":2020,"title_abstract":"Improving Joint Training of Inference Networks and Structured Prediction Energy Networks Deep energy-based models are powerful, but pose challenges for learning and inference (Belanger and McCallum, 2016). Tu and Gimpel (2018) developed an efficient framework for energy-based models by training {``}inference networks{''} to approximate structured inference instead of using gradient descent. However, their alternating optimization approach suffers from instabilities during training, requiring additional loss terms and careful hyperparameter tuning. In this paper, we contribute several strategies to stabilize and improve this joint training of energy functions and inference networks for structured prediction. We design a compound objective to jointly train both cost-augmented and test-time inference networks along with the energy function. We propose joint parameterizations for the inference networks that encourage them to capture complementary functionality during learning. We empirically validate our strategies on two sequence labeling tasks, showing easier paths to strong performance than prior work, as well as further improvements with global energy terms.","social_need":"Affordable and Clean Energy Ensure access to affordable, reliable, sustainable and modern energy for all","cosine_similarity":0.2008638531,"Goal":"Affordable and Clean Energy","Task":["learning and inference","training","structured prediction","learning","sequence labeling tasks"],"Method":["Joint Training of Inference Networks","Structured Prediction Energy Networks","Deep energy - based models","energy - based models","{``}inference networks{''}","structured inference","gradient descent","alternating optimization approach","hyperparameter tuning","joint training of energy functions","inference networks","compound objective","cost - augmented and test - time inference networks","joint parameterizations","inference networks","global energy terms"]},{"ID":"beckage-etal-2021-context","title":"Context or No Context? A preliminary exploration of human-in-the-loop approach for Incremental Temporal Summarization in meetings","abstract":"Incremental meeting temporal summarization, summarizing relevant information of partial multi-party meeting dialogue, is emerging as the next challenge in summarization research. Here we examine the extent to which human abstractive summaries of the preceding increments (context) can be combined with extractive meeting dialogue to generate abstractive summaries. We find that previous context improves ROUGE scores. Our findings further suggest that contexts begin to outweigh the dialogue. Using keyphrase extraction and semantic role labeling (SRL), we find that SRL captures relevant information without overwhelming the the model architecture. By compressing the previous contexts by {\\textasciitilde}70{\\%}, we achieve better ROUGE scores over our baseline models. Collectively, these results suggest that context matters, as does the way in which context is presented to the model.","year":2021,"title_abstract":"Context or No Context? A preliminary exploration of human-in-the-loop approach for Incremental Temporal Summarization in meetings Incremental meeting temporal summarization, summarizing relevant information of partial multi-party meeting dialogue, is emerging as the next challenge in summarization research. Here we examine the extent to which human abstractive summaries of the preceding increments (context) can be combined with extractive meeting dialogue to generate abstractive summaries. We find that previous context improves ROUGE scores. Our findings further suggest that contexts begin to outweigh the dialogue. Using keyphrase extraction and semantic role labeling (SRL), we find that SRL captures relevant information without overwhelming the the model architecture. By compressing the previous contexts by {\\textasciitilde}70{\\%}, we achieve better ROUGE scores over our baseline models. Collectively, these results suggest that context matters, as does the way in which context is presented to the model.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.2007482052,"Goal":"Partnership for the Goals","Task":["Incremental Temporal Summarization","Incremental meeting temporal summarization","partial multi - party meeting dialogue","summarization research","SRL"],"Method":["human - in - the - loop approach","keyphrase extraction","semantic role labeling","model architecture"]},{"ID":"hunt-etal-2019-community","title":"Community lexical access for an endangered polysynthetic language: An electronic dictionary for {S}t. {L}awrence {I}sland {Y}upik","abstract":"In this paper, we introduce a morphologically-aware electronic dictionary for St. Lawrence Island Yupik, an endangered language of the Bering Strait region. Implemented using HTML, Javascript, and CSS, the dictionary is set in an uncluttered interface and permits users to search in Yupik or in English for Yupik root words and Yupik derivational suffixes. For each matching result, our electronic dictionary presents the user with the corresponding entry from the Badten (2008) Yupik-English paper dictionary. Because Yupik is a polysynthetic language, handling of multimorphemic word forms is critical. If a user searches for an inflected Yupik word form, we perform a morphological analysis and return entries for the root word and for any derivational suffixes present in the word. This electronic dictionary should serve not only as a valuable resource for all students and speakers of Yupik, but also for field linguists working towards documentation and conservation of the language.","year":2019,"title_abstract":"Community lexical access for an endangered polysynthetic language: An electronic dictionary for {S}t. {L}awrence {I}sland {Y}upik In this paper, we introduce a morphologically-aware electronic dictionary for St. Lawrence Island Yupik, an endangered language of the Bering Strait region. Implemented using HTML, Javascript, and CSS, the dictionary is set in an uncluttered interface and permits users to search in Yupik or in English for Yupik root words and Yupik derivational suffixes. For each matching result, our electronic dictionary presents the user with the corresponding entry from the Badten (2008) Yupik-English paper dictionary. Because Yupik is a polysynthetic language, handling of multimorphemic word forms is critical. If a user searches for an inflected Yupik word form, we perform a morphological analysis and return entries for the root word and for any derivational suffixes present in the word. This electronic dictionary should serve not only as a valuable resource for all students and speakers of Yupik, but also for field linguists working towards documentation and conservation of the language.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.2006991655,"Goal":"Life Below Water","Task":["Community lexical access","field linguists","documentation","conservation of the language"],"Method":["HTML","Javascript","CSS","electronic dictionary","Yupik","morphological analysis"]},{"ID":"stambolieva-2017-educational","title":"Educational Content Generation for Business and Administration {FL} Courses with the {NBU} {PLT} Platform","abstract":"The paper presents part of an ongoing project of the Laboratory for Language Technologies of New Bulgarian University {--} {``}An e-Platform for Language Teaching (PLT){''} {--} the development of corpus-based teaching content for Business English courses. The presentation offers information on: 1\/ corpus creation and corpus management with PLT; 2\/ PLT corpus annotation; 3\/ language task generation and the Language Task Bank (LTB); 4\/ content transfer to the NBU Moodle platform, test generation and feedback on student performance.","year":2017,"title_abstract":"Educational Content Generation for Business and Administration {FL} Courses with the {NBU} {PLT} Platform The paper presents part of an ongoing project of the Laboratory for Language Technologies of New Bulgarian University {--} {``}An e-Platform for Language Teaching (PLT){''} {--} the development of corpus-based teaching content for Business English courses. The presentation offers information on: 1\/ corpus creation and corpus management with PLT; 2\/ PLT corpus annotation; 3\/ language task generation and the Language Task Bank (LTB); 4\/ content transfer to the NBU Moodle platform, test generation and feedback on student performance.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.200565353,"Goal":"Quality Education","Task":["Educational Content Generation","Business and Administration {FL} Courses","Language Technologies","Language Teaching","corpus - based teaching content","Business English courses","1\/ corpus creation","corpus management","PLT corpus annotation;","language task generation","Language Task Bank","content transfer","test generation"],"Method":["NBU Moodle platform"]},{"ID":"giorgi-etal-2021-discovering","title":"Discovering Black Lives Matter Events in the {U}nited {S}tates: Shared Task 3, {CASE} 2021","abstract":"Evaluating the state-of-the-art event detection systems on determining spatio-temporal distribution of the events on the ground is performed unfrequently. But, the ability to both (1) extract events {``}in the wild{''} from text and (2) properly evaluate event detection systems has potential to support a wide variety of tasks such as monitoring the activity of socio-political movements, examining media coverage and public support of these movements, and informing policy decisions. Therefore, we study performance of the best event detection systems on detecting Black Lives Matter (BLM) events from tweets and news articles. The murder of George Floyd, an unarmed Black man, at the hands of police officers received global attention throughout the second half of 2020. Protests against police violence emerged worldwide and the BLM movement, which was once mostly regulated to the United States, was now seeing activity globally. This shared task asks participants to identify BLM related events from large unstructured data sources, using systems pretrained to extract socio-political events from text. We evaluate several metrics, accessing each system{'}s ability to identify protest events both temporally and spatially. Results show that identifying daily protest counts is an easier task than classifying spatial and temporal protest trends simultaneously, with maximum performance of 0.745 and 0.210 (Pearson $r$), respectively. Additionally, all baselines and participant systems suffered from low recall, with a maximum recall of 5.08.","year":2021,"title_abstract":"Discovering Black Lives Matter Events in the {U}nited {S}tates: Shared Task 3, {CASE} 2021 Evaluating the state-of-the-art event detection systems on determining spatio-temporal distribution of the events on the ground is performed unfrequently. But, the ability to both (1) extract events {``}in the wild{''} from text and (2) properly evaluate event detection systems has potential to support a wide variety of tasks such as monitoring the activity of socio-political movements, examining media coverage and public support of these movements, and informing policy decisions. Therefore, we study performance of the best event detection systems on detecting Black Lives Matter (BLM) events from tweets and news articles. The murder of George Floyd, an unarmed Black man, at the hands of police officers received global attention throughout the second half of 2020. Protests against police violence emerged worldwide and the BLM movement, which was once mostly regulated to the United States, was now seeing activity globally. This shared task asks participants to identify BLM related events from large unstructured data sources, using systems pretrained to extract socio-political events from text. We evaluate several metrics, accessing each system{'}s ability to identify protest events both temporally and spatially. Results show that identifying daily protest counts is an easier task than classifying spatial and temporal protest trends simultaneously, with maximum performance of 0.745 and 0.210 (Pearson $r$), respectively. Additionally, all baselines and participant systems suffered from low recall, with a maximum recall of 5.08.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.2004848421,"Goal":"Sustainable Cities and Communities","Task":["Discovering Black Lives Matter Events","determining spatio - temporal distribution of the events","activity of socio - political movements","policy decisions","detecting Black Lives Matter","identifying daily protest counts"],"Method":["event detection systems","event detection systems","event detection systems","participant systems"]},{"ID":"chen-eisele-2012-multiun","title":"{M}ulti{UN} v2: {UN} Documents with Multilingual Alignments","abstract":"MultiUN is a multilingual parallel corpus extracted from the official documents of the United Nations. It is available in the six official languages of the UN and a small portion of it is also available in German. This paper presents a major update on the first public version of the corpus released in 2010. This version 2 consists of over 513,091 documents, including more than 9{\\%} of new documents retrieved from the United Nations official document system. We applied several modifications to the corpus preparation method. In this paper, we describe the methods we used for processing the UN documents and aligning the sentences. The most significant improvement compared to the previous release is the newly added multilingual sentence alignment information. The alignment information is encoded together with the text in XML instead of additional files. Our representation of the sentence alignment allows quick construction of aligned texts parallel in arbitrary number of languages, which is essential for building machine translation systems.","year":2012,"title_abstract":"{M}ulti{UN} v2: {UN} Documents with Multilingual Alignments MultiUN is a multilingual parallel corpus extracted from the official documents of the United Nations. It is available in the six official languages of the UN and a small portion of it is also available in German. This paper presents a major update on the first public version of the corpus released in 2010. This version 2 consists of over 513,091 documents, including more than 9{\\%} of new documents retrieved from the United Nations official document system. We applied several modifications to the corpus preparation method. In this paper, we describe the methods we used for processing the UN documents and aligning the sentences. The most significant improvement compared to the previous release is the newly added multilingual sentence alignment information. The alignment information is encoded together with the text in XML instead of additional files. Our representation of the sentence alignment allows quick construction of aligned texts parallel in arbitrary number of languages, which is essential for building machine translation systems.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.2002398372,"Goal":"Peace, Justice and Strong Institutions","Task":["Multilingual Alignments","sentence alignment","machine translation systems"],"Method":["corpus preparation method"]},{"ID":"shoemark-etal-2018-inducing","title":"Inducing a lexicon of sociolinguistic variables from code-mixed text","abstract":"Sociolinguistics is often concerned with how variants of a linguistic item (e.g., \\textit{nothing} vs. \\textit{nothin{'}}) are used by different groups or in different situations. We introduce the task of inducing lexical variables from code-mixed text: that is, identifying equivalence pairs such as (\\textit{football}, \\textit{fitba}) along with their linguistic code (\\textit{football}\u2192British, \\textit{fitba}\u2192Scottish). We adapt a framework for identifying gender-biased word pairs to this new task, and present results on three different pairs of English dialects, using tweets as the code-mixed text. Our system achieves precision of over 70{\\%} for two of these three datasets, and produces useful results even without extensive parameter tuning. Our success in adapting this framework from gender to language variety suggests that it could be used to discover other types of analogous pairs as well.","year":2018,"title_abstract":"Inducing a lexicon of sociolinguistic variables from code-mixed text Sociolinguistics is often concerned with how variants of a linguistic item (e.g., \\textit{nothing} vs. \\textit{nothin{'}}) are used by different groups or in different situations. We introduce the task of inducing lexical variables from code-mixed text: that is, identifying equivalence pairs such as (\\textit{football}, \\textit{fitba}) along with their linguistic code (\\textit{football}\u2192British, \\textit{fitba}\u2192Scottish). We adapt a framework for identifying gender-biased word pairs to this new task, and present results on three different pairs of English dialects, using tweets as the code-mixed text. Our system achieves precision of over 70{\\%} for two of these three datasets, and produces useful results even without extensive parameter tuning. Our success in adapting this framework from gender to language variety suggests that it could be used to discover other types of analogous pairs as well.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.2001743317,"Goal":"Gender Equality","Task":["inducing lexical variables","identifying gender - biased word pairs"],"Method":["parameter tuning"]},{"ID":"rios-etal-2018-predicting","title":"Predicting Psychological Health from Childhood Essays with Convolutional Neural Networks for the {CLP}sych 2018 Shared Task (Team {UKNLP})","abstract":"This paper describes the systems we developed for tasks A and B of the 2018 CLPsych shared task. The first task (task A) focuses on predicting behavioral health scores at age 11 using childhood essays. The second task (task B) asks participants to predict future psychological distress at ages 23, 33, 42, and 50 using the age 11 essays. We propose two convolutional neural network based methods that map each task to a regression problem. Among seven teams we ranked third on task A with disattenuated Pearson correlation (DPC) score of 0.5587. Likewise, we ranked third on task B with an average DPC score of 0.3062.","year":2018,"title_abstract":"Predicting Psychological Health from Childhood Essays with Convolutional Neural Networks for the {CLP}sych 2018 Shared Task (Team {UKNLP}) This paper describes the systems we developed for tasks A and B of the 2018 CLPsych shared task. The first task (task A) focuses on predicting behavioral health scores at age 11 using childhood essays. The second task (task B) asks participants to predict future psychological distress at ages 23, 33, 42, and 50 using the age 11 essays. We propose two convolutional neural network based methods that map each task to a regression problem. Among seven teams we ranked third on task A with disattenuated Pearson correlation (DPC) score of 0.5587. Likewise, we ranked third on task B with an average DPC score of 0.3062.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.2000034153,"Goal":"Good Health and Well-Being","Task":["Predicting Psychological Health","predicting behavioral health scores","regression problem"],"Method":["Convolutional Neural Networks","convolutional neural network based methods"]},{"ID":"gupta-etal-2020-heart","title":"Heart Failure Education of {A}frican {A}merican and {H}ispanic\/{L}atino Patients: Data Collection and Analysis","abstract":"Heart failure is a global epidemic with debilitating effects. People with heart failure need to actively participate in home self-care regimens to maintain good health. However, these regimens are not as effective as they could be and are influenced by a variety of factors. Patients from minority communities like African American (AA) and Hispanic\/Latino (H\/L), often have poor outcomes compared to the average Caucasian population. In this paper, we lay the groundwork to develop an interactive dialogue agent that can assist AA and H\/L patients in a culturally sensitive and linguistically accurate manner with their heart health care needs. This will be achieved by extracting relevant educational concepts from the interactions between health educators and patients. Thus far we have recorded and transcribed 20 such interactions. In this paper, we describe our data collection process, thematic and initiative analysis of the interactions, and outline our future steps.","year":2020,"title_abstract":"Heart Failure Education of {A}frican {A}merican and {H}ispanic\/{L}atino Patients: Data Collection and Analysis Heart failure is a global epidemic with debilitating effects. People with heart failure need to actively participate in home self-care regimens to maintain good health. However, these regimens are not as effective as they could be and are influenced by a variety of factors. Patients from minority communities like African American (AA) and Hispanic\/Latino (H\/L), often have poor outcomes compared to the average Caucasian population. In this paper, we lay the groundwork to develop an interactive dialogue agent that can assist AA and H\/L patients in a culturally sensitive and linguistically accurate manner with their heart health care needs. This will be achieved by extracting relevant educational concepts from the interactions between health educators and patients. Thus far we have recorded and transcribed 20 such interactions. In this paper, we describe our data collection process, thematic and initiative analysis of the interactions, and outline our future steps.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.1998056173,"Goal":"Good Health and Well-Being","Task":["Heart Failure Education","Heart failure","home self - care regimens","thematic and initiative analysis"],"Method":["interactive dialogue agent"]},{"ID":"kruspe-etal-2020-cross","title":"Cross-language sentiment analysis of {European} {Twitter} messages during the {COVID-19} pandemic","abstract":"In this paper, we analyze Twitter messages (tweets) collected during the first months of the COVID-19 pandemic in Europe with regard to their sentiment. This is implemented with a neural network for sentiment analysis using multilingual sentence embeddings. We separate the results by country of origin, and correlate their temporal development with events in those countries. This allows us to study the effect of the situation on people{'}s moods. We see, for example, that lockdown announcements correlate with a deterioration of mood in almost all surveyed countries, which recovers within a short time span.","year":2020,"title_abstract":"Cross-language sentiment analysis of {European} {Twitter} messages during the {COVID-19} pandemic In this paper, we analyze Twitter messages (tweets) collected during the first months of the COVID-19 pandemic in Europe with regard to their sentiment. This is implemented with a neural network for sentiment analysis using multilingual sentence embeddings. We separate the results by country of origin, and correlate their temporal development with events in those countries. This allows us to study the effect of the situation on people{'}s moods. We see, for example, that lockdown announcements correlate with a deterioration of mood in almost all surveyed countries, which recovers within a short time span.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1996121109,"Goal":"Reduced Inequalities","Task":["Cross - language sentiment analysis","COVID - 19","sentiment analysis","multilingual sentence embeddings"],"Method":["neural network"]},{"ID":"uyangodage-etal-2021-transformers","title":"Transformers to Fight the {COVID}-19 Infodemic","abstract":"The massive spread of false information on social media has become a global risk especially in a global pandemic situation like COVID-19. False information detection has thus become a surging research topic in recent months. NLP4IF-2021 shared task on fighting the COVID-19 infodemic has been organised to strengthen the research in false information detection where the participants are asked to predict seven different binary labels regarding false information in a tweet. The shared task has been organised in three languages; Arabic, Bulgarian and English. In this paper, we present our approach to tackle the task objective using transformers. Overall, our approach achieves a 0.707 mean F1 score in Arabic, 0.578 mean F1 score in Bulgarian and 0.864 mean F1 score in English ranking 4$^{th}$ place in all the languages.","year":2021,"title_abstract":"Transformers to Fight the {COVID}-19 Infodemic The massive spread of false information on social media has become a global risk especially in a global pandemic situation like COVID-19. False information detection has thus become a surging research topic in recent months. NLP4IF-2021 shared task on fighting the COVID-19 infodemic has been organised to strengthen the research in false information detection where the participants are asked to predict seven different binary labels regarding false information in a tweet. The shared task has been organised in three languages; Arabic, Bulgarian and English. In this paper, we present our approach to tackle the task objective using transformers. Overall, our approach achieves a 0.707 mean F1 score in Arabic, 0.578 mean F1 score in Bulgarian and 0.864 mean F1 score in English ranking 4$^{th}$ place in all the languages.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1993630826,"Goal":"Climate Action","Task":["False information detection","NLP4IF - 2021 shared task","false information detection"],"Method":["Transformers","transformers"]},{"ID":"fraser-etal-2021-understanding","title":"Understanding and Countering Stereotypes: A Computational Approach to the Stereotype Content Model","abstract":"Stereotypical language expresses widely-held beliefs about different social categories. Many stereotypes are overtly negative, while others may appear positive on the surface, but still lead to negative consequences. In this work, we present a computational approach to interpreting stereotypes in text through the Stereotype Content Model (SCM), a comprehensive causal theory from social psychology. The SCM proposes that stereotypes can be understood along two primary dimensions: warmth and competence. We present a method for defining warmth and competence axes in semantic embedding space, and show that the four quadrants defined by this subspace accurately represent the warmth and competence concepts, according to annotated lexicons. We then apply our computational SCM model to textual stereotype data and show that it compares favourably with survey-based studies in the psychological literature. Furthermore, we explore various strategies to counter stereotypical beliefs with anti-stereotypes. It is known that countering stereotypes with anti-stereotypical examples is one of the most effective ways to reduce biased thinking, yet the problem of generating anti-stereotypes has not been previously studied. Thus, a better understanding of how to generate realistic and effective anti-stereotypes can contribute to addressing pressing societal concerns of stereotyping, prejudice, and discrimination.","year":2021,"title_abstract":"Understanding and Countering Stereotypes: A Computational Approach to the Stereotype Content Model Stereotypical language expresses widely-held beliefs about different social categories. Many stereotypes are overtly negative, while others may appear positive on the surface, but still lead to negative consequences. In this work, we present a computational approach to interpreting stereotypes in text through the Stereotype Content Model (SCM), a comprehensive causal theory from social psychology. The SCM proposes that stereotypes can be understood along two primary dimensions: warmth and competence. We present a method for defining warmth and competence axes in semantic embedding space, and show that the four quadrants defined by this subspace accurately represent the warmth and competence concepts, according to annotated lexicons. We then apply our computational SCM model to textual stereotype data and show that it compares favourably with survey-based studies in the psychological literature. Furthermore, we explore various strategies to counter stereotypical beliefs with anti-stereotypes. It is known that countering stereotypes with anti-stereotypical examples is one of the most effective ways to reduce biased thinking, yet the problem of generating anti-stereotypes has not been previously studied. Thus, a better understanding of how to generate realistic and effective anti-stereotypes can contribute to addressing pressing societal concerns of stereotyping, prejudice, and discrimination.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1992798001,"Goal":"Reduced Inequalities","Task":["Understanding and Countering Stereotypes","social psychology","semantic embedding space","biased thinking","anti - stereotypes","societal concerns of stereotyping","discrimination"],"Method":["Computational Approach","Stereotype Content Model","computational approach","Stereotype Content Model","causal theory","SCM","annotated lexicons","computational SCM model"]},{"ID":"stankovic-etal-2010-gis","title":"{GIS} Application Improvement with Multilingual Lexical and Terminological Resources","abstract":"This paper introduces the results of integration of lexical and terminological resources, most of them developed within the Human Language Technology (HLT) Group at the University of Belgrade, with the Geological information system of Serbia (GeolISS) developed at the Faculty of Mining and Geology and funded by the Ministry of the Environmental protection. The approach to GeolISS development, which is aimed at the integration of existing geologic archives, data from published maps on different scales, newly acquired field data, and intranet and internet publishing of geologic is given, followed by the description of the geologic multilingual vocabulary and other lexical and terminological resources used. Two basic results are outlined: multilingual map annotation and improvement of queries for the GeolISS geodatabase. Multilingual labelling and annotation of maps for their graphic display and printing have been tested with Serbian, which describes regional information in the local language, and English, used for sharing geographic information with the world, although the geological vocabulary offers the possibility for integration of other languages as well. The resources also enable semantic and morphological expansion of queries, the latter being very important in highly inflective languages, such as Serbian.","year":2010,"title_abstract":"{GIS} Application Improvement with Multilingual Lexical and Terminological Resources This paper introduces the results of integration of lexical and terminological resources, most of them developed within the Human Language Technology (HLT) Group at the University of Belgrade, with the Geological information system of Serbia (GeolISS) developed at the Faculty of Mining and Geology and funded by the Ministry of the Environmental protection. The approach to GeolISS development, which is aimed at the integration of existing geologic archives, data from published maps on different scales, newly acquired field data, and intranet and internet publishing of geologic is given, followed by the description of the geologic multilingual vocabulary and other lexical and terminological resources used. Two basic results are outlined: multilingual map annotation and improvement of queries for the GeolISS geodatabase. Multilingual labelling and annotation of maps for their graphic display and printing have been tested with Serbian, which describes regional information in the local language, and English, used for sharing geographic information with the world, although the geological vocabulary offers the possibility for integration of other languages as well. The resources also enable semantic and morphological expansion of queries, the latter being very important in highly inflective languages, such as Serbian.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1992015839,"Goal":"Life on Land","Task":["{GIS} Application Improvement","Mining and Geology","Environmental protection","GeolISS development","multilingual map annotation","improvement of queries","Multilingual labelling","annotation of maps","graphic display","printing","sharing geographic information","semantic and morphological expansion of queries"],"Method":["Human Language Technology","Geological information system"]},{"ID":"ye-etal-2021-efficient","title":"Efficient Contrastive Learning via Novel Data Augmentation and Curriculum Learning","abstract":"We introduce EfficientCL, a memory-efficient continual pretraining method that applies contrastive learning with novel data augmentation and curriculum learning. For data augmentation, we stack two types of operation sequentially: cutoff and PCA jittering. While pretraining steps proceed, we apply curriculum learning by incrementing the augmentation degree for each difficulty step. After data augmentation is finished, contrastive learning is applied on projected embeddings of original and augmented examples. When finetuned on GLUE benchmark, our model outperforms baseline models, especially for sentence-level tasks. Additionally, this improvement is capable with only 70{\\%} of computational memory compared to the baseline model.","year":2021,"title_abstract":"Efficient Contrastive Learning via Novel Data Augmentation and Curriculum Learning We introduce EfficientCL, a memory-efficient continual pretraining method that applies contrastive learning with novel data augmentation and curriculum learning. For data augmentation, we stack two types of operation sequentially: cutoff and PCA jittering. While pretraining steps proceed, we apply curriculum learning by incrementing the augmentation degree for each difficulty step. After data augmentation is finished, contrastive learning is applied on projected embeddings of original and augmented examples. When finetuned on GLUE benchmark, our model outperforms baseline models, especially for sentence-level tasks. Additionally, this improvement is capable with only 70{\\%} of computational memory compared to the baseline model.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1991532147,"Goal":"Quality Education","Task":["data augmentation","data augmentation","sentence - level tasks"],"Method":["Contrastive Learning","Data Augmentation","Curriculum Learning","EfficientCL","memory - efficient continual pretraining method","contrastive learning","data augmentation","curriculum learning","cutoff","PCA jittering","curriculum learning","contrastive learning"]},{"ID":"yang-etal-2017-identifying","title":"Identifying and Tracking Sentiments and Topics from Social Media Texts during Natural Disasters","abstract":"We study the problem of identifying the topics and sentiments and tracking their shifts from social media texts in different geographical regions during emergencies and disasters. We propose a location-based dynamic sentiment-topic model (LDST) which can jointly model topic, sentiment, time and Geolocation information. The experimental results demonstrate that LDST performs very well at discovering topics and sentiments from social media and tracking their shifts in different geographical regions during emergencies and disasters. We will release the data and source code after this work is published.","year":2017,"title_abstract":"Identifying and Tracking Sentiments and Topics from Social Media Texts during Natural Disasters We study the problem of identifying the topics and sentiments and tracking their shifts from social media texts in different geographical regions during emergencies and disasters. We propose a location-based dynamic sentiment-topic model (LDST) which can jointly model topic, sentiment, time and Geolocation information. The experimental results demonstrate that LDST performs very well at discovering topics and sentiments from social media and tracking their shifts in different geographical regions during emergencies and disasters. We will release the data and source code after this work is published.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1990284026,"Goal":"Sustainable Cities and Communities","Task":["Identifying and Tracking Sentiments and Topics"],"Method":["location - based dynamic sentiment - topic model","LDST"]},{"ID":"lucy-bamman-2021-gender","title":"Gender and Representation Bias in {GPT}-3 Generated Stories","abstract":"Using topic modeling and lexicon-based word similarity, we find that stories generated by GPT-3 exhibit many known gender stereotypes. Generated stories depict different topics and descriptions depending on GPT-3{'}s perceived gender of the character in a prompt, with feminine characters more likely to be associated with family and appearance, and described as less powerful than masculine characters, even when associated with high power verbs in a prompt. Our study raises questions on how one can avoid unintended social biases when using large language models for storytelling.","year":2021,"title_abstract":"Gender and Representation Bias in {GPT}-3 Generated Stories Using topic modeling and lexicon-based word similarity, we find that stories generated by GPT-3 exhibit many known gender stereotypes. Generated stories depict different topics and descriptions depending on GPT-3{'}s perceived gender of the character in a prompt, with feminine characters more likely to be associated with family and appearance, and described as less powerful than masculine characters, even when associated with high power verbs in a prompt. Our study raises questions on how one can avoid unintended social biases when using large language models for storytelling.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1988973022,"Goal":"Gender Equality","Task":["Gender and Representation Bias","storytelling"],"Method":["topic modeling","lexicon - based word similarity","GPT - 3","large language models"]},{"ID":"marcu-etal-2010-utilizing","title":"Utilizing Automated Translation with Quality Scores to Increase Productivity","abstract":"Automated translation can assist with a variety of translation needs in government, from speeding up access to information for intelligence work to helping human translators increase their productivity. However, government entities need to have a mechanism in place so that they know whether or not they can trust the output from automated translation solutions. In this presentation, Language Weaver will present a new capability ``TrustScore'': an automated scoring algorithm that communicates how good the automated translation is, using a meaningful metric. With this capability, each translation is automatically assigned a score from 1 to 5 in the TrustScore. A score of 1 would indicate that the translation is unintelligible; a score of 3 would indicate that meaning has been conveyed and that the translated content is actionable. A score approaching 4 or higher would indicate that meaning and nuance have been carried through. This automatic prediction of quality has been validated by testing done across significant numbers of data points in different companies and on different types of content. After outlining TrustScore, and how it works, Language Weaver will discuss how a scoring mechanism like TrustScore could be used in a translation productivity workflow in government to assist linguists with day to day translation work. This would enable them to further benefit from their investments in automated translation software. Language Weaver would also share how TrustScore is used in commercial deployments to cost effectively publish information in near real time.","year":2010,"title_abstract":"Utilizing Automated Translation with Quality Scores to Increase Productivity Automated translation can assist with a variety of translation needs in government, from speeding up access to information for intelligence work to helping human translators increase their productivity. However, government entities need to have a mechanism in place so that they know whether or not they can trust the output from automated translation solutions. In this presentation, Language Weaver will present a new capability ``TrustScore'': an automated scoring algorithm that communicates how good the automated translation is, using a meaningful metric. With this capability, each translation is automatically assigned a score from 1 to 5 in the TrustScore. A score of 1 would indicate that the translation is unintelligible; a score of 3 would indicate that meaning has been conveyed and that the translated content is actionable. A score approaching 4 or higher would indicate that meaning and nuance have been carried through. This automatic prediction of quality has been validated by testing done across significant numbers of data points in different companies and on different types of content. After outlining TrustScore, and how it works, Language Weaver will discuss how a scoring mechanism like TrustScore could be used in a translation productivity workflow in government to assist linguists with day to day translation work. This would enable them to further benefit from their investments in automated translation software. Language Weaver would also share how TrustScore is used in commercial deployments to cost effectively publish information in near real time.","social_need":"Decent Work and Economic Growth Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all","cosine_similarity":0.1988059878,"Goal":"Decent Work and Economic Growth","Task":["Automated Translation","Productivity","Automated translation","translation needs","intelligence work","human translators","translation","automated translation","automatic prediction of quality","translation productivity workflow","government","linguists","day to day translation work","automated translation software"],"Method":["automated scoring algorithm","TrustScore","TrustScore","scoring mechanism","TrustScore","TrustScore"]},{"ID":"vempala-blanco-2016-annotating","title":"Annotating Temporally-Anchored Spatial Knowledge on Top of {O}nto{N}otes Semantic Roles","abstract":"This paper presents a two-step methodology to annotate spatial knowledge on top of OntoNotes semantic roles. First, we manipulate semantic roles to automatically generate potential additional spatial knowledge. Second, we crowdsource annotations with Amazon Mechanical Turk to either validate or discard the potential additional spatial knowledge. The resulting annotations indicate whether entities are or are not located somewhere with a degree of certainty, and temporally anchor this spatial information. Crowdsourcing experiments show that the additional spatial knowledge is ubiquitous and intuitive to humans, and experimental results show that it can be inferred automatically using standard supervised machine learning techniques.","year":2016,"title_abstract":"Annotating Temporally-Anchored Spatial Knowledge on Top of {O}nto{N}otes Semantic Roles This paper presents a two-step methodology to annotate spatial knowledge on top of OntoNotes semantic roles. First, we manipulate semantic roles to automatically generate potential additional spatial knowledge. Second, we crowdsource annotations with Amazon Mechanical Turk to either validate or discard the potential additional spatial knowledge. The resulting annotations indicate whether entities are or are not located somewhere with a degree of certainty, and temporally anchor this spatial information. Crowdsourcing experiments show that the additional spatial knowledge is ubiquitous and intuitive to humans, and experimental results show that it can be inferred automatically using standard supervised machine learning techniques.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1987686455,"Goal":"Sustainable Cities and Communities","Task":["Annotating Temporally - Anchored Spatial Knowledge"],"Method":["supervised machine learning"]},{"ID":"jia-etal-2020-mitigating","title":"Mitigating Gender Bias Amplification in Distribution by Posterior Regularization","abstract":"Advanced machine learning techniques have boosted the performance of natural language processing. Nevertheless, recent studies, e.g., (CITATION) show that these techniques inadvertently capture the societal bias hidden in the corpus and further amplify it. However, their analysis is conducted only on models{'} top predictions. In this paper, we investigate the gender bias amplification issue from the distribution perspective and demonstrate that the bias is amplified in the view of predicted probability distribution over labels. We further propose a bias mitigation approach based on posterior regularization. With little performance loss, our method can almost remove the bias amplification in the distribution. Our study sheds the light on understanding the bias amplification.","year":2020,"title_abstract":"Mitigating Gender Bias Amplification in Distribution by Posterior Regularization Advanced machine learning techniques have boosted the performance of natural language processing. Nevertheless, recent studies, e.g., (CITATION) show that these techniques inadvertently capture the societal bias hidden in the corpus and further amplify it. However, their analysis is conducted only on models{'} top predictions. In this paper, we investigate the gender bias amplification issue from the distribution perspective and demonstrate that the bias is amplified in the view of predicted probability distribution over labels. We further propose a bias mitigation approach based on posterior regularization. With little performance loss, our method can almost remove the bias amplification in the distribution. Our study sheds the light on understanding the bias amplification.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1987151802,"Goal":"Gender Equality","Task":["Mitigating Gender Bias Amplification in Distribution","natural language processing","gender bias amplification issue","bias amplification"],"Method":["Posterior Regularization","machine learning techniques","bias mitigation approach","posterior regularization"]},{"ID":"aguirre-etal-2021-gender","title":"Gender and Racial Fairness in Depression Research using Social Media","abstract":"Multiple studies have demonstrated that behaviors expressed on online social media platforms can indicate the mental health state of an individual. The widespread availability of such data has spurred interest in mental health research, using several datasets where individuals are labeled with mental health conditions. While previous research has raised concerns about possible biases in models produced from this data, no study has investigated how these biases manifest themselves with regards to demographic groups in data, such as gender and racial\/ethnic groups. Here, we analyze the fairness of depression classifiers trained on Twitter data with respect to gender and racial demographic groups. We find that model performance differs for underrepresented groups, and we investigate sources of these biases beyond data representation. Our study results in recommendations on how to avoid these biases in future research.","year":2021,"title_abstract":"Gender and Racial Fairness in Depression Research using Social Media Multiple studies have demonstrated that behaviors expressed on online social media platforms can indicate the mental health state of an individual. The widespread availability of such data has spurred interest in mental health research, using several datasets where individuals are labeled with mental health conditions. While previous research has raised concerns about possible biases in models produced from this data, no study has investigated how these biases manifest themselves with regards to demographic groups in data, such as gender and racial\/ethnic groups. Here, we analyze the fairness of depression classifiers trained on Twitter data with respect to gender and racial demographic groups. We find that model performance differs for underrepresented groups, and we investigate sources of these biases beyond data representation. Our study results in recommendations on how to avoid these biases in future research.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1984735131,"Goal":"Reduced Inequalities","Task":["Gender and Racial Fairness","Depression Research","mental health research"],"Method":["depression classifiers","data representation"]},{"ID":"karve-etal-2019-conceptor","title":"Conceptor Debiasing of Word Representations Evaluated on {WEAT}","abstract":"Bias in word representations, such as Word2Vec, has been widely reported and investigated, and efforts made to debias them. We apply the debiasing conceptor for post-processing both traditional and contextualized word embeddings. Our method can simultaneously remove racial and gender biases from word representations. Unlike standard debiasing methods, the debiasing conceptor can utilize heterogeneous lists of biased words without loss in performance. Finally, our empirical experiments show that the debiasing conceptor diminishes racial and gender bias of word representations as measured using the Word Embedding Association Test (WEAT) of Caliskan et al. (2017).","year":2019,"title_abstract":"Conceptor Debiasing of Word Representations Evaluated on {WEAT} Bias in word representations, such as Word2Vec, has been widely reported and investigated, and efforts made to debias them. We apply the debiasing conceptor for post-processing both traditional and contextualized word embeddings. Our method can simultaneously remove racial and gender biases from word representations. Unlike standard debiasing methods, the debiasing conceptor can utilize heterogeneous lists of biased words without loss in performance. Finally, our empirical experiments show that the debiasing conceptor diminishes racial and gender bias of word representations as measured using the Word Embedding Association Test (WEAT) of Caliskan et al. (2017).","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1984225959,"Goal":"Gender Equality","Task":["Conceptor Debiasing of Word Representations","post - processing","traditional and contextualized word embeddings","racial and gender bias of word representations"],"Method":["word representations","Word2Vec","debiasing conceptor","word representations","debiasing methods","debiasing conceptor","debiasing conceptor","Word Embedding Association Test"]},{"ID":"kuhn-etal-2020-indigenous","title":"The Indigenous Languages Technology project at {NRC} {C}anada: An empowerment-oriented approach to developing language software","abstract":"This paper surveys the first, three-year phase of a project at the National Research Council of Canada that is developing software to assist Indigenous communities in Canada in preserving their languages and extending their use. The project aimed to work within the empowerment paradigm, where collaboration with communities and fulfillment of their goals is central. Since many of the technologies we developed were in response to community needs, the project ended up as a collection of diverse subprojects, including the creation of a sophisticated framework for building verb conjugators for highly inflectional polysynthetic languages (such as Kanyen{'}k{\\'e}ha, in the Iroquoian language family), release of what is probably the largest available corpus of sentences in a polysynthetic language (Inuktut) aligned with English sentences and experiments with machine translation (MT) systems trained on this corpus, free online services based on automatic speech recognition (ASR) for easing the transcription bottleneck for recordings of speech in Indigenous languages (and other languages), software for implementing text prediction and read-along audiobooks for Indigenous languages, and several other subprojects.","year":2020,"title_abstract":"The Indigenous Languages Technology project at {NRC} {C}anada: An empowerment-oriented approach to developing language software This paper surveys the first, three-year phase of a project at the National Research Council of Canada that is developing software to assist Indigenous communities in Canada in preserving their languages and extending their use. The project aimed to work within the empowerment paradigm, where collaboration with communities and fulfillment of their goals is central. Since many of the technologies we developed were in response to community needs, the project ended up as a collection of diverse subprojects, including the creation of a sophisticated framework for building verb conjugators for highly inflectional polysynthetic languages (such as Kanyen{'}k{\\'e}ha, in the Iroquoian language family), release of what is probably the largest available corpus of sentences in a polysynthetic language (Inuktut) aligned with English sentences and experiments with machine translation (MT) systems trained on this corpus, free online services based on automatic speech recognition (ASR) for easing the transcription bottleneck for recordings of speech in Indigenous languages (and other languages), software for implementing text prediction and read-along audiobooks for Indigenous languages, and several other subprojects.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.1982700229,"Goal":"Peace, Justice and Strong Institutions","Task":["Indigenous Languages Technology project","language software","machine translation","automatic speech recognition","transcription bottleneck","text prediction","read - along audiobooks"],"Method":["empowerment - oriented approach","empowerment paradigm","verb conjugators","online services"]},{"ID":"muzaffar-etal-2016-issues","title":"Issues and Challenges in Annotating {U}rdu Action Verbs on the {IMAGACT}4{ALL} Platform","abstract":"In South-Asian languages such as Hindi and Urdu, action verbs having compound constructions and serial verbs constructions pose serious problems for natural language processing and other linguistic tasks. Urdu is an Indo-Aryan language spoken by 51, 500, 0001 speakers in India. Action verbs that occur spontaneously in day-to-day communication are highly ambiguous in nature semantically and as a consequence cause disambiguation issues that are relevant and applicable to Language Technologies (LT) like Machine Translation (MT) and Natural Language Processing (NLP). IMAGACT4ALL is an ontology-driven web-based platform developed by the University of Florence for storing action verbs and their inter-relations. This group is currently collaborating with Jawaharlal Nehru University (JNU) in India to connect Indian languages on this platform. Action verbs are frequently used in both written and spoken discourses and refer to various meanings because of their polysemic nature. The IMAGACT4ALL platform stores each 3d animation image, each one of them referring to a variety of possible ontological types, which in turn makes the annotation task for the annotator quite challenging with regard to selecting verb argument structure having a range of probability distribution. The authors, in this paper, discuss the issues and challenges such as complex predicates (compound and conjunct verbs), ambiguously animated video illustrations, semantic discrepancies, and the factors of verb-selection preferences that have produced significant problems in annotating Urdu verbs on the IMAGACT ontology.","year":2016,"title_abstract":"Issues and Challenges in Annotating {U}rdu Action Verbs on the {IMAGACT}4{ALL} Platform In South-Asian languages such as Hindi and Urdu, action verbs having compound constructions and serial verbs constructions pose serious problems for natural language processing and other linguistic tasks. Urdu is an Indo-Aryan language spoken by 51, 500, 0001 speakers in India. Action verbs that occur spontaneously in day-to-day communication are highly ambiguous in nature semantically and as a consequence cause disambiguation issues that are relevant and applicable to Language Technologies (LT) like Machine Translation (MT) and Natural Language Processing (NLP). IMAGACT4ALL is an ontology-driven web-based platform developed by the University of Florence for storing action verbs and their inter-relations. This group is currently collaborating with Jawaharlal Nehru University (JNU) in India to connect Indian languages on this platform. Action verbs are frequently used in both written and spoken discourses and refer to various meanings because of their polysemic nature. The IMAGACT4ALL platform stores each 3d animation image, each one of them referring to a variety of possible ontological types, which in turn makes the annotation task for the annotator quite challenging with regard to selecting verb argument structure having a range of probability distribution. The authors, in this paper, discuss the issues and challenges such as complex predicates (compound and conjunct verbs), ambiguously animated video illustrations, semantic discrepancies, and the factors of verb-selection preferences that have produced significant problems in annotating Urdu verbs on the IMAGACT ontology.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1977285147,"Goal":"Climate Action","Task":["Annotating {U}rdu Action Verbs","natural language processing","linguistic tasks","day - to - day communication","disambiguation issues","Language Technologies","Machine Translation","Natural Language Processing","storing action verbs","annotation task","annotator","annotating Urdu verbs"],"Method":["Urdu","IMAGACT4ALL","ontology - driven web - based platform","IMAGACT4ALL platform"]},{"ID":"ntalampiras-etal-2010-heterogeneous","title":"Heterogeneous Sensor Database in Support of Human Behaviour Analysis in Unrestricted Environments: The Audio Part","abstract":"In the present paper we report on a recent effort that resulted in the establishment of a unique multimodal database, referred to as the PROMETHEUS database. This database was created in support of research and development activities, performed within the European Commission FP7 PROMETHEUS project, aiming at the creation of a framework for monitoring and interpretation of human behaviours in unrestricted indoors and outdoors environments. In the present paper we discuss the design and the implementation of the audio part of the database and offer statistical information about the audio content. Specifically, it contains single-person and multi-person scenarios, but also covers scenarios with interactions between groups of people. The database design was conceived with extended support of research and development activities devoted to detection of typical and atypical events, emergency and crisis situations, which assist for achieving situational awareness and more reliable interpretation of the context in which humans behave. The PROMETHEUS database allows for embracing a wide range of real-world applications, including smart-home and human-robot interaction interfaces, indoors\/outdoors public areas surveillance, airport terminals or city park supervision, etc. A major portion of the PROMETHEUS database will be made publically available by the end of year 2010.","year":2010,"title_abstract":"Heterogeneous Sensor Database in Support of Human Behaviour Analysis in Unrestricted Environments: The Audio Part In the present paper we report on a recent effort that resulted in the establishment of a unique multimodal database, referred to as the PROMETHEUS database. This database was created in support of research and development activities, performed within the European Commission FP7 PROMETHEUS project, aiming at the creation of a framework for monitoring and interpretation of human behaviours in unrestricted indoors and outdoors environments. In the present paper we discuss the design and the implementation of the audio part of the database and offer statistical information about the audio content. Specifically, it contains single-person and multi-person scenarios, but also covers scenarios with interactions between groups of people. The database design was conceived with extended support of research and development activities devoted to detection of typical and atypical events, emergency and crisis situations, which assist for achieving situational awareness and more reliable interpretation of the context in which humans behave. The PROMETHEUS database allows for embracing a wide range of real-world applications, including smart-home and human-robot interaction interfaces, indoors\/outdoors public areas surveillance, airport terminals or city park supervision, etc. A major portion of the PROMETHEUS database will be made publically available by the end of year 2010.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1977236271,"Goal":"Sustainable Cities and Communities","Task":["Human Behaviour Analysis","Unrestricted Environments","monitoring and interpretation of human behaviours","detection of typical and atypical events","emergency and crisis situations","situational awareness","real - world applications","smart - home and human - robot interaction interfaces","indoors\/outdoors public areas surveillance","airport terminals","city park supervision"],"Method":["Heterogeneous Sensor Database","Audio Part","PROMETHEUS database","audio part","database design","PROMETHEUS database"]},{"ID":"weiner-etal-2016-towards","title":"Towards Automatic Transcription of {ILSE} \u2015 an Interdisciplinary Longitudinal Study of Adult Development and Aging","abstract":"The Interdisciplinary Longitudinal Study on Adult Development and Aging (ILSE) was created to facilitate the study of challenges posed by rapidly aging societies in developed countries such as Germany. ILSE contains over 8,000 hours of biographic interviews recorded from more than 1,000 participants over the course of 20 years. Investigations on various aspects of aging, such as cognitive decline, often rely on the analysis of linguistic features which can be derived from spoken content like these interviews. However, transcribing speech is a time and cost consuming manual process and so far only 380 hours of ILSE interviews have been transcribed. Thus, it is the aim of our work to establish technical systems to fully automatically transcribe the ILSE interview data. The joint occurrence of poor recording quality, long audio segments, erroneous transcriptions, varying speaking styles {\\&} crosstalk, and emotional {\\&} dialectal speech in these interviews presents challenges for automatic speech recognition (ASR). We describe our ongoing work towards the fully automatic transcription of all ILSE interviews and the steps we implemented in preparing the transcriptions to meet the interviews{'} challenges. Using a recursive long audio alignment procedure 96 hours of the transcribed data have been made accessible for ASR training.","year":2016,"title_abstract":"Towards Automatic Transcription of {ILSE} \u2015 an Interdisciplinary Longitudinal Study of Adult Development and Aging The Interdisciplinary Longitudinal Study on Adult Development and Aging (ILSE) was created to facilitate the study of challenges posed by rapidly aging societies in developed countries such as Germany. ILSE contains over 8,000 hours of biographic interviews recorded from more than 1,000 participants over the course of 20 years. Investigations on various aspects of aging, such as cognitive decline, often rely on the analysis of linguistic features which can be derived from spoken content like these interviews. However, transcribing speech is a time and cost consuming manual process and so far only 380 hours of ILSE interviews have been transcribed. Thus, it is the aim of our work to establish technical systems to fully automatically transcribe the ILSE interview data. The joint occurrence of poor recording quality, long audio segments, erroneous transcriptions, varying speaking styles {\\&} crosstalk, and emotional {\\&} dialectal speech in these interviews presents challenges for automatic speech recognition (ASR). We describe our ongoing work towards the fully automatic transcription of all ILSE interviews and the steps we implemented in preparing the transcriptions to meet the interviews{'} challenges. Using a recursive long audio alignment procedure 96 hours of the transcribed data have been made accessible for ASR training.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.1976074725,"Goal":"Good Health and Well-Being","Task":["Automatic Transcription of {ILSE}","Interdisciplinary Longitudinal Study of Adult Development and Aging","Adult Development and Aging","ILSE","aging","cognitive decline","analysis of linguistic features","transcribing speech","manual process","automatic speech recognition","automatic transcription","ILSE interviews","transcriptions","ASR"],"Method":["recursive long audio alignment procedure"]},{"ID":"ye-etal-2021-li","title":"\u5229\u7528\u8bed\u4e49\u5173\u8054\u589e\u5f3a\u7684\u8de8\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bd1\u6587\u8d28\u91cf\u8bc4\u4f30(A Cross-language Pre-trained Model with Enhanced Semantic Connection for {MT} Quality Estimation)","abstract":"\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30(QE)\u867d\u7136\u4e0d\u9700\u8981\u53c2\u8003\u8bd1\u6587\u5c31\u80fd\u8fdb\u884c\u81ea\u52a8\u8bc4\u4f30,\u4f46\u5b83\u9700\u8981\u4eba\u5de5\u6807\u6ce8\u7684\u8bc4\u4f30\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\u7684QE\u4e3a\u4e86\u514b\u670d\u4eba\u5de5\u8bc4\u4f30\u6570\u636e\u7684\u7a00\u7f3a\u95ee\u9898,\u901a\u5e38\u5305\u62ec\u4e24\u4e2a\u9636\u6bb5,\u9996\u5148\u501f\u52a9\u5927\u89c4\u6a21\u7684\u5e73\u884c\u8bed\u6599\u5b66\u4e60\u53cc\u8bed\u5bf9\u9f50,\u7136\u540e\u5728\u5c0f\u89c4\u6a21\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u5efa\u6a21\u3002\u8de8\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u4ee5\u7528\u6765\u4ee3\u66ff\u8be5\u4efb\u52a1\u7b2c\u4e00\u9636\u6bb5\u7684\u5b66\u4e60\u8fc7\u7a0b,\u56e0\u6b64\u672c\u6587\u9996\u5148\u5efa\u8bae\u4e00\u4e2a\u57fa\u4e8eXLM-R\u7684\u4e3a\u6e90\/\u76ee\u6807\u8bed\u8a00\u7edf\u4e00\u7f16\u7801\u7684QE\u6a21\u578b\u3002\u5176\u6b21,\u7531\u4e8e\u5927\u591a\u6570\u9884\u8bad\u7ec3\u6a21\u578b\u662f\u5728\u591a\u8bed\u8a00\u7684\u5355\u8bed\u6570\u636e\u96c6\u4e0a\u6784\u5efa\u7684,\u56e0\u6b64\u4e24\u4e24\u8bed\u8a00\u5bf9\u7684\u8bed\u4e49\u5173\u8054\u80fd\u529b\u76f8\u5bf9\u8f83\u5f31\u3002\u4e3a\u4e86\u80fd\u4f7f\u8de8\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u66f4\u597d\u5730\u9002\u5e94QE\u4efb\u52a1,\u672c\u6587\u63d0\u51fa\u7528\u4e09\u79cd\u9884\u8bad\u7ec3\u7b56\u7565\u6765\u589e\u5f3a\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8de8\u8bed\u8a00\u8bed\u4e49\u5173\u8054\u80fd\u529b\u3002\u672c\u6587\u7684\u65b9\u6cd5\u5728WMT2017\u548cWMT2019\u82f1\u5fb7\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u9ad8\u6027\u80fd\u3002","year":2021,"title_abstract":"\u5229\u7528\u8bed\u4e49\u5173\u8054\u589e\u5f3a\u7684\u8de8\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bd1\u6587\u8d28\u91cf\u8bc4\u4f30(A Cross-language Pre-trained Model with Enhanced Semantic Connection for {MT} Quality Estimation) \u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30(QE)\u867d\u7136\u4e0d\u9700\u8981\u53c2\u8003\u8bd1\u6587\u5c31\u80fd\u8fdb\u884c\u81ea\u52a8\u8bc4\u4f30,\u4f46\u5b83\u9700\u8981\u4eba\u5de5\u6807\u6ce8\u7684\u8bc4\u4f30\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\u7684QE\u4e3a\u4e86\u514b\u670d\u4eba\u5de5\u8bc4\u4f30\u6570\u636e\u7684\u7a00\u7f3a\u95ee\u9898,\u901a\u5e38\u5305\u62ec\u4e24\u4e2a\u9636\u6bb5,\u9996\u5148\u501f\u52a9\u5927\u89c4\u6a21\u7684\u5e73\u884c\u8bed\u6599\u5b66\u4e60\u53cc\u8bed\u5bf9\u9f50,\u7136\u540e\u5728\u5c0f\u89c4\u6a21\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u5efa\u6a21\u3002\u8de8\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u4ee5\u7528\u6765\u4ee3\u66ff\u8be5\u4efb\u52a1\u7b2c\u4e00\u9636\u6bb5\u7684\u5b66\u4e60\u8fc7\u7a0b,\u56e0\u6b64\u672c\u6587\u9996\u5148\u5efa\u8bae\u4e00\u4e2a\u57fa\u4e8eXLM-R\u7684\u4e3a\u6e90\/\u76ee\u6807\u8bed\u8a00\u7edf\u4e00\u7f16\u7801\u7684QE\u6a21\u578b\u3002\u5176\u6b21,\u7531\u4e8e\u5927\u591a\u6570\u9884\u8bad\u7ec3\u6a21\u578b\u662f\u5728\u591a\u8bed\u8a00\u7684\u5355\u8bed\u6570\u636e\u96c6\u4e0a\u6784\u5efa\u7684,\u56e0\u6b64\u4e24\u4e24\u8bed\u8a00\u5bf9\u7684\u8bed\u4e49\u5173\u8054\u80fd\u529b\u76f8\u5bf9\u8f83\u5f31\u3002\u4e3a\u4e86\u80fd\u4f7f\u8de8\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u66f4\u597d\u5730\u9002\u5e94QE\u4efb\u52a1,\u672c\u6587\u63d0\u51fa\u7528\u4e09\u79cd\u9884\u8bad\u7ec3\u7b56\u7565\u6765\u589e\u5f3a\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8de8\u8bed\u8a00\u8bed\u4e49\u5173\u8054\u80fd\u529b\u3002\u672c\u6587\u7684\u65b9\u6cd5\u5728WMT2017\u548cWMT2019\u82f1\u5fb7\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u9ad8\u6027\u80fd\u3002","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1975624412,"Goal":"Quality Education","Task":["{MT} Quality Estimation)"],"Method":["Cross - language Pre - trained Model"]},{"ID":"m-k-a-p-2021-ku","title":"{KU}{\\_}{NLP}@{LT}-{EDI}-{EACL}2021: A Multilingual Hope Speech Detection for Equality, Diversity, and Inclusion using Context Aware Embeddings","abstract":"Hope speech detection is a new task for finding and highlighting positive comments or supporting content from user-generated social media comments. For this task, we have used a Shared Task multilingual dataset on Hope Speech Detection for Equality, Diversity, and Inclusion (HopeEDI) for three languages English, code-switched Tamil and Malayalam. In this paper, we present deep learning techniques using context-aware string embeddings for word representations and Recurrent Neural Network (RNN) and pooled document embeddings for text representation. We have evaluated and compared the three models for each language with different approaches. Our proposed methodology works fine and achieved higher performance than baselines. The highest weighted average F-scores of 0.93, 0.58, and 0.84 are obtained on the task organisers{'} final evaluation test set. The proposed models are outperforming the baselines by 3{\\%}, 2{\\%} and 11{\\%} in absolute terms for English, Tamil and Malayalam respectively.","year":2021,"title_abstract":"{KU}{\\_}{NLP}@{LT}-{EDI}-{EACL}2021: A Multilingual Hope Speech Detection for Equality, Diversity, and Inclusion using Context Aware Embeddings Hope speech detection is a new task for finding and highlighting positive comments or supporting content from user-generated social media comments. For this task, we have used a Shared Task multilingual dataset on Hope Speech Detection for Equality, Diversity, and Inclusion (HopeEDI) for three languages English, code-switched Tamil and Malayalam. In this paper, we present deep learning techniques using context-aware string embeddings for word representations and Recurrent Neural Network (RNN) and pooled document embeddings for text representation. We have evaluated and compared the three models for each language with different approaches. Our proposed methodology works fine and achieved higher performance than baselines. The highest weighted average F-scores of 0.93, 0.58, and 0.84 are obtained on the task organisers{'} final evaluation test set. The proposed models are outperforming the baselines by 3{\\%}, 2{\\%} and 11{\\%} in absolute terms for English, Tamil and Malayalam respectively.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1974048316,"Goal":"Gender Equality","Task":["Multilingual Hope Speech Detection","Equality","Inclusion","Hope speech detection","finding and highlighting positive comments","supporting content","Hope Speech Detection","Equality","Diversity","Inclusion","text representation"],"Method":["Context Aware Embeddings","deep learning techniques","context - aware string embeddings","word representations","Recurrent Neural Network","pooled document embeddings"]},{"ID":"camara-etal-2022-mapping","title":"Mapping the Multilingual Margins: Intersectional Biases of Sentiment Analysis Systems in {E}nglish, {S}panish, and {A}rabic","abstract":"As natural language processing systems become more widespread, it is necessary to address fairness issues in their implementation and deployment to ensure that their negative impacts on society are understood and minimized. However, there is limited work that studies fairness using a multilingual and intersectional framework or on downstream tasks. In this paper, we introduce four multilingual Equity Evaluation Corpora, supplementary test sets designed to measure social biases, and a novel statistical framework for studying unisectional and intersectional social biases in natural language processing. We use these tools to measure gender, racial, ethnic, and intersectional social biases across five models trained on emotion regression tasks in English, Spanish, and Arabic. We find that many systems demonstrate statistically significant unisectional and intersectional social biases. We make our code and datasets available for download.","year":2022,"title_abstract":"Mapping the Multilingual Margins: Intersectional Biases of Sentiment Analysis Systems in {E}nglish, {S}panish, and {A}rabic As natural language processing systems become more widespread, it is necessary to address fairness issues in their implementation and deployment to ensure that their negative impacts on society are understood and minimized. However, there is limited work that studies fairness using a multilingual and intersectional framework or on downstream tasks. In this paper, we introduce four multilingual Equity Evaluation Corpora, supplementary test sets designed to measure social biases, and a novel statistical framework for studying unisectional and intersectional social biases in natural language processing. We use these tools to measure gender, racial, ethnic, and intersectional social biases across five models trained on emotion regression tasks in English, Spanish, and Arabic. We find that many systems demonstrate statistically significant unisectional and intersectional social biases. We make our code and datasets available for download.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1973155439,"Goal":"Gender Equality","Task":["fairness issues","downstream tasks","unisectional and intersectional social biases","natural language processing","emotion regression tasks"],"Method":["Sentiment Analysis Systems","natural language processing systems","multilingual and intersectional framework","statistical framework"]},{"ID":"liang-etal-2020-towards","title":"Towards Debiasing Sentence Representations","abstract":"As natural language processing methods are increasingly deployed in real-world scenarios such as healthcare, legal systems, and social science, it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes. Previous work has revealed the presence of social biases in widely used word embeddings involving gender, race, religion, and other social constructs. While some methods were proposed to debias these word-level embeddings, there is a need to perform debiasing at the sentence-level given the recent shift towards new contextualized sentence representations such as ELMo and BERT. In this paper, we investigate the presence of social biases in sentence-level representations and propose a new method, Sent-Debias, to reduce these biases. We show that Sent-Debias is effective in removing biases, and at the same time, preserves performance on sentence-level downstream tasks such as sentiment analysis, linguistic acceptability, and natural language understanding. We hope that our work will inspire future research on characterizing and removing social biases from widely adopted sentence representations for fairer NLP.","year":2020,"title_abstract":"Towards Debiasing Sentence Representations As natural language processing methods are increasingly deployed in real-world scenarios such as healthcare, legal systems, and social science, it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes. Previous work has revealed the presence of social biases in widely used word embeddings involving gender, race, religion, and other social constructs. While some methods were proposed to debias these word-level embeddings, there is a need to perform debiasing at the sentence-level given the recent shift towards new contextualized sentence representations such as ELMo and BERT. In this paper, we investigate the presence of social biases in sentence-level representations and propose a new method, Sent-Debias, to reduce these biases. We show that Sent-Debias is effective in removing biases, and at the same time, preserves performance on sentence-level downstream tasks such as sentiment analysis, linguistic acceptability, and natural language understanding. We hope that our work will inspire future research on characterizing and removing social biases from widely adopted sentence representations for fairer NLP.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1971752048,"Goal":"Gender Equality","Task":["Debiasing Sentence Representations","real - world scenarios","healthcare","legal systems","social science","debiasing","sentence - level downstream tasks","sentiment analysis","linguistic acceptability","natural language understanding","fairer NLP"],"Method":["natural language processing methods","contextualized sentence representations","ELMo","BERT","sentence - level representations","Sent - Debias","Sent - Debias","sentence representations"]},{"ID":"jones-muftic-2020-endangered","title":"Endangered {A}frican Languages Featured in a Digital Collection: The Case of the \u01c2{K}homani {S}an, {H}ugh {B}rody {C}ollection","abstract":"The \u01c2Khomani San, Hugh Brody Collection features the voices and history of indigenous hunter gatherer descendants in three endangered languages namely, N|uu, Kora and Khoekhoe as well as a regional dialect of Afrikaans. A large component of this collection is audio-visual (legacy media) recordings of interviews conducted with members of the community by Hugh Brody and his colleagues between 1997 and 2012, referring as far back as the 1800s. The Digital Library Services team at the University of Cape Town aim to showcase the collection digitally on the UCT-wide Digital Collections platform, Ibali which runs on Omeka-S. In this paper we highlight the importance of such a collection in the context of South Africa, and the ethical steps that were taken to ensure the respect of the \u01c2Khomani San as their stories get uploaded onto a repository and become accessible to all. We will also feature some of the completed collection on Ibali and guide the reader through the organisation of the collection on the Omeka-S backend. Finally, we will outline our development process, from digitisation to repository publishing as well as present some of the challenges in data clean-up, the curation of legacy media, multi-lingual support, and site organisation.","year":2020,"title_abstract":"Endangered {A}frican Languages Featured in a Digital Collection: The Case of the \u01c2{K}homani {S}an, {H}ugh {B}rody {C}ollection The \u01c2Khomani San, Hugh Brody Collection features the voices and history of indigenous hunter gatherer descendants in three endangered languages namely, N|uu, Kora and Khoekhoe as well as a regional dialect of Afrikaans. A large component of this collection is audio-visual (legacy media) recordings of interviews conducted with members of the community by Hugh Brody and his colleagues between 1997 and 2012, referring as far back as the 1800s. The Digital Library Services team at the University of Cape Town aim to showcase the collection digitally on the UCT-wide Digital Collections platform, Ibali which runs on Omeka-S. In this paper we highlight the importance of such a collection in the context of South Africa, and the ethical steps that were taken to ensure the respect of the \u01c2Khomani San as their stories get uploaded onto a repository and become accessible to all. We will also feature some of the completed collection on Ibali and guide the reader through the organisation of the collection on the Omeka-S backend. Finally, we will outline our development process, from digitisation to repository publishing as well as present some of the challenges in data clean-up, the curation of legacy media, multi-lingual support, and site organisation.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1971322298,"Goal":"Sustainable Cities and Communities","Task":["digitisation","repository publishing","data clean - up","curation of legacy media","multi - lingual support","site organisation"],"Method":["Ibali"]},{"ID":"field-tsvetkov-2020-unsupervised","title":"Unsupervised Discovery of Implicit Gender Bias","abstract":"Despite their prevalence in society, social biases are difficult to identify, primarily because human judgements in this domain can be unreliable. We take an unsupervised approach to identifying gender bias against women at a comment level and present a model that can surface text likely to contain bias. Our main challenge is forcing the model to focus on signs of implicit bias, rather than other artifacts in the data. Thus, our methodology involves reducing the influence of confounds through propensity matching and adversarial learning. Our analysis shows how biased comments directed towards female politicians contain mixed criticisms, while comments directed towards other female public figures focus on appearance and sexualization. Ultimately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements.","year":2020,"title_abstract":"Unsupervised Discovery of Implicit Gender Bias Despite their prevalence in society, social biases are difficult to identify, primarily because human judgements in this domain can be unreliable. We take an unsupervised approach to identifying gender bias against women at a comment level and present a model that can surface text likely to contain bias. Our main challenge is forcing the model to focus on signs of implicit bias, rather than other artifacts in the data. Thus, our methodology involves reducing the influence of confounds through propensity matching and adversarial learning. Our analysis shows how biased comments directed towards female politicians contain mixed criticisms, while comments directed towards other female public figures focus on appearance and sexualization. Ultimately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1970689297,"Goal":"Gender Equality","Task":["Unsupervised Discovery of Implicit Gender Bias"],"Method":["unsupervised approach","propensity matching","adversarial learning"]},{"ID":"ajani-etal-2006-development","title":"A Development Tool For Multilingual Ontology-based Conceptual","abstract":"This paper introduces a number theoretical and practical issues related to the \u0093Syllabus\u0094. Syllabusis a multi-lingua ontology based tool, designed to improve the applications of the European Directives in the various European countries.","year":2006,"title_abstract":"A Development Tool For Multilingual Ontology-based Conceptual This paper introduces a number theoretical and practical issues related to the \u0093Syllabus\u0094. Syllabusis a multi-lingua ontology based tool, designed to improve the applications of the European Directives in the various European countries.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.196931988,"Goal":"Partnership for the Goals","Task":["Multilingual Ontology - based Conceptual"],"Method":["multi - lingua ontology based tool"]},{"ID":"mukadam-etal-2020-representation","title":"A Representation Learning Approach to Animal Biodiversity Conservation","abstract":"Generating knowledge from natural language data has aided in solving many artificial intelligence problems. Vector representations of words have been the driving force behind the majority of natural language processing tasks. This paper develops a novel approach for predicting the conservation status of animal species using custom generated scientific name embeddings. We use two different vector embeddings generated using representation learning on Wikipedia text and animal taxonomy data. We generate name embeddings for all species in the animal kingdom using unsupervised learning and build a model on the IUCN Red List dataset to classify species into endangered or least-concern. To our knowledge, this is the first work that makes use of learnt features instead of handcrafted features for this task and achieves competitive results. Based on the high confidence results of our model, we also predict the conservation status of data deficient species whose conservation status is still unknown and thus steering more focus towards them for protection. These embeddings have also been made publicly available here. We believe this will greatly help in solving various downstream tasks and further advance research in the cross-domain involving natural language processing, conservation biology, and life sciences.","year":2020,"title_abstract":"A Representation Learning Approach to Animal Biodiversity Conservation Generating knowledge from natural language data has aided in solving many artificial intelligence problems. Vector representations of words have been the driving force behind the majority of natural language processing tasks. This paper develops a novel approach for predicting the conservation status of animal species using custom generated scientific name embeddings. We use two different vector embeddings generated using representation learning on Wikipedia text and animal taxonomy data. We generate name embeddings for all species in the animal kingdom using unsupervised learning and build a model on the IUCN Red List dataset to classify species into endangered or least-concern. To our knowledge, this is the first work that makes use of learnt features instead of handcrafted features for this task and achieves competitive results. Based on the high confidence results of our model, we also predict the conservation status of data deficient species whose conservation status is still unknown and thus steering more focus towards them for protection. These embeddings have also been made publicly available here. We believe this will greatly help in solving various downstream tasks and further advance research in the cross-domain involving natural language processing, conservation biology, and life sciences.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1968079358,"Goal":"Life on Land","Task":["Animal Biodiversity Conservation","Generating knowledge","artificial intelligence problems","natural language processing tasks","conservation status of animal species","protection","downstream tasks","cross - domain","natural language processing","conservation biology","life sciences"],"Method":["Representation Learning Approach","Vector representations of words","scientific name embeddings","representation learning","unsupervised learning"]},{"ID":"murakami-etal-2021-generating","title":"Generating Weather Comments from Meteorological Simulations","abstract":"The task of generating weather-forecast comments from meteorological simulations has the following requirements: (i) the changes in numerical values for various physical quantities need to be considered, (ii) the weather comments should be dependent on delivery time and area information, and (iii) the comments should provide useful information for users. To meet these requirements, we propose a data-to-text model that incorporates three types of encoders for numerical forecast maps, observation data, and meta-data. We also introduce weather labels representing weather information, such as sunny and rain, for our model to explicitly describe useful information. We conducted automatic and human evaluations. The results indicate that our model performed best against baselines in terms of informativeness. We make our code and data publicly available.","year":2021,"title_abstract":"Generating Weather Comments from Meteorological Simulations The task of generating weather-forecast comments from meteorological simulations has the following requirements: (i) the changes in numerical values for various physical quantities need to be considered, (ii) the weather comments should be dependent on delivery time and area information, and (iii) the comments should provide useful information for users. To meet these requirements, we propose a data-to-text model that incorporates three types of encoders for numerical forecast maps, observation data, and meta-data. We also introduce weather labels representing weather information, such as sunny and rain, for our model to explicitly describe useful information. We conducted automatic and human evaluations. The results indicate that our model performed best against baselines in terms of informativeness. We make our code and data publicly available.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1964621097,"Goal":"Climate Action","Task":["Generating Weather Comments","Meteorological Simulations","generating weather - forecast comments","meteorological simulations","numerical forecast maps"],"Method":["data - to - text model","encoders"]},{"ID":"popel-2018-cuni","title":"{CUNI} Transformer Neural {MT} System for {WMT}18","abstract":"We describe our NMT system submitted to the WMT2018 shared task in news translation. Our system is based on the Transformer model (Vaswani et al., 2017). We use an improved technique of backtranslation, where we iterate the process of translating monolingual data in one direction and training an NMT model for the opposite direction using synthetic parallel data. We apply a simple but effective filtering of the synthetic data. We pre-process the input sentences using coreference resolution in order to disambiguate the gender of pro-dropped personal pronouns. Finally, we apply two simple post-processing substitutions on the translated output. Our system is significantly (p {\\textless} 0.05) better than all other English-Czech and Czech-English systems in WMT2018.","year":2018,"title_abstract":"{CUNI} Transformer Neural {MT} System for {WMT}18 We describe our NMT system submitted to the WMT2018 shared task in news translation. Our system is based on the Transformer model (Vaswani et al., 2017). We use an improved technique of backtranslation, where we iterate the process of translating monolingual data in one direction and training an NMT model for the opposite direction using synthetic parallel data. We apply a simple but effective filtering of the synthetic data. We pre-process the input sentences using coreference resolution in order to disambiguate the gender of pro-dropped personal pronouns. Finally, we apply two simple post-processing substitutions on the translated output. Our system is significantly (p {\\textless} 0.05) better than all other English-Czech and Czech-English systems in WMT2018.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1962440461,"Goal":"Gender Equality","Task":["WMT2018 shared task","news translation"],"Method":["Transformer Neural","NMT","Transformer model","backtranslation","NMT","filtering","coreference resolution","post - processing substitutions"]},{"ID":"simmons-2010-foreign","title":"Foreign Media Collaboration Framework ({FMCF})","abstract":"The Foreign Media Collaboration Framework (FMCF) is the latest approach by NASIC to provide a comprehensive system to process foreign language materials. FMCF is a Services Oriented Architecture (SOA) that provides an infrastructure to manage HLT tools, products, workflows, and services. This federated SOA solution adheres to DISA's NCES SOA Governance Model, DDMS XML for Metadata Capture\/Dissemination, and IC-ISM for Security. The FMCF provides a cutting edge infrastructure that encapsulates multiple capabilities from multiple vendors in one place. This approach will accelerate HLT development, contain sustainment cost, minimize training, and brings the MT, OCR, ASR, audio\/video, entity extraction, analytic tools and database under one umbrella, thus reducing the total cost of ownership.","year":2010,"title_abstract":"Foreign Media Collaboration Framework ({FMCF}) The Foreign Media Collaboration Framework (FMCF) is the latest approach by NASIC to provide a comprehensive system to process foreign language materials. FMCF is a Services Oriented Architecture (SOA) that provides an infrastructure to manage HLT tools, products, workflows, and services. This federated SOA solution adheres to DISA's NCES SOA Governance Model, DDMS XML for Metadata Capture\/Dissemination, and IC-ISM for Security. The FMCF provides a cutting edge infrastructure that encapsulates multiple capabilities from multiple vendors in one place. This approach will accelerate HLT development, contain sustainment cost, minimize training, and brings the MT, OCR, ASR, audio\/video, entity extraction, analytic tools and database under one umbrella, thus reducing the total cost of ownership.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.196226418,"Goal":"Partnership for the Goals","Task":["Metadata Capture\/Dissemination","Security","HLT development","training","MT","ASR","entity extraction","analytic tools"],"Method":["Foreign Media Collaboration Framework","Foreign Media Collaboration Framework","NASIC","FMCF","Services Oriented Architecture","federated SOA solution","DISA's NCES SOA Governance Model","DDMS XML","IC - ISM","FMCF","cutting edge infrastructure","OCR","audio\/video"]},{"ID":"jiang-etal-2019-enhancing","title":"Enhancing Air Quality Prediction with Social Media and Natural Language Processing","abstract":"Accompanied by modern industrial developments, air pollution has already become a major concern for human health. Hence, air quality measures, such as the concentration of PM2.5, have attracted increasing attention. Even some studies apply historical measurements into air quality forecast, the changes of air quality conditions are still hard to monitor. In this paper, we propose to exploit social media and natural language processing techniques to enhance air quality prediction. Social media users are treated as social sensors with their findings and locations. After filtering noisy tweets using word selection and topic modeling, a deep learning model based on convolutional neural networks and over-tweet-pooling is proposed to enhance air quality prediction. We conduct experiments on 7-month real-world Twitter datasets in the five most heavily polluted states in the USA. The results show that our approach significantly improves air quality prediction over the baseline that does not use social media by 6.9{\\%} to 17.7{\\%} in macro-F1 scores.","year":2019,"title_abstract":"Enhancing Air Quality Prediction with Social Media and Natural Language Processing Accompanied by modern industrial developments, air pollution has already become a major concern for human health. Hence, air quality measures, such as the concentration of PM2.5, have attracted increasing attention. Even some studies apply historical measurements into air quality forecast, the changes of air quality conditions are still hard to monitor. In this paper, we propose to exploit social media and natural language processing techniques to enhance air quality prediction. Social media users are treated as social sensors with their findings and locations. After filtering noisy tweets using word selection and topic modeling, a deep learning model based on convolutional neural networks and over-tweet-pooling is proposed to enhance air quality prediction. We conduct experiments on 7-month real-world Twitter datasets in the five most heavily polluted states in the USA. The results show that our approach significantly improves air quality prediction over the baseline that does not use social media by 6.9{\\%} to 17.7{\\%} in macro-F1 scores.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1962245703,"Goal":"Climate Action","Task":["Enhancing Air Quality Prediction","air pollution","human health","historical measurements","air quality forecast","air quality conditions","air quality prediction","air quality prediction","air quality prediction"],"Method":["Natural Language Processing","social media and natural language processing techniques","word selection","topic modeling","deep learning model","convolutional neural networks","over - tweet - pooling"]},{"ID":"viegas-1999-developing","title":"Developing knowledge bases for {MT} with linguistically motivated quality-based learning","abstract":"In this paper we present a proposal to help bypass the bottleneck of knowledge-based systems working under the assumption that the knowledge sources are complete. We show how to create, on the fly, new lexicon entries using lexico-semantic rules and how to create new concepts for unknown words, investigating a new linguistically-motivated model to trigger concepts in context.","year":1999,"title_abstract":"Developing knowledge bases for {MT} with linguistically motivated quality-based learning In this paper we present a proposal to help bypass the bottleneck of knowledge-based systems working under the assumption that the knowledge sources are complete. We show how to create, on the fly, new lexicon entries using lexico-semantic rules and how to create new concepts for unknown words, investigating a new linguistically-motivated model to trigger concepts in context.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1959786564,"Goal":"Quality Education","Task":["{MT}"],"Method":["knowledge bases","linguistically motivated quality - based learning","knowledge - based systems","linguistically - motivated model"]},{"ID":"valvoda-etal-2021-precedent","title":"What About the Precedent: An Information-Theoretic Analysis of Common Law","abstract":"In common law, the outcome of a new case is determined mostly by precedent cases, rather than by existing statutes. However, how exactly does the precedent influence the outcome of a new case? Answering this question is crucial for guaranteeing fair and consistent judicial decision-making. We are the first to approach this question computationally by comparing two longstanding jurisprudential views; Halsbury{'}s, who believes that the arguments of the precedent are the main determinant of the outcome, and Goodhart{'}s, who believes that what matters most is the precedent{'}s facts. We base our study on the corpus of legal cases from the European Court of Human Rights (ECtHR), which allows us to access not only the case itself, but also cases cited in the judges{'} arguments (i.e. the precedent cases). Taking an information-theoretic view, and modelling the question as a case out-come classification task, we find that the precedent{'}s arguments share 0.38 nats of information with the case{'}s outcome, whereas precedent{'}s facts only share 0.18 nats of information (i.e.,58{\\%} less); suggesting Halsbury{'}s view may be more accurate in this specific court. We found however in a qualitative analysis that there are specific statues where Goodhart{'}s view dominates, and present some evidence these are the ones where the legal concept at hand is less straightforward.","year":2021,"title_abstract":"What About the Precedent: An Information-Theoretic Analysis of Common Law In common law, the outcome of a new case is determined mostly by precedent cases, rather than by existing statutes. However, how exactly does the precedent influence the outcome of a new case? Answering this question is crucial for guaranteeing fair and consistent judicial decision-making. We are the first to approach this question computationally by comparing two longstanding jurisprudential views; Halsbury{'}s, who believes that the arguments of the precedent are the main determinant of the outcome, and Goodhart{'}s, who believes that what matters most is the precedent{'}s facts. We base our study on the corpus of legal cases from the European Court of Human Rights (ECtHR), which allows us to access not only the case itself, but also cases cited in the judges{'} arguments (i.e. the precedent cases). Taking an information-theoretic view, and modelling the question as a case out-come classification task, we find that the precedent{'}s arguments share 0.38 nats of information with the case{'}s outcome, whereas precedent{'}s facts only share 0.18 nats of information (i.e.,58{\\%} less); suggesting Halsbury{'}s view may be more accurate in this specific court. We found however in a qualitative analysis that there are specific statues where Goodhart{'}s view dominates, and present some evidence these are the ones where the legal concept at hand is less straightforward.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1955388933,"Goal":"Reduced Inequalities","Task":["judicial decision - making","case out - come classification task","qualitative analysis"],"Method":["Information - Theoretic Analysis of Common Law","jurisprudential views;","information - theoretic view"]},{"ID":"bunt-2021-isa","title":"The {ISA}-17 Quantification Challenge: Background and introduction","abstract":"This paper, intended for the ISA-17 Quantification Annotation track, provides background information for the shared quantification annotation task at the ISA-17 workshop, a.k.a. the Quantification Challenge. In particular, the role of the abstract and concrete syntax of the QuantML markup language are explained, and the semantic interpretation of QuantML annotations in relation to the ISO principles of semantic annotation. Additionally, the choice is motivated of the test suite of the Quantification Challenge, along with the suggested markables for the sentences of the suite.","year":2021,"title_abstract":"The {ISA}-17 Quantification Challenge: Background and introduction This paper, intended for the ISA-17 Quantification Annotation track, provides background information for the shared quantification annotation task at the ISA-17 workshop, a.k.a. the Quantification Challenge. In particular, the role of the abstract and concrete syntax of the QuantML markup language are explained, and the semantic interpretation of QuantML annotations in relation to the ISO principles of semantic annotation. Additionally, the choice is motivated of the test suite of the Quantification Challenge, along with the suggested markables for the sentences of the suite.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1953898072,"Goal":"Partnership for the Goals","Task":["shared quantification annotation task","ISA - 17 workshop","Quantification Challenge","semantic interpretation","semantic annotation","Quantification Challenge"],"Method":["QuantML markup language"]},{"ID":"ljubesic-etal-2016-tweetgeo","title":"{T}weet{G}eo - A Tool for Collecting, Processing and Analysing Geo-encoded Linguistic Data","abstract":"In this paper we present a newly developed tool that enables researchers interested in spatial variation of language to define a geographic perimeter of interest, collect data from the Twitter streaming API published in that perimeter, filter the obtained data by language and country, define and extract variables of interest and analyse the extracted variables by one spatial statistic and two spatial visualisations. We showcase the tool on the area and a selection of languages spoken in former Yugoslavia. By defining the perimeter, languages and a series of linguistic variables of interest we demonstrate the data collection, processing and analysis capabilities of the tool.","year":2016,"title_abstract":"{T}weet{G}eo - A Tool for Collecting, Processing and Analysing Geo-encoded Linguistic Data In this paper we present a newly developed tool that enables researchers interested in spatial variation of language to define a geographic perimeter of interest, collect data from the Twitter streaming API published in that perimeter, filter the obtained data by language and country, define and extract variables of interest and analyse the extracted variables by one spatial statistic and two spatial visualisations. We showcase the tool on the area and a selection of languages spoken in former Yugoslavia. By defining the perimeter, languages and a series of linguistic variables of interest we demonstrate the data collection, processing and analysis capabilities of the tool.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1952565163,"Goal":"Sustainable Cities and Communities","Task":["Collecting , Processing and Analysing Geo - encoded Linguistic Data","data collection","processing and analysis capabilities"],"Method":["spatial statistic","spatial visualisations"]},{"ID":"kovatchev-etal-2018-warp","title":"{WARP}-Text: a Web-Based Tool for Annotating Relationships between Pairs of Texts","abstract":"We present WARP-Text, an open-source web-based tool for annotating relationships between pairs of texts. WARP-Text supports multi-layer annotation and custom definitions of inter-textual and intra-textual relationships. Annotation can be performed at different granularity levels (such as sentences, phrases, or tokens). WARP-Text has an intuitive user-friendly interface both for project managers and annotators. WARP-Text fills a gap in the currently available NLP toolbox, as open-source alternatives for annotation of pairs of text are not readily available. WARP-Text has already been used in several annotation tasks and can be of interest to the researchers working in the areas of Paraphrasing, Entailment, Simplification, and Summarization, among others.","year":2018,"title_abstract":"{WARP}-Text: a Web-Based Tool for Annotating Relationships between Pairs of Texts We present WARP-Text, an open-source web-based tool for annotating relationships between pairs of texts. WARP-Text supports multi-layer annotation and custom definitions of inter-textual and intra-textual relationships. Annotation can be performed at different granularity levels (such as sentences, phrases, or tokens). WARP-Text has an intuitive user-friendly interface both for project managers and annotators. WARP-Text fills a gap in the currently available NLP toolbox, as open-source alternatives for annotation of pairs of text are not readily available. WARP-Text has already been used in several annotation tasks and can be of interest to the researchers working in the areas of Paraphrasing, Entailment, Simplification, and Summarization, among others.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.195091188,"Goal":"Partnership for the Goals","Task":["Relationships","relationships","multi - layer annotation","Annotation","project managers","annotators","NLP","annotation of pairs of text","annotation tasks","Paraphrasing","Entailment","Simplification","Summarization"],"Method":["Web - Based Tool","WARP - Text","web - based tool","WARP - Text","WARP - Text","WARP - Text"]},{"ID":"garcia-cumbreras-etal-2019-sinai","title":"{SINAI}-{DL} at {S}em{E}val-2019 Task 7: Data Augmentation and Temporal Expressions","abstract":"This paper describes the participation of the SINAI-DL team at RumourEval (Task 7 in SemEval 2019, subtask A: SDQC). SDQC addresses the challenge of rumour stance classification as an indirect way of identifying potential rumours. Given a tweet with several replies, our system classifies each reply into either supporting, denying, questioning or commenting on the underlying rumours. We have applied data augmentation, temporal expressions labelling and transfer learning with a four-layer neural classifier. We achieve an accuracy of 0.715 with the official run over reply tweets.","year":2019,"title_abstract":"{SINAI}-{DL} at {S}em{E}val-2019 Task 7: Data Augmentation and Temporal Expressions This paper describes the participation of the SINAI-DL team at RumourEval (Task 7 in SemEval 2019, subtask A: SDQC). SDQC addresses the challenge of rumour stance classification as an indirect way of identifying potential rumours. Given a tweet with several replies, our system classifies each reply into either supporting, denying, questioning or commenting on the underlying rumours. We have applied data augmentation, temporal expressions labelling and transfer learning with a four-layer neural classifier. We achieve an accuracy of 0.715 with the official run over reply tweets.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1950652301,"Goal":"Climate Action","Task":["Data Augmentation","Temporal Expressions","rumour stance classification","identifying potential rumours"],"Method":["SDQC)","SDQC","data augmentation","temporal expressions labelling","transfer learning","four - layer neural classifier"]},{"ID":"rashkin-etal-2017-multilingual","title":"Multilingual Connotation Frames: A Case Study on Social Media for Targeted Sentiment Analysis and Forecast","abstract":"People around the globe respond to major real world events through social media. To study targeted public sentiments across many languages and geographic locations, we introduce multilingual connotation frames: an extension from English connotation frames of Rashkin et al. (2016) with 10 additional European languages, focusing on the implied sentiments among event participants engaged in a frame. As a case study, we present large scale analysis on targeted public sentiments toward salient events and entities using 1.2 million multilingual connotation frames extracted from Twitter.","year":2017,"title_abstract":"Multilingual Connotation Frames: A Case Study on Social Media for Targeted Sentiment Analysis and Forecast People around the globe respond to major real world events through social media. To study targeted public sentiments across many languages and geographic locations, we introduce multilingual connotation frames: an extension from English connotation frames of Rashkin et al. (2016) with 10 additional European languages, focusing on the implied sentiments among event participants engaged in a frame. As a case study, we present large scale analysis on targeted public sentiments toward salient events and entities using 1.2 million multilingual connotation frames extracted from Twitter.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.195001334,"Goal":"Climate Action","Task":["Sentiment Analysis","Forecast","large scale analysis"],"Method":["Multilingual Connotation Frames"]},{"ID":"sieinska-etal-2020-conversational","title":"Conversational Agents for Intelligent Buildings","abstract":"We will demonstrate a deployed conversational AI system that acts as a host of a smart-building on a university campus. The system combines open-domain social conversation with task-based conversation regarding navigation in the building, live resource updates (e.g. available computers) and events in the building. We are able to demonstrate the system on several platforms: Google Home devices, Android phones, and a Furhat robot.","year":2020,"title_abstract":"Conversational Agents for Intelligent Buildings We will demonstrate a deployed conversational AI system that acts as a host of a smart-building on a university campus. The system combines open-domain social conversation with task-based conversation regarding navigation in the building, live resource updates (e.g. available computers) and events in the building. We are able to demonstrate the system on several platforms: Google Home devices, Android phones, and a Furhat robot.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1948081404,"Goal":"Sustainable Cities and Communities","Task":["Intelligent Buildings","smart - building","open - domain social conversation","navigation"],"Method":["Conversational Agents","conversational AI system","task - based conversation","Furhat robot"]},{"ID":"ljubesic-etal-2017-language","title":"Language-independent Gender Prediction on {T}witter","abstract":"In this paper we present a set of experiments and analyses on predicting the gender of Twitter users based on language-independent features extracted either from the text or the metadata of users{'} tweets. We perform our experiments on the TwiSty dataset containing manual gender annotations for users speaking six different languages. Our classification results show that, while the prediction model based on language-independent features performs worse than the bag-of-words model when training and testing on the same language, it regularly outperforms the bag-of-words model when applied to different languages, showing very stable results across various languages. Finally we perform a comparative analysis of feature effect sizes across the six languages and show that differences in our features correspond to cultural distances.","year":2017,"title_abstract":"Language-independent Gender Prediction on {T}witter In this paper we present a set of experiments and analyses on predicting the gender of Twitter users based on language-independent features extracted either from the text or the metadata of users{'} tweets. We perform our experiments on the TwiSty dataset containing manual gender annotations for users speaking six different languages. Our classification results show that, while the prediction model based on language-independent features performs worse than the bag-of-words model when training and testing on the same language, it regularly outperforms the bag-of-words model when applied to different languages, showing very stable results across various languages. Finally we perform a comparative analysis of feature effect sizes across the six languages and show that differences in our features correspond to cultural distances.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1947844326,"Goal":"Gender Equality","Task":["Language - independent Gender Prediction","gender of Twitter users","classification"],"Method":["prediction model","language - independent features","bag - of - words model","bag - of - words model"]},{"ID":"griesel-bosch-2020-navigating","title":"Navigating Challenges of Multilingual Resource Development for Under-Resourced Languages: The Case of the {A}frican {W}ordnet Project","abstract":"Creating a new wordnet is by no means a trivial task and when the target language is under-resourced as is the case for the languages currently included in the multilingual African Wordnet (AfWN), developers need to rely heavily on human expertise. During the different phases of development of the AfWN, we incorporated various methods of fast-tracking to ease the tedious and time-consuming work. Some methods have proven effective while others seem to have little positive impact on the work rate. As in the case of many other under-resourced languages, the expand model was implemented throughout, thus depending on English source data such as the English Princeton Wordnet (PWN) which is then translated into the target language with the assumption that the new language shares an underlying structure with the PWN. The paper discusses some problems encountered along the way and points out various possibilities of (semi) automated quality assurance measures and further refinement of the AfWN to ensure accelerated growth. In this paper we aim to highlight some of the lessons learnt from hands-on experience in order to facilitate similar projects, in particular for languages from other African countries.","year":2020,"title_abstract":"Navigating Challenges of Multilingual Resource Development for Under-Resourced Languages: The Case of the {A}frican {W}ordnet Project Creating a new wordnet is by no means a trivial task and when the target language is under-resourced as is the case for the languages currently included in the multilingual African Wordnet (AfWN), developers need to rely heavily on human expertise. During the different phases of development of the AfWN, we incorporated various methods of fast-tracking to ease the tedious and time-consuming work. Some methods have proven effective while others seem to have little positive impact on the work rate. As in the case of many other under-resourced languages, the expand model was implemented throughout, thus depending on English source data such as the English Princeton Wordnet (PWN) which is then translated into the target language with the assumption that the new language shares an underlying structure with the PWN. The paper discusses some problems encountered along the way and points out various possibilities of (semi) automated quality assurance measures and further refinement of the AfWN to ensure accelerated growth. In this paper we aim to highlight some of the lessons learnt from hands-on experience in order to facilitate similar projects, in particular for languages from other African countries.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1947340369,"Goal":"Partnership for the Goals","Task":["Multilingual Resource Development","fast - tracking"],"Method":["AfWN","expand model","PWN","AfWN"]},{"ID":"lugini-litman-2017-predicting","title":"Predicting Specificity in Classroom Discussion","abstract":"High quality classroom discussion is important to student development, enhancing abilities to express claims, reason about other students{'} claims, and retain information for longer periods of time. Previous small-scale studies have shown that one indicator of classroom discussion quality is specificity. In this paper we tackle the problem of predicting specificity for classroom discussions. We propose several methods and feature sets capable of outperforming the state of the art in specificity prediction. Additionally, we provide a set of meaningful, interpretable features that can be used to analyze classroom discussions at a pedagogical level.","year":2017,"title_abstract":"Predicting Specificity in Classroom Discussion High quality classroom discussion is important to student development, enhancing abilities to express claims, reason about other students{'} claims, and retain information for longer periods of time. Previous small-scale studies have shown that one indicator of classroom discussion quality is specificity. In this paper we tackle the problem of predicting specificity for classroom discussions. We propose several methods and feature sets capable of outperforming the state of the art in specificity prediction. Additionally, we provide a set of meaningful, interpretable features that can be used to analyze classroom discussions at a pedagogical level.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1943856776,"Goal":"Quality Education","Task":["Predicting Specificity","Classroom Discussion","classroom discussion","student development","predicting specificity","classroom discussions","specificity prediction"],"Method":["feature sets"]},{"ID":"anastasiou-2012-speech","title":"A Speech and Gesture Spatial Corpus in Assisted Living","abstract":"Ambient Assisted Living (AAL) is the name for a European technology and innovation funding programme. AAL research field is about intelligent assistant systems for a healthier and safer life in the preferred living environments through the use of Information and Communication Technologies (ICT). We focus specifically on speech and gesture interaction which can enhance the quality of lifestyle of people living in assistive environments, be they seniors or people with physical or cognitive disabilities. In this paper we describe our user study conducted in a lab at the University of Bremen in order to collect empirical speech and gesture data and later create and analyse a multimodal corpus. The user study is about a human user sitting in a wheelchair and performing certain inherently spatial tasks.","year":2012,"title_abstract":"A Speech and Gesture Spatial Corpus in Assisted Living Ambient Assisted Living (AAL) is the name for a European technology and innovation funding programme. AAL research field is about intelligent assistant systems for a healthier and safer life in the preferred living environments through the use of Information and Communication Technologies (ICT). We focus specifically on speech and gesture interaction which can enhance the quality of lifestyle of people living in assistive environments, be they seniors or people with physical or cognitive disabilities. In this paper we describe our user study conducted in a lab at the University of Bremen in order to collect empirical speech and gesture data and later create and analyse a multimodal corpus. The user study is about a human user sitting in a wheelchair and performing certain inherently spatial tasks.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1942070723,"Goal":"Sustainable Cities and Communities","Task":["Assisted Living","Ambient Assisted Living","European technology and innovation funding programme","intelligent assistant systems","speech and gesture interaction","inherently spatial tasks"],"Method":["Information and Communication Technologies"]},{"ID":"soria-etal-2016-fostering","title":"Fostering digital representation of {EU} regional and minority languages: the Digital Language Diversity Project","abstract":"Poor digital representation of minority languages further prevents their usability on digital media and devices. The Digital Language Diversity Project, a three-year project funded under the Erasmus+ programme, aims at addressing the problem of low digital representation of EU regional and minority languages by giving their speakers the intellectual an practical skills to create, share, and reuse online digital content. Availability of digital content and technical support to use it are essential prerequisites for the development of language-based digital applications, which in turn can boost digital usage of these languages. In this paper we introduce the project, its aims, objectives and current activities for sustaining digital usability of minority languages through adult education.","year":2016,"title_abstract":"Fostering digital representation of {EU} regional and minority languages: the Digital Language Diversity Project Poor digital representation of minority languages further prevents their usability on digital media and devices. The Digital Language Diversity Project, a three-year project funded under the Erasmus+ programme, aims at addressing the problem of low digital representation of EU regional and minority languages by giving their speakers the intellectual an practical skills to create, share, and reuse online digital content. Availability of digital content and technical support to use it are essential prerequisites for the development of language-based digital applications, which in turn can boost digital usage of these languages. In this paper we introduce the project, its aims, objectives and current activities for sustaining digital usability of minority languages through adult education.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1939753145,"Goal":"Quality Education","Task":["digital representation","Digital Language Diversity Project","Digital Language Diversity Project","low digital representation","language - based digital applications","digital usability of minority languages"],"Method":["digital representation"]},{"ID":"bourgonje-etal-2017-clickbait","title":"From Clickbait to Fake News Detection: An Approach based on Detecting the Stance of Headlines to Articles","abstract":"We present a system for the detection of the stance of headlines with regard to their corresponding article bodies. The approach can be applied in fake news, especially clickbait detection scenarios. The component is part of a larger platform for the curation of digital content; we consider veracity and relevancy an increasingly important part of curating online information. We want to contribute to the debate on how to deal with fake news and related online phenomena with technological means, by providing means to separate related from unrelated headlines and further classifying the related headlines. On a publicly available data set annotated for the stance of headlines with regard to their corresponding article bodies, we achieve a (weighted) accuracy score of 89.59.","year":2017,"title_abstract":"From Clickbait to Fake News Detection: An Approach based on Detecting the Stance of Headlines to Articles We present a system for the detection of the stance of headlines with regard to their corresponding article bodies. The approach can be applied in fake news, especially clickbait detection scenarios. The component is part of a larger platform for the curation of digital content; we consider veracity and relevancy an increasingly important part of curating online information. We want to contribute to the debate on how to deal with fake news and related online phenomena with technological means, by providing means to separate related from unrelated headlines and further classifying the related headlines. On a publicly available data set annotated for the stance of headlines with regard to their corresponding article bodies, we achieve a (weighted) accuracy score of 89.59.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1938452423,"Goal":"Climate Action","Task":["Fake News Detection","detection of the stance of headlines","fake news","clickbait detection scenarios","curation of digital content;","curating online information"],"Method":["Clickbait"]},{"ID":"otto-2018-team","title":"Team {GESIS} Cologne: An all in all sentence-based approach for {FEVER}","abstract":"In this system description of our pipeline to participate at the Fever Shared Task, we describe our sentence-based approach. Throughout all steps of our pipeline, we regarded single sentences as our processing unit. In our IR-Component, we searched in the set of all possible Wikipedia introduction sentences without limiting sentences to a fixed number of relevant documents. In the entailment module, we judged every sentence separately and combined the result of the classifier for the top 5 sentences with the help of an ensemble classifier to make a judgment whether the truth of a statement can be derived from the given claim.","year":2018,"title_abstract":"Team {GESIS} Cologne: An all in all sentence-based approach for {FEVER} In this system description of our pipeline to participate at the Fever Shared Task, we describe our sentence-based approach. Throughout all steps of our pipeline, we regarded single sentences as our processing unit. In our IR-Component, we searched in the set of all possible Wikipedia introduction sentences without limiting sentences to a fixed number of relevant documents. In the entailment module, we judged every sentence separately and combined the result of the classifier for the top 5 sentences with the help of an ensemble classifier to make a judgment whether the truth of a statement can be derived from the given claim.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1938069463,"Goal":"Climate Action","Task":["Fever Shared Task","IR"],"Method":["sentence - based approach","sentence - based approach","entailment module","classifier","ensemble classifier"]},{"ID":"corvey-etal-2012-foundations","title":"Foundations of a Multilayer Annotation Framework for {T}witter Communications During Crisis Events","abstract":"In times of mass emergency, vast amounts of data are generated via computer-mediated communication (CMC) that are difficult to manually collect and organize into a coherent picture. Yet valuable information is broadcast, and can provide useful insight into time- and safety-critical situations if captured and analyzed efficiently and effectively. We describe a natural language processing component of the EPIC (Empowering the Public with Information in Crisis) Project infrastructure, designed to extract linguistic and behavioral information from tweet text to aid in the task of information integration. The system incorporates linguistic annotation, in the form of Named Entity Tagging, as well as behavioral annotations to capture tweets contributing to situational awareness and analyze the information type of the tweet content. We show classification results and describe future integration of these classifiers in the larger EPIC infrastructure.","year":2012,"title_abstract":"Foundations of a Multilayer Annotation Framework for {T}witter Communications During Crisis Events In times of mass emergency, vast amounts of data are generated via computer-mediated communication (CMC) that are difficult to manually collect and organize into a coherent picture. Yet valuable information is broadcast, and can provide useful insight into time- and safety-critical situations if captured and analyzed efficiently and effectively. We describe a natural language processing component of the EPIC (Empowering the Public with Information in Crisis) Project infrastructure, designed to extract linguistic and behavioral information from tweet text to aid in the task of information integration. The system incorporates linguistic annotation, in the form of Named Entity Tagging, as well as behavioral annotations to capture tweets contributing to situational awareness and analyze the information type of the tweet content. We show classification results and describe future integration of these classifiers in the larger EPIC infrastructure.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.193608731,"Goal":"Climate Action","Task":["{T}witter Communications","time - and safety - critical situations","Information in Crisis) Project infrastructure","information integration","linguistic annotation","situational awareness","classification"],"Method":["Multilayer Annotation Framework","computer - mediated communication","natural language processing component","EPIC","Named Entity Tagging","EPIC infrastructure"]},{"ID":"amine-romdhane-etal-2021-sifting","title":"Sifting {F}rench Tweets to Investigate the Impact of Covid-19 in Triggering Intense Anxiety","abstract":"Sifting French Tweets to Investigate the Impact of Covid-19 in Triggering Intense Anxiety. Social media can be leveraged to understand public sentiment and feelings in real-time, and target public health messages based on user interests and emotions. In this paper, we investigate the impact of the COVID-19 pandemic in triggering intense anxiety, relying on messages exchanged on Twitter. More specifically, we provide : i) a quantitative and qualitative analysis of a corpus of tweets in French related to coronavirus, and ii) a pipeline approach (a filtering mechanism followed by Neural Network methods) to satisfactory classify messages expressing intense anxiety on social media, considering the role played by emotions.","year":2021,"title_abstract":"Sifting {F}rench Tweets to Investigate the Impact of Covid-19 in Triggering Intense Anxiety Sifting French Tweets to Investigate the Impact of Covid-19 in Triggering Intense Anxiety. Social media can be leveraged to understand public sentiment and feelings in real-time, and target public health messages based on user interests and emotions. In this paper, we investigate the impact of the COVID-19 pandemic in triggering intense anxiety, relying on messages exchanged on Twitter. More specifically, we provide : i) a quantitative and qualitative analysis of a corpus of tweets in French related to coronavirus, and ii) a pipeline approach (a filtering mechanism followed by Neural Network methods) to satisfactory classify messages expressing intense anxiety on social media, considering the role played by emotions.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1933778524,"Goal":"Climate Action","Task":["Triggering Intense Anxiety","Triggering Intense Anxiety","triggering intense anxiety"],"Method":["Covid - 19","Covid - 19","COVID - 19","pipeline approach","filtering mechanism","Neural Network methods)"]},{"ID":"ahmed-etal-2019-biofid","title":"{BIO}fid Dataset: Publishing a {G}erman Gold Standard for Named Entity Recognition in Historical Biodiversity Literature","abstract":"The Specialized Information Service Biodiversity Research (BIOfid) has been launched to mobilize valuable biological data from printed literature hidden in German libraries for over the past 250 years. In this project, we annotate German texts converted by OCR from historical scientific literature on the biodiversity of plants, birds, moths and butterflies. Our work enables the automatic extraction of biological information previously buried in the mass of papers and volumes. For this purpose, we generated training data for the tasks of Named Entity Recognition (NER) and Taxa Recognition (TR) in biological documents. We use this data to train a number of leading machine learning tools and create a gold standard for TR in biodiversity literature. More specifically, we perform a practical analysis of our newly generated BIOfid dataset through various downstream-task evaluations and establish a new state of the art for TR with 80.23{\\%} F-score. In this sense, our paper lays the foundations for future work in the field of information extraction in biology texts.","year":2019,"title_abstract":"{BIO}fid Dataset: Publishing a {G}erman Gold Standard for Named Entity Recognition in Historical Biodiversity Literature The Specialized Information Service Biodiversity Research (BIOfid) has been launched to mobilize valuable biological data from printed literature hidden in German libraries for over the past 250 years. In this project, we annotate German texts converted by OCR from historical scientific literature on the biodiversity of plants, birds, moths and butterflies. Our work enables the automatic extraction of biological information previously buried in the mass of papers and volumes. For this purpose, we generated training data for the tasks of Named Entity Recognition (NER) and Taxa Recognition (TR) in biological documents. We use this data to train a number of leading machine learning tools and create a gold standard for TR in biodiversity literature. More specifically, we perform a practical analysis of our newly generated BIOfid dataset through various downstream-task evaluations and establish a new state of the art for TR with 80.23{\\%} F-score. In this sense, our paper lays the foundations for future work in the field of information extraction in biology texts.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.193375349,"Goal":"Life on Land","Task":["Named Entity Recognition","automatic extraction of biological information","Named Entity Recognition","Taxa Recognition","TR","downstream - task evaluations","TR","information extraction in biology texts"],"Method":["OCR","machine learning tools"]},{"ID":"mille-etal-2020-case","title":"A Case Study of {NLG} from Multimedia Data Sources: Generating Architectural Landmark Descriptions","abstract":"In this paper, we present a pipeline system that generates architectural landmark descriptions using textual, visual and structured data. The pipeline comprises five main components:(i) a textual analysis component, which extracts information from Wikipedia pages; (ii)a visual analysis component, which extracts information from copyright-free images; (iii) a retrieval component, which gathers relevant (property, subject, object) triples from DBpedia; (iv) a fusion component, which stores the contents from the different modalities in a Knowledge Base (KB) and resolves the conflicts that stem from using different sources of information; (v) an NLG component, which verbalises the resulting contents of the KB. We show that thanks to the addition of other modalities, we can make the verbalisation of DBpedia triples more relevant and\/or inspirational.","year":2020,"title_abstract":"A Case Study of {NLG} from Multimedia Data Sources: Generating Architectural Landmark Descriptions In this paper, we present a pipeline system that generates architectural landmark descriptions using textual, visual and structured data. The pipeline comprises five main components:(i) a textual analysis component, which extracts information from Wikipedia pages; (ii)a visual analysis component, which extracts information from copyright-free images; (iii) a retrieval component, which gathers relevant (property, subject, object) triples from DBpedia; (iv) a fusion component, which stores the contents from the different modalities in a Knowledge Base (KB) and resolves the conflicts that stem from using different sources of information; (v) an NLG component, which verbalises the resulting contents of the KB. We show that thanks to the addition of other modalities, we can make the verbalisation of DBpedia triples more relevant and\/or inspirational.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1933123767,"Goal":"Sustainable Cities and Communities","Task":["Architectural Landmark Descriptions","architectural landmark descriptions"],"Method":["pipeline system","textual analysis component","visual analysis component","retrieval component","fusion component","NLG component","KB"]},{"ID":"navarretta-etal-2004-human","title":"{``}Human Language Technology Elements in a Knowledge Organisation System - The {VID} Project{''}","abstract":"This paper describes how Human Language Technologies and linguistic resources are used to support the construction of components of a knowledge organisation system. In particular we focus on methodologies and resources for building a corpus-based domain ontology and extracting relevant metadata information for text chunks from domain-specific corpora.","year":2004,"title_abstract":"{``}Human Language Technology Elements in a Knowledge Organisation System - The {VID} Project{''} This paper describes how Human Language Technologies and linguistic resources are used to support the construction of components of a knowledge organisation system. In particular we focus on methodologies and resources for building a corpus-based domain ontology and extracting relevant metadata information for text chunks from domain-specific corpora.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.1933111846,"Goal":"Life Below Water","Task":["corpus - based domain ontology"],"Method":["Language Technology Elements","Knowledge Organisation System","Human Language Technologies","linguistic resources","knowledge organisation system"]},{"ID":"veeman-etal-2020-cross","title":"Cross-lingual Embeddings Reveal Universal and Lineage-Specific Patterns in Grammatical Gender Assignment","abstract":"Grammatical gender is assigned to nouns differently in different languages. Are all factors that influence gender assignment idiosyncratic to languages or are there any that are universal? Using cross-lingual aligned word embeddings, we perform two experiments to address these questions about language typology and human cognition. In both experiments, we predict the gender of nouns in language X using a classifier trained on the nouns of language Y, and take the classifier{'}s accuracy as a measure of transferability of gender systems. First, we show that for 22 Indo-European languages the transferability decreases as the phylogenetic distance increases. This correlation supports the claim that some gender assignment factors are idiosyncratic, and as the languages diverge, the proportion of shared inherited idiosyncrasies diminishes. Second, we show that when the classifier is trained on two Afro-Asiatic languages and tested on the same 22 Indo-European languages (or vice versa), its performance is still significantly above the chance baseline, thus showing that universal factors exist and, moreover, can be captured by word embeddings. When the classifier is tested across families and on inanimate nouns only, the performance is still above baseline, indicating that the universal factors are not limited to biological sex.","year":2020,"title_abstract":"Cross-lingual Embeddings Reveal Universal and Lineage-Specific Patterns in Grammatical Gender Assignment Grammatical gender is assigned to nouns differently in different languages. Are all factors that influence gender assignment idiosyncratic to languages or are there any that are universal? Using cross-lingual aligned word embeddings, we perform two experiments to address these questions about language typology and human cognition. In both experiments, we predict the gender of nouns in language X using a classifier trained on the nouns of language Y, and take the classifier{'}s accuracy as a measure of transferability of gender systems. First, we show that for 22 Indo-European languages the transferability decreases as the phylogenetic distance increases. This correlation supports the claim that some gender assignment factors are idiosyncratic, and as the languages diverge, the proportion of shared inherited idiosyncrasies diminishes. Second, we show that when the classifier is trained on two Afro-Asiatic languages and tested on the same 22 Indo-European languages (or vice versa), its performance is still significantly above the chance baseline, thus showing that universal factors exist and, moreover, can be captured by word embeddings. When the classifier is tested across families and on inanimate nouns only, the performance is still above baseline, indicating that the universal factors are not limited to biological sex.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1932703704,"Goal":"Gender Equality","Task":["Grammatical Gender Assignment","gender assignment","language typology","human cognition","transferability of gender systems","transferability"],"Method":["Cross - lingual Embeddings","classifier","classifier","word embeddings","classifier"]},{"ID":"dunn-etal-2020-measuring","title":"Measuring Linguistic Diversity During {COVID}-19","abstract":"Computational measures of linguistic diversity help us understand the linguistic landscape using digital language data. The contribution of this paper is to calibrate measures of linguistic diversity using restrictions on international travel resulting from the COVID-19 pandemic. Previous work has mapped the distribution of languages using geo-referenced social media and web data. The goal, however, has been to describe these corpora themselves rather than to make inferences about underlying populations. This paper shows that a difference-in-differences method based on the Herfindahl-Hirschman Index can identify the bias in digital corpora that is introduced by non-local populations. These methods tell us where significant changes have taken place and whether this leads to increased or decreased diversity. This is an important step in aligning digital corpora like social media with the real-world populations that have produced them.","year":2020,"title_abstract":"Measuring Linguistic Diversity During {COVID}-19 Computational measures of linguistic diversity help us understand the linguistic landscape using digital language data. The contribution of this paper is to calibrate measures of linguistic diversity using restrictions on international travel resulting from the COVID-19 pandemic. Previous work has mapped the distribution of languages using geo-referenced social media and web data. The goal, however, has been to describe these corpora themselves rather than to make inferences about underlying populations. This paper shows that a difference-in-differences method based on the Herfindahl-Hirschman Index can identify the bias in digital corpora that is introduced by non-local populations. These methods tell us where significant changes have taken place and whether this leads to increased or decreased diversity. This is an important step in aligning digital corpora like social media with the real-world populations that have produced them.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1931847781,"Goal":"Reduced Inequalities","Task":["Measuring Linguistic Diversity"],"Method":["difference - in - differences method","Herfindahl - Hirschman Index"]},{"ID":"koolen-van-cranenburgh-2017-stereotypes","title":"These are not the Stereotypes You are Looking For: Bias and Fairness in Authorial Gender Attribution","abstract":"Stylometric and text categorization results show that author gender can be discerned in texts with relatively high accuracy. However, it is difficult to explain what gives rise to these results and there are many possible confounding factors, such as the domain, genre, and target audience of a text. More fundamentally, such classification efforts risk invoking stereotyping and essentialism. We explore this issue in two datasets of Dutch literary novels, using commonly used descriptive (LIWC, topic modeling) and predictive (machine learning) methods. Our results show the importance of controlling for variables in the corpus and we argue for taking care not to overgeneralize from the results.","year":2017,"title_abstract":"These are not the Stereotypes You are Looking For: Bias and Fairness in Authorial Gender Attribution Stylometric and text categorization results show that author gender can be discerned in texts with relatively high accuracy. However, it is difficult to explain what gives rise to these results and there are many possible confounding factors, such as the domain, genre, and target audience of a text. More fundamentally, such classification efforts risk invoking stereotyping and essentialism. We explore this issue in two datasets of Dutch literary novels, using commonly used descriptive (LIWC, topic modeling) and predictive (machine learning) methods. Our results show the importance of controlling for variables in the corpus and we argue for taking care not to overgeneralize from the results.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1931809485,"Goal":"Gender Equality","Task":["Authorial Gender Attribution","Stylometric","text categorization"],"Method":["classification efforts","descriptive","topic modeling)","predictive (machine learning) methods"]},{"ID":"sahlgren-olsson-2019-gender","title":"Gender Bias in Pretrained {S}wedish Embeddings","abstract":"This paper investigates the presence of gender bias in pretrained Swedish embeddings. We focus on a scenario where names are matched with occupations, and we demonstrate how a number of standard pretrained embeddings handle this task. Our experiments show some significant differences between the pretrained embeddings, with word-based methods showing the most bias and contextualized language models showing the least. We also demonstrate that the previously proposed debiasing method does not affect the performance of the various embeddings in this scenario.","year":2019,"title_abstract":"Gender Bias in Pretrained {S}wedish Embeddings This paper investigates the presence of gender bias in pretrained Swedish embeddings. We focus on a scenario where names are matched with occupations, and we demonstrate how a number of standard pretrained embeddings handle this task. Our experiments show some significant differences between the pretrained embeddings, with word-based methods showing the most bias and contextualized language models showing the least. We also demonstrate that the previously proposed debiasing method does not affect the performance of the various embeddings in this scenario.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1930797845,"Goal":"Gender Equality","Task":["Gender Bias"],"Method":["Pretrained {S}wedish Embeddings","pretrained embeddings","pretrained embeddings","word - based methods","contextualized language models","debiasing method"]},{"ID":"osorio-etal-2020-supervised","title":"Supervised Event Coding from Text Written in {A}rabic: Introducing Hadath","abstract":"This article introduces Hadath, a supervised protocol for coding event data from text written in Arabic. Hadath contributes to recent efforts in advancing multi-language event coding using computer-based solutions. In this application, we focus on extracting event data about the conflict in Afghanistan from 2008 to 2018 using Arabic information sources. The implementation relies first on a Machine Learning algorithm to classify news stories relevant to the Afghan conflict. Then, using Hadath, we implement the Natural Language Processing component for event coding from Arabic script. The output database contains daily geo-referenced information at the district level on who did what to whom, when and where in the Afghan conflict. The data helps to identify trends in the dynamics of violence, the provision of governance, and traditional conflict resolution in Afghanistan for different actors over time and across space.","year":2020,"title_abstract":"Supervised Event Coding from Text Written in {A}rabic: Introducing Hadath This article introduces Hadath, a supervised protocol for coding event data from text written in Arabic. Hadath contributes to recent efforts in advancing multi-language event coding using computer-based solutions. In this application, we focus on extracting event data about the conflict in Afghanistan from 2008 to 2018 using Arabic information sources. The implementation relies first on a Machine Learning algorithm to classify news stories relevant to the Afghan conflict. Then, using Hadath, we implement the Natural Language Processing component for event coding from Arabic script. The output database contains daily geo-referenced information at the district level on who did what to whom, when and where in the Afghan conflict. The data helps to identify trends in the dynamics of violence, the provision of governance, and traditional conflict resolution in Afghanistan for different actors over time and across space.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1930480003,"Goal":"Sustainable Cities and Communities","Task":["Supervised Event Coding","coding event data","multi - language event coding","event coding","governance","conflict resolution"],"Method":["Hadath","Hadath","supervised protocol","Hadath","computer - based solutions","Machine Learning algorithm","Hadath","Natural Language Processing component"]},{"ID":"simoes-almeida-2006-t2o","title":"{T}2{O} - Recycling Thesauri into a Multilingual Ontology","abstract":"In this article we present T2O - a workbench to assist the process of translating heterogeneous resources into ontologies, to enrich and add multilingual information, to help programming with them, and to support ontology publishing. T2O is an ontology algebra.","year":2006,"title_abstract":"{T}2{O} - Recycling Thesauri into a Multilingual Ontology In this article we present T2O - a workbench to assist the process of translating heterogeneous resources into ontologies, to enrich and add multilingual information, to help programming with them, and to support ontology publishing. T2O is an ontology algebra.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1929987371,"Goal":"Partnership for the Goals","Task":["translating heterogeneous resources","ontology publishing"],"Method":["T2O","T2O","ontology algebra"]},{"ID":"steingrimsson-etal-2020-igc","title":"{IGC}-Parl: {I}celandic Corpus of Parliamentary Proceedings","abstract":"We describe the acquisition, annotation and encoding of the corpus of the Althingi parliamentary proceedings. The first version of the corpus includes speeches from 1911-2019. It comprises 406 thousand speeches and over 219 million words. The corpus has been automatically part-of-speech tagged and lemmatised. It is annotated with extensive metadata about the speeches, speakers and political parties, including speech topic, whether the speaker is in the government coalition or opposition, age and gender of speaker at the time of delivery, references to sound and video recordings and more. The corpus is encoded in accordance with the Text Encoding Initiative (TEI) Guidelines and conforms to the Parla-CLARIN schema. We plan to update the corpus annually and its major versions will be archived in the CLARIN.IS repository. It is available for download and search using the KORP concordance tool. Furthermore, information on word frequency are accessible in a custom made web application and an n-gram viewer.","year":2020,"title_abstract":"{IGC}-Parl: {I}celandic Corpus of Parliamentary Proceedings We describe the acquisition, annotation and encoding of the corpus of the Althingi parliamentary proceedings. The first version of the corpus includes speeches from 1911-2019. It comprises 406 thousand speeches and over 219 million words. The corpus has been automatically part-of-speech tagged and lemmatised. It is annotated with extensive metadata about the speeches, speakers and political parties, including speech topic, whether the speaker is in the government coalition or opposition, age and gender of speaker at the time of delivery, references to sound and video recordings and more. The corpus is encoded in accordance with the Text Encoding Initiative (TEI) Guidelines and conforms to the Parla-CLARIN schema. We plan to update the corpus annually and its major versions will be archived in the CLARIN.IS repository. It is available for download and search using the KORP concordance tool. Furthermore, information on word frequency are accessible in a custom made web application and an n-gram viewer.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.1929805279,"Goal":"Peace, Justice and Strong Institutions","Task":["annotation","encoding","search"],"Method":["Parla - CLARIN schema","KORP concordance tool","web application","n - gram viewer"]},{"ID":"shaar-etal-2020-known","title":"That is a Known Lie: Detecting Previously Fact-Checked Claims","abstract":"The recent proliferation of {''}fake news{''} has triggered a number of responses, most notably the emergence of several manual fact-checking initiatives. As a result and over time, a large number of fact-checked claims have been accumulated, which increases the likelihood that a new claim in social media or a new statement by a politician might have already been fact-checked by some trusted fact-checking organization, as viral claims often come back after a while in social media, and politicians like to repeat their favorite statements, true or false, over and over again. As manual fact-checking is very time-consuming (and fully automatic fact-checking has credibility issues), it is important to try to save this effort and to avoid wasting time on claims that have already been fact-checked. Interestingly, despite the importance of the task, it has been largely ignored by the research community so far. Here, we aim to bridge this gap. In particular, we formulate the task and we discuss how it relates to, but also differs from, previous work. We further create a specialized dataset, which we release to the research community. Finally, we present learning-to-rank experiments that demonstrate sizable improvements over state-of-the-art retrieval and textual similarity approaches.","year":2020,"title_abstract":"That is a Known Lie: Detecting Previously Fact-Checked Claims The recent proliferation of {''}fake news{''} has triggered a number of responses, most notably the emergence of several manual fact-checking initiatives. As a result and over time, a large number of fact-checked claims have been accumulated, which increases the likelihood that a new claim in social media or a new statement by a politician might have already been fact-checked by some trusted fact-checking organization, as viral claims often come back after a while in social media, and politicians like to repeat their favorite statements, true or false, over and over again. As manual fact-checking is very time-consuming (and fully automatic fact-checking has credibility issues), it is important to try to save this effort and to avoid wasting time on claims that have already been fact-checked. Interestingly, despite the importance of the task, it has been largely ignored by the research community so far. Here, we aim to bridge this gap. In particular, we formulate the task and we discuss how it relates to, but also differs from, previous work. We further create a specialized dataset, which we release to the research community. Finally, we present learning-to-rank experiments that demonstrate sizable improvements over state-of-the-art retrieval and textual similarity approaches.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1929100752,"Goal":"Climate Action","Task":["Detecting Previously Fact - Checked Claims","manual fact - checking initiatives","manual fact - checking","automatic fact - checking"],"Method":["fact - checking organization","retrieval and textual similarity approaches"]},{"ID":"chang-etal-2019-bias","title":"Bias and Fairness in Natural Language Processing","abstract":"Recent advances in data-driven machine learning techniques (e.g., deep neural networks) have revolutionized many natural language processing applications. These approaches automatically learn how to make decisions based on the statistics and diagnostic information from large amounts of training data. Despite the remarkable accuracy of machine learning in various applications, learning algorithms run the risk of relying on societal biases encoded in the training data to make predictions. This often occurs even when gender and ethnicity information is not explicitly provided to the system because learning algorithms are able to discover implicit associations between individuals and their demographic information based on other variables such as names, titles, home addresses, etc. Therefore, machine learning algorithms risk potentially encouraging unfair and discriminatory decision making and raise serious privacy concerns. Without properly quantifying and reducing the reliance on such correlations, broad adoption of these models might have the undesirable effect of magnifying harmful stereotypes or implicit biases that rely on sensitive demographic attributes.In this tutorial, we will review the history of bias and fairness studies in machine learning and language processing and present recent community effort in quantifying and mitigating bias in natural language processing models for a wide spectrum of tasks, including word embeddings, co-reference resolution, machine translation, and vision-and-language tasks. In particular, we will focus on the following topics:+ Definitions of fairness and bias.+ Data, algorithms, and models that propagate and even amplify social bias to NLP applications and metrics to quantify these biases.+ Algorithmic solutions; learning objective; design principles to prevent social bias in NLP systems and their potential drawbacks.The tutorial will bring researchers and practitioners to be aware of this issue, and encourage the research community to propose innovative solutions to promote fairness in NLP.","year":2019,"title_abstract":"Bias and Fairness in Natural Language Processing Recent advances in data-driven machine learning techniques (e.g., deep neural networks) have revolutionized many natural language processing applications. These approaches automatically learn how to make decisions based on the statistics and diagnostic information from large amounts of training data. Despite the remarkable accuracy of machine learning in various applications, learning algorithms run the risk of relying on societal biases encoded in the training data to make predictions. This often occurs even when gender and ethnicity information is not explicitly provided to the system because learning algorithms are able to discover implicit associations between individuals and their demographic information based on other variables such as names, titles, home addresses, etc. Therefore, machine learning algorithms risk potentially encouraging unfair and discriminatory decision making and raise serious privacy concerns. Without properly quantifying and reducing the reliance on such correlations, broad adoption of these models might have the undesirable effect of magnifying harmful stereotypes or implicit biases that rely on sensitive demographic attributes.In this tutorial, we will review the history of bias and fairness studies in machine learning and language processing and present recent community effort in quantifying and mitigating bias in natural language processing models for a wide spectrum of tasks, including word embeddings, co-reference resolution, machine translation, and vision-and-language tasks. In particular, we will focus on the following topics:+ Definitions of fairness and bias.+ Data, algorithms, and models that propagate and even amplify social bias to NLP applications and metrics to quantify these biases.+ Algorithmic solutions; learning objective; design principles to prevent social bias in NLP systems and their potential drawbacks.The tutorial will bring researchers and practitioners to be aware of this issue, and encourage the research community to propose innovative solutions to promote fairness in NLP.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1928879619,"Goal":"Gender Equality","Task":["Bias and Fairness","Natural Language Processing","natural language processing applications","unfair and discriminatory decision making","bias and fairness studies","machine learning","language processing","bias","word embeddings","co - reference resolution","machine translation","vision - and - language tasks","bias","NLP applications","NLP","NLP"],"Method":["data - driven machine learning techniques","deep neural networks)","machine learning","learning algorithms","learning algorithms","machine learning algorithms","natural language processing models","Algorithmic solutions;","learning objective; design principles"]},{"ID":"pustejovsky-moszkowicz-2012-role","title":"The Role of Model Testing in Standards Development: The Case of {ISO}-Space","abstract":"In this paper, we describe the methodology being used to develop certain aspects of ISO-Space, an annotation language for encoding spatial and spatiotemporal information as expressed in natural language text. After reviewing the requirements of a specification for capturing such knowledge from linguistic descriptions, we describe how ISO-Space has developed to meet the needs of the specification. ISO-Space is an emerging resource that is being developed in the context of an iterative effort to test the specification model with annotation, a methodology called MAMA (Model-Annotate-Model-Annotate) (Pustejovsky and Stubbs, 2012). We describe the genres of text that are being used in a pilot annotation study, in order to both refine and enrich the specification language by way of crowd sourcing simple annotation tasks with Amazon's Mechanical Turk Service.","year":2012,"title_abstract":"The Role of Model Testing in Standards Development: The Case of {ISO}-Space In this paper, we describe the methodology being used to develop certain aspects of ISO-Space, an annotation language for encoding spatial and spatiotemporal information as expressed in natural language text. After reviewing the requirements of a specification for capturing such knowledge from linguistic descriptions, we describe how ISO-Space has developed to meet the needs of the specification. ISO-Space is an emerging resource that is being developed in the context of an iterative effort to test the specification model with annotation, a methodology called MAMA (Model-Annotate-Model-Annotate) (Pustejovsky and Stubbs, 2012). We describe the genres of text that are being used in a pilot annotation study, in order to both refine and enrich the specification language by way of crowd sourcing simple annotation tasks with Amazon's Mechanical Turk Service.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1927617639,"Goal":"Sustainable Cities and Communities","Task":["Model Testing","Standards Development","encoding spatial and spatiotemporal information","annotation","annotation study","annotation tasks"],"Method":["{ISO} - Space","ISO - Space","annotation language","ISO - Space","specification model","MAMA (Model - Annotate - Model - Annotate)"]},{"ID":"jiang-fellbaum-2020-interdependencies","title":"Interdependencies of Gender and Race in Contextualized Word Embeddings","abstract":"Recent years have seen a surge in research on the biases in word embeddings with respect to gender and, to a lesser extent, race. Few of these studies, however, have given attention to the critical intersection of race and gender. In this case study, we analyze the dimensions of gender and race in contextualized word embeddings of given names, taken from BERT, and investigate the nature and nuance of their interaction. We find that these demographic axes, though typically treated as physically and conceptually separate, are in fact interdependent and thus inadvisable to consider in isolation. Further, we show that demographic dimensions predicated on default settings in language, such as in pronouns, may risk rendering groups with multiple marginalized identities invisible. We conclude by discussing the importance and implications of intersectionality for future studies on bias and debiasing in NLP.","year":2020,"title_abstract":"Interdependencies of Gender and Race in Contextualized Word Embeddings Recent years have seen a surge in research on the biases in word embeddings with respect to gender and, to a lesser extent, race. Few of these studies, however, have given attention to the critical intersection of race and gender. In this case study, we analyze the dimensions of gender and race in contextualized word embeddings of given names, taken from BERT, and investigate the nature and nuance of their interaction. We find that these demographic axes, though typically treated as physically and conceptually separate, are in fact interdependent and thus inadvisable to consider in isolation. Further, we show that demographic dimensions predicated on default settings in language, such as in pronouns, may risk rendering groups with multiple marginalized identities invisible. We conclude by discussing the importance and implications of intersectionality for future studies on bias and debiasing in NLP.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1927583814,"Goal":"Gender Equality","Task":["bias","debiasing","NLP"],"Method":["Contextualized Word Embeddings"]},{"ID":"hassan-etal-2021-unpacking-interdependent","title":"Unpacking the Interdependent Systems of Discrimination: Ableist Bias in {NLP} Systems through an Intersectional Lens","abstract":"Much of the world{'}s population experiences some form of disability during their lifetime. Caution must be exercised while designing natural language processing (NLP) systems to prevent systems from inadvertently perpetuating ableist bias against people with disabilities, i.e., prejudice that favors those with typical abilities. We report on various analyses based on word predictions of a large-scale BERT language model. Statistically significant results demonstrate that people with disabilities can be disadvantaged. Findings also explore overlapping forms of discrimination related to interconnected gender and race identities.","year":2021,"title_abstract":"Unpacking the Interdependent Systems of Discrimination: Ableist Bias in {NLP} Systems through an Intersectional Lens Much of the world{'}s population experiences some form of disability during their lifetime. Caution must be exercised while designing natural language processing (NLP) systems to prevent systems from inadvertently perpetuating ableist bias against people with disabilities, i.e., prejudice that favors those with typical abilities. We report on various analyses based on word predictions of a large-scale BERT language model. Statistically significant results demonstrate that people with disabilities can be disadvantaged. Findings also explore overlapping forms of discrimination related to interconnected gender and race identities.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1926572919,"Goal":"Reduced Inequalities","Task":["Interdependent Systems of Discrimination","natural language processing","discrimination"],"Method":["Intersectional Lens","word predictions","large - scale BERT language model"]},{"ID":"can-etal-2021-uetrice","title":"{UET}rice at {MEDIQA} 2021: A Prosper-thy-neighbour Extractive Multi-document Summarization Model","abstract":"This paper describes a system developed to summarize multiple answers challenge in the MEDIQA 2021 shared task collocated with the BioNLP 2021 Workshop. We propose an extractive summarization architecture based on several scores and state-of-the-art techniques. We also present our novel prosper-thy-neighbour strategies to improve performance. Our model has been proven to be effective with the best ROUGE-1\/ROUGE-L scores, being the shared task runner up by ROUGE-2 F1 score (over 13 participated teams).","year":2021,"title_abstract":"{UET}rice at {MEDIQA} 2021: A Prosper-thy-neighbour Extractive Multi-document Summarization Model This paper describes a system developed to summarize multiple answers challenge in the MEDIQA 2021 shared task collocated with the BioNLP 2021 Workshop. We propose an extractive summarization architecture based on several scores and state-of-the-art techniques. We also present our novel prosper-thy-neighbour strategies to improve performance. Our model has been proven to be effective with the best ROUGE-1\/ROUGE-L scores, being the shared task runner up by ROUGE-2 F1 score (over 13 participated teams).","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1926163882,"Goal":"Partnership for the Goals","Task":["Summarization","multiple answers challenge","MEDIQA"],"Method":["Prosper - thy - neighbour","extractive summarization architecture","prosper - thy - neighbour strategies"]},{"ID":"loveys-etal-2018-cross","title":"Cross-cultural differences in language markers of depression online","abstract":"Depression is a global mental health condition that affects all cultures. Despite this, the way depression is expressed varies by culture. Uptake of machine learning technology for diagnosing mental health conditions means that increasingly more depression classifiers are created from online language data. Yet, culture is rarely considered as a factor affecting online language in this literature. This study explores cultural differences in online language data of users with depression. Written language data from 1,593 users with self-reported depression from the online peer support community 7 Cups of Tea was analyzed using the Linguistic Inquiry and Word Count (LIWC), topic modeling, data visualization, and other techniques. We compared the language of users identifying as White, Black or African American, Hispanic or Latino, and Asian or Pacific Islander. Exploratory analyses revealed cross-cultural differences in depression expression in online language data, particularly in relation to emotion expression, cognition, and functioning. The results have important implications for avoiding depression misclassification from machine-driven assessments when used in a clinical setting, and for avoiding inadvertent cultural biases in this line of research more broadly.","year":2018,"title_abstract":"Cross-cultural differences in language markers of depression online Depression is a global mental health condition that affects all cultures. Despite this, the way depression is expressed varies by culture. Uptake of machine learning technology for diagnosing mental health conditions means that increasingly more depression classifiers are created from online language data. Yet, culture is rarely considered as a factor affecting online language in this literature. This study explores cultural differences in online language data of users with depression. Written language data from 1,593 users with self-reported depression from the online peer support community 7 Cups of Tea was analyzed using the Linguistic Inquiry and Word Count (LIWC), topic modeling, data visualization, and other techniques. We compared the language of users identifying as White, Black or African American, Hispanic or Latino, and Asian or Pacific Islander. Exploratory analyses revealed cross-cultural differences in depression expression in online language data, particularly in relation to emotion expression, cognition, and functioning. The results have important implications for avoiding depression misclassification from machine-driven assessments when used in a clinical setting, and for avoiding inadvertent cultural biases in this line of research more broadly.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1925782412,"Goal":"Reduced Inequalities","Task":["language markers of depression","online Depression","diagnosing mental health conditions","depression classifiers","data visualization","depression expression","emotion expression","cognition","depression misclassification","machine - driven assessments","clinical setting"],"Method":["machine learning technology","Linguistic Inquiry and Word Count","topic modeling"]},{"ID":"moneglia-etal-2014-imagact","title":"The {IMAGACT} Visual Ontology. An Extendable Multilingual Infrastructure for the representation of lexical encoding of Action","abstract":"Action verbs have many meanings, covering actions in different ontological types. Moreover, each language categorizes action in its own way. One verb can refer to many different actions and one action can be identified by more than one verb. The range of variations within and across languages is largely unknown, causing trouble for natural language processing tasks. IMAGACT is a corpus-based ontology of action concepts, derived from English and Italian spontaneous speech corpora, which makes use of the universal language of images to identify the different action types extended by verbs referring to action in English, Italian, Chinese and Spanish. This paper presents the infrastructure and the various linguistic information the user can derive from it. IMAGACT makes explicit the variation of meaning of action verbs within one language and allows comparisons of verb variations within and across languages. Because the action concepts are represented with videos, extension into new languages beyond those presently implemented in IMAGACT is done using competence-based judgments by mother-tongue informants without intense lexicographic work involving underdetermined semantic description","year":2014,"title_abstract":"The {IMAGACT} Visual Ontology. An Extendable Multilingual Infrastructure for the representation of lexical encoding of Action Action verbs have many meanings, covering actions in different ontological types. Moreover, each language categorizes action in its own way. One verb can refer to many different actions and one action can be identified by more than one verb. The range of variations within and across languages is largely unknown, causing trouble for natural language processing tasks. IMAGACT is a corpus-based ontology of action concepts, derived from English and Italian spontaneous speech corpora, which makes use of the universal language of images to identify the different action types extended by verbs referring to action in English, Italian, Chinese and Spanish. This paper presents the infrastructure and the various linguistic information the user can derive from it. IMAGACT makes explicit the variation of meaning of action verbs within one language and allows comparisons of verb variations within and across languages. Because the action concepts are represented with videos, extension into new languages beyond those presently implemented in IMAGACT is done using competence-based judgments by mother-tongue informants without intense lexicographic work involving underdetermined semantic description","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1925637722,"Goal":"Climate Action","Task":["lexical encoding of Action Action verbs","natural language processing tasks"],"Method":["{IMAGACT} Visual Ontology","Extendable Multilingual Infrastructure","IMAGACT","corpus - based ontology of action concepts","IMAGACT","IMAGACT","competence - based judgments"]},{"ID":"jurgens-etal-2019-just","title":"A Just and Comprehensive Strategy for Using {NLP} to Address Online Abuse","abstract":"Online abusive behavior affects millions and the NLP community has attempted to mitigate this problem by developing technologies to detect abuse. However, current methods have largely focused on a narrow definition of abuse to detriment of victims who seek both validation and solutions. In this position paper, we argue that the community needs to make three substantive changes: (1) expanding our scope of problems to tackle both more subtle and more serious forms of abuse, (2) developing proactive technologies that counter or inhibit abuse before it harms, and (3) reframing our effort within a framework of justice to promote healthy communities.","year":2019,"title_abstract":"A Just and Comprehensive Strategy for Using {NLP} to Address Online Abuse Online abusive behavior affects millions and the NLP community has attempted to mitigate this problem by developing technologies to detect abuse. However, current methods have largely focused on a narrow definition of abuse to detriment of victims who seek both validation and solutions. In this position paper, we argue that the community needs to make three substantive changes: (1) expanding our scope of problems to tackle both more subtle and more serious forms of abuse, (2) developing proactive technologies that counter or inhibit abuse before it harms, and (3) reframing our effort within a framework of justice to promote healthy communities.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.1923295557,"Goal":"Peace, Justice and Strong Institutions","Task":["Online Abuse","NLP community"],"Method":["{NLP}","proactive technologies"]},{"ID":"singh-etal-2021-cfilt","title":"{CFILT} {IIT} {B}ombay@{LT}-{EDI}-{EACL}2021: Hope Speech Detection for Equality, Diversity, and Inclusion using Multilingual Representation from{T}ransformers","abstract":"With the internet becoming part and parcel of our lives, engagement in social media has increased a lot. Identifying and eliminating offensive content from social media has become of utmost priority to prevent any kind of violence. However, detecting encouraging, supportive and positive content is equally important to prevent misuse of censorship targeted to attack freedom of speech. This paper presents our system for the shared task Hope Speech Detection for Equality, Diversity, and Inclusion at LT-EDI, EACL 2021. The data for this shared task is provided in English, Tamil, and Malayalam which was collected from YouTube comments. It is a multiclass classification problem where each data instance is categorized into one of the three classes: {`}Hope speech{'}, {`}Not hope speech{'}, and {`}Not in intended language{'}. We propose a system that employs multilingual transformer models to obtain the representation of text and classifies it into one of the three classes. We explored the use of multilingual models trained specifically for Indian languages along with generic multilingual models. Our system was ranked 2nd for English, 2nd for Malayalam, and 7th for the Tamil language in the final leader board published by organizers and obtained a weighted F1-score of 0.92, 0.84, 0.55 respectively on the hidden test dataset used for the competition. We have made our system publicly available at GitHub.","year":2021,"title_abstract":"{CFILT} {IIT} {B}ombay@{LT}-{EDI}-{EACL}2021: Hope Speech Detection for Equality, Diversity, and Inclusion using Multilingual Representation from{T}ransformers With the internet becoming part and parcel of our lives, engagement in social media has increased a lot. Identifying and eliminating offensive content from social media has become of utmost priority to prevent any kind of violence. However, detecting encouraging, supportive and positive content is equally important to prevent misuse of censorship targeted to attack freedom of speech. This paper presents our system for the shared task Hope Speech Detection for Equality, Diversity, and Inclusion at LT-EDI, EACL 2021. The data for this shared task is provided in English, Tamil, and Malayalam which was collected from YouTube comments. It is a multiclass classification problem where each data instance is categorized into one of the three classes: {`}Hope speech{'}, {`}Not hope speech{'}, and {`}Not in intended language{'}. We propose a system that employs multilingual transformer models to obtain the representation of text and classifies it into one of the three classes. We explored the use of multilingual models trained specifically for Indian languages along with generic multilingual models. Our system was ranked 2nd for English, 2nd for Malayalam, and 7th for the Tamil language in the final leader board published by organizers and obtained a weighted F1-score of 0.92, 0.84, 0.55 respectively on the hidden test dataset used for the competition. We have made our system publicly available at GitHub.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1923156977,"Goal":"Gender Equality","Task":["Hope Speech Detection","Equality","Diversity","Inclusion","Identifying and eliminating offensive content","shared task Hope Speech Detection","Equality","Inclusion","LT - EDI","multiclass classification problem","hope speech{'}"],"Method":["Multilingual Representation","multilingual transformer models","multilingual models","generic multilingual models"]},{"ID":"an-etal-2019-representation","title":"Representation of Constituents in Neural Language Models: Coordination Phrase as a Case Study","abstract":"Neural language models have achieved state-of-the-art performances on many NLP tasks, and recently have been shown to learn a number of hierarchically-sensitive syntactic dependencies between individual words. However, equally important for language processing is the ability to combine words into phrasal constituents, and use constituent-level features to drive downstream expectations. Here we investigate neural models{'} ability to represent constituent-level features, using coordinated noun phrases as a case study. We assess whether different neural language models trained on English and French represent phrase-level number and gender features, and use those features to drive downstream expectations. Our results suggest that models use a linear combination of NP constituent number to drive CoordNP\/verb number agreement. This behavior is highly regular and even sensitive to local syntactic context, however it differs crucially from observed human behavior. Models have less success with gender agreement. Models trained on large corpora perform best, and there is no obvious advantage for models trained using explicit syntactic supervision.","year":2019,"title_abstract":"Representation of Constituents in Neural Language Models: Coordination Phrase as a Case Study Neural language models have achieved state-of-the-art performances on many NLP tasks, and recently have been shown to learn a number of hierarchically-sensitive syntactic dependencies between individual words. However, equally important for language processing is the ability to combine words into phrasal constituents, and use constituent-level features to drive downstream expectations. Here we investigate neural models{'} ability to represent constituent-level features, using coordinated noun phrases as a case study. We assess whether different neural language models trained on English and French represent phrase-level number and gender features, and use those features to drive downstream expectations. Our results suggest that models use a linear combination of NP constituent number to drive CoordNP\/verb number agreement. This behavior is highly regular and even sensitive to local syntactic context, however it differs crucially from observed human behavior. Models have less success with gender agreement. Models trained on large corpora perform best, and there is no obvious advantage for models trained using explicit syntactic supervision.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1921847314,"Goal":"Gender Equality","Task":["Coordination Phrase","NLP tasks","language processing"],"Method":["Representation of Constituents","Neural Language Models","Neural language models","neural models{'}","neural language models","linear combination of NP constituent number","explicit syntactic supervision"]},{"ID":"darwish-etal-2014-using","title":"Using Stem-Templates to Improve {A}rabic {POS} and Gender\/Number Tagging","abstract":"This paper presents an end-to-end automatic processing system for Arabic. The system performs: correction of common spelling errors pertaining to different forms of alef, ta marbouta and ha, and alef maqsoura and ya; context sensitive word segmentation into underlying clitics, POS tagging, and gender and number tagging of nouns and adjectives. We introduce the use of stem templates as a feature to improve POS tagging by 0.5{\\textbackslash}{\\%} and to help ascertain the gender and number of nouns and adjectives. For gender and number tagging, we report accuracies that are significantly higher on previously unseen words compared to a state-of-the-art system.","year":2014,"title_abstract":"Using Stem-Templates to Improve {A}rabic {POS} and Gender\/Number Tagging This paper presents an end-to-end automatic processing system for Arabic. The system performs: correction of common spelling errors pertaining to different forms of alef, ta marbouta and ha, and alef maqsoura and ya; context sensitive word segmentation into underlying clitics, POS tagging, and gender and number tagging of nouns and adjectives. We introduce the use of stem templates as a feature to improve POS tagging by 0.5{\\textbackslash}{\\%} and to help ascertain the gender and number of nouns and adjectives. For gender and number tagging, we report accuracies that are significantly higher on previously unseen words compared to a state-of-the-art system.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1921622455,"Goal":"Gender Equality","Task":["Gender\/Number Tagging","end - to - end automatic processing system","correction","word segmentation","gender and number tagging","POS tagging","gender and number tagging"],"Method":["Stem - Templates","alef maqsoura","POS","stem templates"]},{"ID":"imran-etal-2016-twitter","title":"{T}witter as a Lifeline: Human-annotated {T}witter Corpora for {NLP} of Crisis-related Messages","abstract":"Microblogging platforms such as Twitter provide active communication channels during mass convergence and emergency events such as earthquakes, typhoons. During the sudden onset of a crisis situation, affected people post useful information on Twitter that can be used for situational awareness and other humanitarian disaster response efforts, if processed timely and effectively. Processing social media information pose multiple challenges such as parsing noisy, brief and informal messages, learning information categories from the incoming stream of messages and classifying them into different classes among others. One of the basic necessities of many of these tasks is the availability of data, in particular human-annotated data. In this paper, we present human-annotated Twitter corpora collected during 19 different crises that took place between 2013 and 2015. To demonstrate the utility of the annotations, we train machine learning classifiers. Moreover, we publish first largest word2vec word embeddings trained on 52 million crisis-related tweets. To deal with tweets language issues, we present human-annotated normalized lexical resources for different lexical variations.","year":2016,"title_abstract":"{T}witter as a Lifeline: Human-annotated {T}witter Corpora for {NLP} of Crisis-related Messages Microblogging platforms such as Twitter provide active communication channels during mass convergence and emergency events such as earthquakes, typhoons. During the sudden onset of a crisis situation, affected people post useful information on Twitter that can be used for situational awareness and other humanitarian disaster response efforts, if processed timely and effectively. Processing social media information pose multiple challenges such as parsing noisy, brief and informal messages, learning information categories from the incoming stream of messages and classifying them into different classes among others. One of the basic necessities of many of these tasks is the availability of data, in particular human-annotated data. In this paper, we present human-annotated Twitter corpora collected during 19 different crises that took place between 2013 and 2015. To demonstrate the utility of the annotations, we train machine learning classifiers. Moreover, we publish first largest word2vec word embeddings trained on 52 million crisis-related tweets. To deal with tweets language issues, we present human-annotated normalized lexical resources for different lexical variations.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1920963675,"Goal":"Climate Action","Task":["{NLP} of Crisis - related Messages","situational awareness","humanitarian disaster response efforts","Processing social media information","tweets language issues"],"Method":["machine learning classifiers","word2vec word embeddings"]},{"ID":"khanna-etal-2022-idiap","title":"{IDIAP}{\\_}{TIET}@{LT}-{EDI}-{ACL}2022 : Hope Speech Detection in Social Media using Contextualized {BERT} with Attention Mechanism","abstract":"With the increase of users on social media platforms, manipulating or provoking masses of people has become a piece of cake. This spread of hatred among people, which has become a loophole for freedom of speech, must be minimized. Hence, it is essential to have a system that automatically classifies the hatred content, especially on social media, to take it down. This paper presents a simple modular pipeline classifier with BERT embeddings and attention mechanism to classify hope speech content in the Hope Speech Detection shared task for Equality, Diversity, and Inclusion-ACL 2022. Our system submission ranks fourth with an F1-score of 0.84. We release our code-base here https:\/\/github.com\/Deepanshu-beep\/hope-speech-attention .","year":2022,"title_abstract":"{IDIAP}{\\_}{TIET}@{LT}-{EDI}-{ACL}2022 : Hope Speech Detection in Social Media using Contextualized {BERT} with Attention Mechanism With the increase of users on social media platforms, manipulating or provoking masses of people has become a piece of cake. This spread of hatred among people, which has become a loophole for freedom of speech, must be minimized. Hence, it is essential to have a system that automatically classifies the hatred content, especially on social media, to take it down. This paper presents a simple modular pipeline classifier with BERT embeddings and attention mechanism to classify hope speech content in the Hope Speech Detection shared task for Equality, Diversity, and Inclusion-ACL 2022. Our system submission ranks fourth with an F1-score of 0.84. We release our code-base here https:\/\/github.com\/Deepanshu-beep\/hope-speech-attention .","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.192042768,"Goal":"Gender Equality","Task":["Hope Speech Detection","Hope Speech Detection shared task","Equality","Diversity","Inclusion - ACL"],"Method":["Contextualized {BERT}","Attention Mechanism","modular pipeline classifier","BERT embeddings","attention mechanism"]},{"ID":"fayoumi-yeniterzi-2020-su","title":"{SU}-{NLP} at {WNUT}-2020 Task 2: The Ensemble Models","abstract":"In this paper, we address the problem of identifying informative tweets related to COVID-19 in the form of a binary classification task as part of our submission for W-NUT 2020 Task 2. Specifically, we focus on ensembling methods to boost the classification performance of classification models such as BERT and CNN. We show that ensembling can reduce the variance in performance, specifically for BERT base models.","year":2020,"title_abstract":"{SU}-{NLP} at {WNUT}-2020 Task 2: The Ensemble Models In this paper, we address the problem of identifying informative tweets related to COVID-19 in the form of a binary classification task as part of our submission for W-NUT 2020 Task 2. Specifically, we focus on ensembling methods to boost the classification performance of classification models such as BERT and CNN. We show that ensembling can reduce the variance in performance, specifically for BERT base models.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.191752702,"Goal":"Climate Action","Task":["identifying informative tweets","COVID - 19","binary classification task","W - NUT","classification"],"Method":["Ensemble Models","ensembling methods","classification models","BERT","CNN","ensembling","BERT"]},{"ID":"lyding-etal-2020-digital","title":"Digital Language Infrastructures {--} Documenting Language Actors","abstract":"The major European language infrastructure initiatives like CLARIN (Hinrichs and Krauwer, 2014), DARIAH (Edmond et al., 2017) or Europeana (Europeana Foundation, 2015) have been built by focusing in the first place on institutions of larger scale, like specialized research departments and larger official units like national libraries, etc. However, besides these principal players also a large number of smaller language actors could contribute to and benefit from language infrastructures. Especially since these smaller institutions, like local libraries, archives and publishers, often collect, manage and host language resources of particular value for their geographical and cultural region, it seems highly relevant to find ways of engaging and connecting them to existing European infrastructure initiatives. In this article, we first highlight the need for reaching out to smaller local language actors and discuss challenges related to this ambition. Then we present the first step in how this objective was approached within a local language infrastructure project, namely by means of a structured documentation of the local language actors landscape in South Tyrol. We describe how the documentation efforts were structured and organized, and what tool we have set up to distribute the collected data online, by adapting existing CLARIN solutions.","year":2020,"title_abstract":"Digital Language Infrastructures {--} Documenting Language Actors The major European language infrastructure initiatives like CLARIN (Hinrichs and Krauwer, 2014), DARIAH (Edmond et al., 2017) or Europeana (Europeana Foundation, 2015) have been built by focusing in the first place on institutions of larger scale, like specialized research departments and larger official units like national libraries, etc. However, besides these principal players also a large number of smaller language actors could contribute to and benefit from language infrastructures. Especially since these smaller institutions, like local libraries, archives and publishers, often collect, manage and host language resources of particular value for their geographical and cultural region, it seems highly relevant to find ways of engaging and connecting them to existing European infrastructure initiatives. In this article, we first highlight the need for reaching out to smaller local language actors and discuss challenges related to this ambition. Then we present the first step in how this objective was approached within a local language infrastructure project, namely by means of a structured documentation of the local language actors landscape in South Tyrol. We describe how the documentation efforts were structured and organized, and what tool we have set up to distribute the collected data online, by adapting existing CLARIN solutions.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1915976405,"Goal":"Sustainable Cities and Communities","Task":["language infrastructures","local language infrastructure project"],"Method":["Digital Language Infrastructures","CLARIN solutions"]},{"ID":"kelly-etal-2020-social","title":"Social media data as a lens onto care-seeking behavior among women veterans of the {US} armed forces","abstract":"In this article, we examine social media data as a lens onto support-seeking among women veterans of the US armed forces. Social media data hold a great deal of promise as a source of information on needs and support-seeking among individuals who are excluded from or systematically prevented from accessing clinical or other institutions ostensibly designed to support them. We apply natural language processing (NLP) techniques to more than 3 million Tweets collected from 20,000 Twitter users. We find evidence that women veterans are more likely to use social media to seek social and community engagement and to discuss mental health and veterans{'} issues significantly more frequently than their male counterparts. By contrast, male veterans tend to use social media to amplify political ideologies or to engage in partisan debate. Our results have implications for how organizations can provide outreach and services to this uniquely vulnerable population, and illustrate the utility of non-traditional observational data sources such as social media to understand the needs of marginalized groups.","year":2020,"title_abstract":"Social media data as a lens onto care-seeking behavior among women veterans of the {US} armed forces In this article, we examine social media data as a lens onto support-seeking among women veterans of the US armed forces. Social media data hold a great deal of promise as a source of information on needs and support-seeking among individuals who are excluded from or systematically prevented from accessing clinical or other institutions ostensibly designed to support them. We apply natural language processing (NLP) techniques to more than 3 million Tweets collected from 20,000 Twitter users. We find evidence that women veterans are more likely to use social media to seek social and community engagement and to discuss mental health and veterans{'} issues significantly more frequently than their male counterparts. By contrast, male veterans tend to use social media to amplify political ideologies or to engage in partisan debate. Our results have implications for how organizations can provide outreach and services to this uniquely vulnerable population, and illustrate the utility of non-traditional observational data sources such as social media to understand the needs of marginalized groups.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.191407457,"Goal":"Gender Equality","Task":["care - seeking behavior","support - seeking","support - seeking","outreach and services"],"Method":["natural language processing","observational data sources"]},{"ID":"wang-2019-msnet","title":"{MS}net: A {BERT}-based Network for Gendered Pronoun Resolution","abstract":"The pre-trained BERT model achieves a remarkable state of the art across a wide range of tasks in natural language processing. For solving the gender bias in gendered pronoun resolution task, I propose a novel neural network model based on the pre-trained BERT. This model is a type of mention score classifier and uses an attention mechanism with no parameters to compute the contextual representation of entity span, and a vector to represent the triple-wise semantic similarity among the pronoun and the entities. In stage 1 of the gendered pronoun resolution task, a variant of this model, trained in the fine-tuning approach, reduced the multi-class logarithmic loss to 0.3033 in the 5-fold cross-validation of training set and 0.2795 in testing set. Besides, this variant won the 2nd place with a score at 0.17289 in stage 2 of the task. The code in this paper is available at: https:\/\/github.com\/ziliwang\/MSnet-for-Gendered-Pronoun-Resolution","year":2019,"title_abstract":"{MS}net: A {BERT}-based Network for Gendered Pronoun Resolution The pre-trained BERT model achieves a remarkable state of the art across a wide range of tasks in natural language processing. For solving the gender bias in gendered pronoun resolution task, I propose a novel neural network model based on the pre-trained BERT. This model is a type of mention score classifier and uses an attention mechanism with no parameters to compute the contextual representation of entity span, and a vector to represent the triple-wise semantic similarity among the pronoun and the entities. In stage 1 of the gendered pronoun resolution task, a variant of this model, trained in the fine-tuning approach, reduced the multi-class logarithmic loss to 0.3033 in the 5-fold cross-validation of training set and 0.2795 in testing set. Besides, this variant won the 2nd place with a score at 0.17289 in stage 2 of the task. The code in this paper is available at: https:\/\/github.com\/ziliwang\/MSnet-for-Gendered-Pronoun-Resolution","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1913795024,"Goal":"Gender Equality","Task":["Gendered Pronoun Resolution","natural language processing","gender bias in gendered pronoun resolution task","gendered pronoun resolution task"],"Method":["{BERT} - based Network","BERT","neural network model","BERT","mention score classifier","attention mechanism","contextual representation","fine - tuning approach"]},{"ID":"ignat-etal-2021-whyact","title":"{W}hy{A}ct: Identifying Action Reasons in Lifestyle Vlogs","abstract":"We aim to automatically identify human action reasons in online videos. We focus on the widespread genre of lifestyle vlogs, in which people perform actions while verbally describing them. We introduce and make publicly available the WhyAct dataset, consisting of 1,077 visual actions manually annotated with their reasons. We describe a multimodal model that leverages visual and textual information to automatically infer the reasons corresponding to an action presented in the video.","year":2021,"title_abstract":"{W}hy{A}ct: Identifying Action Reasons in Lifestyle Vlogs We aim to automatically identify human action reasons in online videos. We focus on the widespread genre of lifestyle vlogs, in which people perform actions while verbally describing them. We introduce and make publicly available the WhyAct dataset, consisting of 1,077 visual actions manually annotated with their reasons. We describe a multimodal model that leverages visual and textual information to automatically infer the reasons corresponding to an action presented in the video.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1913308501,"Goal":"Climate Action","Task":["Identifying Action Reasons","Lifestyle Vlogs","human action reasons"],"Method":["multimodal model"]},{"ID":"miao-etal-2020-twitter","title":"{T}witter Data Augmentation for Monitoring Public Opinion on {COVID}-19 Intervention Measures","abstract":"The COVID-19 outbreak is an ongoing worldwide pandemic that was announced as a global health crisis in March 2020. Due to the enormous challenges and high stakes of this pandemic, governments have implemented a wide range of policies aimed at containing the spread of the virus and its negative effect on multiple aspects of our life. Public responses to various intervention measures imposed over time can be explored by analyzing the social media. Due to the shortage of available labeled data for this new and evolving domain, we apply data distillation methodology to labeled datasets from related tasks and a very small manually labeled dataset. Our experimental results show that data distillation outperforms other data augmentation methods on our task.","year":2020,"title_abstract":"{T}witter Data Augmentation for Monitoring Public Opinion on {COVID}-19 Intervention Measures The COVID-19 outbreak is an ongoing worldwide pandemic that was announced as a global health crisis in March 2020. Due to the enormous challenges and high stakes of this pandemic, governments have implemented a wide range of policies aimed at containing the spread of the virus and its negative effect on multiple aspects of our life. Public responses to various intervention measures imposed over time can be explored by analyzing the social media. Due to the shortage of available labeled data for this new and evolving domain, we apply data distillation methodology to labeled datasets from related tasks and a very small manually labeled dataset. Our experimental results show that data distillation outperforms other data augmentation methods on our task.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1913172305,"Goal":"Climate Action","Task":["Data Augmentation","Monitoring Public Opinion","COVID - 19 outbreak"],"Method":["data distillation methodology","data distillation","data augmentation methods"]},{"ID":"wang-etal-2018-teacher","title":"A Teacher-Student Framework for Maintainable Dialog Manager","abstract":"Reinforcement learning (RL) is an attractive solution for task-oriented dialog systems. However, extending RL-based systems to handle new intents and slots requires a system redesign. The high maintenance cost makes it difficult to apply RL methods to practical systems on a large scale. To address this issue, we propose a practical teacher-student framework to extend RL-based dialog systems without retraining from scratch. Specifically, the {``}student{''} is an extended dialog manager based on a new ontology, and the {``}teacher{''} is existing resources used for guiding the learning process of the {``}student{''}. By specifying constraints held in the new dialog manager, we transfer knowledge of the {``}teacher{''} to the {``}student{''} without additional resources. Experiments show that the performance of the extended system is comparable to the system trained from scratch. More importantly, the proposed framework makes no assumption about the unsupported intents and slots, which makes it possible to improve RL-based systems incrementally.","year":2018,"title_abstract":"A Teacher-Student Framework for Maintainable Dialog Manager Reinforcement learning (RL) is an attractive solution for task-oriented dialog systems. However, extending RL-based systems to handle new intents and slots requires a system redesign. The high maintenance cost makes it difficult to apply RL methods to practical systems on a large scale. To address this issue, we propose a practical teacher-student framework to extend RL-based dialog systems without retraining from scratch. Specifically, the {``}student{''} is an extended dialog manager based on a new ontology, and the {``}teacher{''} is existing resources used for guiding the learning process of the {``}student{''}. By specifying constraints held in the new dialog manager, we transfer knowledge of the {``}teacher{''} to the {``}student{''} without additional resources. Experiments show that the performance of the extended system is comparable to the system trained from scratch. More importantly, the proposed framework makes no assumption about the unsupported intents and slots, which makes it possible to improve RL-based systems incrementally.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1912774444,"Goal":"Quality Education","Task":["task - oriented dialog systems","system redesign","learning process"],"Method":["Teacher - Student Framework","Maintainable Dialog Manager Reinforcement learning","RL - based systems","RL methods","teacher - student framework","RL - based dialog systems","dialog manager","dialog manager","RL - based systems"]},{"ID":"wang-etal-2021-self","title":"Self Promotion in {US} Congressional Tweets","abstract":"Prior studies have found that women self-promote less than men due to gender stereotypes. In this study we built a BERT-based NLP model to predict whether a Congressional tweet shows self-promotion or not and then used this model to examine whether a gender gap in self-promotion exists among Congressional tweets. After analyzing 2 million Congressional tweets from July 2017 to March 2021, controlling for a number of factors that include political party, chamber, age, number of terms in Congress, number of daily tweets, and number of followers, we found that women in Congress actually perform more self-promotion on Twitter, indicating a reversal of traditional gender norms where women self-promote less than men.","year":2021,"title_abstract":"Self Promotion in {US} Congressional Tweets Prior studies have found that women self-promote less than men due to gender stereotypes. In this study we built a BERT-based NLP model to predict whether a Congressional tweet shows self-promotion or not and then used this model to examine whether a gender gap in self-promotion exists among Congressional tweets. After analyzing 2 million Congressional tweets from July 2017 to March 2021, controlling for a number of factors that include political party, chamber, age, number of terms in Congress, number of daily tweets, and number of followers, we found that women in Congress actually perform more self-promotion on Twitter, indicating a reversal of traditional gender norms where women self-promote less than men.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1908853352,"Goal":"Gender Equality","Task":["Self Promotion","self - promotion"],"Method":["BERT - based NLP model"]},{"ID":"excell-al-moubayed-2021-towards","title":"Towards Equal Gender Representation in the Annotations of Toxic Language Detection","abstract":"Classifiers tend to propagate biases present in the data on which they are trained. Hence, it is important to understand how the demographic identities of the annotators of comments affect the fairness of the resulting model. In this paper, we focus on the differences in the ways men and women annotate comments for toxicity, investigating how these differences result in models that amplify the opinions of male annotators. We find that the BERT model associates toxic comments containing offensive words with male annotators, causing the model to predict 67.7{\\%} of toxic comments as having been annotated by men. We show that this disparity between gender predictions can be mitigated by removing offensive words and highly toxic comments from the training data. We then apply the learned associations between gender and language to toxic language classifiers, finding that models trained exclusively on female-annotated data perform 1.8{\\%} better than those trained solely on male-annotated data, and that training models on data after removing all offensive words reduces bias in the model by 55.5{\\%} while increasing the sensitivity by 0.4{\\%}.","year":2021,"title_abstract":"Towards Equal Gender Representation in the Annotations of Toxic Language Detection Classifiers tend to propagate biases present in the data on which they are trained. Hence, it is important to understand how the demographic identities of the annotators of comments affect the fairness of the resulting model. In this paper, we focus on the differences in the ways men and women annotate comments for toxicity, investigating how these differences result in models that amplify the opinions of male annotators. We find that the BERT model associates toxic comments containing offensive words with male annotators, causing the model to predict 67.7{\\%} of toxic comments as having been annotated by men. We show that this disparity between gender predictions can be mitigated by removing offensive words and highly toxic comments from the training data. We then apply the learned associations between gender and language to toxic language classifiers, finding that models trained exclusively on female-annotated data perform 1.8{\\%} better than those trained solely on male-annotated data, and that training models on data after removing all offensive words reduces bias in the model by 55.5{\\%} while increasing the sensitivity by 0.4{\\%}.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1906841397,"Goal":"Gender Equality","Task":["gender predictions"],"Method":["Equal Gender Representation","Toxic Language Detection Classifiers","BERT model","toxic language classifiers"]},{"ID":"temnikova-etal-2014-building","title":"Building a Crisis Management Term Resource for Social Media: The Case of Floods and Protests","abstract":"Extracting information from social media is being currently exploited for a variety of tasks, including the recognition of emergency events in Twitter. This is done in order to supply Crisis Management agencies with additional crisis information. The existing approaches, however, mostly rely on geographic location and hashtags\/keywords, obtained via a manual Twitter search. As we expect that Twitter crisis terminology would differ from existing crisis glossaries, we start collecting a specialized terminological resource to support this task. The aim of this resource is to contain sets of crisis-related Twitter terms which are the same for different instances of the same type of event. This article presents a preliminary investigation of the nature of terms used in four events of two crisis types, tests manual and automatic ways to collect these terms and comes up with an initial collection of terms for these two types of events. As contributions, a novel annotation schema is presented, along with important insights into the differences in annotations between different specialists, descriptive term statistics, and performance results of existing automatic terminology recognition approaches for this task.","year":2014,"title_abstract":"Building a Crisis Management Term Resource for Social Media: The Case of Floods and Protests Extracting information from social media is being currently exploited for a variety of tasks, including the recognition of emergency events in Twitter. This is done in order to supply Crisis Management agencies with additional crisis information. The existing approaches, however, mostly rely on geographic location and hashtags\/keywords, obtained via a manual Twitter search. As we expect that Twitter crisis terminology would differ from existing crisis glossaries, we start collecting a specialized terminological resource to support this task. The aim of this resource is to contain sets of crisis-related Twitter terms which are the same for different instances of the same type of event. This article presents a preliminary investigation of the nature of terms used in four events of two crisis types, tests manual and automatic ways to collect these terms and comes up with an initial collection of terms for these two types of events. As contributions, a novel annotation schema is presented, along with important insights into the differences in annotations between different specialists, descriptive term statistics, and performance results of existing automatic terminology recognition approaches for this task.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1906301081,"Goal":"Climate Action","Task":["Crisis Management Term Resource","recognition of emergency events","Crisis Management agencies","manual Twitter search"],"Method":["annotation schema","automatic terminology recognition approaches"]},{"ID":"jansen-ustalov-2020-textgraphs","title":"{T}ext{G}raphs 2020 Shared Task on Multi-Hop Inference for Explanation Regeneration","abstract":"The 2020 Shared Task on Multi-Hop Inference for Explanation Regeneration tasks participants with regenerating large detailed multi-fact explanations for standardized science exam questions. Given a question, correct answer, and knowledge base, models must rank each fact in the knowledge base such that facts most likely to appear in the explanation are ranked highest. Explanations consist of an average of 6 (and as many as 16) facts that span both core scientific knowledge and world knowledge, and form an explicit lexically-connected {``}explanation graph{''} describing how the facts interrelate. In this second iteration of the explanation regeneration shared task, participants are supplied with more than double the training and evaluation data of the first shared task, as well as a knowledge base nearly double in size, both of which expand into more challenging scientific topics that increase the difficulty of the task. In total 10 teams participated, and 5 teams submitted system description papers. The best-performing teams significantly increased state-of-the-art performance both in terms of ranking (mean average precision) and inference speed on this challenge task.","year":2020,"title_abstract":"{T}ext{G}raphs 2020 Shared Task on Multi-Hop Inference for Explanation Regeneration The 2020 Shared Task on Multi-Hop Inference for Explanation Regeneration tasks participants with regenerating large detailed multi-fact explanations for standardized science exam questions. Given a question, correct answer, and knowledge base, models must rank each fact in the knowledge base such that facts most likely to appear in the explanation are ranked highest. Explanations consist of an average of 6 (and as many as 16) facts that span both core scientific knowledge and world knowledge, and form an explicit lexically-connected {``}explanation graph{''} describing how the facts interrelate. In this second iteration of the explanation regeneration shared task, participants are supplied with more than double the training and evaluation data of the first shared task, as well as a knowledge base nearly double in size, both of which expand into more challenging scientific topics that increase the difficulty of the task. In total 10 teams participated, and 5 teams submitted system description papers. The best-performing teams significantly increased state-of-the-art performance both in terms of ranking (mean average precision) and inference speed on this challenge task.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.190389961,"Goal":"Quality Education","Task":["Multi - Hop Inference","Explanation Regeneration","2020 Shared Task","Multi - Hop Inference","Explanation Regeneration tasks","explanation regeneration shared task","ranking"],"Method":["lexically - connected {``}explanation graph{''}"]},{"ID":"tran-etal-2020-uit","title":"{UIT}-{HSE} at {WNUT}-2020 Task 2: Exploiting {CT}-{BERT} for Identifying {COVID}-19 Information on the {T}witter Social Network","abstract":"Recently, COVID-19 has affected a variety of real-life aspects of the world and led to dreadful consequences. More and more tweets about COVID-19 has been shared publicly on Twitter. However, the plurality of those Tweets are uninformative, which is challenging to build automatic systems to detect the informative ones for useful AI applications. In this paper, we present our results at the W-NUT 2020 Shared Task 2: Identification of Informative COVID-19 English Tweets. In particular, we propose our simple but effective approach using the transformer-based models based on COVID-Twitter-BERT (CT-BERT) with different fine-tuning techniques. As a result, we achieve the F1-Score of 90.94{\\%} with the third place on the leaderboard of this task which attracted 56 submitted teams in total.","year":2020,"title_abstract":"{UIT}-{HSE} at {WNUT}-2020 Task 2: Exploiting {CT}-{BERT} for Identifying {COVID}-19 Information on the {T}witter Social Network Recently, COVID-19 has affected a variety of real-life aspects of the world and led to dreadful consequences. More and more tweets about COVID-19 has been shared publicly on Twitter. However, the plurality of those Tweets are uninformative, which is challenging to build automatic systems to detect the informative ones for useful AI applications. In this paper, we present our results at the W-NUT 2020 Shared Task 2: Identification of Informative COVID-19 English Tweets. In particular, we propose our simple but effective approach using the transformer-based models based on COVID-Twitter-BERT (CT-BERT) with different fine-tuning techniques. As a result, we achieve the F1-Score of 90.94{\\%} with the third place on the leaderboard of this task which attracted 56 submitted teams in total.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1903019994,"Goal":"Climate Action","Task":["Identifying {COVID} - 19","AI applications","W - NUT","Identification of Informative COVID - 19"],"Method":["{CT}","automatic systems","transformer - based models","COVID - Twitter - BERT","fine - tuning techniques"]},{"ID":"suwaileh-etal-2020-ready","title":"Are We Ready for this Disaster? Towards Location Mention Recognition from Crisis Tweets","abstract":"The widespread usage of Twitter during emergencies has provided a new opportunity and timely resource to crisis responders for various disaster management tasks. Geolocation information of pertinent tweets is crucial for gaining situational awareness and delivering aid. However, the majority of tweets do not come with geoinformation. In this work, we focus on the task of location mention recognition from crisis-related tweets. Specifically, we investigate the influence of different types of labeled training data on the performance of a BERT-based classification model. We explore several training settings such as combing in- and out-domain data from news articles and general-purpose and crisis-related tweets. Furthermore, we investigate the effect of geospatial proximity while training on near or far-away events from the target event. Using five different datasets, our extensive experiments provide answers to several critical research questions that are useful for the research community to foster research in this important direction. For example, results show that, for training a location mention recognition model, Twitter-based data is preferred over general-purpose data; and crisis-related data is preferred over general-purpose Twitter data. Furthermore, training on data from geographically-nearby disaster events to the target event boosts the performance compared to training on distant events.","year":2020,"title_abstract":"Are We Ready for this Disaster? Towards Location Mention Recognition from Crisis Tweets The widespread usage of Twitter during emergencies has provided a new opportunity and timely resource to crisis responders for various disaster management tasks. Geolocation information of pertinent tweets is crucial for gaining situational awareness and delivering aid. However, the majority of tweets do not come with geoinformation. In this work, we focus on the task of location mention recognition from crisis-related tweets. Specifically, we investigate the influence of different types of labeled training data on the performance of a BERT-based classification model. We explore several training settings such as combing in- and out-domain data from news articles and general-purpose and crisis-related tweets. Furthermore, we investigate the effect of geospatial proximity while training on near or far-away events from the target event. Using five different datasets, our extensive experiments provide answers to several critical research questions that are useful for the research community to foster research in this important direction. For example, results show that, for training a location mention recognition model, Twitter-based data is preferred over general-purpose data; and crisis-related data is preferred over general-purpose Twitter data. Furthermore, training on data from geographically-nearby disaster events to the target event boosts the performance compared to training on distant events.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1902290583,"Goal":"Climate Action","Task":["Location Mention Recognition","disaster management tasks","situational awareness","delivering aid","location mention recognition"],"Method":["BERT - based classification model","location mention recognition model"]},{"ID":"ni-chasaide-etal-2022-challenges","title":"Challenges in assistive technology development for an endangered language: an {I}rish ({G}aelic) perspective","abstract":"This paper describes three areas of assistive technology development which deploy the resources and speech technology for Irish (Gaelic), newly emerging from the ABAIR initiative. These include (i) a screenreading facility for visually impaired people, (ii) an application to help develop phonological awareness and early literacy for dyslexic people (iii) a speech-enabled AAC system for non-speaking people. Each of these is at a different stage of development and poses unique challenges: these are dis-cussed along with the approaches adopted to address them. Three guiding principles underlie development. Firstly, the sociolinguistic context and the needs of the community are essential considerations in setting priorities. Secondly, development needs to be language sensitive. The need for skilled researchers with a deep knowledge of Irish structure is illustrated in the case of (ii) and (iii), where aspects of Irish linguistic structure (phonological, morphological and grammatical) and the striking differences from English pose challenges for systems aimed at bilingual Irish-English users. Thirdly, and most importantly, the users and their support networks are central {--} not as passive recipients of ready-made technologies, but as active partners at every stage of development, from design to implementation, evaluation and dissemination.","year":2022,"title_abstract":"Challenges in assistive technology development for an endangered language: an {I}rish ({G}aelic) perspective This paper describes three areas of assistive technology development which deploy the resources and speech technology for Irish (Gaelic), newly emerging from the ABAIR initiative. These include (i) a screenreading facility for visually impaired people, (ii) an application to help develop phonological awareness and early literacy for dyslexic people (iii) a speech-enabled AAC system for non-speaking people. Each of these is at a different stage of development and poses unique challenges: these are dis-cussed along with the approaches adopted to address them. Three guiding principles underlie development. Firstly, the sociolinguistic context and the needs of the community are essential considerations in setting priorities. Secondly, development needs to be language sensitive. The need for skilled researchers with a deep knowledge of Irish structure is illustrated in the case of (ii) and (iii), where aspects of Irish linguistic structure (phonological, morphological and grammatical) and the striking differences from English pose challenges for systems aimed at bilingual Irish-English users. Thirdly, and most importantly, the users and their support networks are central {--} not as passive recipients of ready-made technologies, but as active partners at every stage of development, from design to implementation, evaluation and dissemination.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1900394261,"Goal":"Sustainable Cities and Communities","Task":["assistive technology development","assistive technology development","ABAIR initiative","phonological awareness","early literacy","development","development","evaluation","dissemination"],"Method":["speech technology","screenreading facility","speech - enabled AAC system"]},{"ID":"blackburn-etal-2020-corpus","title":"Corpus Development for Studying Online Disinformation Campaign: A Narrative + Stance Approach","abstract":"Disinformation on social media is impacting our personal life and society. The outbreak of the new coronavirus is the most recent example for which a wealth of disinformation provoked fear, hate, and even social panic. While there are emerging interests in studying how disinformation campaigns form, spread, and influence target audiences, developing disinformation campaign corpora is challenging given the high volume, fast evolution, and wide variation of messages associated with each campaign. Disinformation cannot always be captured by simple factchecking, which makes it even more challenging to validate and create ground truth. This paper presents our approach to develop a corpus for studying disinformation campaigns targeting the White Helmets of Syria. We bypass directly classifying a piece of information as disinformation or not. Instead, we label the narrative and stance of tweets and YouTube comments about White Helmets. Narratives is defined as a recurring statement that is used to express a point of view. Stance is a high-level point of view on a topic. We demonstrate that narrative and stance together can provide a dynamic method for real world users, e.g., intelligence analysts, to quickly identify and counter disinformation campaigns based on their knowledge at the time.","year":2020,"title_abstract":"Corpus Development for Studying Online Disinformation Campaign: A Narrative + Stance Approach Disinformation on social media is impacting our personal life and society. The outbreak of the new coronavirus is the most recent example for which a wealth of disinformation provoked fear, hate, and even social panic. While there are emerging interests in studying how disinformation campaigns form, spread, and influence target audiences, developing disinformation campaign corpora is challenging given the high volume, fast evolution, and wide variation of messages associated with each campaign. Disinformation cannot always be captured by simple factchecking, which makes it even more challenging to validate and create ground truth. This paper presents our approach to develop a corpus for studying disinformation campaigns targeting the White Helmets of Syria. We bypass directly classifying a piece of information as disinformation or not. Instead, we label the narrative and stance of tweets and YouTube comments about White Helmets. Narratives is defined as a recurring statement that is used to express a point of view. Stance is a high-level point of view on a topic. We demonstrate that narrative and stance together can provide a dynamic method for real world users, e.g., intelligence analysts, to quickly identify and counter disinformation campaigns based on their knowledge at the time.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1899913549,"Goal":"Climate Action","Task":["Corpus Development","Online Disinformation Campaign","disinformation campaigns","intelligence analysts","counter disinformation campaigns"],"Method":["Narrative + Stance Approach","factchecking"]},{"ID":"chalkidis-etal-2022-fairlex","title":"{F}air{L}ex: A Multilingual Benchmark for Evaluating Fairness in Legal Text Processing","abstract":"We present a benchmark suite of four datasets for evaluating the fairness of pre-trained language models and the techniques used to fine-tune them for downstream tasks. Our benchmarks cover four jurisdictions (European Council, USA, Switzerland, and China), five languages (English, German, French, Italian and Chinese) and fairness across five attributes (gender, age, region, language, and legal area). In our experiments, we evaluate pre-trained language models using several group-robust fine-tuning techniques and show that performance group disparities are vibrant in many cases, while none of these techniques guarantee fairness, nor consistently mitigate group disparities. Furthermore, we provide a quantitative and qualitative analysis of our results, highlighting open challenges in the development of robustness methods in legal NLP.","year":2022,"title_abstract":"{F}air{L}ex: A Multilingual Benchmark for Evaluating Fairness in Legal Text Processing We present a benchmark suite of four datasets for evaluating the fairness of pre-trained language models and the techniques used to fine-tune them for downstream tasks. Our benchmarks cover four jurisdictions (European Council, USA, Switzerland, and China), five languages (English, German, French, Italian and Chinese) and fairness across five attributes (gender, age, region, language, and legal area). In our experiments, we evaluate pre-trained language models using several group-robust fine-tuning techniques and show that performance group disparities are vibrant in many cases, while none of these techniques guarantee fairness, nor consistently mitigate group disparities. Furthermore, we provide a quantitative and qualitative analysis of our results, highlighting open challenges in the development of robustness methods in legal NLP.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1899840534,"Goal":"Reduced Inequalities","Task":["Fairness","Legal Text Processing","downstream tasks","legal NLP"],"Method":["language models","language models","group - robust fine - tuning techniques","robustness methods"]},{"ID":"zhang-etal-2019-girls","title":"Are Girls Neko or Sh{\\=o}jo? Cross-Lingual Alignment of Non-Isomorphic Embeddings with Iterative Normalization","abstract":"Cross-lingual word embeddings (CLWE) underlie many multilingual natural language processing systems, often through orthogonal transformations of pre-trained monolingual embeddings. However, orthogonal mapping only works on language pairs whose embeddings are naturally isomorphic. For non-isomorphic pairs, our method (Iterative Normalization) transforms monolingual embeddings to make orthogonal alignment easier by simultaneously enforcing that (1) individual word vectors are unit length, and (2) each language{'}s average vector is zero. Iterative Normalization consistently improves word translation accuracy of three CLWE methods, with the largest improvement observed on English-Japanese (from 2{\\%} to 44{\\%} test accuracy).","year":2019,"title_abstract":"Are Girls Neko or Sh{\\=o}jo? Cross-Lingual Alignment of Non-Isomorphic Embeddings with Iterative Normalization Cross-lingual word embeddings (CLWE) underlie many multilingual natural language processing systems, often through orthogonal transformations of pre-trained monolingual embeddings. However, orthogonal mapping only works on language pairs whose embeddings are naturally isomorphic. For non-isomorphic pairs, our method (Iterative Normalization) transforms monolingual embeddings to make orthogonal alignment easier by simultaneously enforcing that (1) individual word vectors are unit length, and (2) each language{'}s average vector is zero. Iterative Normalization consistently improves word translation accuracy of three CLWE methods, with the largest improvement observed on English-Japanese (from 2{\\%} to 44{\\%} test accuracy).","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1898257881,"Goal":"Gender Equality","Task":["Cross - Lingual Alignment of Non - Isomorphic Embeddings","multilingual natural language processing systems","Normalization)","orthogonal alignment"],"Method":["Iterative Normalization","Cross - lingual word embeddings","orthogonal transformations of pre - trained monolingual embeddings","orthogonal mapping","Iterative Normalization","CLWE methods"]},{"ID":"ziems-yang-2021-protect-serve","title":"To Protect and To Serve? Analyzing Entity-Centric Framing of Police Violence","abstract":"Framing has significant but subtle effects on public opinion and policy. We propose an NLP framework to measure entity-centric frames. We use it to understand media coverage on police violence in the United States in a new Police Violence Frames Corpus of 82k news articles spanning 7k police killings. Our work uncovers more than a dozen framing devices and reveals significant differences in the way liberal and conservative news sources frame both the issue of police violence and the entities involved. Conservative sources emphasize when the victim is armed or attacking an officer and are more likely to mention the victim{'}s criminal record. Liberal sources focus more on the underlying systemic injustice, highlighting the victim{'}s race and that they were unarmed. We discover temporary spikes in these injustice frames near high-profile shooting events, and finally, we show protest volume correlates with and precedes media framing decisions.","year":2021,"title_abstract":"To Protect and To Serve? Analyzing Entity-Centric Framing of Police Violence Framing has significant but subtle effects on public opinion and policy. We propose an NLP framework to measure entity-centric frames. We use it to understand media coverage on police violence in the United States in a new Police Violence Frames Corpus of 82k news articles spanning 7k police killings. Our work uncovers more than a dozen framing devices and reveals significant differences in the way liberal and conservative news sources frame both the issue of police violence and the entities involved. Conservative sources emphasize when the victim is armed or attacking an officer and are more likely to mention the victim{'}s criminal record. Liberal sources focus more on the underlying systemic injustice, highlighting the victim{'}s race and that they were unarmed. We discover temporary spikes in these injustice frames near high-profile shooting events, and finally, we show protest volume correlates with and precedes media framing decisions.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.1898218393,"Goal":"Peace, Justice and Strong Institutions","Task":["Entity - Centric Framing of Police Violence Framing","public opinion and policy","media coverage","police violence"],"Method":["NLP framework"]},{"ID":"jorg-etal-2010-lt","title":"{LT} World: Ontology and Reference Information Portal","abstract":"LT World (www.lt-world.org) is an ontology-driven web portal aimed at serving the global language technology community. Ontology-driven means, that the system is driven by an ontological schema to manage the research information and knowledge life-cycles: identify relevant concepts of information, structure and formalize them, assign relationships, functions and views, add states and rules, modify them. For modelling such a complex structure, we employ (i) concepts from the research domain, such as person, organisation, project, tool, data, patent, news, event (ii) concepts from the LT domain, such as technology and resource (iii) concepts from closely related domains, such as language, linguistics, and mathematics. Whereas the research entities represent the general context, that is, a research environment as such, the LT entities define the information and knowledge space of the field, enhanced by entities from closely related areas. By managing information holistically \u2015 that is, within a research context \u2015 its inherent semantics becomes much more transparent. This paper introduces LT World as a reference information portal through ontological eyes: its content, its system, its method for maintaining knowledge-rich items, its ontology as an asset.","year":2010,"title_abstract":"{LT} World: Ontology and Reference Information Portal LT World (www.lt-world.org) is an ontology-driven web portal aimed at serving the global language technology community. Ontology-driven means, that the system is driven by an ontological schema to manage the research information and knowledge life-cycles: identify relevant concepts of information, structure and formalize them, assign relationships, functions and views, add states and rules, modify them. For modelling such a complex structure, we employ (i) concepts from the research domain, such as person, organisation, project, tool, data, patent, news, event (ii) concepts from the LT domain, such as technology and resource (iii) concepts from closely related domains, such as language, linguistics, and mathematics. Whereas the research entities represent the general context, that is, a research environment as such, the LT entities define the information and knowledge space of the field, enhanced by entities from closely related areas. By managing information holistically \u2015 that is, within a research context \u2015 its inherent semantics becomes much more transparent. This paper introduces LT World as a reference information portal through ontological eyes: its content, its system, its method for maintaining knowledge-rich items, its ontology as an asset.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1898076981,"Goal":"Life on Land","Task":["Ontology and Reference Information Portal","global language technology community","Ontology - driven","reference information portal","maintaining knowledge - rich items"],"Method":["LT World","lt - world","ontology - driven web portal","ontological schema","LT World","ontological eyes"]},{"ID":"cromieres-etal-2017-kyoto","title":"{K}yoto {U}niversity Participation to {WAT} 2017","abstract":"We describe here our approaches and results on the WAT 2017 shared translation tasks. Following our good results with Neural Machine Translation in the previous shared task, we continue this approach this year, with incremental improvements in models and training methods. We focused on the ASPEC dataset and could improve the state-of-the-art results for Chinese-to-Japanese and Japanese-to-Chinese translations.","year":2017,"title_abstract":"{K}yoto {U}niversity Participation to {WAT} 2017 We describe here our approaches and results on the WAT 2017 shared translation tasks. Following our good results with Neural Machine Translation in the previous shared task, we continue this approach this year, with incremental improvements in models and training methods. We focused on the ASPEC dataset and could improve the state-of-the-art results for Chinese-to-Japanese and Japanese-to-Chinese translations.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1897951365,"Goal":"Gender Equality","Task":["WAT 2017 shared translation tasks","Neural Machine Translation","Chinese - to - Japanese","Japanese - to - Chinese translations"],"Method":["training methods"]},{"ID":"balloccu-reiter-2022-beyond","title":"Beyond calories: evaluating how tailored communication reduces emotional load in diet-coaching","abstract":"Dieting is a behaviour change task that is difficult for many people to conduct successfully. This is due to many factors, including stress and cost. Mobile applications offer an alternative to traditional coaching. However, previous work on apps evaluation only focused on dietary outcomes, ignoring users{'} emotional state despite its influence on eating habits. In this work, we introduce a novel evaluation of the effects that tailored communication can have on the emotional load of dieting. We implement this by augmenting a traditional diet-app with affective NLG, text-tailoring and persuasive communication techniques. We then run a short 2-weeks experiment and check dietary outcomes, user feedback of produced text and, most importantly, its impact on emotional state, through PANAS questionnaire. Results show that tailored communication significantly improved users{'} emotional state, compared to an app-only control group.","year":2022,"title_abstract":"Beyond calories: evaluating how tailored communication reduces emotional load in diet-coaching Dieting is a behaviour change task that is difficult for many people to conduct successfully. This is due to many factors, including stress and cost. Mobile applications offer an alternative to traditional coaching. However, previous work on apps evaluation only focused on dietary outcomes, ignoring users{'} emotional state despite its influence on eating habits. In this work, we introduce a novel evaluation of the effects that tailored communication can have on the emotional load of dieting. We implement this by augmenting a traditional diet-app with affective NLG, text-tailoring and persuasive communication techniques. We then run a short 2-weeks experiment and check dietary outcomes, user feedback of produced text and, most importantly, its impact on emotional state, through PANAS questionnaire. Results show that tailored communication significantly improved users{'} emotional state, compared to an app-only control group.","social_need":"No Hunger End hunger, achieve food security and improved nutrition and promote sustainable agriculture","cosine_similarity":0.1895230561,"Goal":"No Hunger","Task":["tailored communication","emotional load","diet - coaching Dieting","behaviour change task","coaching","apps evaluation","dietary outcomes","tailored communication","dieting"],"Method":["Mobile applications","diet - app","affective NLG","text - tailoring","persuasive communication techniques","tailored communication","app - only control"]},{"ID":"chinta-etal-2021-study","title":"Study of Manifestation of Civil Unrest on {T}witter","abstract":"Twitter is commonly used for civil unrest detection and forecasting tasks, but there is a lack of work in evaluating \\textit{how} civil unrest manifests on Twitter across countries and events. We present two in-depth case studies for two specific large-scale events, one in a country with high (English) Twitter usage (Johannesburg riots in South Africa) and one in a country with low Twitter usage (Burayu massacre protests in Ethiopia). We show that while there is event signal during the events, there is little signal leading up to the events. In addition to the case studies, we train Ngram-based models on a larger set of Twitter civil unrest data across time, events, and countries and use machine learning explainability tools (SHAP) to identify important features. The models were able to find words indicative of civil unrest that generalized across countries. The 42 countries span Africa, Middle East, and Southeast Asia and the events range occur between 2014 and 2019.","year":2021,"title_abstract":"Study of Manifestation of Civil Unrest on {T}witter Twitter is commonly used for civil unrest detection and forecasting tasks, but there is a lack of work in evaluating \\textit{how} civil unrest manifests on Twitter across countries and events. We present two in-depth case studies for two specific large-scale events, one in a country with high (English) Twitter usage (Johannesburg riots in South Africa) and one in a country with low Twitter usage (Burayu massacre protests in Ethiopia). We show that while there is event signal during the events, there is little signal leading up to the events. In addition to the case studies, we train Ngram-based models on a larger set of Twitter civil unrest data across time, events, and countries and use machine learning explainability tools (SHAP) to identify important features. The models were able to find words indicative of civil unrest that generalized across countries. The 42 countries span Africa, Middle East, and Southeast Asia and the events range occur between 2014 and 2019.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1893620193,"Goal":"Reduced Inequalities","Task":["Manifestation of Civil Unrest","civil unrest detection and forecasting tasks","\\textit{how} civil unrest"],"Method":["Ngram - based models","machine learning explainability tools"]},{"ID":"chen-etal-2018-feature","title":"Feature Engineering for Second Language Acquisition Modeling","abstract":"Knowledge tracing serves as a keystone in delivering personalized education. However, few works attempted to model students{'} knowledge state in the setting of Second Language Acquisition. The Duolingo Shared Task on Second Language Acquisition Modeling provides students{'} trace data that we extensively analyze and engineer features from for the task of predicting whether a student will correctly solve a vocabulary exercise. Our analyses of students{'} learning traces reveal that factors like exercise format and engagement impact their exercise performance to a large extent. Overall, we extracted 23 different features as input to a Gradient Tree Boosting framework, which resulted in an AUC score of between 0.80 and 0.82 on the official test set.","year":2018,"title_abstract":"Feature Engineering for Second Language Acquisition Modeling Knowledge tracing serves as a keystone in delivering personalized education. However, few works attempted to model students{'} knowledge state in the setting of Second Language Acquisition. The Duolingo Shared Task on Second Language Acquisition Modeling provides students{'} trace data that we extensively analyze and engineer features from for the task of predicting whether a student will correctly solve a vocabulary exercise. Our analyses of students{'} learning traces reveal that factors like exercise format and engagement impact their exercise performance to a large extent. Overall, we extracted 23 different features as input to a Gradient Tree Boosting framework, which resulted in an AUC score of between 0.80 and 0.82 on the official test set.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.189124912,"Goal":"Quality Education","Task":["Second Language Acquisition Modeling Knowledge tracing","personalized education","Second Language Acquisition","Duolingo Shared Task","Second Language Acquisition Modeling","vocabulary exercise"],"Method":["Feature Engineering","Gradient Tree Boosting framework"]},{"ID":"ortega-etal-2019-adviser","title":"{ADVISER}: A Dialog System Framework for Education {\\&} Research","abstract":"In this paper, we present ADVISER - an open source dialog system framework for education and research purposes. This system supports multi-domain task-oriented conversations in two languages. It additionally provides a flexible architecture in which modules can be arbitrarily combined or exchanged - allowing for easy switching between rules-based and neural network based implementations. Furthermore, ADVISER offers a transparent, user-friendly framework designed for interdisciplinary collaboration: from a flexible back end, allowing easy integration of new features, to an intuitive graphical user interface supporting nontechnical users.","year":2019,"title_abstract":"{ADVISER}: A Dialog System Framework for Education {\\&} Research In this paper, we present ADVISER - an open source dialog system framework for education and research purposes. This system supports multi-domain task-oriented conversations in two languages. It additionally provides a flexible architecture in which modules can be arbitrarily combined or exchanged - allowing for easy switching between rules-based and neural network based implementations. Furthermore, ADVISER offers a transparent, user-friendly framework designed for interdisciplinary collaboration: from a flexible back end, allowing easy integration of new features, to an intuitive graphical user interface supporting nontechnical users.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.189050138,"Goal":"Quality Education","Task":["Education","education and research purposes","multi - domain task - oriented conversations","interdisciplinary collaboration"],"Method":["Dialog System Framework","ADVISER","open source dialog system framework","rules - based and neural network based implementations","ADVISER","user - friendly framework"]},{"ID":"joseph-etal-2021-mis","title":"(Mis)alignment Between Stance Expressed in Social Media Data and Public Opinion Surveys","abstract":"Stance detection, which aims to determine whether an individual is for or against a target concept, promises to uncover public opinion from large streams of social media data. Yet even human annotation of social media content does not always capture {``}stance{''} as measured by public opinion polls. We demonstrate this by directly comparing an individual{'}s self-reported stance to the stance inferred from their social media data. Leveraging a longitudinal public opinion survey with respondent Twitter handles, we conducted this comparison for 1,129 individuals across four salient targets. We find that recall is high for both {``}Pro{'}{'} and {``}Anti{'}{'} stance classifications but precision is variable in a number of cases. We identify three factors leading to the disconnect between text and author stance: temporal inconsistencies, differences in constructs, and measurement errors from both survey respondents and annotators. By presenting a framework for assessing the limitations of stance detection models, this work provides important insight into what stance detection truly measures.","year":2021,"title_abstract":"(Mis)alignment Between Stance Expressed in Social Media Data and Public Opinion Surveys Stance detection, which aims to determine whether an individual is for or against a target concept, promises to uncover public opinion from large streams of social media data. Yet even human annotation of social media content does not always capture {``}stance{''} as measured by public opinion polls. We demonstrate this by directly comparing an individual{'}s self-reported stance to the stance inferred from their social media data. Leveraging a longitudinal public opinion survey with respondent Twitter handles, we conducted this comparison for 1,129 individuals across four salient targets. We find that recall is high for both {``}Pro{'}{'} and {``}Anti{'}{'} stance classifications but precision is variable in a number of cases. We identify three factors leading to the disconnect between text and author stance: temporal inconsistencies, differences in constructs, and measurement errors from both survey respondents and annotators. By presenting a framework for assessing the limitations of stance detection models, this work provides important insight into what stance detection truly measures.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1890027225,"Goal":"Climate Action","Task":["Stance detection","human annotation","stance detection"],"Method":["stance classifications","stance detection models"]},{"ID":"sancheti-etal-2020-lynyrdskynyrd","title":"{L}ynyrd{S}kynyrd at {WNUT}-2020 Task 2: Semi-Supervised Learning for Identification of Informative {COVID}-19 {E}nglish Tweets","abstract":"In this work, we describe our system for WNUT-2020 shared task on the identification of informative COVID-19 English tweets. Our system is an ensemble of various machine learning methods, leveraging both traditional feature-based classifiers as well as recent advances in pre-trained language models that help in capturing the syntactic, semantic, and contextual features from the tweets. We further employ pseudo-labelling to incorporate the unlabelled Twitter data released on the pandemic. Our best performing model achieves an F1-score of 0.9179 on the provided validation set and 0.8805 on the blind test-set.","year":2020,"title_abstract":"{L}ynyrd{S}kynyrd at {WNUT}-2020 Task 2: Semi-Supervised Learning for Identification of Informative {COVID}-19 {E}nglish Tweets In this work, we describe our system for WNUT-2020 shared task on the identification of informative COVID-19 English tweets. Our system is an ensemble of various machine learning methods, leveraging both traditional feature-based classifiers as well as recent advances in pre-trained language models that help in capturing the syntactic, semantic, and contextual features from the tweets. We further employ pseudo-labelling to incorporate the unlabelled Twitter data released on the pandemic. Our best performing model achieves an F1-score of 0.9179 on the provided validation set and 0.8805 on the blind test-set.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1889030635,"Goal":"Climate Action","Task":["Semi - Supervised Learning","Identification of Informative {COVID} - 19","WNUT - 2020","identification of informative COVID - 19"],"Method":["machine learning methods","feature - based classifiers","language models","pseudo - labelling"]},{"ID":"portelli-etal-2020-distilling","title":"Distilling the Evidence to Augment Fact Verification Models","abstract":"The alarming spread of fake news in social media, together with the impossibility of scaling manual fact verification, motivated the development of natural language processing techniques to automatically verify the veracity of claims. Most approaches perform a claim-evidence classification without providing any insights about why the claim is trustworthy or not. We propose, instead, a model-agnostic framework that consists of two modules: (1) a span extractor, which identifies the crucial information connecting claim and evidence; and (2) a classifier that combines claim, evidence, and the extracted spans to predict the veracity of the claim. We show that the spans are informative for the classifier, improving performance and robustness. Tested on several state-of-the-art models over the Fever dataset, the enhanced classifiers consistently achieve higher accuracy while also showing reduced sensitivity to artifacts in the claims.","year":2020,"title_abstract":"Distilling the Evidence to Augment Fact Verification Models The alarming spread of fake news in social media, together with the impossibility of scaling manual fact verification, motivated the development of natural language processing techniques to automatically verify the veracity of claims. Most approaches perform a claim-evidence classification without providing any insights about why the claim is trustworthy or not. We propose, instead, a model-agnostic framework that consists of two modules: (1) a span extractor, which identifies the crucial information connecting claim and evidence; and (2) a classifier that combines claim, evidence, and the extracted spans to predict the veracity of the claim. We show that the spans are informative for the classifier, improving performance and robustness. Tested on several state-of-the-art models over the Fever dataset, the enhanced classifiers consistently achieve higher accuracy while also showing reduced sensitivity to artifacts in the claims.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.188843891,"Goal":"Climate Action","Task":["fact verification","veracity of claims","claim - evidence classification"],"Method":["Fact Verification Models","natural language processing techniques","model - agnostic framework","span extractor","classifier","classifier","classifiers"]},{"ID":"sharifirad-jacovi-2019-learning","title":"Learning and Understanding Different Categories of Sexism Using Convolutional Neural Network{'}s Filters","abstract":"Sexism is very common in social media and makes the boundaries of free speech tighter for female users. Automatically flagging and removing sexist content requires niche identification and description of the categories. In this study, inspired by social science work, we propose three categories of sexism toward women as follows: {``}Indirect sexism{''}, {``}Sexual sexism{''} and {``}Physical sexism{''}. We build classifiers such as Convolutional Neural Network (CNN) to automatically detect different types of sexism and address problems of annotation. Even though inherent non-interpretability of CNN is a challenge for users who detect sexism, as the reason classifying a given speech instance with regard to sexism is difficult to glance from a CNN. However, recent research developed interpretable CNN filters for text data. In a CNN, filters followed by different activation patterns along with global max-pooling can help us tease apart the most important ngrams from the rest. In this paper, we interpret a CNN model trained to classify sexism in order to understand different categories of sexism by detecting semantic categories of ngrams and clustering them. Then, these ngrams in each category are used to improve the performance of the classification task. It is a preliminary work using machine learning and natural language techniques to learn the concept of sexism and distinguishes itself by looking at more precise categories of sexism in social media along with an in-depth investigation of CNN{'}s filters.","year":2019,"title_abstract":"Learning and Understanding Different Categories of Sexism Using Convolutional Neural Network{'}s Filters Sexism is very common in social media and makes the boundaries of free speech tighter for female users. Automatically flagging and removing sexist content requires niche identification and description of the categories. In this study, inspired by social science work, we propose three categories of sexism toward women as follows: {``}Indirect sexism{''}, {``}Sexual sexism{''} and {``}Physical sexism{''}. We build classifiers such as Convolutional Neural Network (CNN) to automatically detect different types of sexism and address problems of annotation. Even though inherent non-interpretability of CNN is a challenge for users who detect sexism, as the reason classifying a given speech instance with regard to sexism is difficult to glance from a CNN. However, recent research developed interpretable CNN filters for text data. In a CNN, filters followed by different activation patterns along with global max-pooling can help us tease apart the most important ngrams from the rest. In this paper, we interpret a CNN model trained to classify sexism in order to understand different categories of sexism by detecting semantic categories of ngrams and clustering them. Then, these ngrams in each category are used to improve the performance of the classification task. It is a preliminary work using machine learning and natural language techniques to learn the concept of sexism and distinguishes itself by looking at more precise categories of sexism in social media along with an in-depth investigation of CNN{'}s filters.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1888414919,"Goal":"Gender Equality","Task":["Learning and Understanding Different Categories of Sexism","Sexism","Automatically flagging and removing sexist content","niche identification","social science work","annotation","sexism","classification task"],"Method":["Convolutional Neural Network{'}s Filters","classifiers","Convolutional Neural Network","CNN","CNN","CNN","CNN","global max - pooling","CNN model","machine learning","natural language techniques","CNN{'}s filters"]},{"ID":"rudinger-etal-2018-gender","title":"Gender Bias in Coreference Resolution","abstract":"We present an empirical study of gender bias in coreference resolution systems. We first introduce a novel, Winograd schema-style set of minimal pair sentences that differ only by pronoun gender. With these {``}Winogender schemas,{''} we evaluate and confirm systematic gender bias in three publicly-available coreference resolution systems, and correlate this bias with real-world and textual gender statistics.","year":2018,"title_abstract":"Gender Bias in Coreference Resolution We present an empirical study of gender bias in coreference resolution systems. We first introduce a novel, Winograd schema-style set of minimal pair sentences that differ only by pronoun gender. With these {``}Winogender schemas,{''} we evaluate and confirm systematic gender bias in three publicly-available coreference resolution systems, and correlate this bias with real-world and textual gender statistics.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1887580156,"Goal":"Gender Equality","Task":["Gender Bias","Coreference Resolution","gender bias","coreference resolution"],"Method":["coreference resolution systems"]},{"ID":"reuver-etal-2021-nlp","title":"No {NLP} Task Should be an Island: Multi-disciplinarity for Diversity in News Recommender Systems","abstract":"Natural Language Processing (NLP) is defined by specific, separate tasks, with each their own literature, benchmark datasets, and definitions. In this position paper, we argue that for a complex problem such as the threat to democracy by non-diverse news recommender systems, it is important to take into account a higher-order, normative goal and its implications. Experts in ethics, political science and media studies have suggested that news recommendation systems could be used to support a deliberative democracy. We reflect on the role of NLP in recommendation systems with this specific goal in mind and show that this theory of democracy helps to identify which NLP tasks and techniques can support this goal, and what work still needs to be done. This leads to recommendations for NLP researchers working on this specific problem as well as researchers working on other complex multidisciplinary problems.","year":2021,"title_abstract":"No {NLP} Task Should be an Island: Multi-disciplinarity for Diversity in News Recommender Systems Natural Language Processing (NLP) is defined by specific, separate tasks, with each their own literature, benchmark datasets, and definitions. In this position paper, we argue that for a complex problem such as the threat to democracy by non-diverse news recommender systems, it is important to take into account a higher-order, normative goal and its implications. Experts in ethics, political science and media studies have suggested that news recommendation systems could be used to support a deliberative democracy. We reflect on the role of NLP in recommendation systems with this specific goal in mind and show that this theory of democracy helps to identify which NLP tasks and techniques can support this goal, and what work still needs to be done. This leads to recommendations for NLP researchers working on this specific problem as well as researchers working on other complex multidisciplinary problems.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.1886933595,"Goal":"Peace, Justice and Strong Institutions","Task":["Diversity","News Recommender Systems","Natural Language Processing","ethics","political science","media studies","news recommendation systems","recommendation systems","NLP tasks","NLP researchers","multidisciplinary problems"],"Method":["news recommender systems","NLP"]},{"ID":"van-den-heuvel-etal-2010-veterantapes","title":"The {V}eteran{T}apes: Research Corpus, Fragment Processing Tool, and Enhanced Publications for the e-Humanities","abstract":"Enhanced Publications are a new way to publish scientific and other results in an electronic article. The advantage of EPs is that the relation between the article and the underlying data facilitate the peer review process and other quality assessment activities. Due to the link between de publication and the research data the publication can be much richer than a paper edition permits. We present an example of EPs in which links are made to interview fragments that include transcripts, audio segments, annotations and metadata. EPs call for a new paradigm of research methodology in which digital persistent access to research data are a central issue. In this contribution we highlight 1. The research data as it is archived and curated, 2. the concept ''``enhanced publication'''' and its scientific value, 3. the ''``fragment fitter tool'''', a language processing tool to facilitate the creation of EPs, 4. IPR issues related to the re-use of the interview data.","year":2010,"title_abstract":"The {V}eteran{T}apes: Research Corpus, Fragment Processing Tool, and Enhanced Publications for the e-Humanities Enhanced Publications are a new way to publish scientific and other results in an electronic article. The advantage of EPs is that the relation between the article and the underlying data facilitate the peer review process and other quality assessment activities. Due to the link between de publication and the research data the publication can be much richer than a paper edition permits. We present an example of EPs in which links are made to interview fragments that include transcripts, audio segments, annotations and metadata. EPs call for a new paradigm of research methodology in which digital persistent access to research data are a central issue. In this contribution we highlight 1. The research data as it is archived and curated, 2. the concept ''``enhanced publication'''' and its scientific value, 3. the ''``fragment fitter tool'''', a language processing tool to facilitate the creation of EPs, 4. IPR issues related to the re-use of the interview data.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1885430664,"Goal":"Sustainable Cities and Communities","Task":["peer review process","quality assessment activities","research methodology","IPR issues"],"Method":["Fragment Processing Tool","EPs","EPs","''``fragment fitter tool''''","language processing tool","EPs"]},{"ID":"depraetere-etal-2020-ape","title":"{APE}-{QUEST}: an {MT} Quality Gate","abstract":"The APE-QUEST project (2018--2020) sets up a quality gate and crowdsourcing workflow for the eTranslation system of EC{'}s Connecting Europe Facility to improve translation quality in specific domains. It packages these services as a translation portal for machine-to-machine and machine-to-human scenarios.","year":2020,"title_abstract":"{APE}-{QUEST}: an {MT} Quality Gate The APE-QUEST project (2018--2020) sets up a quality gate and crowdsourcing workflow for the eTranslation system of EC{'}s Connecting Europe Facility to improve translation quality in specific domains. It packages these services as a translation portal for machine-to-machine and machine-to-human scenarios.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1884227097,"Goal":"Quality Education","Task":["translation","translation portal","machine - to - machine and machine - to - human scenarios"],"Method":["crowdsourcing workflow","eTranslation system"]},{"ID":"ethayarajh-etal-2019-understanding","title":"Understanding Undesirable Word Embedding Associations","abstract":"Word embeddings are often criticized for capturing undesirable word associations such as gender stereotypes. However, methods for measuring and removing such biases remain poorly understood. We show that for any embedding model that implicitly does matrix factorization, debiasing vectors post hoc using subspace projection (Bolukbasi et al., 2016) is, under certain conditions, equivalent to training on an unbiased corpus. We also prove that WEAT, the most common association test for word embeddings, systematically overestimates bias. Given that the subspace projection method is provably effective, we use it to derive a new measure of association called the relational inner product association (RIPA). Experiments with RIPA reveal that, on average, skipgram with negative sampling (SGNS) does not make most words any more gendered than they are in the training corpus. However, for gender-stereotyped words, SGNS actually amplifies the gender association in the corpus.","year":2019,"title_abstract":"Understanding Undesirable Word Embedding Associations Word embeddings are often criticized for capturing undesirable word associations such as gender stereotypes. However, methods for measuring and removing such biases remain poorly understood. We show that for any embedding model that implicitly does matrix factorization, debiasing vectors post hoc using subspace projection (Bolukbasi et al., 2016) is, under certain conditions, equivalent to training on an unbiased corpus. We also prove that WEAT, the most common association test for word embeddings, systematically overestimates bias. Given that the subspace projection method is provably effective, we use it to derive a new measure of association called the relational inner product association (RIPA). Experiments with RIPA reveal that, on average, skipgram with negative sampling (SGNS) does not make most words any more gendered than they are in the training corpus. However, for gender-stereotyped words, SGNS actually amplifies the gender association in the corpus.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1884022355,"Goal":"Gender Equality","Task":["Undesirable Word Embedding Associations","Word embeddings","word embeddings"],"Method":["embedding model","matrix factorization","subspace projection","WEAT","subspace projection method","relational inner product association","RIPA","skipgram with negative sampling","SGNS"]},{"ID":"zeinert-etal-2021-annotating","title":"Annotating Online Misogyny","abstract":"Online misogyny, a category of online abusive language, has serious and harmful social consequences. Automatic detection of misogynistic language online, while imperative, poses complicated challenges to both data gathering, data annotation, and bias mitigation, as this type of data is linguistically complex and diverse. This paper makes three contributions in this area: Firstly, we describe the detailed design of our iterative annotation process and codebook. Secondly, we present a comprehensive taxonomy of labels for annotating misogyny in natural written language, and finally, we introduce a high-quality dataset of annotated posts sampled from social media posts.","year":2021,"title_abstract":"Annotating Online Misogyny Online misogyny, a category of online abusive language, has serious and harmful social consequences. Automatic detection of misogynistic language online, while imperative, poses complicated challenges to both data gathering, data annotation, and bias mitigation, as this type of data is linguistically complex and diverse. This paper makes three contributions in this area: Firstly, we describe the detailed design of our iterative annotation process and codebook. Secondly, we present a comprehensive taxonomy of labels for annotating misogyny in natural written language, and finally, we introduce a high-quality dataset of annotated posts sampled from social media posts.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1882168353,"Goal":"Gender Equality","Task":["Online misogyny","Automatic detection of misogynistic language online","data gathering","data annotation","bias mitigation","annotating misogyny"],"Method":["iterative annotation process"]},{"ID":"pradeep-etal-2021-scientific","title":"Scientific Claim Verification with {V}er{T}5erini","abstract":"This work describes the adaptation of a pretrained sequence-to-sequence model to the task of scientific claim verification in the biomedical domain. We propose a system called VerT5erini that exploits T5 for abstract retrieval, sentence selection, and label prediction, which are three critical sub-tasks of claim verification. We evaluate our pipeline on SciFACT, a newly curated dataset that requires models to not just predict the veracity of claims but also provide relevant sentences from a corpus of scientific literature that support the prediction. Empirically, our system outperforms a strong baseline in each of the three sub-tasks. We further show VerT5erini{'}s ability to generalize to two new datasets of COVID-19 claims using evidence from the CORD-19 corpus.","year":2021,"title_abstract":"Scientific Claim Verification with {V}er{T}5erini This work describes the adaptation of a pretrained sequence-to-sequence model to the task of scientific claim verification in the biomedical domain. We propose a system called VerT5erini that exploits T5 for abstract retrieval, sentence selection, and label prediction, which are three critical sub-tasks of claim verification. We evaluate our pipeline on SciFACT, a newly curated dataset that requires models to not just predict the veracity of claims but also provide relevant sentences from a corpus of scientific literature that support the prediction. Empirically, our system outperforms a strong baseline in each of the three sub-tasks. We further show VerT5erini{'}s ability to generalize to two new datasets of COVID-19 claims using evidence from the CORD-19 corpus.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1881585121,"Goal":"Climate Action","Task":["Scientific Claim Verification","scientific claim verification","abstract retrieval","sentence selection","label prediction","claim verification","prediction"],"Method":["{V}er{T}5erini","pretrained sequence - to - sequence model","VerT5erini","T5","VerT5erini{'}s"]},{"ID":"gao-etal-2018-action","title":"What Action Causes This? Towards Naive Physical Action-Effect Prediction","abstract":"Despite recent advances in knowledge representation, automated reasoning, and machine learning, artificial agents still lack the ability to understand basic action-effect relations regarding the physical world, for example, the action of cutting a cucumber most likely leads to the state where the cucumber is broken apart into smaller pieces. If artificial agents (e.g., robots) ever become our partners in joint tasks, it is critical to empower them with such action-effect understanding so that they can reason about the state of the world and plan for actions. Towards this goal, this paper introduces a new task on naive physical action-effect prediction, which addresses the relations between concrete actions (expressed in the form of verb-noun pairs) and their effects on the state of the physical world as depicted by images. We collected a dataset for this task and developed an approach that harnesses web image data through distant supervision to facilitate learning for action-effect prediction. Our empirical results have shown that web data can be used to complement a small number of seed examples (e.g., three examples for each action) for model learning. This opens up possibilities for agents to learn physical action-effect relations for tasks at hand through communication with humans with a few examples.","year":2018,"title_abstract":"What Action Causes This? Towards Naive Physical Action-Effect Prediction Despite recent advances in knowledge representation, automated reasoning, and machine learning, artificial agents still lack the ability to understand basic action-effect relations regarding the physical world, for example, the action of cutting a cucumber most likely leads to the state where the cucumber is broken apart into smaller pieces. If artificial agents (e.g., robots) ever become our partners in joint tasks, it is critical to empower them with such action-effect understanding so that they can reason about the state of the world and plan for actions. Towards this goal, this paper introduces a new task on naive physical action-effect prediction, which addresses the relations between concrete actions (expressed in the form of verb-noun pairs) and their effects on the state of the physical world as depicted by images. We collected a dataset for this task and developed an approach that harnesses web image data through distant supervision to facilitate learning for action-effect prediction. Our empirical results have shown that web data can be used to complement a small number of seed examples (e.g., three examples for each action) for model learning. This opens up possibilities for agents to learn physical action-effect relations for tasks at hand through communication with humans with a few examples.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1881353408,"Goal":"Climate Action","Task":["Naive Physical Action - Effect Prediction","knowledge representation","machine learning","joint tasks","action - effect understanding","naive physical action - effect prediction","distant supervision","learning","action - effect prediction","model learning","physical action - effect relations"],"Method":["automated reasoning","artificial agents","artificial agents"]},{"ID":"lee-etal-2021-ncuee","title":"{NCUEE}-{NLP} at {MEDIQA} 2021: Health Question Summarization Using {PEGASUS} Transformers","abstract":"This study describes the model design of the NCUEE-NLP system for the MEDIQA challenge at the BioNLP 2021 workshop. We use the PEGASUS transformers and fine-tune the downstream summarization task using our collected and processed datasets. A total of 22 teams participated in the consumer health question summarization task of MEDIQA 2021. Each participating team was allowed to submit a maximum of ten runs. Our best submission, achieving a ROUGE2-F1 score of 0.1597, ranked third among all 128 submissions.","year":2021,"title_abstract":"{NCUEE}-{NLP} at {MEDIQA} 2021: Health Question Summarization Using {PEGASUS} Transformers This study describes the model design of the NCUEE-NLP system for the MEDIQA challenge at the BioNLP 2021 workshop. We use the PEGASUS transformers and fine-tune the downstream summarization task using our collected and processed datasets. A total of 22 teams participated in the consumer health question summarization task of MEDIQA 2021. Each participating team was allowed to submit a maximum of ten runs. Our best submission, achieving a ROUGE2-F1 score of 0.1597, ranked third among all 128 submissions.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.1881196499,"Goal":"Good Health and Well-Being","Task":["Summarization","MEDIQA challenge","BioNLP 2021 workshop","summarization","consumer health question summarization task","MEDIQA 2021"],"Method":["NCUEE - NLP","PEGASUS transformers"]},{"ID":"gorog-2014-taus-post","title":"{TAUS} post-editing productivity tool","abstract":"While there is a massive adoption of MT post-editing as a new service in the global translation industry, a common reference to skills and best practices to do this work well has been missing. TAUS took up the challenge to provide a course that would integrate with the DQF tools and the post-editing best practices developed by TAUS members in the previous years and offers both theory and practice to develop post-editing skills. The contribution of language service providers who are involved in MT and post-editing on a daily basis allowed TAUS to deliver fast on this industry need. This online course addresses the challenges for linguists and translators deciding to work on post-editing assignments and is aimed at those who want to learn the best practices and skills to become more efficient and proficient in the activity of post-editing.","year":2014,"title_abstract":"{TAUS} post-editing productivity tool While there is a massive adoption of MT post-editing as a new service in the global translation industry, a common reference to skills and best practices to do this work well has been missing. TAUS took up the challenge to provide a course that would integrate with the DQF tools and the post-editing best practices developed by TAUS members in the previous years and offers both theory and practice to develop post-editing skills. The contribution of language service providers who are involved in MT and post-editing on a daily basis allowed TAUS to deliver fast on this industry need. This online course addresses the challenges for linguists and translators deciding to work on post-editing assignments and is aimed at those who want to learn the best practices and skills to become more efficient and proficient in the activity of post-editing.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1877194643,"Goal":"Quality Education","Task":["post - editing productivity tool","MT post - editing","global translation industry","post - editing skills","language service providers","MT","post - editing","linguists","translators","post - editing assignments","post - editing"],"Method":["TAUS","DQF tools","post - editing best practices"]},{"ID":"liu-2019-anonymized","title":"Anonymized {BERT}: An Augmentation Approach to the Gendered Pronoun Resolution Challenge","abstract":"We present our 7th place solution to the Gendered Pronoun Resolution challenge, which uses BERT without fine-tuning and a novel augmentation strategy designed for contextual embedding token-level tasks. Our method anonymizes the referent by replacing candidate names with a set of common placeholder names. Besides the usual benefits of effectively increasing training data size, this approach diversifies idiosyncratic information embedded in names. Using same set of common first names can also help the model recognize names better, shorten token length, and remove gender and regional biases associated with names. The system scored 0.1947 log loss in stage 2, where the augmentation contributed to an improvements of 0.04. Post-competition analysis shows that, when using different embedding layers, the system scores 0.1799 which would be third place.","year":2019,"title_abstract":"Anonymized {BERT}: An Augmentation Approach to the Gendered Pronoun Resolution Challenge We present our 7th place solution to the Gendered Pronoun Resolution challenge, which uses BERT without fine-tuning and a novel augmentation strategy designed for contextual embedding token-level tasks. Our method anonymizes the referent by replacing candidate names with a set of common placeholder names. Besides the usual benefits of effectively increasing training data size, this approach diversifies idiosyncratic information embedded in names. Using same set of common first names can also help the model recognize names better, shorten token length, and remove gender and regional biases associated with names. The system scored 0.1947 log loss in stage 2, where the augmentation contributed to an improvements of 0.04. Post-competition analysis shows that, when using different embedding layers, the system scores 0.1799 which would be third place.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1876950264,"Goal":"Gender Equality","Task":["Gendered Pronoun Resolution Challenge","Gendered Pronoun Resolution challenge","contextual embedding token - level tasks"],"Method":["Anonymized {BERT}","Augmentation Approach","7th place solution","BERT","fine - tuning","augmentation strategy","augmentation","embedding layers"]},{"ID":"pais-etal-2020-mwsa","title":"{MWSA} Task at {G}loba{L}ex 2020: {RACAI}{'}s Word Sense Alignment System using a Similarity Measurement of Dictionary Definitions","abstract":"This paper describes RACAI{'}s word sense alignment system, which participated in the Monolingual Word Sense Alignment shared task organized at GlobaLex 2020 workshop. We discuss the system architecture, some of the challenges that we faced as well as present our results on several of the languages available for the task.","year":2020,"title_abstract":"{MWSA} Task at {G}loba{L}ex 2020: {RACAI}{'}s Word Sense Alignment System using a Similarity Measurement of Dictionary Definitions This paper describes RACAI{'}s word sense alignment system, which participated in the Monolingual Word Sense Alignment shared task organized at GlobaLex 2020 workshop. We discuss the system architecture, some of the challenges that we faced as well as present our results on several of the languages available for the task.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1876684427,"Goal":"Gender Equality","Task":["Similarity Measurement of Dictionary Definitions","Monolingual Word Sense Alignment","GlobaLex 2020 workshop"],"Method":["Word Sense Alignment System","RACAI{'}s word sense alignment system"]},{"ID":"ashraf-etal-2022-nayel","title":"{NAYEL} @{LT}-{EDI}-{ACL}2022: Homophobia\/Transphobia Detection for Equality, Diversity, and Inclusion using {SVM}","abstract":"Analysing the contents of social media platforms such as YouTube, Facebook and Twitter gained interest due to the vast number of users. One of the important tasks is homophobia\/transphobia detection. This paper illustrates the system submitted by our team for the homophobia\/transphobia detection in social media comments shared task. A machine learning-based model has been designed and various classification algorithms have been implemented for automatic detection of homophobia in YouTube comments. TF\/IDF has been used with a range of bigram model for vectorization of comments. Support Vector Machines has been used to develop the proposed model and our submission reported 0.91, 0.92, 0.88 weighted f1-score for English, Tamil and Tamil-English datasets respectively.","year":2022,"title_abstract":"{NAYEL} @{LT}-{EDI}-{ACL}2022: Homophobia\/Transphobia Detection for Equality, Diversity, and Inclusion using {SVM} Analysing the contents of social media platforms such as YouTube, Facebook and Twitter gained interest due to the vast number of users. One of the important tasks is homophobia\/transphobia detection. This paper illustrates the system submitted by our team for the homophobia\/transphobia detection in social media comments shared task. A machine learning-based model has been designed and various classification algorithms have been implemented for automatic detection of homophobia in YouTube comments. TF\/IDF has been used with a range of bigram model for vectorization of comments. Support Vector Machines has been used to develop the proposed model and our submission reported 0.91, 0.92, 0.88 weighted f1-score for English, Tamil and Tamil-English datasets respectively.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1876099408,"Goal":"Gender Equality","Task":["Homophobia\/Transphobia Detection","Equality","Inclusion","homophobia\/transphobia detection","homophobia\/transphobia detection","automatic detection of homophobia in YouTube comments","vectorization of comments"],"Method":["{SVM}","machine learning - based model","classification algorithms","TF\/IDF","bigram model","Support Vector Machines"]},{"ID":"shang-etal-2018-unsupervised","title":"Unsupervised Abstractive Meeting Summarization with Multi-Sentence Compression and Budgeted Submodular Maximization","abstract":"We introduce a novel graph-based framework for abstractive meeting speech summarization that is fully unsupervised and does not rely on any annotations. Our work combines the strengths of multiple recent approaches while addressing their weaknesses. Moreover, we leverage recent advances in word embeddings and graph degeneracy applied to NLP to take exterior semantic knowledge into account, and to design custom diversity and informativeness measures. Experiments on the AMI and ICSI corpus show that our system improves on the state-of-the-art. Code and data are publicly available, and our system can be interactively tested.","year":2018,"title_abstract":"Unsupervised Abstractive Meeting Summarization with Multi-Sentence Compression and Budgeted Submodular Maximization We introduce a novel graph-based framework for abstractive meeting speech summarization that is fully unsupervised and does not rely on any annotations. Our work combines the strengths of multiple recent approaches while addressing their weaknesses. Moreover, we leverage recent advances in word embeddings and graph degeneracy applied to NLP to take exterior semantic knowledge into account, and to design custom diversity and informativeness measures. Experiments on the AMI and ICSI corpus show that our system improves on the state-of-the-art. Code and data are publicly available, and our system can be interactively tested.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1875409484,"Goal":"Partnership for the Goals","Task":["Unsupervised Abstractive Meeting Summarization","Multi - Sentence Compression","abstractive meeting speech summarization","word embeddings","NLP"],"Method":["Budgeted Submodular Maximization","graph - based framework","graph degeneracy"]},{"ID":"que-2021-simon-lt","title":"Simon @ {LT}-{EDI}-{EACL}2021: Detecting Hope Speech with {BERT}","abstract":"In today{'}s society, the rapid development of communication technology allows us to communicate with people from different parts of the world. In the process of communication, each person treats others differently. Some people are used to using offensive and sarcastic language to express their views. These words cause pain to others and make people feel down. Some people are used to sharing happiness with others and encouraging others. Such people bring joy and hope to others through their words. On social media platforms, these two kinds of language are all over the place. If people want to make the online world a better place, they will have to deal with both. So identifying offensive language and hope language is an essential task. There have been many assignments about offensive language. Shared Task on Hope Speech Detection for Equality, Diversity, and Inclusion at LT-EDI 2021-EACL 2021 uses another unique perspective {--} to identify the language of Hope to make contributions to society. The XLM-Roberta model is an excellent multilingual model. Our team used a fine-tuned XLM-Roberta model to accomplish this task.","year":2021,"title_abstract":"Simon @ {LT}-{EDI}-{EACL}2021: Detecting Hope Speech with {BERT} In today{'}s society, the rapid development of communication technology allows us to communicate with people from different parts of the world. In the process of communication, each person treats others differently. Some people are used to using offensive and sarcastic language to express their views. These words cause pain to others and make people feel down. Some people are used to sharing happiness with others and encouraging others. Such people bring joy and hope to others through their words. On social media platforms, these two kinds of language are all over the place. If people want to make the online world a better place, they will have to deal with both. So identifying offensive language and hope language is an essential task. There have been many assignments about offensive language. Shared Task on Hope Speech Detection for Equality, Diversity, and Inclusion at LT-EDI 2021-EACL 2021 uses another unique perspective {--} to identify the language of Hope to make contributions to society. The XLM-Roberta model is an excellent multilingual model. Our team used a fine-tuned XLM-Roberta model to accomplish this task.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1870349646,"Goal":"Gender Equality","Task":["Detecting Hope Speech","communication technology","communication","Hope Speech Detection","Equality","LT - EDI","EACL 2021"],"Method":["XLM - Roberta model","multilingual model","XLM - Roberta model"]},{"ID":"haber-waks-2021-classification-geotemporal","title":"Classification and Geotemporal Analysis of Quality-of-Life Issues in Tenant Reviews","abstract":"Online tenant reviews of multifamily residential properties present a unique source of information for commercial real estate investing and research. Real estate professionals frequently read tenant reviews to uncover property-related issues that are otherwise difficult to detect, a process that is both biased and time-consuming. Using this as motivation, we asked whether a text classification-based approach can automate the detection of four carefully defined, major quality-of-life issues: severe crime, noise nuisance, pest burden, and parking difficulties. We aggregate 5.5 million tenant reviews from five sources and use two-stage crowdsourced labeling on 0.1{\\%} of the data to produce high-quality labels for subsequent text classification. Following fine-tuning of pretrained language models on millions of reviews, we train a multi-label reviews classifier that achieves a mean AUROC of 0.965 on these labels. We next use the model to reveal temporal and spatial patterns among tens of thousands of multifamily properties. Collectively, these results highlight the feasibility of automated analysis of housing trends and investment opportunities using tenant-perspective data.","year":2021,"title_abstract":"Classification and Geotemporal Analysis of Quality-of-Life Issues in Tenant Reviews Online tenant reviews of multifamily residential properties present a unique source of information for commercial real estate investing and research. Real estate professionals frequently read tenant reviews to uncover property-related issues that are otherwise difficult to detect, a process that is both biased and time-consuming. Using this as motivation, we asked whether a text classification-based approach can automate the detection of four carefully defined, major quality-of-life issues: severe crime, noise nuisance, pest burden, and parking difficulties. We aggregate 5.5 million tenant reviews from five sources and use two-stage crowdsourced labeling on 0.1{\\%} of the data to produce high-quality labels for subsequent text classification. Following fine-tuning of pretrained language models on millions of reviews, we train a multi-label reviews classifier that achieves a mean AUROC of 0.965 on these labels. We next use the model to reveal temporal and spatial patterns among tens of thousands of multifamily properties. Collectively, these results highlight the feasibility of automated analysis of housing trends and investment opportunities using tenant-perspective data.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1869689524,"Goal":"Sustainable Cities and Communities","Task":["Classification and Geotemporal Analysis of Quality - of - Life Issues","commercial real estate investing and research","detection","quality - of - life issues","text classification","automated analysis of housing trends","investment opportunities"],"Method":["text classification - based approach","crowdsourced labeling","pretrained language models","multi - label reviews classifier"]},{"ID":"whang-vosoughi-2020-dartmouth","title":"{D}artmouth {CS} at {WNUT}-2020 Task 2: Informative {COVID}-19 Tweet Classification Using {BERT}","abstract":"We describe the systems developed for the WNUT-2020 shared task 2, identification of informative COVID-19 English Tweets. BERT is a highly performant model for Natural Language Processing tasks. We increased BERT{'}s performance in this classification task by fine-tuning BERT and concatenating its embeddings with Tweet-specific features and training a Support Vector Machine (SVM) for classification (henceforth called BERT+). We compared its performance to a suite of machine learning models. We used a Twitter specific data cleaning pipeline and word-level TF-IDF to extract features for the non-BERT models. BERT+ was the top performing model with an F1-score of 0.8713.","year":2020,"title_abstract":"{D}artmouth {CS} at {WNUT}-2020 Task 2: Informative {COVID}-19 Tweet Classification Using {BERT} We describe the systems developed for the WNUT-2020 shared task 2, identification of informative COVID-19 English Tweets. BERT is a highly performant model for Natural Language Processing tasks. We increased BERT{'}s performance in this classification task by fine-tuning BERT and concatenating its embeddings with Tweet-specific features and training a Support Vector Machine (SVM) for classification (henceforth called BERT+). We compared its performance to a suite of machine learning models. We used a Twitter specific data cleaning pipeline and word-level TF-IDF to extract features for the non-BERT models. BERT+ was the top performing model with an F1-score of 0.8713.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1869514138,"Goal":"Climate Action","Task":["Tweet Classification","identification of informative COVID","Natural Language Processing tasks","classification task","classification"],"Method":["{BERT}","BERT","BERT{'}s","BERT","Support Vector Machine","BERT+)","machine learning models","Twitter specific data cleaning pipeline","word - level TF - IDF","BERT","BERT+"]},{"ID":"konat-etal-2016-corpus","title":"A Corpus of Argument Networks: Using Graph Properties to Analyse Divisive Issues","abstract":"Governments are increasingly utilising online platforms in order to engage with, and ascertain the opinions of, their citizens. Whilst policy makers could potentially benefit from such enormous feedback from society, they first face the challenge of making sense out of the large volumes of data produced. This creates a demand for tools and technologies which will enable governments to quickly and thoroughly digest the points being made and to respond accordingly. By determining the argumentative and dialogical structures contained within a debate, we are able to determine the issues which are divisive and those which attract agreement. This paper proposes a method of graph-based analytics which uses properties of graphs representing networks of arguments pro- {\\&} con- in order to automatically analyse issues which divide citizens about new regulations. By future application of the most recent advances in argument mining, the results reported here will have a chance to scale up to enable sense-making of the vast amount of feedback received from citizens on directions that policy should take.","year":2016,"title_abstract":"A Corpus of Argument Networks: Using Graph Properties to Analyse Divisive Issues Governments are increasingly utilising online platforms in order to engage with, and ascertain the opinions of, their citizens. Whilst policy makers could potentially benefit from such enormous feedback from society, they first face the challenge of making sense out of the large volumes of data produced. This creates a demand for tools and technologies which will enable governments to quickly and thoroughly digest the points being made and to respond accordingly. By determining the argumentative and dialogical structures contained within a debate, we are able to determine the issues which are divisive and those which attract agreement. This paper proposes a method of graph-based analytics which uses properties of graphs representing networks of arguments pro- {\\&} con- in order to automatically analyse issues which divide citizens about new regulations. By future application of the most recent advances in argument mining, the results reported here will have a chance to scale up to enable sense-making of the vast amount of feedback received from citizens on directions that policy should take.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1867371649,"Goal":"Climate Action","Task":["Analyse Divisive Issues","argument mining"],"Method":["Corpus of Argument Networks","Graph Properties","graph - based analytics"]},{"ID":"xie-etal-2017-eica","title":"{EICA} Team at {S}em{E}val-2017 Task 3: Semantic and Metadata-based Features for Community Question Answering","abstract":"We describe our system for participating in SemEval-2017 Task 3 on Community Question Answering. Our approach relies on combining a rich set of various types of features: semantic and metadata. The most important group turned out to be the metadata feature and the semantic vectors trained on QatarLiving data. In the main Subtask C, our primary submission was ranked fourth, with a MAP of 13.48 and accuracy of 97.08. In Subtask A, our primary submission get into the top 50{\\%}.","year":2017,"title_abstract":"{EICA} Team at {S}em{E}val-2017 Task 3: Semantic and Metadata-based Features for Community Question Answering We describe our system for participating in SemEval-2017 Task 3 on Community Question Answering. Our approach relies on combining a rich set of various types of features: semantic and metadata. The most important group turned out to be the metadata feature and the semantic vectors trained on QatarLiving data. In the main Subtask C, our primary submission was ranked fourth, with a MAP of 13.48 and accuracy of 97.08. In Subtask A, our primary submission get into the top 50{\\%}.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.186412096,"Goal":"Sustainable Cities and Communities","Task":["Community Question Answering","Community Question Answering"],"Method":["semantic vectors"]},{"ID":"bosca-etal-2014-gold","title":"A Gold Standard for {CLIR} evaluation in the Organic Agriculture Domain","abstract":"We present a gold standard for the evaluation of Cross Language Information Retrieval systems in the domain of Organic Agriculture and AgroEcology. The presented resource is free to use for research purposes and it includes a collection of multilingual documents annotated with respect to a domain ontology, the ontology used for annotating the resources, a set of 48 queries in 12 languages and a gold standard with the correct resources for the proposed queries. The goal of this work consists in contributing to the research community with a resource for evaluating multilingual retrieval algorithms, with particular focus on domain adaptation strategies for \u0093general purpose\u0094 multilingual information retrieval systems and on the effective exploitation of semantic annotations. Domain adaptation is in fact an important activity for tuning the retrieval system, reducing the ambiguities and improving the precision of information retrieval. Domain ontologies constitute a diffuse practice for defining the conceptual space of a corpus and mapping resources to specific topics and in our lab we propose as well to investigate and evaluate the impact of this information in enhancing the retrieval of contents. An initial experiment is described, giving a baseline for further research with the proposed gold standard.","year":2014,"title_abstract":"A Gold Standard for {CLIR} evaluation in the Organic Agriculture Domain We present a gold standard for the evaluation of Cross Language Information Retrieval systems in the domain of Organic Agriculture and AgroEcology. The presented resource is free to use for research purposes and it includes a collection of multilingual documents annotated with respect to a domain ontology, the ontology used for annotating the resources, a set of 48 queries in 12 languages and a gold standard with the correct resources for the proposed queries. The goal of this work consists in contributing to the research community with a resource for evaluating multilingual retrieval algorithms, with particular focus on domain adaptation strategies for \u0093general purpose\u0094 multilingual information retrieval systems and on the effective exploitation of semantic annotations. Domain adaptation is in fact an important activity for tuning the retrieval system, reducing the ambiguities and improving the precision of information retrieval. Domain ontologies constitute a diffuse practice for defining the conceptual space of a corpus and mapping resources to specific topics and in our lab we propose as well to investigate and evaluate the impact of this information in enhancing the retrieval of contents. An initial experiment is described, giving a baseline for further research with the proposed gold standard.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.186355412,"Goal":"Life on Land","Task":["{CLIR} evaluation","Organic Agriculture Domain","Cross Language Information Retrieval systems","Organic Agriculture","AgroEcology","multilingual retrieval algorithms","multilingual information retrieval systems","semantic annotations","retrieval system","information retrieval","retrieval of contents"],"Method":["domain adaptation strategies","Domain adaptation","Domain ontologies"]},{"ID":"xiang-etal-2020-learning","title":"Learning to Stop: A Simple yet Effective Approach to Urban Vision-Language Navigation","abstract":"Vision-and-Language Navigation (VLN) is a natural language grounding task where an agent learns to follow language instructions and navigate to specified destinations in real-world environments. A key challenge is to recognize and stop at the correct location, especially for complicated outdoor environments. Existing methods treat the STOP action equally as other actions, which results in undesirable behaviors that the agent often fails to stop at the destination even though it might be on the right path. Therefore, we propose Learning to Stop (L2Stop), a simple yet effective policy module that differentiates STOP and other actions. Our approach achieves the new state of the art on a challenging urban VLN dataset Touchdown, outperforming the baseline by 6.89{\\%} (absolute improvement) on Success weighted by Edit Distance (SED).","year":2020,"title_abstract":"Learning to Stop: A Simple yet Effective Approach to Urban Vision-Language Navigation Vision-and-Language Navigation (VLN) is a natural language grounding task where an agent learns to follow language instructions and navigate to specified destinations in real-world environments. A key challenge is to recognize and stop at the correct location, especially for complicated outdoor environments. Existing methods treat the STOP action equally as other actions, which results in undesirable behaviors that the agent often fails to stop at the destination even though it might be on the right path. Therefore, we propose Learning to Stop (L2Stop), a simple yet effective policy module that differentiates STOP and other actions. Our approach achieves the new state of the art on a challenging urban VLN dataset Touchdown, outperforming the baseline by 6.89{\\%} (absolute improvement) on Success weighted by Edit Distance (SED).","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.185981214,"Goal":"Sustainable Cities and Communities","Task":["Learning to Stop","Urban Vision - Language Navigation","Vision - and - Language Navigation","natural language grounding task","Learning to Stop"],"Method":["policy module"]},{"ID":"yu-etal-2020-measuring","title":"Measuring Correlation-to-Causation Exaggeration in Press Releases","abstract":"Press releases have an increasingly strong influence on media coverage of health research; however, they have been found to contain seriously exaggerated claims that can misinform the public and undermine public trust in science. In this study we propose an NLP approach to identify exaggerated causal claims made in health press releases that report on observational studies, which are designed to establish correlational findings, but are often exaggerated as causal. We developed a new corpus and trained models that can identify causal claims in the main statements in a press release. By comparing the claims made in a press release with the corresponding claims in the original research paper, we found that 22{\\%} of press releases made exaggerated causal claims from correlational findings in observational studies. Furthermore, universities exaggerated more often than journal publishers by a ratio of 1.5 to 1. Encouragingly, the exaggeration rate has slightly decreased over the past 10 years, despite the increase of the total number of press releases. More research is needed to understand the cause of the decreasing pattern.","year":2020,"title_abstract":"Measuring Correlation-to-Causation Exaggeration in Press Releases Press releases have an increasingly strong influence on media coverage of health research; however, they have been found to contain seriously exaggerated claims that can misinform the public and undermine public trust in science. In this study we propose an NLP approach to identify exaggerated causal claims made in health press releases that report on observational studies, which are designed to establish correlational findings, but are often exaggerated as causal. We developed a new corpus and trained models that can identify causal claims in the main statements in a press release. By comparing the claims made in a press release with the corresponding claims in the original research paper, we found that 22{\\%} of press releases made exaggerated causal claims from correlational findings in observational studies. Furthermore, universities exaggerated more often than journal publishers by a ratio of 1.5 to 1. Encouragingly, the exaggeration rate has slightly decreased over the past 10 years, despite the increase of the total number of press releases. More research is needed to understand the cause of the decreasing pattern.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1858724505,"Goal":"Climate Action","Task":["Measuring Correlation - to - Causation Exaggeration","media coverage of health research;"],"Method":["NLP approach"]},{"ID":"b-etal-2022-ssncse","title":"{SSNCSE}{\\_}{NLP}@{LT}-{EDI}-{ACL}2022:Hope Speech Detection for Equality, Diversity and Inclusion using sentence transformers","abstract":"In recent times, applications have been developed to regulate and control the spread of negativity and toxicity on online platforms. The world is filled with serious problems like political {\\&} religious conflicts, wars, pandemics, and offensive hate speech is the last thing we desire. Our task was to classify a text into {`}Hope Speech{'} and {`}Non-Hope Speech{'}. We searched for datasets acquired from YouTube comments that offer support, reassurance, inspiration, and insight, and the ones that don{'}t. The datasets were provided to us by the LTEDI organizers in English, Tamil, Spanish, Kannada, and Malayalam. To successfully identify and classify them, we employed several machine learning transformer models such as m-BERT, MLNet, BERT, XLMRoberta, and XLM{\\_}MLM. The observed results indicate that the BERT and m-BERT have obtained the best results among all the other techniques, gaining a weighted F1- score of 0.92, 0.71, 0.76, 0.87, and 0.83 for English, Tamil, Spanish, Kannada, and Malayalam respectively. This paper depicts our work for the Shared Task on Hope Speech Detection for Equality, Diversity, and Inclusion at LTEDI 2021.","year":2022,"title_abstract":"{SSNCSE}{\\_}{NLP}@{LT}-{EDI}-{ACL}2022:Hope Speech Detection for Equality, Diversity and Inclusion using sentence transformers In recent times, applications have been developed to regulate and control the spread of negativity and toxicity on online platforms. The world is filled with serious problems like political {\\&} religious conflicts, wars, pandemics, and offensive hate speech is the last thing we desire. Our task was to classify a text into {`}Hope Speech{'} and {`}Non-Hope Speech{'}. We searched for datasets acquired from YouTube comments that offer support, reassurance, inspiration, and insight, and the ones that don{'}t. The datasets were provided to us by the LTEDI organizers in English, Tamil, Spanish, Kannada, and Malayalam. To successfully identify and classify them, we employed several machine learning transformer models such as m-BERT, MLNet, BERT, XLMRoberta, and XLM{\\_}MLM. The observed results indicate that the BERT and m-BERT have obtained the best results among all the other techniques, gaining a weighted F1- score of 0.92, 0.71, 0.76, 0.87, and 0.83 for English, Tamil, Spanish, Kannada, and Malayalam respectively. This paper depicts our work for the Shared Task on Hope Speech Detection for Equality, Diversity, and Inclusion at LTEDI 2021.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1857309937,"Goal":"Gender Equality","Task":["Hope Speech Detection","Equality","Diversity","Inclusion","Hope Speech Detection","Equality","Diversity","Inclusion"],"Method":["sentence transformers","machine learning transformer models","m - BERT","MLNet","BERT","XLMRoberta","XLM{\\_}MLM","BERT","m - BERT"]},{"ID":"van-den-bogaert-etal-2020-mice","title":"{MICE}: a middleware layer for {MT}","abstract":"The MICE project (2018-2020) will deliver a middleware layer for improving the output quality of the eTranslation system of EC{'}s Connecting Europe Facility through additional services, such as domain adaptation and named entity recognition. It will also deliver a user portal, allowing for human post-editing.","year":2020,"title_abstract":"{MICE}: a middleware layer for {MT} The MICE project (2018-2020) will deliver a middleware layer for improving the output quality of the eTranslation system of EC{'}s Connecting Europe Facility through additional services, such as domain adaptation and named entity recognition. It will also deliver a user portal, allowing for human post-editing.","social_need":"Industry, Innovation and Infrastrucure Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation","cosine_similarity":0.185569793,"Goal":"Industry, Innovation and Infrastrucure","Task":["human post - editing"],"Method":["middleware layer","middleware layer","eTranslation system","domain adaptation","named entity recognition"]},{"ID":"arranz-choukri-2010-elras","title":"{ELRA}{'}s Services 15 Years on...Sharing and Anticipating the Community","abstract":"15 years have gone by and ELRA continues embracing the needs of the HLT community to design its services and to implement them through its operational body, ELDA. The needs of the community have become much more ambitious...Larger language resources (LR), better quality ones (how do we reach a compromise between price \u2015 maybe free \u2015 and quality?), more annotations, at different levels and for different modalities...easy access to these LRs and solved IPR issues, appropriate and adaptable licensing schemas...large activity in HLT evaluation, both in terms of setting up the evaluation and in helping produce all necessary data, protocols, specifications as well as conducting the whole process...producing the LRs researchers and developers need, LRs for a wide variety of activities and technologies...for development, for training, for evaluation...Disseminating all knowledge in the field, whether generated at ELRA or elsewhere...keeping the community up to date with what goes on regularly (LREC conferences, LangTech, Newsletters, HLT Evaluation Portal, etc.). Needless to say, part of ELRA\u0092s evolution implies facing and anticipating the realities of the new Internet and data exchange era and remaining a LR backbone...looking into new models of LR data centres and platforms, LR access and exchange via web services, new models for infrastructures and repositories with even higher collaboration to make it happen. ELRA\/ELDA participate in a number of international projects focused on this new production and sharing schema that will be detailed in the current paper.","year":2010,"title_abstract":"{ELRA}{'}s Services 15 Years on...Sharing and Anticipating the Community 15 years have gone by and ELRA continues embracing the needs of the HLT community to design its services and to implement them through its operational body, ELDA. The needs of the community have become much more ambitious...Larger language resources (LR), better quality ones (how do we reach a compromise between price \u2015 maybe free \u2015 and quality?), more annotations, at different levels and for different modalities...easy access to these LRs and solved IPR issues, appropriate and adaptable licensing schemas...large activity in HLT evaluation, both in terms of setting up the evaluation and in helping produce all necessary data, protocols, specifications as well as conducting the whole process...producing the LRs researchers and developers need, LRs for a wide variety of activities and technologies...for development, for training, for evaluation...Disseminating all knowledge in the field, whether generated at ELRA or elsewhere...keeping the community up to date with what goes on regularly (LREC conferences, LangTech, Newsletters, HLT Evaluation Portal, etc.). Needless to say, part of ELRA\u0092s evolution implies facing and anticipating the realities of the new Internet and data exchange era and remaining a LR backbone...looking into new models of LR data centres and platforms, LR access and exchange via web services, new models for infrastructures and repositories with even higher collaboration to make it happen. ELRA\/ELDA participate in a number of international projects focused on this new production and sharing schema that will be detailed in the current paper.","social_need":"Industry, Innovation and Infrastrucure Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation","cosine_similarity":0.1852861345,"Goal":"Industry, Innovation and Infrastrucure","Task":["IPR issues","HLT evaluation","evaluation","development","training","evaluation","Internet and data exchange era","LR","LR","LR access and exchange","infrastructures","repositories","production and sharing schema"],"Method":["HLT","licensing schemas","LRs","ELRA\u0092s evolution","web services"]},{"ID":"nir-melnick-2005-constant","title":"Constant-Sense Connection Paths","abstract":"A multilingual sense code may chart ``constant-sense connection paths'' across languages. A writer, not versed in any target language, may nonetheless proofread the sense for translation and edit it, to ensure that his meaning is conveyed as he wishes it, to other languages. A translation-ready format may be thus produced, to serve as a printing-press plate, for precise and automatic translation to any language, or to a plurality of languages. The translation-ready format may describe each word and the full document with a comprehensive code, which specifies the multilingual sense code and other relevant information about the word, in a standardized fashion, digitally, forming a unified, language-independent tagging system and a unified, language-independent lexicon.","year":2005,"title_abstract":"Constant-Sense Connection Paths A multilingual sense code may chart ``constant-sense connection paths'' across languages. A writer, not versed in any target language, may nonetheless proofread the sense for translation and edit it, to ensure that his meaning is conveyed as he wishes it, to other languages. A translation-ready format may be thus produced, to serve as a printing-press plate, for precise and automatic translation to any language, or to a plurality of languages. The translation-ready format may describe each word and the full document with a comprehensive code, which specifies the multilingual sense code and other relevant information about the word, in a standardized fashion, digitally, forming a unified, language-independent tagging system and a unified, language-independent lexicon.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1852236688,"Goal":"Partnership for the Goals","Task":["translation","translation"],"Method":["language - independent tagging system"]},{"ID":"christiansen-etal-2021-effect","title":"The Effect of Round-Trip Translation on Fairness in Sentiment Analysis","abstract":"Sentiment analysis systems have been shown to exhibit sensitivity to protected attributes. Round-trip translation, on the other hand, has been shown to normalize text. We explore the impact of round-trip translation on the demographic parity of sentiment classifiers and show how round-trip translation consistently improves classification fairness at test time (reducing up to 47{\\%} of between-group gaps). We also explore the idea of retraining sentiment classifiers on round-trip-translated data.","year":2021,"title_abstract":"The Effect of Round-Trip Translation on Fairness in Sentiment Analysis Sentiment analysis systems have been shown to exhibit sensitivity to protected attributes. Round-trip translation, on the other hand, has been shown to normalize text. We explore the impact of round-trip translation on the demographic parity of sentiment classifiers and show how round-trip translation consistently improves classification fairness at test time (reducing up to 47{\\%} of between-group gaps). We also explore the idea of retraining sentiment classifiers on round-trip-translated data.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1850630045,"Goal":"Reduced Inequalities","Task":["Translation","Fairness","Sentiment Analysis","Sentiment analysis","Round - trip translation","translation","translation"],"Method":["sentiment classifiers","sentiment classifiers"]},{"ID":"fourla-yannoutsou-1998-implementing","title":"Implementing {MT} in the {G}reek public sector","abstract":"This paper presents the activities of Euromat (European Machine Translation) office in Greece, which has been functioning as a centre for Machine Translation Services for the Greek Public Sector since 1994. It describes the user profile, his\/her attitude towards MT, strategies of promotion and the collected corpus for the first three years. User data were collected by questionnaires, interviews and corpus statistics. The general conclusions which have come out from our surveys are discussed.","year":1998,"title_abstract":"Implementing {MT} in the {G}reek public sector This paper presents the activities of Euromat (European Machine Translation) office in Greece, which has been functioning as a centre for Machine Translation Services for the Greek Public Sector since 1994. It describes the user profile, his\/her attitude towards MT, strategies of promotion and the collected corpus for the first three years. User data were collected by questionnaires, interviews and corpus statistics. The general conclusions which have come out from our surveys are discussed.","social_need":"Decent Work and Economic Growth Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all","cosine_similarity":0.1850136667,"Goal":"Decent Work and Economic Growth","Task":["{G}reek public sector","Machine Translation) office","Machine Translation Services","MT","promotion"],"Method":["{MT}"]},{"ID":"richter-etal-2017-heidelplace","title":"{H}eidel{P}lace: An Extensible Framework for Geoparsing","abstract":"Geographic information extraction from textual data sources, called geoparsing, is a key task in text processing and central to subsequent spatial analysis approaches. Several geoparsers are available that support this task, each with its own (often limited or specialized) gazetteer and its own approaches to toponym detection and resolution. In this demonstration paper, we present HeidelPlace, an extensible framework in support of geoparsing. Key features of HeidelPlace include a generic gazetteer model that supports the integration of place information from different knowledge bases, and a pipeline approach that enables an effective combination of diverse modules tailored to specific geoparsing tasks. This makes HeidelPlace a valuable tool for testing and evaluating different gazetteer sources and geoparsing methods. In the demonstration, we show how to set up a geoparsing workflow with HeidelPlace and how it can be used to compare and consolidate the output of different geoparsing approaches.","year":2017,"title_abstract":"{H}eidel{P}lace: An Extensible Framework for Geoparsing Geographic information extraction from textual data sources, called geoparsing, is a key task in text processing and central to subsequent spatial analysis approaches. Several geoparsers are available that support this task, each with its own (often limited or specialized) gazetteer and its own approaches to toponym detection and resolution. In this demonstration paper, we present HeidelPlace, an extensible framework in support of geoparsing. Key features of HeidelPlace include a generic gazetteer model that supports the integration of place information from different knowledge bases, and a pipeline approach that enables an effective combination of diverse modules tailored to specific geoparsing tasks. This makes HeidelPlace a valuable tool for testing and evaluating different gazetteer sources and geoparsing methods. In the demonstration, we show how to set up a geoparsing workflow with HeidelPlace and how it can be used to compare and consolidate the output of different geoparsing approaches.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1849945933,"Goal":"Sustainable Cities and Communities","Task":["Geoparsing Geographic information extraction","text processing","toponym detection","resolution","geoparsing","geoparsing tasks"],"Method":["Extensible Framework","geoparsing","spatial analysis approaches","geoparsers","HeidelPlace","extensible framework","HeidelPlace","generic gazetteer model","pipeline approach","geoparsing methods","geoparsing workflow","HeidelPlace","geoparsing approaches"]},{"ID":"abraham-etal-2020-crowdsourcing","title":"Crowdsourcing Speech Data for Low-Resource Languages from Low-Income Workers","abstract":"Voice-based technologies are essential to cater to the hundreds of millions of new smartphone users. However, most of the languages spoken by these new users have little to no labelled speech data. Unfortunately, collecting labelled speech data in any language is an expensive and resource-intensive task. Moreover, existing platforms typically collect speech data only from urban speakers familiar with digital technology whose dialects are often very different from low-income users. In this paper, we explore the possibility of collecting labelled speech data directly from low-income workers. In addition to providing diversity to the speech dataset, we believe this approach can also provide valuable supplemental earning opportunities to these communities. To this end, we conducted a study where we collected labelled speech data in the Marathi language from three different user groups: low-income rural users, low-income urban users, and university students. Overall, we collected 109 hours of data from 36 participants. Our results show that the data collected from low-income participants is of comparable quality to the data collected from university students (who are typically employed to do this work) and that crowdsourcing speech data from low-income rural and urban workers is a viable method of gathering speech data.","year":2020,"title_abstract":"Crowdsourcing Speech Data for Low-Resource Languages from Low-Income Workers Voice-based technologies are essential to cater to the hundreds of millions of new smartphone users. However, most of the languages spoken by these new users have little to no labelled speech data. Unfortunately, collecting labelled speech data in any language is an expensive and resource-intensive task. Moreover, existing platforms typically collect speech data only from urban speakers familiar with digital technology whose dialects are often very different from low-income users. In this paper, we explore the possibility of collecting labelled speech data directly from low-income workers. In addition to providing diversity to the speech dataset, we believe this approach can also provide valuable supplemental earning opportunities to these communities. To this end, we conducted a study where we collected labelled speech data in the Marathi language from three different user groups: low-income rural users, low-income urban users, and university students. Overall, we collected 109 hours of data from 36 participants. Our results show that the data collected from low-income participants is of comparable quality to the data collected from university students (who are typically employed to do this work) and that crowdsourcing speech data from low-income rural and urban workers is a viable method of gathering speech data.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1849886924,"Goal":"Sustainable Cities and Communities","Task":["speech data"],"Method":["Voice - based technologies"]},{"ID":"kalpakchi-boye-2019-spacerefnet","title":"{S}pace{R}ef{N}et: a neural approach to spatial reference resolution in a real city environment","abstract":"Adding interactive capabilities to pedestrian wayfinding systems in the form of spoken dialogue will make them more natural to humans. Such an interactive wayfinding system needs to continuously understand and interpret pedestrian{'}s utterances referring to the spatial context. Achieving this requires the system to identify exophoric referring expressions in the utterances, and link these expressions to the geographic entities in the vicinity. This exophoric spatial reference resolution problem is difficult, as there are often several dozens of candidate referents. We present a neural network-based approach for identifying pedestrian{'}s references (using a network called RefNet) and resolving them to appropriate geographic objects (using a network called SpaceRefNet). Both methods show promising results beating the respective baselines and earlier reported results in the literature.","year":2019,"title_abstract":"{S}pace{R}ef{N}et: a neural approach to spatial reference resolution in a real city environment Adding interactive capabilities to pedestrian wayfinding systems in the form of spoken dialogue will make them more natural to humans. Such an interactive wayfinding system needs to continuously understand and interpret pedestrian{'}s utterances referring to the spatial context. Achieving this requires the system to identify exophoric referring expressions in the utterances, and link these expressions to the geographic entities in the vicinity. This exophoric spatial reference resolution problem is difficult, as there are often several dozens of candidate referents. We present a neural network-based approach for identifying pedestrian{'}s references (using a network called RefNet) and resolving them to appropriate geographic objects (using a network called SpaceRefNet). Both methods show promising results beating the respective baselines and earlier reported results in the literature.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1848143637,"Goal":"Sustainable Cities and Communities","Task":["spatial reference resolution","real city environment","pedestrian wayfinding systems","exophoric spatial reference resolution problem","pedestrian{'}s references"],"Method":["neural approach","interactive capabilities","interactive wayfinding system","neural network - based approach","RefNet)","SpaceRefNet)"]},{"ID":"treviso-etal-2021-ist","title":"{IST}-Unbabel 2021 Submission for the Explainable Quality Estimation Shared Task","abstract":"We present the joint contribution of Instituto Superior T{\\'e}cnico (IST) and Unbabel to the Explainable Quality Estimation (QE) shared task, where systems were submitted to two tracks: constrained (without word-level supervision) and unconstrained (with word-level supervision). For the constrained track, we experimented with several explainability methods to extract the relevance of input tokens from sentence-level QE models built on top of multilingual pre-trained transformers. Among the different tested methods, composing explanations in the form of attention weights scaled by the norm of value vectors yielded the best results. When word-level labels are used during training, our best results were obtained by using word-level predicted probabilities. We further improve the performance of our methods on the two tracks by ensembling explanation scores extracted from models trained with different pre-trained transformers, achieving strong results for in-domain and zero-shot language pairs.","year":2021,"title_abstract":"{IST}-Unbabel 2021 Submission for the Explainable Quality Estimation Shared Task We present the joint contribution of Instituto Superior T{\\'e}cnico (IST) and Unbabel to the Explainable Quality Estimation (QE) shared task, where systems were submitted to two tracks: constrained (without word-level supervision) and unconstrained (with word-level supervision). For the constrained track, we experimented with several explainability methods to extract the relevance of input tokens from sentence-level QE models built on top of multilingual pre-trained transformers. Among the different tested methods, composing explanations in the form of attention weights scaled by the norm of value vectors yielded the best results. When word-level labels are used during training, our best results were obtained by using word-level predicted probabilities. We further improve the performance of our methods on the two tracks by ensembling explanation scores extracted from models trained with different pre-trained transformers, achieving strong results for in-domain and zero-shot language pairs.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1848090887,"Goal":"Quality Education","Task":["Explainable Quality Estimation Shared Task","Explainable Quality Estimation","shared task","constrained track","in - domain and zero - shot language pairs"],"Method":["explainability methods","sentence - level QE models"]},{"ID":"august-etal-2020-writing","title":"Writing Strategies for Science Communication: Data and Computational Analysis","abstract":"Communicating complex scientific ideas without misleading or overwhelming the public is challenging. While science communication guides exist, they rarely offer empirical evidence for how their strategies are used in practice. Writing strategies that can be automatically recognized could greatly support science communication efforts by enabling tools to detect and suggest strategies for writers. We compile a set of writing strategies drawn from a wide range of prescriptive sources and develop an annotation scheme allowing humans to recognize them. We collect a corpus of 128k science writing documents in English and annotate a subset of this corpus. We use the annotations to train transformer-based classifiers and measure the strategies{'} use in the larger corpus. We find that the use of strategies, such as storytelling and emphasizing the most important findings, varies significantly across publications with different reader audiences.","year":2020,"title_abstract":"Writing Strategies for Science Communication: Data and Computational Analysis Communicating complex scientific ideas without misleading or overwhelming the public is challenging. While science communication guides exist, they rarely offer empirical evidence for how their strategies are used in practice. Writing strategies that can be automatically recognized could greatly support science communication efforts by enabling tools to detect and suggest strategies for writers. We compile a set of writing strategies drawn from a wide range of prescriptive sources and develop an annotation scheme allowing humans to recognize them. We collect a corpus of 128k science writing documents in English and annotate a subset of this corpus. We use the annotations to train transformer-based classifiers and measure the strategies{'} use in the larger corpus. We find that the use of strategies, such as storytelling and emphasizing the most important findings, varies significantly across publications with different reader audiences.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1848037243,"Goal":"Climate Action","Task":["Science Communication","Data and Computational Analysis","Communicating complex scientific ideas","science communication guides","science communication efforts"],"Method":["Writing Strategies","Writing strategies","writing strategies","annotation scheme","transformer - based classifiers"]},{"ID":"hauksdottir-2014-innovative","title":"An Innovative World Language Centre : Challenges for the Use of Language Technology","abstract":"The Vigdis International Centre of Multilingualism and Intercultural Understanding at the University of Iceland will work under the auspices of UNESCO. The main objective of the Centre is to promote linguistic diversity and to raise awareness of the importance of multilingualism. The focus will be on research on translations, foreign language learning, language policy and language planning. The centre will also serve as a platform for promoting collaborative activities on languages and cultures, in particular, organizing exhibitions and other events aimed at both the academic community and the general public. The Centre will work in close collaboration with the national and international research community. The Centre aims to create state-of-the-art infrastructure, using Language Technology resources in research and academic studies, in particular in translations and language learning (Computer-Assisted Language Learning). In addition, the centre will provide scholars with a means to conduct corpus-based research for synchronic investigations and for comparative studies. The Centre will also function as a repository for language data corpora. Facilities will be provided so that these corpora can be used by the research community on site and online. Computer technology resources will also be exploited in creating tools and exhibitions for the general audience.","year":2014,"title_abstract":"An Innovative World Language Centre : Challenges for the Use of Language Technology The Vigdis International Centre of Multilingualism and Intercultural Understanding at the University of Iceland will work under the auspices of UNESCO. The main objective of the Centre is to promote linguistic diversity and to raise awareness of the importance of multilingualism. The focus will be on research on translations, foreign language learning, language policy and language planning. The centre will also serve as a platform for promoting collaborative activities on languages and cultures, in particular, organizing exhibitions and other events aimed at both the academic community and the general public. The Centre will work in close collaboration with the national and international research community. The Centre aims to create state-of-the-art infrastructure, using Language Technology resources in research and academic studies, in particular in translations and language learning (Computer-Assisted Language Learning). In addition, the centre will provide scholars with a means to conduct corpus-based research for synchronic investigations and for comparative studies. The Centre will also function as a repository for language data corpora. Facilities will be provided so that these corpora can be used by the research community on site and online. Computer technology resources will also be exploited in creating tools and exhibitions for the general audience.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1847654134,"Goal":"Quality Education","Task":["Intercultural Understanding","translations","foreign language learning","language policy","language planning","academic studies","translations","language learning","Language Learning)","corpus - based research","synchronic investigations","comparative studies"],"Method":["World Language Centre","Language Technology","Language Technology resources"]},{"ID":"bond-etal-2016-cili","title":"{CILI}: the Collaborative Interlingual Index","abstract":"This paper introduces the motivation for and design of the Collaborative InterLingual Index (CILI). It is designed to make possible coordination between multiple loosely coupled wordnet projects. The structure of the CILI is based on the Interlingual index first proposed in the EuroWordNet project with several pragmatic extensions: an explicit open license, definitions in English and links to wordnets in the Global Wordnet Grid.","year":2016,"title_abstract":"{CILI}: the Collaborative Interlingual Index This paper introduces the motivation for and design of the Collaborative InterLingual Index (CILI). It is designed to make possible coordination between multiple loosely coupled wordnet projects. The structure of the CILI is based on the Interlingual index first proposed in the EuroWordNet project with several pragmatic extensions: an explicit open license, definitions in English and links to wordnets in the Global Wordnet Grid.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1846239269,"Goal":"Partnership for the Goals","Task":["EuroWordNet project"],"Method":["Collaborative Interlingual Index","Collaborative InterLingual Index","Interlingual index"]},{"ID":"liu-etal-2021-marginal","title":"Marginal Utility Diminishes: Exploring the Minimum Knowledge for {BERT} Knowledge Distillation","abstract":"Recently, knowledge distillation (KD) has shown great success in BERT compression. Instead of only learning from the teacher{'}s soft label as in conventional KD, researchers find that the rich information contained in the hidden layers of BERT is conducive to the student{'}s performance. To better exploit the hidden knowledge, a common practice is to force the student to deeply mimic the teacher{'}s hidden states of all the tokens in a layer-wise manner. In this paper, however, we observe that although distilling the teacher{'}s hidden state knowledge (HSK) is helpful, the performance gain (marginal utility) diminishes quickly as more HSK is distilled. To understand this effect, we conduct a series of analysis. Specifically, we divide the HSK of BERT into three dimensions, namely depth, length and width. We first investigate a variety of strategies to extract crucial knowledge for each single dimension and then jointly compress the three dimensions. In this way, we show that 1) the student{'}s performance can be improved by extracting and distilling the crucial HSK, and 2) using a tiny fraction of HSK can achieve the same performance as extensive HSK distillation. Based on the second finding, we further propose an efficient KD paradigm to compress BERT, which does not require loading the teacher during the training of student. For two kinds of student models and computing devices, the proposed KD paradigm gives rise to training speedup of 2.7x 3.4x.","year":2021,"title_abstract":"Marginal Utility Diminishes: Exploring the Minimum Knowledge for {BERT} Knowledge Distillation Recently, knowledge distillation (KD) has shown great success in BERT compression. Instead of only learning from the teacher{'}s soft label as in conventional KD, researchers find that the rich information contained in the hidden layers of BERT is conducive to the student{'}s performance. To better exploit the hidden knowledge, a common practice is to force the student to deeply mimic the teacher{'}s hidden states of all the tokens in a layer-wise manner. In this paper, however, we observe that although distilling the teacher{'}s hidden state knowledge (HSK) is helpful, the performance gain (marginal utility) diminishes quickly as more HSK is distilled. To understand this effect, we conduct a series of analysis. Specifically, we divide the HSK of BERT into three dimensions, namely depth, length and width. We first investigate a variety of strategies to extract crucial knowledge for each single dimension and then jointly compress the three dimensions. In this way, we show that 1) the student{'}s performance can be improved by extracting and distilling the crucial HSK, and 2) using a tiny fraction of HSK can achieve the same performance as extensive HSK distillation. Based on the second finding, we further propose an efficient KD paradigm to compress BERT, which does not require loading the teacher during the training of student. For two kinds of student models and computing devices, the proposed KD paradigm gives rise to training speedup of 2.7x 3.4x.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1845997125,"Goal":"Quality Education","Task":["{BERT} Knowledge Distillation","BERT compression","BERT"],"Method":["knowledge distillation","KD","BERT","HSK","HSK","HSK","HSK","HSK distillation","KD paradigm","student models","KD paradigm"]},{"ID":"radford-2021-regressing","title":"Regressing Location on Text for Probabilistic Geocoding","abstract":"Text data are an important source of detailed information about social and political events. Automated systems parse large volumes of text data to infer or extract structured information that describes actors, actions, dates, times, and locations. One of these sub-tasks is geocoding: predicting the geographic coordinates associated with events or locations described by a given text. I present an end-to-end probabilistic model for geocoding text data. Additionally, I collect a novel data set for evaluating the performance of geocoding systems. I compare the model-based solution, called ELECTRo-map, to the current state-of-the-art open source system for geocoding texts for event data. Finally, I discuss the benefits of end-to-end model-based geocoding, including principled uncertainty estimation and the ability of these models to leverage contextual information.","year":2021,"title_abstract":"Regressing Location on Text for Probabilistic Geocoding Text data are an important source of detailed information about social and political events. Automated systems parse large volumes of text data to infer or extract structured information that describes actors, actions, dates, times, and locations. One of these sub-tasks is geocoding: predicting the geographic coordinates associated with events or locations described by a given text. I present an end-to-end probabilistic model for geocoding text data. Additionally, I collect a novel data set for evaluating the performance of geocoding systems. I compare the model-based solution, called ELECTRo-map, to the current state-of-the-art open source system for geocoding texts for event data. Finally, I discuss the benefits of end-to-end model-based geocoding, including principled uncertainty estimation and the ability of these models to leverage contextual information.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.184515968,"Goal":"Sustainable Cities and Communities","Task":["Probabilistic Geocoding","geocoding","predicting the geographic coordinates","geocoding text data","geocoding texts"],"Method":["Automated systems","end - to - end probabilistic model","geocoding systems","model - based solution","ELECTRo - map","open source system","end model - based geocoding","principled uncertainty estimation"]},{"ID":"dinu-etal-2017-stylistic","title":"On the stylistic evolution from communism to democracy: {S}olomon {M}arcus study case","abstract":"In this article we propose a stylistic analysis of Solomon Marcus{'} non-scientific published texts, gathered in six volumes, aiming to uncover some of his quantitative and qualitative fingerprints. Moreover, we compare and cluster two distinct periods of time in his writing style: 22 years of communist regime (1967-1989) and 27 years of democracy (1990-2016). The distributional analysis of Marcus{'} text reveals that the passing from the communist regime period to democracy is sharply marked by two complementary changes in Marcus{'} writing: in the pre-democracy period, the communist norms of writing style demanded on the one hand long phrases, long words and clich{\\'e}s, and on the other hand, a short list of preferred {``}official{''} topics; in democracy tendency was towards shorten phrases and words while approaching a broader area of topics.","year":2017,"title_abstract":"On the stylistic evolution from communism to democracy: {S}olomon {M}arcus study case In this article we propose a stylistic analysis of Solomon Marcus{'} non-scientific published texts, gathered in six volumes, aiming to uncover some of his quantitative and qualitative fingerprints. Moreover, we compare and cluster two distinct periods of time in his writing style: 22 years of communist regime (1967-1989) and 27 years of democracy (1990-2016). The distributional analysis of Marcus{'} text reveals that the passing from the communist regime period to democracy is sharply marked by two complementary changes in Marcus{'} writing: in the pre-democracy period, the communist norms of writing style demanded on the one hand long phrases, long words and clich{\\'e}s, and on the other hand, a short list of preferred {``}official{''} topics; in democracy tendency was towards shorten phrases and words while approaching a broader area of topics.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.1845029891,"Goal":"Peace, Justice and Strong Institutions","Task":["stylistic evolution","distributional analysis"],"Method":["stylistic analysis"]},{"ID":"qi-etal-2021-improving-abstractive","title":"Improving Abstractive Dialogue Summarization with Hierarchical Pretraining and Topic Segment","abstract":"With the increasing abundance of meeting transcripts, meeting summary has attracted more and more attention from researchers. The unsupervised pre-training method based on transformer structure combined with fine-tuning of downstream tasks has achieved great success in the field of text summarization. However, the semantic structure and style of meeting transcripts are quite different from that of articles. In this work, we propose a hierarchical transformer encoder-decoder network with multi-task pre-training. Specifically, we mask key sentences at the word-level encoder and generate them at the decoder. Besides, we randomly mask some of the role alignments in the input text and force the model to recover the original role tags to complete the alignments. In addition, we introduce a topic segmentation mechanism to further improve the quality of the generated summaries. The experimental results show that our model is superior to the previous methods in meeting summary datasets AMI and ICSI.","year":2021,"title_abstract":"Improving Abstractive Dialogue Summarization with Hierarchical Pretraining and Topic Segment With the increasing abundance of meeting transcripts, meeting summary has attracted more and more attention from researchers. The unsupervised pre-training method based on transformer structure combined with fine-tuning of downstream tasks has achieved great success in the field of text summarization. However, the semantic structure and style of meeting transcripts are quite different from that of articles. In this work, we propose a hierarchical transformer encoder-decoder network with multi-task pre-training. Specifically, we mask key sentences at the word-level encoder and generate them at the decoder. Besides, we randomly mask some of the role alignments in the input text and force the model to recover the original role tags to complete the alignments. In addition, we introduce a topic segmentation mechanism to further improve the quality of the generated summaries. The experimental results show that our model is superior to the previous methods in meeting summary datasets AMI and ICSI.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1844796985,"Goal":"Partnership for the Goals","Task":["Abstractive Dialogue Summarization","downstream tasks","text summarization","multi - task pre - training"],"Method":["Hierarchical Pretraining","Topic Segment","unsupervised pre - training method","transformer structure","hierarchical transformer encoder - decoder network","word - level encoder","decoder","topic segmentation mechanism"]},{"ID":"desilets-etal-2008-reliable","title":"Reliable Innovation: A Tecchie{'}s Travels in the Land of Translators","abstract":"Machine Translation (MT) is rapidly progressing towards quality levels that might make it appropriate for broad user populations in a range of scenarios, including gisting and post-editing in unconstrained domains. For this to happen, the field may however need to switch gear and move away from its current technology driven paradigm to a more user-centered approach. In this paper, we discuss how ethnographic techniques like Contextual Inquiry could help in that respect, by providing researchers and developers with rich information about the world and needs of potential end-users. We discuss how data from Contextual Inquiries with professional translators was used to concretely and positively influence several research and development projects in the area of Computer Assisted Translation technology. These inquiries had many benefits, including: (i) grounding developers and researchers in the world of their end-users, (ii) generating new technology ideas, (iii) selecting between competing development project ideas, (iv) finding how to alleviate friction for important ideas that go against the grain of current user practices, (v) evaluating existing or experimental technologies, (vi) helping with micro level design decision, (vii) building credibility with translators, and (viii) fostering multidisciplinary discussion between researchers.","year":2008,"title_abstract":"Reliable Innovation: A Tecchie{'}s Travels in the Land of Translators Machine Translation (MT) is rapidly progressing towards quality levels that might make it appropriate for broad user populations in a range of scenarios, including gisting and post-editing in unconstrained domains. For this to happen, the field may however need to switch gear and move away from its current technology driven paradigm to a more user-centered approach. In this paper, we discuss how ethnographic techniques like Contextual Inquiry could help in that respect, by providing researchers and developers with rich information about the world and needs of potential end-users. We discuss how data from Contextual Inquiries with professional translators was used to concretely and positively influence several research and development projects in the area of Computer Assisted Translation technology. These inquiries had many benefits, including: (i) grounding developers and researchers in the world of their end-users, (ii) generating new technology ideas, (iii) selecting between competing development project ideas, (iv) finding how to alleviate friction for important ideas that go against the grain of current user practices, (v) evaluating existing or experimental technologies, (vi) helping with micro level design decision, (vii) building credibility with translators, and (viii) fostering multidisciplinary discussion between researchers.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1842772663,"Goal":"Sustainable Cities and Communities","Task":["Reliable Innovation","Machine Translation","gisting","post - editing","unconstrained domains","Computer Assisted Translation technology","micro level design decision"],"Method":["Tecchie{'}s","technology driven paradigm","user - centered approach","ethnographic techniques","Contextual Inquiry"]},{"ID":"koay-etal-2020-domain","title":"How Domain Terminology Affects Meeting Summarization Performance","abstract":"Meetings are essential to modern organizations. Numerous meetings are held and recorded daily, more than can ever be comprehended. A meeting summarization system that identifies salient utterances from the transcripts to automatically generate meeting minutes can help. It empowers users to rapidly search and sift through large meeting collections. To date, the impact of domain terminology on the performance of meeting summarization remains understudied, despite that meetings are rich with domain knowledge. In this paper, we create gold-standard annotations for domain terminology on a sizable meeting corpus; they are known as jargon terms. We then analyze the performance of a meeting summarization system with and without jargon terms. Our findings reveal that domain terminology can have a substantial impact on summarization performance. We publicly release all domain terminology to advance research in meeting summarization.","year":2020,"title_abstract":"How Domain Terminology Affects Meeting Summarization Performance Meetings are essential to modern organizations. Numerous meetings are held and recorded daily, more than can ever be comprehended. A meeting summarization system that identifies salient utterances from the transcripts to automatically generate meeting minutes can help. It empowers users to rapidly search and sift through large meeting collections. To date, the impact of domain terminology on the performance of meeting summarization remains understudied, despite that meetings are rich with domain knowledge. In this paper, we create gold-standard annotations for domain terminology on a sizable meeting corpus; they are known as jargon terms. We then analyze the performance of a meeting summarization system with and without jargon terms. Our findings reveal that domain terminology can have a substantial impact on summarization performance. We publicly release all domain terminology to advance research in meeting summarization.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.184267059,"Goal":"Partnership for the Goals","Task":["Meeting Summarization","meeting summarization","summarization","summarization"],"Method":["meeting summarization system","meeting summarization system"]},{"ID":"aburaed-etal-2017-sentence","title":"What Sentence are you Referring to and Why? Identifying Cited Sentences in Scientific Literature","abstract":"In the current context of scientific information overload, text mining tools are of paramount importance for researchers who have to read scientific papers and assess their value. Current citation networks, which link papers by citation relationships (reference and citing paper), are useful to quantitatively understand the value of a piece of scientific work, however they are limited in that they do not provide information about what specific part of the reference paper the citing paper is referring to. This qualitative information is very important, for example, in the context of current community-based scientific summarization activities. In this paper, and relying on an annotated dataset of co-citation sentences, we carry out a number of experiments aimed at, given a citation sentence, automatically identify a part of a reference paper being cited. Additionally our algorithm predicts the specific reason why such reference sentence has been cited out of five possible reasons.","year":2017,"title_abstract":"What Sentence are you Referring to and Why? Identifying Cited Sentences in Scientific Literature In the current context of scientific information overload, text mining tools are of paramount importance for researchers who have to read scientific papers and assess their value. Current citation networks, which link papers by citation relationships (reference and citing paper), are useful to quantitatively understand the value of a piece of scientific work, however they are limited in that they do not provide information about what specific part of the reference paper the citing paper is referring to. This qualitative information is very important, for example, in the context of current community-based scientific summarization activities. In this paper, and relying on an annotated dataset of co-citation sentences, we carry out a number of experiments aimed at, given a citation sentence, automatically identify a part of a reference paper being cited. Additionally our algorithm predicts the specific reason why such reference sentence has been cited out of five possible reasons.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1842492968,"Goal":"Sustainable Cities and Communities","Task":["scientific information overload","community - based scientific summarization activities"],"Method":["text mining tools","citation networks"]},{"ID":"bond-etal-2019-comparison","title":"A Comparison of Sense-level Sentiment Scores","abstract":"In this paper, we compare a variety of sense-tagged sentiment resources, including SentiWordNet, ML-Senticon, plWordNet emo and the NTU Multilingual Corpus. The goal is to investigate the quality of the resources and see how well the sentiment polarity annotation maps across languages.","year":2019,"title_abstract":"A Comparison of Sense-level Sentiment Scores In this paper, we compare a variety of sense-tagged sentiment resources, including SentiWordNet, ML-Senticon, plWordNet emo and the NTU Multilingual Corpus. The goal is to investigate the quality of the resources and see how well the sentiment polarity annotation maps across languages.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1841556132,"Goal":"Reduced Inequalities","Task":["sentiment polarity annotation"],"Method":["ML - Senticon","plWordNet emo"]},{"ID":"porco-goldwasser-2020-predicting","title":"Predicting Stance Change Using Modular Architectures","abstract":"The ability to change a person{'}s mind on a given issue depends both on the arguments they are presented with and on their underlying perspectives and biases on that issue. Predicting stance changes require characterizing both aspects and the interaction between them, especially in realistic settings in which stance changes are very rare. In this paper, we suggest a modular learning approach, which decomposes the task into multiple modules, focusing on different aspects of the interaction between users, their beliefs, and the arguments they are exposed to. Our experiments show that our modular approach archives significantly better results compared to the end-to-end approach using BERT over the same inputs.","year":2020,"title_abstract":"Predicting Stance Change Using Modular Architectures The ability to change a person{'}s mind on a given issue depends both on the arguments they are presented with and on their underlying perspectives and biases on that issue. Predicting stance changes require characterizing both aspects and the interaction between them, especially in realistic settings in which stance changes are very rare. In this paper, we suggest a modular learning approach, which decomposes the task into multiple modules, focusing on different aspects of the interaction between users, their beliefs, and the arguments they are exposed to. Our experiments show that our modular approach archives significantly better results compared to the end-to-end approach using BERT over the same inputs.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1840139627,"Goal":"Climate Action","Task":["Predicting Stance Change","Predicting stance changes"],"Method":["Modular Architectures","modular learning approach","modular approach","end approach"]},{"ID":"vijjali-etal-2020-two","title":"Two Stage Transformer Model for {COVID}-19 Fake News Detection and Fact Checking","abstract":"The rapid advancement of technology in online communication via social media platforms has led to a prolific rise in the spread of misinformation and fake news. Fake news is especially rampant in the current COVID-19 pandemic, leading to people believing in false and potentially harmful claims and stories. Detecting fake news quickly can alleviate the spread of panic, chaos and potential health hazards. We developed a two stage automated pipeline for COVID-19 fake news detection using state of the art machine learning models for natural language processing. The first model leverages a novel fact checking algorithm that retrieves the most relevant facts concerning user queries about particular COVID-19 claims. The second model verifies the level of {``}truth{''} in the queried claim by computing the textual entailment between the claim and the true facts retrieved from a manually curated COVID-19 dataset. The dataset is based on a publicly available knowledge source consisting of more than 5000 COVID-19 false claims and verified explanations, a subset of which was internally annotated and cross-validated to train and evaluate our models. We evaluate a series of models based on classical text-based features to more contextual Transformer based models and observe that a model pipeline based on BERT and ALBERT for the two stages respectively yields the best results.","year":2020,"title_abstract":"Two Stage Transformer Model for {COVID}-19 Fake News Detection and Fact Checking The rapid advancement of technology in online communication via social media platforms has led to a prolific rise in the spread of misinformation and fake news. Fake news is especially rampant in the current COVID-19 pandemic, leading to people believing in false and potentially harmful claims and stories. Detecting fake news quickly can alleviate the spread of panic, chaos and potential health hazards. We developed a two stage automated pipeline for COVID-19 fake news detection using state of the art machine learning models for natural language processing. The first model leverages a novel fact checking algorithm that retrieves the most relevant facts concerning user queries about particular COVID-19 claims. The second model verifies the level of {``}truth{''} in the queried claim by computing the textual entailment between the claim and the true facts retrieved from a manually curated COVID-19 dataset. The dataset is based on a publicly available knowledge source consisting of more than 5000 COVID-19 false claims and verified explanations, a subset of which was internally annotated and cross-validated to train and evaluate our models. We evaluate a series of models based on classical text-based features to more contextual Transformer based models and observe that a model pipeline based on BERT and ALBERT for the two stages respectively yields the best results.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1840126216,"Goal":"Climate Action","Task":["Fake News Detection","Fact Checking","online communication","COVID - 19 pandemic","Detecting fake news","COVID - 19 fake news detection","natural language processing"],"Method":["Transformer Model","automated pipeline","machine learning models","fact checking algorithm","text - based features","contextual Transformer based models","model pipeline","BERT"]},{"ID":"yela-bello-etal-2021-multihumes","title":"{M}ulti{H}um{ES}: Multilingual Humanitarian Dataset for Extractive Summarization","abstract":"When responding to a disaster, humanitarian experts must rapidly process large amounts of secondary data sources to derive situational awareness and guide decision-making. While these documents contain valuable information, manually processing them is extremely time-consuming when an expedient response is necessary. To improve this process, effective summarization models are a valuable tool for humanitarian response experts as they provide digestible overviews of essential information in secondary data. This paper focuses on extractive summarization for the humanitarian response domain and describes and makes public a new multilingual data collection for this purpose. The collection {--} called MultiHumES{--} provides multilingual documents coupled with informative snippets that have been annotated by humanitarian analysts over the past four years. We report the performance results of a recent neural networks-based summarization model together with other baselines. We hope that the released data collection can further grow the research on multilingual extractive summarization in the humanitarian response domain.","year":2021,"title_abstract":"{M}ulti{H}um{ES}: Multilingual Humanitarian Dataset for Extractive Summarization When responding to a disaster, humanitarian experts must rapidly process large amounts of secondary data sources to derive situational awareness and guide decision-making. While these documents contain valuable information, manually processing them is extremely time-consuming when an expedient response is necessary. To improve this process, effective summarization models are a valuable tool for humanitarian response experts as they provide digestible overviews of essential information in secondary data. This paper focuses on extractive summarization for the humanitarian response domain and describes and makes public a new multilingual data collection for this purpose. The collection {--} called MultiHumES{--} provides multilingual documents coupled with informative snippets that have been annotated by humanitarian analysts over the past four years. We report the performance results of a recent neural networks-based summarization model together with other baselines. We hope that the released data collection can further grow the research on multilingual extractive summarization in the humanitarian response domain.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1840102971,"Goal":"Sustainable Cities and Communities","Task":["Extractive Summarization","situational awareness","decision - making","humanitarian response experts","extractive summarization","humanitarian response domain","summarization","multilingual extractive summarization","humanitarian response domain"],"Method":["summarization models","neural networks"]},{"ID":"li-dickinson-2017-gender","title":"Gender Prediction for {C}hinese Social Media Data","abstract":"Social media provides users a platform to publish messages and socialize with others, and microblogs have gained more users than ever in recent years. With such usage, user profiling is a popular task in computational linguistics and text mining. Different approaches have been used to predict users{'} gender, age, and other information, but most of this work has been done on English and other Western languages. The goal of this project is to predict the gender of users based on their posts on Weibo, a Chinese micro-blogging platform. Given issues in Chinese word segmentation, we explore character and word n-grams as features for this task, as well as using character and word embeddings for classification. Given how the data is extracted, we approach the task on a per-post basis, and we show the difficulties of the task for both humans and computers. Nonetheless, we present encouraging results and point to future improvements.","year":2017,"title_abstract":"Gender Prediction for {C}hinese Social Media Data Social media provides users a platform to publish messages and socialize with others, and microblogs have gained more users than ever in recent years. With such usage, user profiling is a popular task in computational linguistics and text mining. Different approaches have been used to predict users{'} gender, age, and other information, but most of this work has been done on English and other Western languages. The goal of this project is to predict the gender of users based on their posts on Weibo, a Chinese micro-blogging platform. Given issues in Chinese word segmentation, we explore character and word n-grams as features for this task, as well as using character and word embeddings for classification. Given how the data is extracted, we approach the task on a per-post basis, and we show the difficulties of the task for both humans and computers. Nonetheless, we present encouraging results and point to future improvements.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1837614924,"Goal":"Gender Equality","Task":["Gender Prediction","user profiling","computational linguistics","text mining","Chinese word segmentation","classification"],"Method":["character and word embeddings"]},{"ID":"xiao-etal-2022-datalab","title":"{D}ata{L}ab: A Platform for Data Analysis and Intervention","abstract":"Despite data{'}s crucial role in machine learning, most existing tools and research tend to focus on systems on top of existing data rather than how to interpret and manipulate data.In this paper, we propose DataLab, a unified data-oriented platform that not only allows users to interactively analyze the characteristics of data but also provides a standardized interface so that many data processing operations can be provided within a unified interface. Additionally, in view of the ongoing surge in the proliferation of datasets, DataLab has features for dataset recommendation and global vision analysis that help researchers form a better view of the data ecosystem. So far, DataLab covers 1,300 datasets and 3,583 of its transformed version, where 313 datasets support different types of analysis (e.g., with respect to gender bias) with the help of 119M samples annotated by 318 feature functions. DataLab is under active development and will be supported going forward. We have released a web platform, web API, Python SDK, and PyPI published package, which hopefully, can meet the diverse needs of researchers.","year":2022,"title_abstract":"{D}ata{L}ab: A Platform for Data Analysis and Intervention Despite data{'}s crucial role in machine learning, most existing tools and research tend to focus on systems on top of existing data rather than how to interpret and manipulate data.In this paper, we propose DataLab, a unified data-oriented platform that not only allows users to interactively analyze the characteristics of data but also provides a standardized interface so that many data processing operations can be provided within a unified interface. Additionally, in view of the ongoing surge in the proliferation of datasets, DataLab has features for dataset recommendation and global vision analysis that help researchers form a better view of the data ecosystem. So far, DataLab covers 1,300 datasets and 3,583 of its transformed version, where 313 datasets support different types of analysis (e.g., with respect to gender bias) with the help of 119M samples annotated by 318 feature functions. DataLab is under active development and will be supported going forward. We have released a web platform, web API, Python SDK, and PyPI published package, which hopefully, can meet the diverse needs of researchers.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1836881936,"Goal":"Life on Land","Task":["Data Analysis and Intervention","machine learning","data processing operations","dataset recommendation","global vision analysis","analysis"],"Method":["DataLab","unified data - oriented platform","DataLab","web platform","Python SDK","PyPI published package"]},{"ID":"khramer-2020-automatically","title":"Automatically explaining health information","abstract":"Modern AI systems automatically learn from data using sophisticated statistical models. Explaining how these systems work and how they make their predictions therefore increasingly involves producing descriptions of how different probabilities are weighted and which uncertainties underlie these numbers. But what is the best way to (automatically) present such probabilistic explanations, do people actually understand them, and what is the potential impact of such information on people{'}s wellbeing? In this talk, I adress these questions in the context of systems that automatically generate personalised health information. The emergence of large national health registeries, such as the Dutch cancer registry, now make it possible to automatically generate descriptions of treatment options for new cancer patients based on data of comparable patients, including health and quality of life predictions following different treatments. I describe a series of studies, in which our team has investigated to what extent this information is currently provided to people, and under which conditions people actually want to have access to these kind of data-driven explanations. Additionally, we have studied whether there are different profiles in information needs, and what the best way is to provide probabilistic information and the associated undertainties to people.","year":2020,"title_abstract":"Automatically explaining health information Modern AI systems automatically learn from data using sophisticated statistical models. Explaining how these systems work and how they make their predictions therefore increasingly involves producing descriptions of how different probabilities are weighted and which uncertainties underlie these numbers. But what is the best way to (automatically) present such probabilistic explanations, do people actually understand them, and what is the potential impact of such information on people{'}s wellbeing? In this talk, I adress these questions in the context of systems that automatically generate personalised health information. The emergence of large national health registeries, such as the Dutch cancer registry, now make it possible to automatically generate descriptions of treatment options for new cancer patients based on data of comparable patients, including health and quality of life predictions following different treatments. I describe a series of studies, in which our team has investigated to what extent this information is currently provided to people, and under which conditions people actually want to have access to these kind of data-driven explanations. Additionally, we have studied whether there are different profiles in information needs, and what the best way is to provide probabilistic information and the associated undertainties to people.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.1836695373,"Goal":"Good Health and Well-Being","Task":["Automatically explaining health information","personalised health information"],"Method":["AI systems","statistical models","data - driven explanations"]},{"ID":"johnson-goldwasser-2016-know","title":"{``}All {I} know about politics is what {I} read in {T}witter{''}: Weakly Supervised Models for Extracting Politicians{'} Stances From {T}witter","abstract":"During the 2016 United States presidential election, politicians have increasingly used Twitter to express their beliefs, stances on current political issues, and reactions concerning national and international events. Given the limited length of tweets and the scrutiny politicians face for what they choose or neglect to say, they must craft and time their tweets carefully. The content and delivery of these tweets is therefore highly indicative of a politician{'}s stances. We present a weakly supervised method for extracting how issues are framed and temporal activity patterns on Twitter for popular politicians and issues of the 2016 election. These behavioral components are combined into a global model which collectively infers the most likely stance and agreement patterns among politicians, with respective accuracies of 86.44{\\%} and 84.6{\\%} on average.","year":2016,"title_abstract":"{``}All {I} know about politics is what {I} read in {T}witter{''}: Weakly Supervised Models for Extracting Politicians{'} Stances From {T}witter During the 2016 United States presidential election, politicians have increasingly used Twitter to express their beliefs, stances on current political issues, and reactions concerning national and international events. Given the limited length of tweets and the scrutiny politicians face for what they choose or neglect to say, they must craft and time their tweets carefully. The content and delivery of these tweets is therefore highly indicative of a politician{'}s stances. We present a weakly supervised method for extracting how issues are framed and temporal activity patterns on Twitter for popular politicians and issues of the 2016 election. These behavioral components are combined into a global model which collectively infers the most likely stance and agreement patterns among politicians, with respective accuracies of 86.44{\\%} and 84.6{\\%} on average.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1836687773,"Goal":"Climate Action","Task":["Extracting Politicians{'} Stances"],"Method":["Weakly Supervised Models","weakly supervised method","behavioral components","global model"]},{"ID":"tamchyna-2021-deploying","title":"Deploying {MT} Quality Estimation on a large scale: Lessons learned and open questions","abstract":"This talk will focus on Memsource{'}s experience implementing MT Quality Estimation on a large scale within a translation management system. We will cover the whole development journey: from our early experimentation and the challenges we faced adapting academic models for a real world setting, all the way through to the practical implementation. Since the launch of this feature, we{'}ve accumulated a significant amount of experience and feedback, which has informed our subsequent development. Lastly we will discuss several open questions regarding the future role of quality estimation in translation.","year":2021,"title_abstract":"Deploying {MT} Quality Estimation on a large scale: Lessons learned and open questions This talk will focus on Memsource{'}s experience implementing MT Quality Estimation on a large scale within a translation management system. We will cover the whole development journey: from our early experimentation and the challenges we faced adapting academic models for a real world setting, all the way through to the practical implementation. Since the launch of this feature, we{'}ve accumulated a significant amount of experience and feedback, which has informed our subsequent development. Lastly we will discuss several open questions regarding the future role of quality estimation in translation.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1835535169,"Goal":"Quality Education","Task":["MT Quality Estimation","translation management system","quality estimation","translation"],"Method":["{MT} Quality Estimation","academic models"]},{"ID":"vaughan-2017-tutorial","title":"{T}utorial: Making Better Use of the Crowd","abstract":"Over the last decade, crowdsourcing has been used to harness the power of human computation to solve tasks that are notoriously difficult to solve with computers alone, such as determining whether or not an image contains a tree, rating the relevance of a website, or verifying the phone number of a business. The natural language processing community was early to embrace crowdsourcing as a tool for quickly and inexpensively obtaining annotated data to train NLP systems. Once this data is collected, it can be handed off to algorithms that learn to perform basic NLP tasks such as translation or parsing. Usually this handoff is where interaction with the crowd ends. The crowd provides the data, but the ultimate goal is to eventually take humans out of the loop. Are there better ways to make use of the crowd?In this tutorial, I will begin with a showcase of innovative uses of crowdsourcing that go beyond data collection and annotation. I will discuss applications to natural language processing and machine learning, hybrid intelligence or {``}human in the loop{''} AI systems that leverage the complementary strengths of humans and machines in order to achieve more than either could achieve alone, and large scale studies of human behavior online. I will then spend the majority of the tutorial diving into recent research aimed at understanding who crowdworkers are, how they behave, and what this should teach us about best practices for interacting with the crowd.","year":2017,"title_abstract":"{T}utorial: Making Better Use of the Crowd Over the last decade, crowdsourcing has been used to harness the power of human computation to solve tasks that are notoriously difficult to solve with computers alone, such as determining whether or not an image contains a tree, rating the relevance of a website, or verifying the phone number of a business. The natural language processing community was early to embrace crowdsourcing as a tool for quickly and inexpensively obtaining annotated data to train NLP systems. Once this data is collected, it can be handed off to algorithms that learn to perform basic NLP tasks such as translation or parsing. Usually this handoff is where interaction with the crowd ends. The crowd provides the data, but the ultimate goal is to eventually take humans out of the loop. Are there better ways to make use of the crowd?In this tutorial, I will begin with a showcase of innovative uses of crowdsourcing that go beyond data collection and annotation. I will discuss applications to natural language processing and machine learning, hybrid intelligence or {``}human in the loop{''} AI systems that leverage the complementary strengths of humans and machines in order to achieve more than either could achieve alone, and large scale studies of human behavior online. I will then spend the majority of the tutorial diving into recent research aimed at understanding who crowdworkers are, how they behave, and what this should teach us about best practices for interacting with the crowd.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1834224463,"Goal":"Sustainable Cities and Communities","Task":["natural language processing community","NLP","NLP tasks","translation","parsing","data collection","annotation","natural language processing","machine learning","hybrid intelligence","AI"],"Method":["crowdsourcing","human computation","crowdsourcing","crowdsourcing"]},{"ID":"ros-etal-2020-mining","title":"Mining Wages in Nineteenth-Century Job Advertisements. The Application of Language Resources and Language Technology to study Economic and Social Inequality","abstract":"For the analysis of historical wage development, no structured data is available. Job advertisements, as found in newspapers can provide insights into what different types of jobs paid, but require language technology to structure in a format conducive to quantitative analysis. In this paper, we report on our experiments to mine wages from 19th century newspaper advertisements and detail the challenges that need to be overcome to perform a socio-economic analysis of textual data sources.","year":2020,"title_abstract":"Mining Wages in Nineteenth-Century Job Advertisements. The Application of Language Resources and Language Technology to study Economic and Social Inequality For the analysis of historical wage development, no structured data is available. Job advertisements, as found in newspapers can provide insights into what different types of jobs paid, but require language technology to structure in a format conducive to quantitative analysis. In this paper, we report on our experiments to mine wages from 19th century newspaper advertisements and detail the challenges that need to be overcome to perform a socio-economic analysis of textual data sources.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1833779216,"Goal":"Reduced Inequalities","Task":["Mining Wages","Language Resources","Economic and Social Inequality","analysis of historical wage development","quantitative analysis","socio - economic analysis of textual data sources"],"Method":["Language Technology","language technology"]},{"ID":"rehm-2016-language","title":"The Language Resource Life Cycle: Towards a Generic Model for Creating, Maintaining, Using and Distributing Language Resources","abstract":"Language Resources (LRs) are an essential ingredient of current approaches in Linguistics, Computational Linguistics, Language Technology and related fields. LRs are collections of spoken or written language data, typically annotated with linguistic analysis information. Different types of LRs exist, for example, corpora, ontologies, lexicons, collections of spoken language data (audio), or collections that also include video (multimedia, multimodal). Often, LRs are distributed with specific tools, documentation, manuals or research publications. The different phases that involve creating and distributing an LR can be conceptualised as a life cycle. While the idea of handling the LR production and maintenance process in terms of a life cycle has been brought up quite some time ago, a best practice model or common approach can still be considered a research gap. This article wants to help fill this gap by proposing an initial version of a generic Language Resource Life Cycle that can be used to inform, direct, control and evaluate LR research and development activities (including description, management, production, validation and evaluation workflows).","year":2016,"title_abstract":"The Language Resource Life Cycle: Towards a Generic Model for Creating, Maintaining, Using and Distributing Language Resources Language Resources (LRs) are an essential ingredient of current approaches in Linguistics, Computational Linguistics, Language Technology and related fields. LRs are collections of spoken or written language data, typically annotated with linguistic analysis information. Different types of LRs exist, for example, corpora, ontologies, lexicons, collections of spoken language data (audio), or collections that also include video (multimedia, multimodal). Often, LRs are distributed with specific tools, documentation, manuals or research publications. The different phases that involve creating and distributing an LR can be conceptualised as a life cycle. While the idea of handling the LR production and maintenance process in terms of a life cycle has been brought up quite some time ago, a best practice model or common approach can still be considered a research gap. This article wants to help fill this gap by proposing an initial version of a generic Language Resource Life Cycle that can be used to inform, direct, control and evaluate LR research and development activities (including description, management, production, validation and evaluation workflows).","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.1833532304,"Goal":"Life Below Water","Task":["Maintaining","Distributing Language Resources","Language Resources","Linguistics","Computational Linguistics","Language Technology","LR production and maintenance process","life cycle","LR research and development activities","description","management","production","validation and evaluation workflows)"],"Method":["Language Resource Life Cycle","LRs","LRs","LRs","LR","Language Resource Life Cycle"]},{"ID":"osika-etal-2018-second","title":"Second Language Acquisition Modeling: An Ensemble Approach","abstract":"Accurate prediction of students{'} knowledge is a fundamental building block of personalized learning systems. Here, we propose an ensemble model to predict student knowledge gaps. Applying our approach to student trace data from the online educational platform Duolingo we achieved highest score on all three datasets in the 2018 Shared Task on Second Language Acquisition Modeling. We describe our model and discuss relevance of the task compared to how it would be setup in a production environment for personalized education.","year":2018,"title_abstract":"Second Language Acquisition Modeling: An Ensemble Approach Accurate prediction of students{'} knowledge is a fundamental building block of personalized learning systems. Here, we propose an ensemble model to predict student knowledge gaps. Applying our approach to student trace data from the online educational platform Duolingo we achieved highest score on all three datasets in the 2018 Shared Task on Second Language Acquisition Modeling. We describe our model and discuss relevance of the task compared to how it would be setup in a production environment for personalized education.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1833524704,"Goal":"Quality Education","Task":["Second Language Acquisition Modeling","prediction of students{'} knowledge","student knowledge gaps","Second Language Acquisition Modeling","production environment","personalized education"],"Method":["Ensemble Approach","personalized learning systems","ensemble model"]},{"ID":"roberts-etal-2012-annotating","title":"Annotating Spatial Containment Relations Between Events","abstract":"A significant amount of spatial information in textual documents is hidden within the relationship between events. While humans have an intuitive understanding of these relationships that allow us to recover an object's or event's location, currently no annotated data exists to allow automatic discovery of spatial containment relations between events. We present our process for building such a corpus of manually annotated spatial relations between events. Events form complex predicate-argument structures that model the participants in the event, their roles, as well as the temporal and spatial grounding. In addition, events are not presented in isolation in text; there are explicit and implicit interactions between events that often participate in event structures. In this paper, we focus on five spatial containment relations that may exist between events: (1) SAME, (2) CONTAINS, (3) OVERLAPS, (4) NEAR, and (5) DIFFERENT. Using the transitive closure across these spatial relations, the implicit location of many events and their participants can be discovered. We discuss our annotation schema for spatial containment relations, placing it within the pre-existing theories of spatial representation. We also discuss our annotation guidelines for maintaining annotation quality as well as our process for augmenting SpatialML with spatial containment relations between events. Additionally, we outline some baseline experiments to evaluate the feasibility of developing supervised systems based on this corpus. These results indicate that although the task is challenging, automated methods are capable of discovering spatial containment relations between events.","year":2012,"title_abstract":"Annotating Spatial Containment Relations Between Events A significant amount of spatial information in textual documents is hidden within the relationship between events. While humans have an intuitive understanding of these relationships that allow us to recover an object's or event's location, currently no annotated data exists to allow automatic discovery of spatial containment relations between events. We present our process for building such a corpus of manually annotated spatial relations between events. Events form complex predicate-argument structures that model the participants in the event, their roles, as well as the temporal and spatial grounding. In addition, events are not presented in isolation in text; there are explicit and implicit interactions between events that often participate in event structures. In this paper, we focus on five spatial containment relations that may exist between events: (1) SAME, (2) CONTAINS, (3) OVERLAPS, (4) NEAR, and (5) DIFFERENT. Using the transitive closure across these spatial relations, the implicit location of many events and their participants can be discovered. We discuss our annotation schema for spatial containment relations, placing it within the pre-existing theories of spatial representation. We also discuss our annotation guidelines for maintaining annotation quality as well as our process for augmenting SpatialML with spatial containment relations between events. Additionally, we outline some baseline experiments to evaluate the feasibility of developing supervised systems based on this corpus. These results indicate that although the task is challenging, automated methods are capable of discovering spatial containment relations between events.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1833403558,"Goal":"Sustainable Cities and Communities","Task":["Annotating Spatial Containment Relations","automatic discovery of spatial containment relations between events","spatial containment relations","SpatialML","spatial containment relations between events"],"Method":["annotation schema","spatial representation","annotation guidelines","supervised systems","automated methods"]},{"ID":"schwartz-etal-2017-assessing","title":"Assessing Objective Recommendation Quality through Political Forecasting","abstract":"Recommendations are often rated for their subjective quality, but few researchers have studied comment quality in terms of objective utility. We explore recommendation quality assessment with respect to both subjective (i.e. users{'} ratings) and objective (i.e., did it influence? did it improve decisions?) metrics in a massive online geopolitical forecasting system, ultimately comparing linguistic characteristics of each quality metric. Using a variety of features, we predict all types of quality with better accuracy than the simple yet strong baseline of comment length. Looking at the most predictive content illustrates rater biases; for example, forecasters are subjectively biased in favor of comments mentioning business transactions or dealings as well as material things, even though such comments do not indeed prove any more useful objectively. Additionally, more complex sentence constructions, as evidenced by subordinate conjunctions, are characteristic of comments leading to objective improvements in forecasting.","year":2017,"title_abstract":"Assessing Objective Recommendation Quality through Political Forecasting Recommendations are often rated for their subjective quality, but few researchers have studied comment quality in terms of objective utility. We explore recommendation quality assessment with respect to both subjective (i.e. users{'} ratings) and objective (i.e., did it influence? did it improve decisions?) metrics in a massive online geopolitical forecasting system, ultimately comparing linguistic characteristics of each quality metric. Using a variety of features, we predict all types of quality with better accuracy than the simple yet strong baseline of comment length. Looking at the most predictive content illustrates rater biases; for example, forecasters are subjectively biased in favor of comments mentioning business transactions or dealings as well as material things, even though such comments do not indeed prove any more useful objectively. Additionally, more complex sentence constructions, as evidenced by subordinate conjunctions, are characteristic of comments leading to objective improvements in forecasting.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1833120883,"Goal":"Climate Action","Task":["Political Forecasting Recommendations","recommendation quality assessment","forecasting"],"Method":["online geopolitical forecasting system"]},{"ID":"ehara-2016-translation","title":"Translation systems and experimental results of the {EHR} group for {WAT}2016 tasks","abstract":"System architecture, experimental settings and experimental results of the group for the WAT2016 tasks are described. We participate in six tasks: en-ja, zh-ja, JPCzh-ja, JPCko-ja, HINDENen-hi and HINDENhi-ja. Although the basic architecture of our sys-tems is PBSMT with reordering, several techniques are conducted. Especially, the system for the HINDENhi-ja task with pivoting by English uses the reordering technique. Be-cause Hindi and Japanese are both OV type languages and English is a VO type language, we can use reordering technique to the pivot language. We can improve BLEU score from 7.47 to 7.66 by the reordering technique for the sentence level pivoting of this task.","year":2016,"title_abstract":"Translation systems and experimental results of the {EHR} group for {WAT}2016 tasks System architecture, experimental settings and experimental results of the group for the WAT2016 tasks are described. We participate in six tasks: en-ja, zh-ja, JPCzh-ja, JPCko-ja, HINDENen-hi and HINDENhi-ja. Although the basic architecture of our sys-tems is PBSMT with reordering, several techniques are conducted. Especially, the system for the HINDENhi-ja task with pivoting by English uses the reordering technique. Be-cause Hindi and Japanese are both OV type languages and English is a VO type language, we can use reordering technique to the pivot language. We can improve BLEU score from 7.47 to 7.66 by the reordering technique for the sentence level pivoting of this task.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1832802743,"Goal":"Gender Equality","Task":["Translation","WAT2016 tasks","HINDENhi - ja task","sentence level pivoting"],"Method":["PBSMT","reordering","reordering technique","reordering technique","reordering technique"]},{"ID":"van-der-meulen-reijnierse-2020-factcorp","title":"{F}act{C}orp: A Corpus of {D}utch Fact-checks and its Multiple Usages","abstract":"Fact-checking information before publication has long been a core task for journalists, but recent times have seen the emergence of dedicated news items specifically aimed at fact-checking after publication. This relatively new form of fact-checking receives a fair amount of attention from academics, with current research focusing mostly on journalists{'} motivations for publishing post-hoc fact-checks, the effects of fact-checking on the perceived accuracy of false claims, and the creation of computational tools for automatic fact-checking. In this paper, we propose to study fact-checks from a corpus linguistic perspective. This will enable us to gain insight in the scope and contents of fact-checks, to investigate what fact-checks can teach us about the way in which science appears (incorrectly) in the news, and to see how fact-checks behave in the science communication landscape. We report on the creation of FactCorp, a 1,16 million-word corpus containing 1,974 fact-checks from three major Dutch newspapers. We also present results of several exploratory analyses, including a rhetorical moves analysis, a qualitative content elements analysis, and keyword analyses. Through these analyses, we aim to demonstrate the wealth of possible applications that FactCorp allows, thereby stressing the importance of creating such resources.","year":2020,"title_abstract":"{F}act{C}orp: A Corpus of {D}utch Fact-checks and its Multiple Usages Fact-checking information before publication has long been a core task for journalists, but recent times have seen the emergence of dedicated news items specifically aimed at fact-checking after publication. This relatively new form of fact-checking receives a fair amount of attention from academics, with current research focusing mostly on journalists{'} motivations for publishing post-hoc fact-checks, the effects of fact-checking on the perceived accuracy of false claims, and the creation of computational tools for automatic fact-checking. In this paper, we propose to study fact-checks from a corpus linguistic perspective. This will enable us to gain insight in the scope and contents of fact-checks, to investigate what fact-checks can teach us about the way in which science appears (incorrectly) in the news, and to see how fact-checks behave in the science communication landscape. We report on the creation of FactCorp, a 1,16 million-word corpus containing 1,974 fact-checks from three major Dutch newspapers. We also present results of several exploratory analyses, including a rhetorical moves analysis, a qualitative content elements analysis, and keyword analyses. Through these analyses, we aim to demonstrate the wealth of possible applications that FactCorp allows, thereby stressing the importance of creating such resources.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1831568331,"Goal":"Climate Action","Task":["Fact - checking information","fact - checking","fact - checking","post - hoc fact - checks","fact - checking","automatic fact - checking","fact - checks","fact - checks","science communication landscape"],"Method":["Fact - checks","computational tools","exploratory analyses","rhetorical moves analysis","qualitative content elements analysis","keyword analyses","FactCorp"]},{"ID":"kumar-etal-2021-narnia","title":"{NARNIA} at {NLP}4{IF}-2021: Identification of Misinformation in {COVID}-19 Tweets Using {BERT}weet","abstract":"The spread of COVID-19 has been accompanied with widespread misinformation on social media. In particular, Twitterverse has seen a huge increase in dissemination of distorted facts and figures. The present work aims at identifying tweets regarding COVID-19 which contains harmful and false information. We have experimented with a number of Deep Learning-based models, including different word embeddings, such as Glove, ELMo, among others. BERTweet model achieved the best overall F1-score of 0.881 and secured the third rank on the above task.","year":2021,"title_abstract":"{NARNIA} at {NLP}4{IF}-2021: Identification of Misinformation in {COVID}-19 Tweets Using {BERT}weet The spread of COVID-19 has been accompanied with widespread misinformation on social media. In particular, Twitterverse has seen a huge increase in dissemination of distorted facts and figures. The present work aims at identifying tweets regarding COVID-19 which contains harmful and false information. We have experimented with a number of Deep Learning-based models, including different word embeddings, such as Glove, ELMo, among others. BERTweet model achieved the best overall F1-score of 0.881 and secured the third rank on the above task.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1830746979,"Goal":"Climate Action","Task":["Identification of Misinformation"],"Method":["Deep Learning - based models","Glove","ELMo","BERTweet"]},{"ID":"yang-etal-2019-fill","title":"Fill the {GAP}: Exploiting {BERT} for Pronoun Resolution","abstract":"In this paper, we describe our entry in the gendered pronoun resolution competition which achieved fourth place without data augmentation. Our method is an ensemble system of BERTs which resolves co-reference in an interaction space. We report four insights from our work: BERT{'}s representations involve significant redundancy; modeling interaction effects similar to natural language inference models is useful for this task; there is an optimal BERT layer to extract representations for pronoun resolution; and the difference between the attention weights from the pronoun to the candidate entities was highly correlated with the correct label, with interesting implications for future work.","year":2019,"title_abstract":"Fill the {GAP}: Exploiting {BERT} for Pronoun Resolution In this paper, we describe our entry in the gendered pronoun resolution competition which achieved fourth place without data augmentation. Our method is an ensemble system of BERTs which resolves co-reference in an interaction space. We report four insights from our work: BERT{'}s representations involve significant redundancy; modeling interaction effects similar to natural language inference models is useful for this task; there is an optimal BERT layer to extract representations for pronoun resolution; and the difference between the attention weights from the pronoun to the candidate entities was highly correlated with the correct label, with interesting implications for future work.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1829696,"Goal":"Gender Equality","Task":["Pronoun Resolution","gendered pronoun resolution competition","co - reference","pronoun resolution;"],"Method":["{BERT}","data augmentation","ensemble system","BERTs","BERT{'}s","natural language inference models","BERT"]},{"ID":"mao-etal-2021-eliciting","title":"Eliciting Bias in Question Answering Models through Ambiguity","abstract":"Question answering (QA) models use retriever and reader systems to answer questions. Reliance on training data by QA systems can amplify or reflect inequity through their responses. Many QA models, such as those for the SQuAD dataset, are trained and tested on a subset of Wikipedia articles which encode their own biases and also reproduce real-world inequality. Understanding how training data affects bias in QA systems can inform methods to mitigate inequity. We develop two sets of questions for closed and open domain questions respectively, which use ambiguous questions to probe QA models for bias. We feed three deep-learning-based QA systems with our question sets and evaluate responses for bias via the metrics. Using our metrics, we find that open-domain QA models amplify biases more than their closed-domain counterparts and propose that biases in the retriever surface more readily due to greater freedom of choice.","year":2021,"title_abstract":"Eliciting Bias in Question Answering Models through Ambiguity Question answering (QA) models use retriever and reader systems to answer questions. Reliance on training data by QA systems can amplify or reflect inequity through their responses. Many QA models, such as those for the SQuAD dataset, are trained and tested on a subset of Wikipedia articles which encode their own biases and also reproduce real-world inequality. Understanding how training data affects bias in QA systems can inform methods to mitigate inequity. We develop two sets of questions for closed and open domain questions respectively, which use ambiguous questions to probe QA models for bias. We feed three deep-learning-based QA systems with our question sets and evaluate responses for bias via the metrics. Using our metrics, we find that open-domain QA models amplify biases more than their closed-domain counterparts and propose that biases in the retriever surface more readily due to greater freedom of choice.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1829590201,"Goal":"Quality Education","Task":["Eliciting Bias","Question Answering Models","QA","QA","QA","QA","QA"],"Method":["Ambiguity Question answering","retriever and reader systems","QA models","deep - learning"]},{"ID":"yu-etal-2019-detecting","title":"Detecting Causal Language Use in Science Findings","abstract":"Causal interpretation of correlational findings from observational studies has been a major type of misinformation in science communication. Prior studies on identifying inappropriate use of causal language relied on manual content analysis, which is not scalable for examining a large volume of science publications. In this study, we first annotated a corpus of over 3,000 PubMed research conclusion sentences, then developed a BERT-based prediction model that classifies conclusion sentences into {``}no relationship{''}, {``}correlational{''}, {``}conditional causal{''}, and {``}direct causal{''} categories, achieving an accuracy of 0.90 and a macro-F1 of 0.88. We then applied the prediction model to measure the causal language use in the research conclusions of about 38,000 observational studies in PubMed. The prediction result shows that 21.7{\\%} studies used direct causal language exclusively in their conclusions, and 32.4{\\%} used some direct causal language. We also found that the ratio of causal language use differs among authors from different countries, challenging the notion of a shared consensus on causal language use in the global science community. Our prediction model could also be used to help identify the inappropriate use of causal language in science publications.","year":2019,"title_abstract":"Detecting Causal Language Use in Science Findings Causal interpretation of correlational findings from observational studies has been a major type of misinformation in science communication. Prior studies on identifying inappropriate use of causal language relied on manual content analysis, which is not scalable for examining a large volume of science publications. In this study, we first annotated a corpus of over 3,000 PubMed research conclusion sentences, then developed a BERT-based prediction model that classifies conclusion sentences into {``}no relationship{''}, {``}correlational{''}, {``}conditional causal{''}, and {``}direct causal{''} categories, achieving an accuracy of 0.90 and a macro-F1 of 0.88. We then applied the prediction model to measure the causal language use in the research conclusions of about 38,000 observational studies in PubMed. The prediction result shows that 21.7{\\%} studies used direct causal language exclusively in their conclusions, and 32.4{\\%} used some direct causal language. We also found that the ratio of causal language use differs among authors from different countries, challenging the notion of a shared consensus on causal language use in the global science community. Our prediction model could also be used to help identify the inappropriate use of causal language in science publications.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1829209626,"Goal":"Climate Action","Task":["Detecting Causal Language Use","Causal interpretation of correlational findings","misinformation in science communication"],"Method":["manual content analysis","BERT - based prediction model","prediction model","prediction model"]},{"ID":"leng-etal-2021-fastcorrect-2","title":"{F}ast{C}orrect 2: Fast Error Correction on Multiple Candidates for Automatic Speech Recognition","abstract":"Error correction is widely used in automatic speech recognition (ASR) to post-process the generated sentence, and can further reduce the word error rate (WER). Although multiple candidates are generated by an ASR system through beam search, current error correction approaches can only correct one sentence at a time, failing to leverage the voting effect from multiple candidates to better detect and correct error tokens. In this work, we propose FastCorrect 2, an error correction model that takes multiple ASR candidates as input for better correction accuracy. FastCorrect 2 adopts non-autoregressive generation for fast inference, which consists of an encoder that processes multiple source sentences and a decoder that generates the target sentence in parallel from the adjusted source sentence, where the adjustment is based on the predicted duration of each source token. However, there are some issues when handling multiple source sentences. First, it is non-trivial to leverage the voting effect from multiple source sentences since they usually vary in length. Thus, we propose a novel alignment algorithm to maximize the degree of token alignment among multiple sentences in terms of token and pronunciation similarity. Second, the decoder can only take one adjusted source sentence as input, while there are multiple source sentences. Thus, we develop a candidate predictor to detect the most suitable candidate for the decoder. Experiments on our inhouse dataset and AISHELL-1 show that FastCorrect 2 can further reduce the WER over the previous correction model with single candidate by 3.2{\\%} and 2.6{\\%}, demonstrating the effectiveness of leveraging multiple candidates in ASR error correction. FastCorrect 2 achieves better performance than the cascaded re-scoring and correction pipeline and can serve as a unified post-processing module for ASR.","year":2021,"title_abstract":"{F}ast{C}orrect 2: Fast Error Correction on Multiple Candidates for Automatic Speech Recognition Error correction is widely used in automatic speech recognition (ASR) to post-process the generated sentence, and can further reduce the word error rate (WER). Although multiple candidates are generated by an ASR system through beam search, current error correction approaches can only correct one sentence at a time, failing to leverage the voting effect from multiple candidates to better detect and correct error tokens. In this work, we propose FastCorrect 2, an error correction model that takes multiple ASR candidates as input for better correction accuracy. FastCorrect 2 adopts non-autoregressive generation for fast inference, which consists of an encoder that processes multiple source sentences and a decoder that generates the target sentence in parallel from the adjusted source sentence, where the adjustment is based on the predicted duration of each source token. However, there are some issues when handling multiple source sentences. First, it is non-trivial to leverage the voting effect from multiple source sentences since they usually vary in length. Thus, we propose a novel alignment algorithm to maximize the degree of token alignment among multiple sentences in terms of token and pronunciation similarity. Second, the decoder can only take one adjusted source sentence as input, while there are multiple source sentences. Thus, we develop a candidate predictor to detect the most suitable candidate for the decoder. Experiments on our inhouse dataset and AISHELL-1 show that FastCorrect 2 can further reduce the WER over the previous correction model with single candidate by 3.2{\\%} and 2.6{\\%}, demonstrating the effectiveness of leveraging multiple candidates in ASR error correction. FastCorrect 2 achieves better performance than the cascaded re-scoring and correction pipeline and can serve as a unified post-processing module for ASR.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1829022169,"Goal":"Gender Equality","Task":["Fast Error Correction","Automatic Speech Recognition","Error correction","automatic speech recognition","ASR","fast inference","ASR error correction","ASR"],"Method":["beam search","error correction approaches","FastCorrect 2","error correction model","FastCorrect 2","non - autoregressive generation","encoder","decoder","alignment algorithm","decoder","candidate predictor","decoder","FastCorrect 2","correction model","FastCorrect 2","cascaded re - scoring and correction pipeline","unified post - processing module"]},{"ID":"broeder-etal-2014-experiences","title":"Experiences with the {ISO}cat Data Category Registry","abstract":"The ISOcat Data Category Registry has been a joint project of both ISO TC 37 and the European CLARIN infrastructure. In this paper the experiences of using ISOcat in CLARIN are described and evaluated. This evaluation clarifies the requirements of CLARIN with regard to a semantic registry to support its semantic interoperability needs. A simpler model based on concepts instead of data cate-gories and a simpler workflow based on community recommendations will address these needs better and offer the required flexibility.","year":2014,"title_abstract":"Experiences with the {ISO}cat Data Category Registry The ISOcat Data Category Registry has been a joint project of both ISO TC 37 and the European CLARIN infrastructure. In this paper the experiences of using ISOcat in CLARIN are described and evaluated. This evaluation clarifies the requirements of CLARIN with regard to a semantic registry to support its semantic interoperability needs. A simpler model based on concepts instead of data cate-gories and a simpler workflow based on community recommendations will address these needs better and offer the required flexibility.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1828335822,"Goal":"Life on Land","Task":["semantic interoperability needs"],"Method":["{ISO}cat Data Category Registry","ISOcat Data Category Registry","ISO TC","CLARIN infrastructure","ISOcat","CLARIN","CLARIN","semantic registry"]},{"ID":"rehm-etal-2016-fostering","title":"Fostering the Next Generation of {E}uropean Language Technology: Recent Developments \u2015 Emerging Initiatives \u2015 Challenges and Opportunities","abstract":"META-NET is a European network of excellence, founded in 2010, that consists of 60 research centres in 34 European countries. One of the key visions and goals of META-NET is a truly multilingual Europe, which is substantially supported and realised through language technologies. In this article we provide an overview of recent developments around the multilingual Europe topic, we also describe recent and upcoming events as well as recent and upcoming strategy papers. Furthermore, we provide overviews of two new emerging initiatives, the CEF.AT and ELRC activity on the one hand and the Cracking the Language Barrier federation on the other. The paper closes with several suggested next steps in order to address the current challenges and to open up new opportunities.","year":2016,"title_abstract":"Fostering the Next Generation of {E}uropean Language Technology: Recent Developments \u2015 Emerging Initiatives \u2015 Challenges and Opportunities META-NET is a European network of excellence, founded in 2010, that consists of 60 research centres in 34 European countries. One of the key visions and goals of META-NET is a truly multilingual Europe, which is substantially supported and realised through language technologies. In this article we provide an overview of recent developments around the multilingual Europe topic, we also describe recent and upcoming events as well as recent and upcoming strategy papers. Furthermore, we provide overviews of two new emerging initiatives, the CEF.AT and ELRC activity on the one hand and the Cracking the Language Barrier federation on the other. The paper closes with several suggested next steps in order to address the current challenges and to open up new opportunities.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1827365458,"Goal":"Partnership for the Goals","Task":["{E}uropean Language Technology","multilingual Europe topic"],"Method":["META - NET","META - NET","language technologies","ELRC activity","Language Barrier federation"]},{"ID":"bacciu-etal-2014-accommodations","title":"Accommodations in Tuscany as Linked Data","abstract":"The OpeNER Linked Dataset (OLD) contains 19.140 entries about accommodations in Tuscany (Italy). For each accommodation, it describes the type, e.g. hotel, bed and breakfast, hostel, camping etc., and other useful information, such as a short description, the Web address, its location and the features it provides. OLD is the linked data version of the open dataset provided by Fondazione Sistema Toscana, the representative system for tourism in Tuscany. In addition, to the original dataset, OLD provides also the link of each accommodation to the most common social media (Facebook, Foursquare, Google Places and Booking). OLD exploits three common ontologies of the accommodation domain: Acco, Hontology and GoodRelations. The idea is to provide a flexible dataset, which speaks more than one ontology. OLD is available as a SPARQL node and is released under the Creative Commons release. Finally, OLD is developed within the OpeNER European project, which aims at building a set of ready to use tools to recognize and disambiguate entity mentions and perform sentiment analysis and opinion detection on texts. Within the project, OLD provides a named entity repository for entity disambiguation.","year":2014,"title_abstract":"Accommodations in Tuscany as Linked Data The OpeNER Linked Dataset (OLD) contains 19.140 entries about accommodations in Tuscany (Italy). For each accommodation, it describes the type, e.g. hotel, bed and breakfast, hostel, camping etc., and other useful information, such as a short description, the Web address, its location and the features it provides. OLD is the linked data version of the open dataset provided by Fondazione Sistema Toscana, the representative system for tourism in Tuscany. In addition, to the original dataset, OLD provides also the link of each accommodation to the most common social media (Facebook, Foursquare, Google Places and Booking). OLD exploits three common ontologies of the accommodation domain: Acco, Hontology and GoodRelations. The idea is to provide a flexible dataset, which speaks more than one ontology. OLD is available as a SPARQL node and is released under the Creative Commons release. Finally, OLD is developed within the OpeNER European project, which aims at building a set of ready to use tools to recognize and disambiguate entity mentions and perform sentiment analysis and opinion detection on texts. Within the project, OLD provides a named entity repository for entity disambiguation.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1825731993,"Goal":"Sustainable Cities and Communities","Task":["tourism","accommodation domain","entity mentions","sentiment analysis","opinion detection","entity disambiguation"],"Method":["OLD","OLD","OLD","OLD","OLD","named entity repository"]},{"ID":"murgolo-2021-lab","title":"Lab vs. Production: Two Approaches to Productivity Evaluation for {MTPE} for {LSP}","abstract":"In the paper we propose both kind of tests as viable post-editing productivity evaluation solutions as they both deliver a clear overview of the difference in speed between HT and PE of the translators involved. The decision on whether to use the first approach or the second can be based on a number of factors, such as: availability of actual orders in the domain and language combination to be tested; time; availability of Post-editors in the domain and in the language combination to be tested. The aim of this paper will be to show that both methodologies can be useful in different settings for a preliminary evaluation of possible productivity gain with MTPE.","year":2021,"title_abstract":"Lab vs. Production: Two Approaches to Productivity Evaluation for {MTPE} for {LSP} In the paper we propose both kind of tests as viable post-editing productivity evaluation solutions as they both deliver a clear overview of the difference in speed between HT and PE of the translators involved. The decision on whether to use the first approach or the second can be based on a number of factors, such as: availability of actual orders in the domain and language combination to be tested; time; availability of Post-editors in the domain and in the language combination to be tested. The aim of this paper will be to show that both methodologies can be useful in different settings for a preliminary evaluation of possible productivity gain with MTPE.","social_need":"Decent Work and Economic Growth Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all","cosine_similarity":0.1824533939,"Goal":"Decent Work and Economic Growth","Task":["Production","Productivity Evaluation","post - editing productivity evaluation solutions","MTPE"],"Method":["{MTPE}","{LSP}"]},{"ID":"rogers-etal-2021-just-think","title":"{`}Just What do You Think You{'}re Doing, Dave?{'} A Checklist for Responsible Data Use in {NLP}","abstract":"A key part of the NLP ethics movement is responsible use of data, but exactly what that means or how it can be best achieved remain unclear. This position paper discusses the core legal and ethical principles for collection and sharing of textual data, and the tensions between them. We propose a potential checklist for responsible data (re-)use that could both standardise the peer review of conference submissions, as well as enable a more in-depth view of published research across the community. Our proposal aims to contribute to the development of a consistent standard for data (re-)use, embraced across NLP conferences.","year":2021,"title_abstract":"{`}Just What do You Think You{'}re Doing, Dave?{'} A Checklist for Responsible Data Use in {NLP} A key part of the NLP ethics movement is responsible use of data, but exactly what that means or how it can be best achieved remain unclear. This position paper discusses the core legal and ethical principles for collection and sharing of textual data, and the tensions between them. We propose a potential checklist for responsible data (re-)use that could both standardise the peer review of conference submissions, as well as enable a more in-depth view of published research across the community. Our proposal aims to contribute to the development of a consistent standard for data (re-)use, embraced across NLP conferences.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.1823660284,"Goal":"Life Below Water","Task":["Responsible Data Use","NLP ethics movement","collection and sharing of textual data","responsible data","peer review","data (re - )use"],"Method":["NLP"]},{"ID":"ziems-etal-2022-inducing","title":"Inducing Positive Perspectives with Text Reframing","abstract":"Sentiment transfer is one popular example of a text style transfer task, where the goal is to reverse the sentiment polarity of a text. With a sentiment reversal comes also a reversal in meaning. We introduce a different but related task called positive reframing in which we neutralize a negative point of view and generate a more positive perspective for the author without contradicting the original meaning. Our insistence on meaning preservation makes positive reframing a challenging and semantically rich task. To facilitate rapid progress, we introduce a large-scale benchmark, Positive Psychology Frames, with 8,349 sentence pairs and 12,755 structured annotations to explain positive reframing in terms of six theoretically-motivated reframing strategies. Then we evaluate a set of state-of-the-art text style transfer models, and conclude by discussing key challenges and directions for future work.","year":2022,"title_abstract":"Inducing Positive Perspectives with Text Reframing Sentiment transfer is one popular example of a text style transfer task, where the goal is to reverse the sentiment polarity of a text. With a sentiment reversal comes also a reversal in meaning. We introduce a different but related task called positive reframing in which we neutralize a negative point of view and generate a more positive perspective for the author without contradicting the original meaning. Our insistence on meaning preservation makes positive reframing a challenging and semantically rich task. To facilitate rapid progress, we introduce a large-scale benchmark, Positive Psychology Frames, with 8,349 sentence pairs and 12,755 structured annotations to explain positive reframing in terms of six theoretically-motivated reframing strategies. Then we evaluate a set of state-of-the-art text style transfer models, and conclude by discussing key challenges and directions for future work.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1822209358,"Goal":"Reduced Inequalities","Task":["Inducing Positive Perspectives","Text Reframing","Sentiment transfer","text style transfer task","sentiment reversal","positive reframing","meaning preservation","positive reframing","semantically rich task"],"Method":["theoretically - motivated reframing strategies","text style transfer models"]},{"ID":"ferrero-etal-2017-intoevents","title":"{I}n{T}o{E}vent{S}: An Interactive Toolkit for Discovering and Building Event Schemas","abstract":"Event Schema Induction is the task of learning a representation of events (e.g., bombing) and the roles involved in them (e.g, victim and perpetrator). This paper presents InToEventS, an interactive tool for learning these schemas. InToEventS allows users to explore a corpus and discover which kind of events are present. We show how users can create useful event schemas using two interactive clustering steps.","year":2017,"title_abstract":"{I}n{T}o{E}vent{S}: An Interactive Toolkit for Discovering and Building Event Schemas Event Schema Induction is the task of learning a representation of events (e.g., bombing) and the roles involved in them (e.g, victim and perpetrator). This paper presents InToEventS, an interactive tool for learning these schemas. InToEventS allows users to explore a corpus and discover which kind of events are present. We show how users can create useful event schemas using two interactive clustering steps.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1822159886,"Goal":"Sustainable Cities and Communities","Task":["Discovering and Building Event Schemas","Event Schema Induction","representation of events"],"Method":["Interactive Toolkit","InToEventS","interactive tool","InToEventS","interactive clustering steps"]},{"ID":"benton-etal-2017-ethical","title":"Ethical Research Protocols for Social Media Health Research","abstract":"Social media have transformed data-driven research in political science, the social sciences, health, and medicine. Since health research often touches on sensitive topics that relate to ethics of treatment and patient privacy, similar ethical considerations should be acknowledged when using social media data in health research. While much has been said regarding the ethical considerations of social media research, health research leads to an additional set of concerns. We provide practical suggestions in the form of guidelines for researchers working with social media data in health research. These guidelines can inform an IRB proposal for researchers new to social media health research.","year":2017,"title_abstract":"Ethical Research Protocols for Social Media Health Research Social media have transformed data-driven research in political science, the social sciences, health, and medicine. Since health research often touches on sensitive topics that relate to ethics of treatment and patient privacy, similar ethical considerations should be acknowledged when using social media data in health research. While much has been said regarding the ethical considerations of social media research, health research leads to an additional set of concerns. We provide practical suggestions in the form of guidelines for researchers working with social media data in health research. These guidelines can inform an IRB proposal for researchers new to social media health research.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.1822116375,"Goal":"Good Health and Well-Being","Task":["Social Media Health Research","data - driven research","political science","health","medicine","health research","health research","health research","health research"],"Method":["Ethical Research Protocols"]},{"ID":"schuster-etal-2019-towards","title":"Towards Debiasing Fact Verification Models","abstract":"Fact verification requires validating a claim in the context of evidence. We show, however, that in the popular FEVER dataset this might not necessarily be the case. Claim-only classifiers perform competitively with top evidence-aware models. In this paper, we investigate the cause of this phenomenon, identifying strong cues for predicting labels solely based on the claim, without considering any evidence. We create an evaluation set that avoids those idiosyncrasies. The performance of FEVER-trained models significantly drops when evaluated on this test set. Therefore, we introduce a regularization method which alleviates the effect of bias in the training data, obtaining improvements on the newly created test set. This work is a step towards a more sound evaluation of reasoning capabilities in fact verification models.","year":2019,"title_abstract":"Towards Debiasing Fact Verification Models Fact verification requires validating a claim in the context of evidence. We show, however, that in the popular FEVER dataset this might not necessarily be the case. Claim-only classifiers perform competitively with top evidence-aware models. In this paper, we investigate the cause of this phenomenon, identifying strong cues for predicting labels solely based on the claim, without considering any evidence. We create an evaluation set that avoids those idiosyncrasies. The performance of FEVER-trained models significantly drops when evaluated on this test set. Therefore, we introduce a regularization method which alleviates the effect of bias in the training data, obtaining improvements on the newly created test set. This work is a step towards a more sound evaluation of reasoning capabilities in fact verification models.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1820231676,"Goal":"Climate Action","Task":["Fact verification","predicting labels","reasoning capabilities","fact verification models"],"Method":["Fact Verification Models","Claim - only classifiers","evidence - aware models","FEVER - trained models","regularization method"]},{"ID":"stajner-etal-2021-motivates","title":"What Motivates You? Benchmarking Automatic Detection of Basic Needs from Short Posts","abstract":"According to the self-determination theory, the levels of satisfaction of three basic needs (competence, autonomy and relatedness) have implications on people{'}s everyday life and career. We benchmark the novel task of automatically detecting those needs on short posts in English, by modelling it as a ternary classification task, and as three binary classification tasks. A detailed manual analysis shows that the latter has advantages in the real-world scenario, and that our best models achieve similar performances as a trained human annotator.","year":2021,"title_abstract":"What Motivates You? Benchmarking Automatic Detection of Basic Needs from Short Posts According to the self-determination theory, the levels of satisfaction of three basic needs (competence, autonomy and relatedness) have implications on people{'}s everyday life and career. We benchmark the novel task of automatically detecting those needs on short posts in English, by modelling it as a ternary classification task, and as three binary classification tasks. A detailed manual analysis shows that the latter has advantages in the real-world scenario, and that our best models achieve similar performances as a trained human annotator.","social_need":"Decent Work and Economic Growth Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all","cosine_similarity":0.1820202917,"Goal":"Decent Work and Economic Growth","Task":["Automatic Detection of Basic Needs","automatically detecting those needs","ternary classification task","binary classification tasks"],"Method":["self - determination theory","human annotator"]},{"ID":"eck-2021-keynote","title":"Keynote Abstract: Machine Learning in Conflict Studies: Reflections on Ethics, Collaboration, and Ongoing Challenges","abstract":"Advances in machine learning are nothing short of revolutionary in their potential to analyze massive amounts of data and in doing so, create new knowledge bases. But there is a responsibility in wielding the power to analyze these data since the public attributes a high degree of confidence to results which are based on big datasets. In this keynote, I will first address our ethical imperative as scholars to {``}get it right.{''} This imperative relates not only to model precision but also to the quality of the underlying data, and to whether the models inadvertently reproduce or obscure political biases in the source material. In considering the ethical imperative to get it right, it is also important to define what is {``}right{''}: what is considered an acceptable threshold for classification success needs to be understood in light of the project{'}s objectives. I then reflect on the different topics and data which are sourced in this field. Much of the existing research has focused on identifying conflict events (e.g. battles), but scholars are also increasingly turning to ML approaches to address other facets of the conflict environment. Conflict event extraction has long been a challenge for the natural language processing (NLP) community because it requires sophisticated methods for defining event ontologies, creating language resources, and developing algorithmic approaches. NLP machine-learning tools are ill-adapted to the complex, often messy, and diverse data generated during conflicts. Relative to other types of NLP text corpora, conflicts tend to generate less textual data, and texts are generated non-systematically. Conflict-related texts are often lexically idiosyncratic and tend to be written differently across actors, periods, and conflicts. Event definition and adjudication present tough challenges in the context of conflict corpora. Topics which rely on other types of data may be better-suited to NLP and machine learning methods. For example, Twitter and other social media data lend themselves well to studying hate speech, public opinion, social polarization, or discursive aspects of conflictual environments. Likewise, government-produced policy documents have typically been analyzed with historical, qualitative methods but their standardized formats and quantity suggest that ML methods can provide new traction. ML approaches may also allow scholars to exploit local sources and multi-language sources to a greater degree than has been possible. Many challenges remain, and these are best addressed in collaborative projects which build on interdisciplinary expertise. Classification projects need to be anchored in the theoretical interests of scholars of political violence if the data they produce are to be put to analytical use. There are few ontologies for classification that adequately reflect conflict researchers{'} interests, which highlights the need for conceptual as well as technical development.","year":2021,"title_abstract":"Keynote Abstract: Machine Learning in Conflict Studies: Reflections on Ethics, Collaboration, and Ongoing Challenges Advances in machine learning are nothing short of revolutionary in their potential to analyze massive amounts of data and in doing so, create new knowledge bases. But there is a responsibility in wielding the power to analyze these data since the public attributes a high degree of confidence to results which are based on big datasets. In this keynote, I will first address our ethical imperative as scholars to {``}get it right.{''} This imperative relates not only to model precision but also to the quality of the underlying data, and to whether the models inadvertently reproduce or obscure political biases in the source material. In considering the ethical imperative to get it right, it is also important to define what is {``}right{''}: what is considered an acceptable threshold for classification success needs to be understood in light of the project{'}s objectives. I then reflect on the different topics and data which are sourced in this field. Much of the existing research has focused on identifying conflict events (e.g. battles), but scholars are also increasingly turning to ML approaches to address other facets of the conflict environment. Conflict event extraction has long been a challenge for the natural language processing (NLP) community because it requires sophisticated methods for defining event ontologies, creating language resources, and developing algorithmic approaches. NLP machine-learning tools are ill-adapted to the complex, often messy, and diverse data generated during conflicts. Relative to other types of NLP text corpora, conflicts tend to generate less textual data, and texts are generated non-systematically. Conflict-related texts are often lexically idiosyncratic and tend to be written differently across actors, periods, and conflicts. Event definition and adjudication present tough challenges in the context of conflict corpora. Topics which rely on other types of data may be better-suited to NLP and machine learning methods. For example, Twitter and other social media data lend themselves well to studying hate speech, public opinion, social polarization, or discursive aspects of conflictual environments. Likewise, government-produced policy documents have typically been analyzed with historical, qualitative methods but their standardized formats and quantity suggest that ML methods can provide new traction. ML approaches may also allow scholars to exploit local sources and multi-language sources to a greater degree than has been possible. Many challenges remain, and these are best addressed in collaborative projects which build on interdisciplinary expertise. Classification projects need to be anchored in the theoretical interests of scholars of political violence if the data they produce are to be put to analytical use. There are few ontologies for classification that adequately reflect conflict researchers{'} interests, which highlights the need for conceptual as well as technical development.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.18200773,"Goal":"Peace, Justice and Strong Institutions","Task":["Conflict Studies","Ethics","Collaboration","classification","Conflict event extraction","natural language processing","event ontologies","Event definition","adjudication","Classification projects","classification"],"Method":["Machine Learning","machine learning","ML approaches","algorithmic approaches","NLP","NLP","NLP and machine learning methods","historical , qualitative methods","ML methods","ML approaches"]},{"ID":"madnani-etal-2017-building","title":"Building Better Open-Source Tools to Support Fairness in Automated Scoring","abstract":"Automated scoring of written and spoken responses is an NLP application that can significantly impact lives especially when deployed as part of high-stakes tests such as the GRE\u00ae and the TOEFL\u00ae. Ethical considerations require that automated scoring algorithms treat all test-takers fairly. The educational measurement community has done significant research on fairness in assessments and automated scoring systems must incorporate their recommendations. The best way to do that is by making available automated, non-proprietary tools to NLP researchers that directly incorporate these recommendations and generate the analyses needed to help identify and resolve biases in their scoring systems. In this paper, we attempt to provide such a solution.","year":2017,"title_abstract":"Building Better Open-Source Tools to Support Fairness in Automated Scoring Automated scoring of written and spoken responses is an NLP application that can significantly impact lives especially when deployed as part of high-stakes tests such as the GRE\u00ae and the TOEFL\u00ae. Ethical considerations require that automated scoring algorithms treat all test-takers fairly. The educational measurement community has done significant research on fairness in assessments and automated scoring systems must incorporate their recommendations. The best way to do that is by making available automated, non-proprietary tools to NLP researchers that directly incorporate these recommendations and generate the analyses needed to help identify and resolve biases in their scoring systems. In this paper, we attempt to provide such a solution.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1819551438,"Goal":"Quality Education","Task":["Fairness","Automated Scoring","Automated scoring of written and spoken responses","educational measurement community","assessments"],"Method":["Open - Source Tools","NLP application","GRE\u00ae","automated scoring algorithms","automated scoring systems","NLP researchers","scoring systems"]},{"ID":"dhana-laxmi-etal-2020-dsc","title":"{DSC}-{IIT} {ISM} at {WNUT}-2020 Task 2: Detection of {COVID}-19 informative tweets using {R}o{BERT}a","abstract":"Social media such as Twitter is a hotspot of user-generated information. In this ongoing Covid-19 pandemic, there has been an abundance of data on social media which can be classified as informative and uninformative content. In this paper, we present our work to detect informative Covid-19 English tweets using RoBERTa model as a part of the W-NUT workshop 2020. We show the efficacy of our model on a public dataset with an F1-score of 0.89 on the validation dataset and 0.87 on the leaderboard.","year":2020,"title_abstract":"{DSC}-{IIT} {ISM} at {WNUT}-2020 Task 2: Detection of {COVID}-19 informative tweets using {R}o{BERT}a Social media such as Twitter is a hotspot of user-generated information. In this ongoing Covid-19 pandemic, there has been an abundance of data on social media which can be classified as informative and uninformative content. In this paper, we present our work to detect informative Covid-19 English tweets using RoBERTa model as a part of the W-NUT workshop 2020. We show the efficacy of our model on a public dataset with an F1-score of 0.89 on the validation dataset and 0.87 on the leaderboard.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1819013059,"Goal":"Climate Action","Task":["Detection of {COVID} - 19","Covid - 19","W - NUT workshop"],"Method":["RoBERTa model"]},{"ID":"sommerauer-fokkens-2019-conceptual","title":"Conceptual Change and Distributional Semantic Models: an Exploratory Study on Pitfalls and Possibilities","abstract":"Studying conceptual change using embedding models has become increasingly popular in the Digital Humanities community while critical observations about them have received less attention. This paper investigates what the impact of known pitfalls can be on the conclusions drawn in a digital humanities study through the use case of {``}Racism{''}. In addition, we suggest an approach for modeling a complex concept in terms of words and relations representative of the conceptual system. Our results show that different models created from the same data yield different results, but also indicate that using different model architectures, comparing different corpora and comparing to control words and relations can help to identify which results are solid and which may be due to artefact. We propose guidelines to conduct similar studies, but also note that more work is needed to fully understand how we can distinguish artefacts from actual conceptual changes.","year":2019,"title_abstract":"Conceptual Change and Distributional Semantic Models: an Exploratory Study on Pitfalls and Possibilities Studying conceptual change using embedding models has become increasingly popular in the Digital Humanities community while critical observations about them have received less attention. This paper investigates what the impact of known pitfalls can be on the conclusions drawn in a digital humanities study through the use case of {``}Racism{''}. In addition, we suggest an approach for modeling a complex concept in terms of words and relations representative of the conceptual system. Our results show that different models created from the same data yield different results, but also indicate that using different model architectures, comparing different corpora and comparing to control words and relations can help to identify which results are solid and which may be due to artefact. We propose guidelines to conduct similar studies, but also note that more work is needed to fully understand how we can distinguish artefacts from actual conceptual changes.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1818664372,"Goal":"Reduced Inequalities","Task":["Conceptual Change","conceptual change","digital humanities study"],"Method":["Distributional Semantic Models","embedding models","conceptual system","model architectures"]},{"ID":"field-etal-2018-framing","title":"Framing and Agenda-setting in {R}ussian News: a Computational Analysis of Intricate Political Strategies","abstract":"Amidst growing concern over media manipulation, NLP attention has focused on overt strategies like censorship and {``}fake news{''}. Here, we draw on two concepts from political science literature to explore subtler strategies for government media manipulation: agenda-setting (selecting what topics to cover) and framing (deciding how topics are covered). We analyze 13 years (100K articles) of the Russian newspaper Izvestia and identify a strategy of distraction: articles mention the U.S. more frequently in the month directly following an economic downturn in Russia. We introduce embedding-based methods for cross-lingually projecting English frames to Russian, and discover that these articles emphasize U.S. moral failings and threats to the U.S. Our work offers new ways to identify subtle media manipulation strategies at the intersection of agenda-setting and framing.","year":2018,"title_abstract":"Framing and Agenda-setting in {R}ussian News: a Computational Analysis of Intricate Political Strategies Amidst growing concern over media manipulation, NLP attention has focused on overt strategies like censorship and {``}fake news{''}. Here, we draw on two concepts from political science literature to explore subtler strategies for government media manipulation: agenda-setting (selecting what topics to cover) and framing (deciding how topics are covered). We analyze 13 years (100K articles) of the Russian newspaper Izvestia and identify a strategy of distraction: articles mention the U.S. more frequently in the month directly following an economic downturn in Russia. We introduce embedding-based methods for cross-lingually projecting English frames to Russian, and discover that these articles emphasize U.S. moral failings and threats to the U.S. Our work offers new ways to identify subtle media manipulation strategies at the intersection of agenda-setting and framing.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.181797877,"Goal":"Climate Action","Task":["Framing","Agenda - setting","media manipulation","NLP attention","political science literature","government media manipulation","agenda - setting","distraction","cross - lingually projecting English frames","agenda - setting","framing"],"Method":["Computational Analysis of Intricate Political Strategies","subtler strategies","embedding - based methods","media manipulation strategies"]},{"ID":"varadi-etal-2008-clarin","title":"{CLARIN}: Common Language Resources and Technology Infrastructure","abstract":"The paper provides a general introduction to the CLARIN project, a large-scale European research infrastructure project designed to establish an integrated and interoperable infrastructure of language resources and technologies. The goal is to make language resources and technology much more accessible to all researchers working with language material, particularly non-expert users in the Humanities and Social Sciences. CLARIN intends to build a virtual, distributed infrastructure consisting of a federation of trusted digital archives and repositories where language resources and tools are accessible through web services. The CLARIN project consists of 32 partners from 22 countries and is currently engaged in the preparatory phase of developing the infrastructure. The paper describes the objectives of the project in terms of its technical, legal, linguistic and user dimensions.","year":2008,"title_abstract":"{CLARIN}: Common Language Resources and Technology Infrastructure The paper provides a general introduction to the CLARIN project, a large-scale European research infrastructure project designed to establish an integrated and interoperable infrastructure of language resources and technologies. The goal is to make language resources and technology much more accessible to all researchers working with language material, particularly non-expert users in the Humanities and Social Sciences. CLARIN intends to build a virtual, distributed infrastructure consisting of a federation of trusted digital archives and repositories where language resources and tools are accessible through web services. The CLARIN project consists of 32 partners from 22 countries and is currently engaged in the preparatory phase of developing the infrastructure. The paper describes the objectives of the project in terms of its technical, legal, linguistic and user dimensions.","social_need":"Industry, Innovation and Infrastrucure Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation","cosine_similarity":0.1816752851,"Goal":"Industry, Innovation and Infrastrucure","Task":["CLARIN project","virtual , distributed infrastructure"],"Method":["Technology Infrastructure"]},{"ID":"alsudias-rayson-2020-covid","title":"{COVID-19} and {Arabic} {Twitter}: How can {Arab} World Governments and Public Health Organizations Learn from Social Media?","abstract":"In March 2020, the World Health Organization announced the COVID-19 outbreak as a pandemic. Most previous social media related research has been on English tweets and COVID-19. In this study, we collect approximately 1 million Arabic tweets from the Twitter streaming API related to COVID-19. Focussing on outcomes that we believe will be useful for Public Health Organizations, we analyse them in three different ways: identifying the topics discussed during the period, detecting rumours, and predicting the source of the tweets. We use the k-means algorithm for the first goal with k=5. The topics discussed can be grouped as follows: COVID-19 statistics, prayers for God, COVID-19 locations, advise and education for prevention, and advertising. We sample 2000 tweets and label them manually for false information, correct information, and unrelated. Then, we apply three different machine learning algorithms, Logistic Regression, Support Vector Classification, and Na{\\\"\\i}ve Bayes with two sets of features, word frequency approach and word embeddings. We find that Machine Learning classifiers are able to correctly identify the rumour related tweets with 84{\\%} accuracy. We also try to predict the source of the rumour related tweets depending on our previous model which is about classifying tweets into five categories: academic, media, government, health professional, and public. Around (60{\\%}) of the rumour related tweets are classified as written by health professionals and academics.","year":2020,"title_abstract":"{COVID-19} and {Arabic} {Twitter}: How can {Arab} World Governments and Public Health Organizations Learn from Social Media? In March 2020, the World Health Organization announced the COVID-19 outbreak as a pandemic. Most previous social media related research has been on English tweets and COVID-19. In this study, we collect approximately 1 million Arabic tweets from the Twitter streaming API related to COVID-19. Focussing on outcomes that we believe will be useful for Public Health Organizations, we analyse them in three different ways: identifying the topics discussed during the period, detecting rumours, and predicting the source of the tweets. We use the k-means algorithm for the first goal with k=5. The topics discussed can be grouped as follows: COVID-19 statistics, prayers for God, COVID-19 locations, advise and education for prevention, and advertising. We sample 2000 tweets and label them manually for false information, correct information, and unrelated. Then, we apply three different machine learning algorithms, Logistic Regression, Support Vector Classification, and Na{\\\"\\i}ve Bayes with two sets of features, word frequency approach and word embeddings. We find that Machine Learning classifiers are able to correctly identify the rumour related tweets with 84{\\%} accuracy. We also try to predict the source of the rumour related tweets depending on our previous model which is about classifying tweets into five categories: academic, media, government, health professional, and public. Around (60{\\%}) of the rumour related tweets are classified as written by health professionals and academics.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1815091819,"Goal":"Climate Action","Task":["Public Health Organizations","detecting rumours","prevention","advertising"],"Method":["k - means algorithm","machine learning algorithms","Logistic Regression","Support Vector Classification","Na{\\\"\\i}ve Bayes","word frequency approach","word embeddings","Machine Learning classifiers"]},{"ID":"lim-etal-2021-papagos","title":"Papago{'}s Submission for the {WMT}21 Quality Estimation Shared Task","abstract":"This paper describes Papago submission to the WMT 2021 Quality Estimation Task 1: Sentence-level Direct Assessment. Our multilingual Quality Estimation system explores the combination of Pretrained Language Models and Multi-task Learning architectures. We propose an iterative training pipeline based on pretraining with large amounts of in-domain synthetic data and finetuning with gold (labeled) data. We then compress our system via knowledge distillation in order to reduce parameters yet maintain strong performance. Our submitted multilingual systems perform competitively in multilingual and all 11 individual language pair settings including zero-shot.","year":2021,"title_abstract":"Papago{'}s Submission for the {WMT}21 Quality Estimation Shared Task This paper describes Papago submission to the WMT 2021 Quality Estimation Task 1: Sentence-level Direct Assessment. Our multilingual Quality Estimation system explores the combination of Pretrained Language Models and Multi-task Learning architectures. We propose an iterative training pipeline based on pretraining with large amounts of in-domain synthetic data and finetuning with gold (labeled) data. We then compress our system via knowledge distillation in order to reduce parameters yet maintain strong performance. Our submitted multilingual systems perform competitively in multilingual and all 11 individual language pair settings including zero-shot.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1814274192,"Goal":"Quality Education","Task":["WMT 2021 Quality Estimation","Sentence - level Direct Assessment","multilingual"],"Method":["multilingual Quality Estimation system","Pretrained Language Models","Multi - task Learning architectures","iterative training pipeline","pretraining","knowledge distillation","multilingual systems"]},{"ID":"situ-etal-2021-lifelong","title":"Lifelong Explainer for Lifelong Learners","abstract":"Lifelong Learning (LL) black-box models are dynamic in that they keep learning from new tasks and constantly update their parameters. Owing to the need to utilize information from previously seen tasks, and capture commonalities in potentially diverse data, it is hard for automatic explanation methods to explain the outcomes of these models. In addition, existing explanation methods, e.g., LIME, which are computationally expensive when explaining a static black-box model, are even more inefficient in the LL setting. In this paper, we propose a novel Lifelong Explanation (LLE) approach that continuously trains a student explainer under the supervision of a teacher {--} an arbitrary explanation algorithm {--} on different tasks undertaken in LL. We also leverage the Experience Replay (ER) mechanism to prevent catastrophic forgetting in the student explainer. Our experiments comparing LLE to three baselines on text classification tasks show that LLE can enhance the stability of the explanations for all seen tasks and maintain the same level of faithfulness to the black-box model as the teacher, while being up to 10{\\^{}}2 times faster at test time. Our ablation study shows that the ER mechanism in our LLE approach enhances the learning capabilities of the student explainer. Our code is available at https:\/\/github.com\/situsnow\/LLE.","year":2021,"title_abstract":"Lifelong Explainer for Lifelong Learners Lifelong Learning (LL) black-box models are dynamic in that they keep learning from new tasks and constantly update their parameters. Owing to the need to utilize information from previously seen tasks, and capture commonalities in potentially diverse data, it is hard for automatic explanation methods to explain the outcomes of these models. In addition, existing explanation methods, e.g., LIME, which are computationally expensive when explaining a static black-box model, are even more inefficient in the LL setting. In this paper, we propose a novel Lifelong Explanation (LLE) approach that continuously trains a student explainer under the supervision of a teacher {--} an arbitrary explanation algorithm {--} on different tasks undertaken in LL. We also leverage the Experience Replay (ER) mechanism to prevent catastrophic forgetting in the student explainer. Our experiments comparing LLE to three baselines on text classification tasks show that LLE can enhance the stability of the explanations for all seen tasks and maintain the same level of faithfulness to the black-box model as the teacher, while being up to 10{\\^{}}2 times faster at test time. Our ablation study shows that the ER mechanism in our LLE approach enhances the learning capabilities of the student explainer. Our code is available at https:\/\/github.com\/situsnow\/LLE.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1814024448,"Goal":"Quality Education","Task":["Lifelong Learners","Lifelong Learning","LL setting","text classification tasks"],"Method":["Lifelong Explainer","black - box models","automatic explanation methods","explanation methods","LIME","static black - box model","Lifelong Explanation","student explainer","explanation algorithm","Experience Replay (ER) mechanism","student explainer","LLE","LLE","black - box model","ER mechanism","LLE approach","student explainer"]},{"ID":"yang-etal-2019-sentence","title":"Sentence-Level Agreement for Neural Machine Translation","abstract":"The training objective of neural machine translation (NMT) is to minimize the loss between the words in the translated sentences and those in the references. In NMT, there is a natural correspondence between the source sentence and the target sentence. However, this relationship has only been represented using the entire neural network and the training objective is computed in word-level. In this paper, we propose a sentence-level agreement module to directly minimize the difference between the representation of source and target sentence. The proposed agreement module can be integrated into NMT as an additional training objective function and can also be used to enhance the representation of the source sentences. Empirical results on the NIST Chinese-to-English and WMT English-to-German tasks show the proposed agreement module can significantly improve the NMT performance.","year":2019,"title_abstract":"Sentence-Level Agreement for Neural Machine Translation The training objective of neural machine translation (NMT) is to minimize the loss between the words in the translated sentences and those in the references. In NMT, there is a natural correspondence between the source sentence and the target sentence. However, this relationship has only been represented using the entire neural network and the training objective is computed in word-level. In this paper, we propose a sentence-level agreement module to directly minimize the difference between the representation of source and target sentence. The proposed agreement module can be integrated into NMT as an additional training objective function and can also be used to enhance the representation of the source sentences. Empirical results on the NIST Chinese-to-English and WMT English-to-German tasks show the proposed agreement module can significantly improve the NMT performance.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1813817173,"Goal":"Gender Equality","Task":["Sentence - Level Agreement","Neural Machine Translation","training objective","neural machine translation","NMT","WMT English - to - German tasks","NMT"],"Method":["neural network","sentence - level agreement module","agreement module","NMT","agreement module"]},{"ID":"algotiml-etal-2019-arabic","title":"{A}rabic Tweet-Act: Speech Act Recognition for {A}rabic Asynchronous Conversations","abstract":"Speech acts are the actions that a speaker intends when performing an utterance within conversations. In this paper, we proposed speech act classification for asynchronous conversations on Twitter using multiple machine learning methods including SVM and deep neural networks. We applied the proposed methods on the ArSAS tweets dataset. The obtained results show that superiority of deep learning methods compared to SVMs, where Bi-LSTM managed to achieve an accuracy of 87.5{\\%} and a macro-averaged F1 score 61.5{\\%}. We believe that our results are the first to be reported on the task of speech-act recognition for asynchronous conversations on Arabic Twitter.","year":2019,"title_abstract":"{A}rabic Tweet-Act: Speech Act Recognition for {A}rabic Asynchronous Conversations Speech acts are the actions that a speaker intends when performing an utterance within conversations. In this paper, we proposed speech act classification for asynchronous conversations on Twitter using multiple machine learning methods including SVM and deep neural networks. We applied the proposed methods on the ArSAS tweets dataset. The obtained results show that superiority of deep learning methods compared to SVMs, where Bi-LSTM managed to achieve an accuracy of 87.5{\\%} and a macro-averaged F1 score 61.5{\\%}. We believe that our results are the first to be reported on the task of speech-act recognition for asynchronous conversations on Arabic Twitter.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1812733561,"Goal":"Climate Action","Task":["Speech Act Recognition","speech act classification","asynchronous conversations","speech - act recognition","asynchronous conversations"],"Method":["machine learning methods","SVM","deep neural networks","deep learning methods","SVMs","Bi - LSTM"]},{"ID":"andy-etal-2021-understanding","title":"Understanding Social Support Expressed in a {COVID}-19 Online Forum","abstract":"In online forums focused on health and wellbeing, individuals tend to seek and give the following social support: emotional and informational support. Understanding the expressions of these social supports in an online COVID- 19 forum is important for: (a) the forum and its members to provide the right type of support to individuals and (b) determining the long term effects of the COVID-19 pandemic on the well-being of the public, thereby informing interventions. In this work, we build four machine learning models to measure the extent of the following social supports expressed in each post in a COVID-19 online forum: (a) emotional support given (b) emotional support sought (c) informational support given, and (d) informational support sought. Using these models, we aim to: (i) determine if there is a correlation between the different social supports expressed in posts e.g. when members of the forum give emotional support in posts, do they also tend to give or seek informational support in the same post? (ii) determine how these social supports sought and given changes over time in published posts. We find that (i) there is a positive correlation between the informational support given in posts and the emotional support given and emotional support sought, respectively, in these posts and (ii) over time, users tended to seek more emotional support and give less emotional support.","year":2021,"title_abstract":"Understanding Social Support Expressed in a {COVID}-19 Online Forum In online forums focused on health and wellbeing, individuals tend to seek and give the following social support: emotional and informational support. Understanding the expressions of these social supports in an online COVID- 19 forum is important for: (a) the forum and its members to provide the right type of support to individuals and (b) determining the long term effects of the COVID-19 pandemic on the well-being of the public, thereby informing interventions. In this work, we build four machine learning models to measure the extent of the following social supports expressed in each post in a COVID-19 online forum: (a) emotional support given (b) emotional support sought (c) informational support given, and (d) informational support sought. Using these models, we aim to: (i) determine if there is a correlation between the different social supports expressed in posts e.g. when members of the forum give emotional support in posts, do they also tend to give or seek informational support in the same post? (ii) determine how these social supports sought and given changes over time in published posts. We find that (i) there is a positive correlation between the informational support given in posts and the emotional support given and emotional support sought, respectively, in these posts and (ii) over time, users tended to seek more emotional support and give less emotional support.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.1811714768,"Goal":"Good Health and Well-Being","Task":["Understanding Social Support","COVID - 19"],"Method":["machine learning models"]},{"ID":"aker-etal-2017-simple","title":"Simple Open Stance Classification for Rumour Analysis","abstract":"Stance classification determines the attitude, or stance, in a (typically short) text. The task has powerful applications, such as the detection of fake news or the automatic extraction of attitudes toward entities or events in the media. This paper describes a surprisingly simple and efficient classification approach to open stance classification in Twitter, for rumour and veracity classification. The approach profits from a novel set of automatically identifiable problem-specific features, which significantly boost classifier accuracy and achieve above state-of-the-art results on recent benchmark datasets. This calls into question the value of using complex sophisticated models for stance classification without first doing informed feature extraction.","year":2017,"title_abstract":"Simple Open Stance Classification for Rumour Analysis Stance classification determines the attitude, or stance, in a (typically short) text. The task has powerful applications, such as the detection of fake news or the automatic extraction of attitudes toward entities or events in the media. This paper describes a surprisingly simple and efficient classification approach to open stance classification in Twitter, for rumour and veracity classification. The approach profits from a novel set of automatically identifiable problem-specific features, which significantly boost classifier accuracy and achieve above state-of-the-art results on recent benchmark datasets. This calls into question the value of using complex sophisticated models for stance classification without first doing informed feature extraction.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1810807288,"Goal":"Climate Action","Task":["Open Stance Classification","Rumour Analysis","Stance classification","detection of fake news","automatic extraction of attitudes","classification","open stance classification","rumour and veracity classification","stance classification"],"Method":["informed feature extraction"]},{"ID":"gillani-levy-2019-simple","title":"Simple dynamic word embeddings for mapping perceptions in the public sphere","abstract":"Word embeddings trained on large-scale historical corpora can illuminate human biases and stereotypes that perpetuate social inequalities. These embeddings are often trained in separate vector space models defined according to different attributes of interest. In this paper, we introduce a single, unified dynamic embedding model that learns attribute-specific word embeddings and apply it to a novel dataset{---}talk radio shows from around the US{---}to analyze perceptions about refugees. We validate our model on a benchmark dataset and apply it to two corpora of talk radio shows averaging 117 million words produced over one month across 83 stations and 64 cities. Our findings suggest that dynamic word embeddings are capable of identifying nuanced differences in public discourse about contentious topics, suggesting their usefulness as a tool for better understanding how the public perceives and engages with different issues across time, geography, and other dimensions.","year":2019,"title_abstract":"Simple dynamic word embeddings for mapping perceptions in the public sphere Word embeddings trained on large-scale historical corpora can illuminate human biases and stereotypes that perpetuate social inequalities. These embeddings are often trained in separate vector space models defined according to different attributes of interest. In this paper, we introduce a single, unified dynamic embedding model that learns attribute-specific word embeddings and apply it to a novel dataset{---}talk radio shows from around the US{---}to analyze perceptions about refugees. We validate our model on a benchmark dataset and apply it to two corpora of talk radio shows averaging 117 million words produced over one month across 83 stations and 64 cities. Our findings suggest that dynamic word embeddings are capable of identifying nuanced differences in public discourse about contentious topics, suggesting their usefulness as a tool for better understanding how the public perceives and engages with different issues across time, geography, and other dimensions.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1807838529,"Goal":"Reduced Inequalities","Task":["mapping perceptions","Word embeddings"],"Method":["dynamic word embeddings","vector space models","unified dynamic embedding model","dynamic word embeddings"]},{"ID":"kaneko-bollegala-2021-debiasing","title":"Debiasing Pre-trained Contextualised Embeddings","abstract":"In comparison to the numerous debiasing methods proposed for the static non-contextualised word embeddings, the discriminative biases in contextualised embeddings have received relatively little attention. We propose a fine-tuning method that can be applied at token- or sentence-levels to debias pre-trained contextualised embeddings. Our proposed method can be applied to any pre-trained contextualised embedding model, without requiring to retrain those models. Using gender bias as an illustrative example, we then conduct a systematic study using several state-of-the-art (SoTA) contextualised representations on multiple benchmark datasets to evaluate the level of biases encoded in different contextualised embeddings before and after debiasing using the proposed method. We find that applying token-level debiasing for all tokens and across all layers of a contextualised embedding model produces the best performance. Interestingly, we observe that there is a trade-off between creating an accurate vs. unbiased contextualised embedding model, and different contextualised embedding models respond differently to this trade-off.","year":2021,"title_abstract":"Debiasing Pre-trained Contextualised Embeddings In comparison to the numerous debiasing methods proposed for the static non-contextualised word embeddings, the discriminative biases in contextualised embeddings have received relatively little attention. We propose a fine-tuning method that can be applied at token- or sentence-levels to debias pre-trained contextualised embeddings. Our proposed method can be applied to any pre-trained contextualised embedding model, without requiring to retrain those models. Using gender bias as an illustrative example, we then conduct a systematic study using several state-of-the-art (SoTA) contextualised representations on multiple benchmark datasets to evaluate the level of biases encoded in different contextualised embeddings before and after debiasing using the proposed method. We find that applying token-level debiasing for all tokens and across all layers of a contextualised embedding model produces the best performance. Interestingly, we observe that there is a trade-off between creating an accurate vs. unbiased contextualised embedding model, and different contextualised embedding models respond differently to this trade-off.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1806492656,"Goal":"Gender Equality","Task":["static non - contextualised word embeddings"],"Method":["Debiasing Pre - trained Contextualised Embeddings","debiasing methods","contextualised embeddings","fine - tuning method","contextualised embeddings","contextualised embedding model","contextualised representations","token - level debiasing","contextualised embedding model","unbiased contextualised embedding model","contextualised embedding models"]},{"ID":"sidorov-etal-2014-comparison","title":"Comparison of Gender- and Speaker-adaptive Emotion Recognition","abstract":"Deriving the emotion of a human speaker is a hard task, especially if only the audio stream is taken into account. While state-of-the-art approaches already provide good results, adaptive methods have been proposed in order to further improve the recognition accuracy. A recent approach is to add characteristics of the speaker, e.g., the gender of the speaker. In this contribution, we argue that adding information unique for each speaker, i.e., by using speaker identification techniques, improves emotion recognition simply by adding this additional information to the feature vector of the statistical classification algorithm. Moreover, we compare this approach to emotion recognition adding only the speaker gender being a non-unique speaker attribute. We justify this by performing adaptive emotion recognition using both gender and speaker information on four different corpora of different languages containing acted and non-acted speech. The final results show that adding speaker information significantly outperforms both adding gender information and solely using a generic speaker-independent approach.","year":2014,"title_abstract":"Comparison of Gender- and Speaker-adaptive Emotion Recognition Deriving the emotion of a human speaker is a hard task, especially if only the audio stream is taken into account. While state-of-the-art approaches already provide good results, adaptive methods have been proposed in order to further improve the recognition accuracy. A recent approach is to add characteristics of the speaker, e.g., the gender of the speaker. In this contribution, we argue that adding information unique for each speaker, i.e., by using speaker identification techniques, improves emotion recognition simply by adding this additional information to the feature vector of the statistical classification algorithm. Moreover, we compare this approach to emotion recognition adding only the speaker gender being a non-unique speaker attribute. We justify this by performing adaptive emotion recognition using both gender and speaker information on four different corpora of different languages containing acted and non-acted speech. The final results show that adding speaker information significantly outperforms both adding gender information and solely using a generic speaker-independent approach.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1805306673,"Goal":"Gender Equality","Task":["Gender - and Speaker - adaptive Emotion Recognition","emotion recognition","emotion recognition","adaptive emotion recognition"],"Method":["adaptive methods","speaker identification techniques","statistical classification algorithm","generic speaker - independent approach"]},{"ID":"piotrkowicz-etal-2017-automatic","title":"Automatic Extraction of News Values from Headline Text","abstract":"Headlines play a crucial role in attracting audiences{'} attention to online artefacts (e.g. news articles, videos, blogs). The ability to carry out an automatic, large-scale analysis of headlines is critical to facilitate the selection and prioritisation of a large volume of digital content. In journalism studies news content has been extensively studied using manually annotated news values - factors used implicitly and explicitly when making decisions on the selection and prioritisation of news items. This paper presents the first attempt at a fully automatic extraction of news values from headline text. The news values extraction methods are applied on a large headlines corpus collected from The Guardian, and evaluated by comparing it with a manually annotated gold standard. A crowdsourcing survey indicates that news values affect people{'}s decisions to click on a headline, supporting the need for an automatic news values detection.","year":2017,"title_abstract":"Automatic Extraction of News Values from Headline Text Headlines play a crucial role in attracting audiences{'} attention to online artefacts (e.g. news articles, videos, blogs). The ability to carry out an automatic, large-scale analysis of headlines is critical to facilitate the selection and prioritisation of a large volume of digital content. In journalism studies news content has been extensively studied using manually annotated news values - factors used implicitly and explicitly when making decisions on the selection and prioritisation of news items. This paper presents the first attempt at a fully automatic extraction of news values from headline text. The news values extraction methods are applied on a large headlines corpus collected from The Guardian, and evaluated by comparing it with a manually annotated gold standard. A crowdsourcing survey indicates that news values affect people{'}s decisions to click on a headline, supporting the need for an automatic news values detection.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1805144995,"Goal":"Climate Action","Task":["Automatic Extraction of News Values","automatic , large - scale analysis of headlines","selection and prioritisation of a large volume of digital content","journalism studies news content","selection and prioritisation of news items","automatic extraction of news values","automatic news values detection"],"Method":["news values extraction methods"]},{"ID":"le-ferrand-etal-2022-learning","title":"Learning From Failure: Data Capture in an {A}ustralian Aboriginal Community","abstract":"Most low resource language technology development is premised on the need to collect data for training statistical models. When we follow the typical process of recording and transcribing text for small Indigenous languages, we hit up against the so-called {``}transcription bottleneck.{''} Therefore it is worth exploring new ways of engaging with speakers which generate data while avoiding the transcription bottleneck. We have deployed a prototype app for speakers to use for confirming system guesses in an approach to transcription based on word spotting. However, in the process of testing the app we encountered many new problems for engagement with speakers. This paper presents a close-up study of the process of deploying data capture technology on the ground in an Australian Aboriginal community. We reflect on our interactions with participants and draw lessons that apply to anyone seeking to develop methods for language data collection in an Indigenous community.","year":2022,"title_abstract":"Learning From Failure: Data Capture in an {A}ustralian Aboriginal Community Most low resource language technology development is premised on the need to collect data for training statistical models. When we follow the typical process of recording and transcribing text for small Indigenous languages, we hit up against the so-called {``}transcription bottleneck.{''} Therefore it is worth exploring new ways of engaging with speakers which generate data while avoiding the transcription bottleneck. We have deployed a prototype app for speakers to use for confirming system guesses in an approach to transcription based on word spotting. However, in the process of testing the app we encountered many new problems for engagement with speakers. This paper presents a close-up study of the process of deploying data capture technology on the ground in an Australian Aboriginal community. We reflect on our interactions with participants and draw lessons that apply to anyone seeking to develop methods for language data collection in an Indigenous community.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1803973019,"Goal":"Sustainable Cities and Communities","Task":["Data Capture","low resource language technology development","transcription bottleneck","confirming system guesses","transcription","language data collection"],"Method":["statistical models","word spotting","data capture technology"]},{"ID":"maegaard-etal-2010-cooperation","title":"Cooperation for {A}rabic Language Resources and Tools {---} The {MEDAR} Project","abstract":"The paper describes some of the work carried out within the European funded project MEDAR. The project has three streams of activity: the technical stream, the cooperation stream and the dissemination stream. MEDAR has first updated the existing surveys and BLARK for Arabic, and then the technical stream focused on machine translation. The consortium identified a number of freely available MT systems and then customized two versions of the famous MOSES package. The Consortium addressed the needs to package MOSES for English to Arabic (while the main MT stream is on Arabic to English). For performance assessment purposes, the partners produced test data that allowed carrying out an evaluation campaign with 5 different systems (including from outside the consortium) and two online ones. Both the MT baselines and the collected data will be made available via ELRA catalogue. The cooperation stream focuses mostly on the cooperation roadmap for Human Language Technologies for Arabic. Cooperation Roadmap for the region directed towards the Arabic HLT in general. It is the purpose of the roadmap to outline areas and priorities for collaboration, in terms of collaboration between EU countries and Arabic speaking countries, as well as cooperation in general: between countries, between universities, and last but not least between universities and industry.","year":2010,"title_abstract":"Cooperation for {A}rabic Language Resources and Tools {---} The {MEDAR} Project The paper describes some of the work carried out within the European funded project MEDAR. The project has three streams of activity: the technical stream, the cooperation stream and the dissemination stream. MEDAR has first updated the existing surveys and BLARK for Arabic, and then the technical stream focused on machine translation. The consortium identified a number of freely available MT systems and then customized two versions of the famous MOSES package. The Consortium addressed the needs to package MOSES for English to Arabic (while the main MT stream is on Arabic to English). For performance assessment purposes, the partners produced test data that allowed carrying out an evaluation campaign with 5 different systems (including from outside the consortium) and two online ones. Both the MT baselines and the collected data will be made available via ELRA catalogue. The cooperation stream focuses mostly on the cooperation roadmap for Human Language Technologies for Arabic. Cooperation Roadmap for the region directed towards the Arabic HLT in general. It is the purpose of the roadmap to outline areas and priorities for collaboration, in terms of collaboration between EU countries and Arabic speaking countries, as well as cooperation in general: between countries, between universities, and last but not least between universities and industry.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1803574562,"Goal":"Partnership for the Goals","Task":["{A}rabic Language Resources","dissemination stream","Arabic","machine translation","MT","MT","MT","cooperation roadmap","Human Language Technologies","Arabic","Cooperation Roadmap"],"Method":["MEDAR","MOSES package","Arabic HLT"]},{"ID":"sahlgren-etal-2021-basically","title":"It{'}s Basically the Same Language Anyway: the Case for a Nordic Language Model","abstract":"When is it beneficial for a research community to organize a broader collaborative effort on a topic, and when should we instead promote individual efforts? In this opinion piece, we argue that we are at a stage in the development of large-scale language models where a collaborative effort is desirable, despite the fact that the preconditions for making individual contributions have never been better. We consider a number of arguments for collaboratively developing a large-scale Nordic language model, include environmental considerations, cost, data availability, language typology, cultural similarity, and transparency. Our primary goal is to raise awareness and foster a discussion about our potential impact and responsibility as NLP community.","year":2021,"title_abstract":"It{'}s Basically the Same Language Anyway: the Case for a Nordic Language Model When is it beneficial for a research community to organize a broader collaborative effort on a topic, and when should we instead promote individual efforts? In this opinion piece, we argue that we are at a stage in the development of large-scale language models where a collaborative effort is desirable, despite the fact that the preconditions for making individual contributions have never been better. We consider a number of arguments for collaboratively developing a large-scale Nordic language model, include environmental considerations, cost, data availability, language typology, cultural similarity, and transparency. Our primary goal is to raise awareness and foster a discussion about our potential impact and responsibility as NLP community.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1803412288,"Goal":"Partnership for the Goals","Task":["large - scale language models","NLP community"],"Method":["Nordic Language Model","Nordic language model"]},{"ID":"vanopstal-etal-2010-assessing","title":"Assessing the Impact of {E}nglish Language Skills and Education Level on {P}ub{M}ed Searches by {D}utch-speaking Users","abstract":"The aim of this study was to assess the retrieval effectiveness of nursing students in the Dutch-speaking part of Belgium. We tested two groups: students from the master of Nursing and Midwifery training, and students from the bachelor of Nursing program. The test consisted of five parts: first, the students completed an enquiry about their computer skills, experiences with PubMed and how they assessed their own language skills. Secondly, an introduction into the use of MeSH in PubMed was given, followed by a PubMed search. After the literature search, a second enquiry was completed in which the students were asked to give their opinion about the test. To conclude, an official language test was completed. The results of the PubMed search, i.e. a list of articles the students deemed relevant for a particular question, were compared to a gold standard. Precision, recall and F-score were calculated in order to evaluate the efficiency of the PubMed search. We used information from the search process, such as search term formulation and MeSH term selection to evaluate the search process and examined their relationship with the results of the language test and the level of education.","year":2010,"title_abstract":"Assessing the Impact of {E}nglish Language Skills and Education Level on {P}ub{M}ed Searches by {D}utch-speaking Users The aim of this study was to assess the retrieval effectiveness of nursing students in the Dutch-speaking part of Belgium. We tested two groups: students from the master of Nursing and Midwifery training, and students from the bachelor of Nursing program. The test consisted of five parts: first, the students completed an enquiry about their computer skills, experiences with PubMed and how they assessed their own language skills. Secondly, an introduction into the use of MeSH in PubMed was given, followed by a PubMed search. After the literature search, a second enquiry was completed in which the students were asked to give their opinion about the test. To conclude, an official language test was completed. The results of the PubMed search, i.e. a list of articles the students deemed relevant for a particular question, were compared to a gold standard. Precision, recall and F-score were calculated in order to evaluate the efficiency of the PubMed search. We used information from the search process, such as search term formulation and MeSH term selection to evaluate the search process and examined their relationship with the results of the language test and the level of education.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1802954674,"Goal":"Quality Education","Task":["retrieval","Nursing and Midwifery training","PubMed search","PubMed search"],"Method":["PubMed search","search process","search term formulation","MeSH term selection","search process"]},{"ID":"petukhova-etal-2016-modelling","title":"Modelling Multi-issue Bargaining Dialogues: Data Collection, Annotation Design and Corpus","abstract":"The paper describes experimental dialogue data collection activities, as well semantically annotated corpus creation undertaken within EU-funded METALOGUE project(www.metalogue.eu). The project aims to develop a dialogue system with flexible dialogue management to enable system{'}s adaptive, reactive, interactive and proactive dialogue behavior in setting goals, choosing appropriate strategies and monitoring numerous parallel interpretation and management processes. To achieve these goals negotiation (or more precisely multi-issue bargaining) scenario has been considered as the specific setting and application domain. The dialogue corpus forms the basis for the design of task and interaction models of participants negotiation behavior, and subsequently for dialogue system development which would be capable to replace one of the negotiators. The METALOGUE corpus will be released to the community for research purposes.","year":2016,"title_abstract":"Modelling Multi-issue Bargaining Dialogues: Data Collection, Annotation Design and Corpus The paper describes experimental dialogue data collection activities, as well semantically annotated corpus creation undertaken within EU-funded METALOGUE project(www.metalogue.eu). The project aims to develop a dialogue system with flexible dialogue management to enable system{'}s adaptive, reactive, interactive and proactive dialogue behavior in setting goals, choosing appropriate strategies and monitoring numerous parallel interpretation and management processes. To achieve these goals negotiation (or more precisely multi-issue bargaining) scenario has been considered as the specific setting and application domain. The dialogue corpus forms the basis for the design of task and interaction models of participants negotiation behavior, and subsequently for dialogue system development which would be capable to replace one of the negotiators. The METALOGUE corpus will be released to the community for research purposes.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1802437454,"Goal":"Partnership for the Goals","Task":["Modelling Multi - issue Bargaining Dialogues","Data Collection","Annotation Design","semantically annotated corpus creation","dialogue management","parallel interpretation and management processes","negotiation","multi - issue bargaining) scenario","task and interaction models of participants negotiation behavior","dialogue system development"],"Method":["dialogue system"]},{"ID":"vecchi-etal-2021-towards","title":"Towards Argument Mining for Social Good: A Survey","abstract":"This survey builds an interdisciplinary picture of Argument Mining (AM), with a strong focus on its potential to address issues related to Social and Political Science. More specifically, we focus on AM challenges related to its applications to social media and in the multilingual domain, and then proceed to the widely debated notion of argument quality. We propose a novel definition of argument quality which is integrated with that of deliberative quality from the Social Science literature. Under our definition, the quality of a contribution needs to be assessed at multiple levels: the contribution itself, its preceding context, and the consequential effect on the development of the upcoming discourse. The latter has not received the deserved attention within the community. We finally define an application of AM for Social Good: (semi-)automatic moderation, a highly integrative application which (a) represents a challenging testbed for the integrated notion of quality we advocate, (b) allows the empirical quantification of argument\/deliberative quality to benefit from the developments in other NLP fields (i.e. hate speech detection, fact checking, debiasing), and (c) has a clearly beneficial potential at the level of its societal thanks to its real-world application (even if extremely ambitious).","year":2021,"title_abstract":"Towards Argument Mining for Social Good: A Survey This survey builds an interdisciplinary picture of Argument Mining (AM), with a strong focus on its potential to address issues related to Social and Political Science. More specifically, we focus on AM challenges related to its applications to social media and in the multilingual domain, and then proceed to the widely debated notion of argument quality. We propose a novel definition of argument quality which is integrated with that of deliberative quality from the Social Science literature. Under our definition, the quality of a contribution needs to be assessed at multiple levels: the contribution itself, its preceding context, and the consequential effect on the development of the upcoming discourse. The latter has not received the deserved attention within the community. We finally define an application of AM for Social Good: (semi-)automatic moderation, a highly integrative application which (a) represents a challenging testbed for the integrated notion of quality we advocate, (b) allows the empirical quantification of argument\/deliberative quality to benefit from the developments in other NLP fields (i.e. hate speech detection, fact checking, debiasing), and (c) has a clearly beneficial potential at the level of its societal thanks to its real-world application (even if extremely ambitious).","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1801810712,"Goal":"Reduced Inequalities","Task":["Argument Mining","Social Good","Argument Mining","Social and Political Science","AM challenges","multilingual domain","argument quality","Social Good","(semi - )automatic moderation","NLP fields","hate speech detection","fact checking","debiasing)"],"Method":["AM"]},{"ID":"li-etal-2019-word","title":"On the Word Alignment from Neural Machine Translation","abstract":"Prior researches suggest that neural machine translation (NMT) captures word alignment through its attention mechanism, however, this paper finds attention may almost fail to capture word alignment for some NMT models. This paper thereby proposes two methods to induce word alignment which are general and agnostic to specific NMT models. Experiments show that both methods induce much better word alignment than attention. This paper further visualizes the translation through the word alignment induced by NMT. In particular, it analyzes the effect of alignment errors on translation errors at word level and its quantitative analysis over many testing examples consistently demonstrate that alignment errors are likely to lead to translation errors measured by different metrics.","year":2019,"title_abstract":"On the Word Alignment from Neural Machine Translation Prior researches suggest that neural machine translation (NMT) captures word alignment through its attention mechanism, however, this paper finds attention may almost fail to capture word alignment for some NMT models. This paper thereby proposes two methods to induce word alignment which are general and agnostic to specific NMT models. Experiments show that both methods induce much better word alignment than attention. This paper further visualizes the translation through the word alignment induced by NMT. In particular, it analyzes the effect of alignment errors on translation errors at word level and its quantitative analysis over many testing examples consistently demonstrate that alignment errors are likely to lead to translation errors measured by different metrics.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1800009012,"Goal":"Gender Equality","Task":["Word Alignment","Neural Machine Translation","neural machine translation","word alignment","word alignment","word alignment","word alignment","translation","translation"],"Method":["attention mechanism","NMT models","NMT models","NMT"]},{"ID":"zinn-etal-2010-evolving","title":"An Evolving e{S}cience Environment for Research Data in Linguistics","abstract":"The amount of research data in the Humanities is increasing at fast speed. Metadata helps describing and making accessible this data to interested researchers within and across institutions. While metadata interoperability is an issue that is being recognised and addressed, the systematic and user-driven provision of annotations and the linking together of resources into new organisational layers have received much less attention. This paper gives an overview of our evolving technological eScience environment to support such functionality. It describes two tools, ADDIT and ViCoS, which enable researchers, rather than archive managers, to organise and reorganise research data to fit their particular needs. The two tools, which are embedded into our institute's existing software landscape, are an initial step towards an eScience environment that gives our scientists easy access to (multimodal) research data of their interest, and empowers them to structure, enrich, link together, and share such data as they wish.","year":2010,"title_abstract":"An Evolving e{S}cience Environment for Research Data in Linguistics The amount of research data in the Humanities is increasing at fast speed. Metadata helps describing and making accessible this data to interested researchers within and across institutions. While metadata interoperability is an issue that is being recognised and addressed, the systematic and user-driven provision of annotations and the linking together of resources into new organisational layers have received much less attention. This paper gives an overview of our evolving technological eScience environment to support such functionality. It describes two tools, ADDIT and ViCoS, which enable researchers, rather than archive managers, to organise and reorganise research data to fit their particular needs. The two tools, which are embedded into our institute's existing software landscape, are an initial step towards an eScience environment that gives our scientists easy access to (multimodal) research data of their interest, and empowers them to structure, enrich, link together, and share such data as they wish.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1799833775,"Goal":"Life on Land","Task":["Linguistics","metadata interoperability","linking together of resources"],"Method":["e{S}cience Environment","Metadata","technological eScience environment","ADDIT","ViCoS","archive managers","eScience environment"]},{"ID":"nakamachi-etal-2020-tmuou","title":"{TMUOU} Submission for {WMT}20 Quality Estimation Shared Task","abstract":"We introduce the TMUOU submission for the WMT20 Quality Estimation Shared Task 1: Sentence-Level Direct Assessment. Our system is an ensemble model of four regression models based on XLM-RoBERTa with language tags. We ranked 4th in Pearson and 2nd in MAE and RMSE on a multilingual track.","year":2020,"title_abstract":"{TMUOU} Submission for {WMT}20 Quality Estimation Shared Task We introduce the TMUOU submission for the WMT20 Quality Estimation Shared Task 1: Sentence-Level Direct Assessment. Our system is an ensemble model of four regression models based on XLM-RoBERTa with language tags. We ranked 4th in Pearson and 2nd in MAE and RMSE on a multilingual track.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1799710542,"Goal":"Quality Education","Task":["WMT20 Quality Estimation Shared Task","Sentence - Level Direct Assessment"],"Method":["TMUOU","ensemble model","regression models","XLM - RoBERTa"]},{"ID":"alhindi-etal-2021-arastance","title":"{A}ra{S}tance: A Multi-Country and Multi-Domain Dataset of {A}rabic Stance Detection for Fact Checking","abstract":"With the continuing spread of misinformation and disinformation online, it is of increasing importance to develop combating mechanisms at scale in the form of automated systems that support multiple languages. One task of interest is claim veracity prediction, which can be addressed using stance detection with respect to relevant documents retrieved online. To this end, we present our new Arabic Stance Detection dataset (AraStance) of 4,063 claim{--}article pairs from a diverse set of sources comprising three fact-checking websites and one news website. AraStance covers false and true claims from multiple domains (e.g., politics, sports, health) and several Arab countries, and it is well-balanced between related and unrelated documents with respect to the claims. We benchmark AraStance, along with two other stance detection datasets, using a number of BERT-based models. Our best model achieves an accuracy of 85{\\%} and a macro F1 score of 78{\\%}, which leaves room for improvement and reflects the challenging nature of AraStance and the task of stance detection in general.","year":2021,"title_abstract":"{A}ra{S}tance: A Multi-Country and Multi-Domain Dataset of {A}rabic Stance Detection for Fact Checking With the continuing spread of misinformation and disinformation online, it is of increasing importance to develop combating mechanisms at scale in the form of automated systems that support multiple languages. One task of interest is claim veracity prediction, which can be addressed using stance detection with respect to relevant documents retrieved online. To this end, we present our new Arabic Stance Detection dataset (AraStance) of 4,063 claim{--}article pairs from a diverse set of sources comprising three fact-checking websites and one news website. AraStance covers false and true claims from multiple domains (e.g., politics, sports, health) and several Arab countries, and it is well-balanced between related and unrelated documents with respect to the claims. We benchmark AraStance, along with two other stance detection datasets, using a number of BERT-based models. Our best model achieves an accuracy of 85{\\%} and a macro F1 score of 78{\\%}, which leaves room for improvement and reflects the challenging nature of AraStance and the task of stance detection in general.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1799595356,"Goal":"Climate Action","Task":["Stance Detection","Fact Checking","claim veracity prediction","stance detection","stance detection","stance detection"],"Method":["combating mechanisms","automated systems","AraStance","BERT - based models","AraStance"]},{"ID":"van-miltenburg-etal-2016-vu","title":"The {VU} Sound Corpus: Adding More Fine-grained Annotations to the Freesound Database","abstract":"This paper presents a collection of annotations (tags or keywords) for a set of 2,133 environmental sounds taken from the Freesound database (www.freesound.org). The annotations are acquired through an open-ended crowd-labeling task, in which participants were asked to provide keywords for each of three sounds. The main goal of this study is to find out (i) whether it is feasible to collect keywords for a large collection of sounds through crowdsourcing, and (ii) how people talk about sounds, and what information they can infer from hearing a sound in isolation. Our main finding is that it is not only feasible to perform crowd-labeling for a large collection of sounds, it is also very useful to highlight different aspects of the sounds that authors may fail to mention. Our data is freely available, and can be used to ground semantic models, improve search in audio databases, and to study the language of sound.","year":2016,"title_abstract":"The {VU} Sound Corpus: Adding More Fine-grained Annotations to the Freesound Database This paper presents a collection of annotations (tags or keywords) for a set of 2,133 environmental sounds taken from the Freesound database (www.freesound.org). The annotations are acquired through an open-ended crowd-labeling task, in which participants were asked to provide keywords for each of three sounds. The main goal of this study is to find out (i) whether it is feasible to collect keywords for a large collection of sounds through crowdsourcing, and (ii) how people talk about sounds, and what information they can infer from hearing a sound in isolation. Our main finding is that it is not only feasible to perform crowd-labeling for a large collection of sounds, it is also very useful to highlight different aspects of the sounds that authors may fail to mention. Our data is freely available, and can be used to ground semantic models, improve search in audio databases, and to study the language of sound.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1798657328,"Goal":"Sustainable Cities and Communities","Task":["open - ended crowd - labeling task","crowd - labeling","search","language of sound"],"Method":["semantic models"]},{"ID":"hettiarachchi-etal-2021-daai","title":"{DAAI} at {CASE} 2021 Task 1: Transformer-based Multilingual Socio-political and Crisis Event Detection","abstract":"Automatic socio-political and crisis event detection has been a challenge for natural language processing as well as social and political science communities, due to the diversity and nuance in such events and high accuracy requirements. In this paper, we propose an approach which can handle both document and cross-sentence level event detection in a multilingual setting using pretrained transformer models. Our approach became the winning solution in document level predictions and secured the 3rd place in cross-sentence level predictions for the English language. We could also achieve competitive results for other languages to prove the effectiveness and universality of our approach.","year":2021,"title_abstract":"{DAAI} at {CASE} 2021 Task 1: Transformer-based Multilingual Socio-political and Crisis Event Detection Automatic socio-political and crisis event detection has been a challenge for natural language processing as well as social and political science communities, due to the diversity and nuance in such events and high accuracy requirements. In this paper, we propose an approach which can handle both document and cross-sentence level event detection in a multilingual setting using pretrained transformer models. Our approach became the winning solution in document level predictions and secured the 3rd place in cross-sentence level predictions for the English language. We could also achieve competitive results for other languages to prove the effectiveness and universality of our approach.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1797938943,"Goal":"Climate Action","Task":["Transformer - based Multilingual Socio - political and Crisis Event Detection","Automatic socio - political and crisis event detection","natural language processing","social and political science communities","document and cross - sentence level event detection","multilingual setting","document level predictions","cross - sentence level predictions"],"Method":["pretrained transformer models"]},{"ID":"muti-barron-cedeno-2022-checkpoint","title":"A Checkpoint on Multilingual Misogyny Identification","abstract":"We address the problem of identifying misogyny in tweets in mono and multilingual settings in three languages: English, Italian, and Spanish. We explore model variations considering single and multiple languages both in the pre-training of the transformer and in the training of the downstream taskto explore the feasibility of detecting misogyny through a transfer learning approach across multiple languages. That is, we train monolingual transformers with monolingual data, and multilingual transformers with both monolingual and multilingual data.Our models reach state-of-the-art performance on all three languages. The single-language BERT models perform the best, closely followed by different configurations of multilingual BERT models. The performance drops in zero-shot classification across languages. Our error analysis shows that multilingual and monolingual models tend to make the same mistakes.","year":2022,"title_abstract":"A Checkpoint on Multilingual Misogyny Identification We address the problem of identifying misogyny in tweets in mono and multilingual settings in three languages: English, Italian, and Spanish. We explore model variations considering single and multiple languages both in the pre-training of the transformer and in the training of the downstream taskto explore the feasibility of detecting misogyny through a transfer learning approach across multiple languages. That is, we train monolingual transformers with monolingual data, and multilingual transformers with both monolingual and multilingual data.Our models reach state-of-the-art performance on all three languages. The single-language BERT models perform the best, closely followed by different configurations of multilingual BERT models. The performance drops in zero-shot classification across languages. Our error analysis shows that multilingual and monolingual models tend to make the same mistakes.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1796895266,"Goal":"Gender Equality","Task":["Multilingual Misogyny Identification","identifying misogyny in tweets","pre - training","transformer","downstream taskto","detecting misogyny","zero - shot classification"],"Method":["transfer learning approach","monolingual transformers","multilingual transformers","single - language BERT models","multilingual BERT models","error analysis","multilingual and monolingual models"]},{"ID":"cui-etal-2020-njus","title":"{NJU}{'}s submission to the {WMT}20 {QE} Shared Task","abstract":"This paper describes our system of the sentence-level and word-level Quality Estimation Shared Task of WMT20. Our system is based on the QE Brain, and we simply enhance it by injecting noise at the target side. And to obtain the deep bi-directional information, we use a masked language model at the target side instead of two single directional decoders. Meanwhile, we try to use the extra QE data from the WMT17 and WMT19 to improve our system{'}s performance. Finally, we ensemble the features or the results from different models to get our best results. Our system finished fifth in the end at sentence-level on both EN-ZH and EN-DE language pairs.","year":2020,"title_abstract":"{NJU}{'}s submission to the {WMT}20 {QE} Shared Task This paper describes our system of the sentence-level and word-level Quality Estimation Shared Task of WMT20. Our system is based on the QE Brain, and we simply enhance it by injecting noise at the target side. And to obtain the deep bi-directional information, we use a masked language model at the target side instead of two single directional decoders. Meanwhile, we try to use the extra QE data from the WMT17 and WMT19 to improve our system{'}s performance. Finally, we ensemble the features or the results from different models to get our best results. Our system finished fifth in the end at sentence-level on both EN-ZH and EN-DE language pairs.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1795969903,"Goal":"Quality Education","Task":["sentence - level","word - level Quality Estimation Shared Task"],"Method":["WMT20","masked language model","directional decoders","WMT17","WMT19"]},{"ID":"fortuna-etal-2021-cartography","title":"Cartography of Natural Language Processing for Social Good ({NLP}4{SG}): Searching for Definitions, Statistics and White Spots","abstract":"The range of works that can be considered as developing NLP for social good (NLP4SG) is enormous. While many of them target the identification of hate speech or fake news, there are others that address, e.g., text simplification to alleviate consequences of dyslexia, or coaching strategies to fight depression. However, so far, there is no clear picture of what areas are targeted by NLP4SG, who are the actors, which are the main scenarios and what are the topics that have been left aside. In order to obtain a clearer view in this respect, we first propose a working definition of NLP4SG and identify some primary aspects that are crucial for NLP4SG, including, e.g., areas, ethics, privacy and bias. Then, we draw upon a corpus of around 50,000 articles downloaded from the ACL Anthology. Based on a list of keywords retrieved from the literature and revised in view of the task, we select from this corpus articles that can be considered to be on NLP4SG according to our definition and analyze them in terms of trends along the time line, etc. The result is a map of the current NLP4SG research and insights concerning the white spots on this map.","year":2021,"title_abstract":"Cartography of Natural Language Processing for Social Good ({NLP}4{SG}): Searching for Definitions, Statistics and White Spots The range of works that can be considered as developing NLP for social good (NLP4SG) is enormous. While many of them target the identification of hate speech or fake news, there are others that address, e.g., text simplification to alleviate consequences of dyslexia, or coaching strategies to fight depression. However, so far, there is no clear picture of what areas are targeted by NLP4SG, who are the actors, which are the main scenarios and what are the topics that have been left aside. In order to obtain a clearer view in this respect, we first propose a working definition of NLP4SG and identify some primary aspects that are crucial for NLP4SG, including, e.g., areas, ethics, privacy and bias. Then, we draw upon a corpus of around 50,000 articles downloaded from the ACL Anthology. Based on a list of keywords retrieved from the literature and revised in view of the task, we select from this corpus articles that can be considered to be on NLP4SG according to our definition and analyze them in terms of trends along the time line, etc. The result is a map of the current NLP4SG research and insights concerning the white spots on this map.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1795900762,"Goal":"Reduced Inequalities","Task":["Cartography of Natural Language Processing","Social Good","social good","identification of hate speech","text simplification","NLP4SG"],"Method":["NLP","coaching strategies","NLP4SG","NLP4SG","NLP4SG"]},{"ID":"passonneau-2006-measuring","title":"Measuring Agreement on Set-valued Items ({MASI}) for Semantic and Pragmatic Annotation","abstract":"Annotation projects dealing with complex semantic or pragmatic phenomena face the dilemma of creating annotation schemes that oversimplify the phenomena, or that capture distinctions conventional reliability metrics cannot measure adequately. The solution to the dilemma is to develop metrics that quantify the decisions that annotators are asked to make. This paper discusses MASI, distance metric for comparing sets, and illustrates its use in quantifying the reliability of a specific dataset. Annotations of Summary Content Units (SCUs) generate models referred to as pyramids which can be used to evaluate unseen human summaries or machine summaries. The paper presents reliability results for five pairs of pyramids created for document sets from the 2003 Document Understanding Conference (DUC). The annotators worked independently of each other. Differences between application of MASI to pyramid annotation and its previous application to co-reference annotation are discussed. In addition, it is argued that a paradigmatic reliability study should relate measures of inter-annotator agreement to independent assessments, such as significance tests of the annotated variables with respect to other phenomena. In effect, what counts as sufficiently reliable intera-annotator agreement depends on the use the annotated data will be put to.","year":2006,"title_abstract":"Measuring Agreement on Set-valued Items ({MASI}) for Semantic and Pragmatic Annotation Annotation projects dealing with complex semantic or pragmatic phenomena face the dilemma of creating annotation schemes that oversimplify the phenomena, or that capture distinctions conventional reliability metrics cannot measure adequately. The solution to the dilemma is to develop metrics that quantify the decisions that annotators are asked to make. This paper discusses MASI, distance metric for comparing sets, and illustrates its use in quantifying the reliability of a specific dataset. Annotations of Summary Content Units (SCUs) generate models referred to as pyramids which can be used to evaluate unseen human summaries or machine summaries. The paper presents reliability results for five pairs of pyramids created for document sets from the 2003 Document Understanding Conference (DUC). The annotators worked independently of each other. Differences between application of MASI to pyramid annotation and its previous application to co-reference annotation are discussed. In addition, it is argued that a paradigmatic reliability study should relate measures of inter-annotator agreement to independent assessments, such as significance tests of the annotated variables with respect to other phenomena. In effect, what counts as sufficiently reliable intera-annotator agreement depends on the use the annotated data will be put to.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1795472801,"Goal":"Partnership for the Goals","Task":["Measuring Agreement","Semantic and Pragmatic Annotation Annotation projects","comparing sets","Document Understanding Conference","pyramid annotation","co - reference annotation"],"Method":["annotation schemes","MASI","MASI","significance tests"]},{"ID":"ngo-ho-yvon-2021-optimizing","title":"Optimizing Word Alignments with Better Subword Tokenization","abstract":"Word alignment identify translational correspondences between words in a parallel sentence pair and are used and for example and to train statistical machine translation and learn bilingual dictionaries or to perform quality estimation. Subword tokenization has become a standard preprocessing step for a large number of applications and notably for state-of-the-art open vocabulary machine translation systems. In this paper and we thoroughly study how this preprocessing step interacts with the word alignment task and propose several tokenization strategies to obtain well-segmented parallel corpora. Using these new techniques and we were able to improve baseline word-based alignment models for six language pairs.","year":2021,"title_abstract":"Optimizing Word Alignments with Better Subword Tokenization Word alignment identify translational correspondences between words in a parallel sentence pair and are used and for example and to train statistical machine translation and learn bilingual dictionaries or to perform quality estimation. Subword tokenization has become a standard preprocessing step for a large number of applications and notably for state-of-the-art open vocabulary machine translation systems. In this paper and we thoroughly study how this preprocessing step interacts with the word alignment task and propose several tokenization strategies to obtain well-segmented parallel corpora. Using these new techniques and we were able to improve baseline word-based alignment models for six language pairs.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1793908775,"Goal":"Gender Equality","Task":["Optimizing Word Alignments","Word alignment","statistical machine translation","bilingual dictionaries","quality estimation","Subword tokenization","machine translation systems","word alignment task"],"Method":["preprocessing step","preprocessing step","tokenization strategies","word - based alignment models"]},{"ID":"gira-etal-2022-debiasing","title":"Debiasing Pre-Trained Language Models via Efficient Fine-Tuning","abstract":"An explosion in the popularity of transformer-based language models (such as GPT-3, BERT, RoBERTa, and ALBERT) has opened the doors to new machine learning applications involving language modeling, text generation, and more. However, recent scrutiny reveals that these language models contain inherent biases towards certain demographics reflected in their training data. While research has tried mitigating this problem, existing approaches either fail to remove the bias completely, degrade performance ({``}catastrophic forgetting{''}), or are costly to execute. This work examines how to reduce gender bias in a GPT-2 language model by fine-tuning less than 1{\\%} of its parameters. Through quantitative benchmarks, we show that this is a viable way to reduce prejudice in pre-trained language models while remaining cost-effective at scale.","year":2022,"title_abstract":"Debiasing Pre-Trained Language Models via Efficient Fine-Tuning An explosion in the popularity of transformer-based language models (such as GPT-3, BERT, RoBERTa, and ALBERT) has opened the doors to new machine learning applications involving language modeling, text generation, and more. However, recent scrutiny reveals that these language models contain inherent biases towards certain demographics reflected in their training data. While research has tried mitigating this problem, existing approaches either fail to remove the bias completely, degrade performance ({``}catastrophic forgetting{''}), or are costly to execute. This work examines how to reduce gender bias in a GPT-2 language model by fine-tuning less than 1{\\%} of its parameters. Through quantitative benchmarks, we show that this is a viable way to reduce prejudice in pre-trained language models while remaining cost-effective at scale.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1793705672,"Goal":"Gender Equality","Task":["machine learning applications","language modeling","text generation"],"Method":["Debiasing Pre - Trained Language Models","Fine - Tuning","transformer - based language models","GPT - 3","BERT","ALBERT)","language models","GPT - 2 language model","language models"]},{"ID":"alharbi-lee-2021-kawarith","title":"Kawarith: an {A}rabic {T}witter Corpus for Crisis Events","abstract":"Social media (SM) platforms such as Twitter provide large quantities of real-time data that can be leveraged during mass emergencies. Developing tools to support crisis-affected communities requires available datasets, which often do not exist for low resource languages. This paper introduces Kawarith a multi-dialect Arabic Twitter corpus for crisis events, comprising more than a million Arabic tweets collected during 22 crises that occurred between 2018 and 2020 and involved several types of hazard. Exploration of this content revealed the most discussed topics and information types, and the paper presents a labelled dataset from seven emergency events that serves as a gold standard for several tasks in crisis informatics research. Using annotated data from the same event, a BERT model is fine-tuned to classify tweets into different categories in the multi- label setting. Results show that BERT-based models yield good performance on this task even with small amounts of task-specific training data.","year":2021,"title_abstract":"Kawarith: an {A}rabic {T}witter Corpus for Crisis Events Social media (SM) platforms such as Twitter provide large quantities of real-time data that can be leveraged during mass emergencies. Developing tools to support crisis-affected communities requires available datasets, which often do not exist for low resource languages. This paper introduces Kawarith a multi-dialect Arabic Twitter corpus for crisis events, comprising more than a million Arabic tweets collected during 22 crises that occurred between 2018 and 2020 and involved several types of hazard. Exploration of this content revealed the most discussed topics and information types, and the paper presents a labelled dataset from seven emergency events that serves as a gold standard for several tasks in crisis informatics research. Using annotated data from the same event, a BERT model is fine-tuned to classify tweets into different categories in the multi- label setting. Results show that BERT-based models yield good performance on this task even with small amounts of task-specific training data.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1793072522,"Goal":"Climate Action","Task":["crisis - affected communities","crisis informatics research","multi - label setting"],"Method":["Kawarith","Kawarith","BERT","BERT"]},{"ID":"ramiandrisoa-mothe-2020-irit","title":"{IRIT} at {TRAC} 2020","abstract":"This paper describes the participation of the IRIT team in the TRAC (Trolling, Aggression and Cyberbullying) 2020 shared task (Bhattacharya et al., 2020) on Aggression Identification and more precisely to the shared task in English language. The shared task was further divided into two sub-tasks: (a) aggression identification and (b) misogynistic aggression identification. We proposed to use the transformer based language model BERT (Bidirectional Encoder Representation from Transformer) for the two sub-tasks. Our team was qualified as twelfth out of sixteen participants on sub-task (a) and eleventh out of fifteen participants on sub-task (b).","year":2020,"title_abstract":"{IRIT} at {TRAC} 2020 This paper describes the participation of the IRIT team in the TRAC (Trolling, Aggression and Cyberbullying) 2020 shared task (Bhattacharya et al., 2020) on Aggression Identification and more precisely to the shared task in English language. The shared task was further divided into two sub-tasks: (a) aggression identification and (b) misogynistic aggression identification. We proposed to use the transformer based language model BERT (Bidirectional Encoder Representation from Transformer) for the two sub-tasks. Our team was qualified as twelfth out of sixteen participants on sub-task (a) and eleventh out of fifteen participants on sub-task (b).","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1791800559,"Goal":"Gender Equality","Task":["IRIT","Aggression","Cyberbullying) 2020 shared task","Aggression Identification","shared task","shared task","aggression identification","misogynistic aggression identification","sub - task"],"Method":["transformer based language model","Encoder Representation","Transformer)"]},{"ID":"mboning-tchiaze-etal-2020-building","title":"Building Collaboration-based Resources in Endowed {A}frican Languages: Case of {NT}e{AL}an Dictionaries Platform","abstract":"In a context where open-source NLP resources and tools in African languages are scarce and dispersed, it is difficult for researchers to truly fit African languages into current algorithms of artificial intelligence. Created in 2017, with the aim of building communities of voluntary contributors around African native and\/or national languages, cultures, NLP technologies and artificial intelligence, the NTeALan association has set up a series of web collaborative platforms intended to allow the aforementioned communities to create and administer their own lexicographic resources. In this article, we present on the one hand the first versions of the three platforms: the REST API for saving lexicographical resources, the dictionary management platform and the collaborative dictionary platform; on the other hand, we describe the data format chosen and used to encapsulate our resources. After experimenting with a few dictionaries and some users feedback, we are convinced that only collaboration-based approach and platforms can effectively respond to the production of good resources in African native and\/or national languages.","year":2020,"title_abstract":"Building Collaboration-based Resources in Endowed {A}frican Languages: Case of {NT}e{AL}an Dictionaries Platform In a context where open-source NLP resources and tools in African languages are scarce and dispersed, it is difficult for researchers to truly fit African languages into current algorithms of artificial intelligence. Created in 2017, with the aim of building communities of voluntary contributors around African native and\/or national languages, cultures, NLP technologies and artificial intelligence, the NTeALan association has set up a series of web collaborative platforms intended to allow the aforementioned communities to create and administer their own lexicographic resources. In this article, we present on the one hand the first versions of the three platforms: the REST API for saving lexicographical resources, the dictionary management platform and the collaborative dictionary platform; on the other hand, we describe the data format chosen and used to encapsulate our resources. After experimenting with a few dictionaries and some users feedback, we are convinced that only collaboration-based approach and platforms can effectively respond to the production of good resources in African native and\/or national languages.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1791689694,"Goal":"Partnership for the Goals","Task":["Collaboration - based Resources","artificial intelligence","artificial intelligence","saving lexicographical resources"],"Method":["NLP technologies","NTeALan association","web collaborative platforms","REST API","dictionary management platform","collaborative dictionary platform;","collaboration - based approach"]},{"ID":"chen-etal-2020-accurate","title":"Accurate Word Alignment Induction from Neural Machine Translation","abstract":"Despite its original goal to jointly learn to align and translate, prior researches suggest that Transformer captures poor word alignments through its attention mechanism. In this paper, we show that attention weights do capture accurate word alignments and propose two novel word alignment induction methods Shift-Att and Shift-AET. The main idea is to induce alignments at the step when the to-be-aligned target token is the decoder input rather than the decoder output as in previous work. Shift-Att is an interpretation method that induces alignments from the attention weights of Transformer and does not require parameter update or architecture change. Shift-AET extracts alignments from an additional alignment module which is tightly integrated into Transformer and trained in isolation with supervision from symmetrized Shift-Att alignments. Experiments on three publicly available datasets demonstrate that both methods perform better than their corresponding neural baselines and Shift-AET significantly outperforms GIZA++ by 1.4-4.8 AER points.","year":2020,"title_abstract":"Accurate Word Alignment Induction from Neural Machine Translation Despite its original goal to jointly learn to align and translate, prior researches suggest that Transformer captures poor word alignments through its attention mechanism. In this paper, we show that attention weights do capture accurate word alignments and propose two novel word alignment induction methods Shift-Att and Shift-AET. The main idea is to induce alignments at the step when the to-be-aligned target token is the decoder input rather than the decoder output as in previous work. Shift-Att is an interpretation method that induces alignments from the attention weights of Transformer and does not require parameter update or architecture change. Shift-AET extracts alignments from an additional alignment module which is tightly integrated into Transformer and trained in isolation with supervision from symmetrized Shift-Att alignments. Experiments on three publicly available datasets demonstrate that both methods perform better than their corresponding neural baselines and Shift-AET significantly outperforms GIZA++ by 1.4-4.8 AER points.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1791563332,"Goal":"Gender Equality","Task":["Word Alignment Induction","Neural Machine Translation","translate"],"Method":["Transformer","attention mechanism","word alignment induction methods","Shift - Att","Shift - AET","Shift - Att","interpretation method","Transformer","Shift - AET","alignment module","Transformer","symmetrized Shift - Att alignments","neural baselines","Shift - AET","GIZA++"]},{"ID":"marinelli-etal-2008-encoding","title":"Encoding Terms from a Scientific Domain in a Terminological Database: Methodology and Criteria","abstract":"This paper reports on the main phases of a research which aims at enhancing a maritime terminological database by means of a set of terms belonging to meteorology. The structure of the terminological database, according to EuroWordNet\/ItalWordNet model is described; the criteria used to build corpora of specialized texts are explained as well as the use of the corpora as source for term selection and extraction. The contribution of the semantic databases is taken into account: on the one hand, the most recent version of the Princeton WordNet has been exploited as reference for comparing and evaluating synsets; on the other hand, the Italian WordNet has been employed as source for exporting synsets to be coded in the terminological resource. The set of semantic relations useful to codify new terms belonging to the discipline of meteorology is examined, revising the semantic relations provided by the IWN model, introducing new relations which are more suitably tailored to specific requirements either scientific or pragmatic. The need for a particular relation is highlighted to represent the mental association which is made when a term intuitively recalls another term, but they are neither synonyms nor connected by means of a hyperonymy\/hyponymy relation.","year":2008,"title_abstract":"Encoding Terms from a Scientific Domain in a Terminological Database: Methodology and Criteria This paper reports on the main phases of a research which aims at enhancing a maritime terminological database by means of a set of terms belonging to meteorology. The structure of the terminological database, according to EuroWordNet\/ItalWordNet model is described; the criteria used to build corpora of specialized texts are explained as well as the use of the corpora as source for term selection and extraction. The contribution of the semantic databases is taken into account: on the one hand, the most recent version of the Princeton WordNet has been exploited as reference for comparing and evaluating synsets; on the other hand, the Italian WordNet has been employed as source for exporting synsets to be coded in the terminological resource. The set of semantic relations useful to codify new terms belonging to the discipline of meteorology is examined, revising the semantic relations provided by the IWN model, introducing new relations which are more suitably tailored to specific requirements either scientific or pragmatic. The need for a particular relation is highlighted to represent the mental association which is made when a term intuitively recalls another term, but they are neither synonyms nor connected by means of a hyperonymy\/hyponymy relation.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.1790278405,"Goal":"Life Below Water","Task":["Encoding Terms","maritime terminological database","term selection and extraction","evaluating synsets;","exporting synsets","meteorology"],"Method":["EuroWordNet\/ItalWordNet model","IWN model","hyperonymy\/hyponymy relation"]},{"ID":"mccrae-etal-2020-english","title":"{E}nglish {W}ord{N}et 2020: Improving and Extending a {W}ord{N}et for {E}nglish using an Open-Source Methodology","abstract":"WordNet, while one of the most widely used resources for NLP, has not been updated for a long time, and as such a new project English WordNet has arisen to continue the development of the model under an open-source paradigm. In this paper, we detail the second release of this resource entitled {``}English WordNet 2020{''}. The work has focused firstly, on the introduction of new synsets and senses and developing guidelines for this and secondly, on the integration of contributions from other projects. We present the changes in this edition, which total over 15,000 changes over the previous release.","year":2020,"title_abstract":"{E}nglish {W}ord{N}et 2020: Improving and Extending a {W}ord{N}et for {E}nglish using an Open-Source Methodology WordNet, while one of the most widely used resources for NLP, has not been updated for a long time, and as such a new project English WordNet has arisen to continue the development of the model under an open-source paradigm. In this paper, we detail the second release of this resource entitled {``}English WordNet 2020{''}. The work has focused firstly, on the introduction of new synsets and senses and developing guidelines for this and secondly, on the integration of contributions from other projects. We present the changes in this edition, which total over 15,000 changes over the previous release.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1790105104,"Goal":"Partnership for the Goals","Task":["NLP"],"Method":["WordNet"]},{"ID":"kohn-etal-2020-generating","title":"Generating Instructions at Different Levels of Abstraction","abstract":"When generating technical instructions, it is often convenient to describe complex objects in the world at different levels of abstraction. A novice user might need an object explained piece by piece, while for an expert, talking about the complex object (e.g. a wall or railing) directly may be more succinct and efficient. We show how to generate building instructions at different levels of abstraction in Minecraft. We introduce the use of hierarchical planning to this end, a method from AI planning which can capture the structure of complex objects neatly. A crowdsourcing evaluation shows that the choice of abstraction level matters to users, and that an abstraction strategy which balances low-level and high-level object descriptions compares favorably to ones which don{'}t.","year":2020,"title_abstract":"Generating Instructions at Different Levels of Abstraction When generating technical instructions, it is often convenient to describe complex objects in the world at different levels of abstraction. A novice user might need an object explained piece by piece, while for an expert, talking about the complex object (e.g. a wall or railing) directly may be more succinct and efficient. We show how to generate building instructions at different levels of abstraction in Minecraft. We introduce the use of hierarchical planning to this end, a method from AI planning which can capture the structure of complex objects neatly. A crowdsourcing evaluation shows that the choice of abstraction level matters to users, and that an abstraction strategy which balances low-level and high-level object descriptions compares favorably to ones which don{'}t.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1789552867,"Goal":"Sustainable Cities and Communities","Task":["Generating Instructions","technical instructions","building instructions","Minecraft","AI planning","crowdsourcing evaluation"],"Method":["hierarchical planning","abstraction strategy"]},{"ID":"goeuriot-etal-2016-building","title":"Building Evaluation Datasets for Consumer-Oriented Information Retrieval","abstract":"Common people often experience difficulties in accessing relevant, correct, accurate and understandable health information online. Developing search techniques that aid these information needs is challenging. In this paper we present the datasets created by CLEF eHealth Lab from 2013-2015 for evaluation of search solutions to support common people finding health information online. Specifically, the CLEF eHealth information retrieval (IR) task of this Lab has provided the research community with benchmarks for evaluating consumer-centered health information retrieval, thus fostering research and development aimed to address this challenging problem. Given consumer queries, the goal of the task is to retrieve relevant documents from the provided collection of web pages. The shared datasets provide a large health web crawl, queries representing people{'}s real world information needs, and relevance assessment judgements for the queries.","year":2016,"title_abstract":"Building Evaluation Datasets for Consumer-Oriented Information Retrieval Common people often experience difficulties in accessing relevant, correct, accurate and understandable health information online. Developing search techniques that aid these information needs is challenging. In this paper we present the datasets created by CLEF eHealth Lab from 2013-2015 for evaluation of search solutions to support common people finding health information online. Specifically, the CLEF eHealth information retrieval (IR) task of this Lab has provided the research community with benchmarks for evaluating consumer-centered health information retrieval, thus fostering research and development aimed to address this challenging problem. Given consumer queries, the goal of the task is to retrieve relevant documents from the provided collection of web pages. The shared datasets provide a large health web crawl, queries representing people{'}s real world information needs, and relevance assessment judgements for the queries.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.1789072752,"Goal":"Good Health and Well-Being","Task":["Consumer - Oriented Information Retrieval","common people finding health information","consumer - centered health information retrieval"],"Method":["search techniques","search solutions"]},{"ID":"alex-etal-2016-homing","title":"Homing in on {T}witter Users: Evaluating an Enhanced Geoparser for User Profile Locations","abstract":"Twitter-related studies often need to geo-locate Tweets or Twitter users, identifying their real-world geographic locations. As tweet-level geotagging remains rare, most prior work exploited tweet content, timezone and network information to inform geolocation, or else relied on off-the-shelf tools to geolocate users from location information in their user profiles. However, such user location metadata is not consistently structured, causing such tools to fail regularly, especially if a string contains multiple locations, or if locations are very fine-grained. We argue that user profile location (UPL) and tweet location need to be treated as distinct types of information from which differing inferences can be drawn. Here, we apply geoparsing to UPLs, and demonstrate how task performance can be improved by adapting our Edinburgh Geoparser, which was originally developed for processing English text. We present a detailed evaluation method and results, including inter-coder agreement. We demonstrate that the optimised geoparser can effectively extract and geo-reference multiple locations at different levels of granularity with an F1-score of around 0.90. We also illustrate how geoparsed UPLs can be exploited for international information trade studies and country-level sentiment analysis.","year":2016,"title_abstract":"Homing in on {T}witter Users: Evaluating an Enhanced Geoparser for User Profile Locations Twitter-related studies often need to geo-locate Tweets or Twitter users, identifying their real-world geographic locations. As tweet-level geotagging remains rare, most prior work exploited tweet content, timezone and network information to inform geolocation, or else relied on off-the-shelf tools to geolocate users from location information in their user profiles. However, such user location metadata is not consistently structured, causing such tools to fail regularly, especially if a string contains multiple locations, or if locations are very fine-grained. We argue that user profile location (UPL) and tweet location need to be treated as distinct types of information from which differing inferences can be drawn. Here, we apply geoparsing to UPLs, and demonstrate how task performance can be improved by adapting our Edinburgh Geoparser, which was originally developed for processing English text. We present a detailed evaluation method and results, including inter-coder agreement. We demonstrate that the optimised geoparser can effectively extract and geo-reference multiple locations at different levels of granularity with an F1-score of around 0.90. We also illustrate how geoparsed UPLs can be exploited for international information trade studies and country-level sentiment analysis.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1788994968,"Goal":"Sustainable Cities and Communities","Task":["User Profile Locations","Twitter - related studies","geotagging","international information trade studies","country - level sentiment analysis"],"Method":["Geoparser","geoparsing","UPLs","Edinburgh Geoparser","geoparser","geoparsed UPLs"]},{"ID":"hurriyetoglu-etal-2021-multilingual","title":"Multilingual Protest News Detection - Shared Task 1, {CASE} 2021","abstract":"Benchmarking state-of-the-art text classification and information extraction systems in multilingual, cross-lingual, few-shot, and zero-shot settings for socio-political event information collection is achieved in the scope of the shared task Socio-political and Crisis Events Detection at the workshop CASE @ ACL-IJCNLP 2021. Socio-political event data is utilized for national and international policy- and decision-making. Therefore, the reliability and validity of these datasets are of the utmost importance. We split the shared task into three parts to address the three aspects of data collection (Task 1), fine-grained semantic classification (Task 2), and evaluation (Task 3). Task 1, which is the focus of this report, is on multilingual protest news detection and comprises four subtasks that are document classification (subtask 1), sentence classification (subtask 2), event sentence coreference identification (subtask 3), and event extraction (subtask 4). All subtasks had English, Portuguese, and Spanish for both training and evaluation data. Data in Hindi language was available only for the evaluation of subtask 1. The majority of the submissions, which are 238 in total, are created using multi- and cross-lingual approaches. Best scores are above 77.27 F1-macro for subtask 1, above 85.32 F1-macro for subtask 2, above 84.23 CoNLL 2012 average score for subtask 3, and above 66.20 F1-macro for subtask 4 in all evaluation settings. The performance of the best system for subtask 4 is above 66.20 F1 for all available languages. Although there is still a significant room for improvement in cross-lingual and zero-shot settings, the best submissions for each evaluation scenario yield remarkable results. Monolingual models outperformed the multilingual models in a few evaluation scenarios.","year":2021,"title_abstract":"Multilingual Protest News Detection - Shared Task 1, {CASE} 2021 Benchmarking state-of-the-art text classification and information extraction systems in multilingual, cross-lingual, few-shot, and zero-shot settings for socio-political event information collection is achieved in the scope of the shared task Socio-political and Crisis Events Detection at the workshop CASE @ ACL-IJCNLP 2021. Socio-political event data is utilized for national and international policy- and decision-making. Therefore, the reliability and validity of these datasets are of the utmost importance. We split the shared task into three parts to address the three aspects of data collection (Task 1), fine-grained semantic classification (Task 2), and evaluation (Task 3). Task 1, which is the focus of this report, is on multilingual protest news detection and comprises four subtasks that are document classification (subtask 1), sentence classification (subtask 2), event sentence coreference identification (subtask 3), and event extraction (subtask 4). All subtasks had English, Portuguese, and Spanish for both training and evaluation data. Data in Hindi language was available only for the evaluation of subtask 1. The majority of the submissions, which are 238 in total, are created using multi- and cross-lingual approaches. Best scores are above 77.27 F1-macro for subtask 1, above 85.32 F1-macro for subtask 2, above 84.23 CoNLL 2012 average score for subtask 3, and above 66.20 F1-macro for subtask 4 in all evaluation settings. The performance of the best system for subtask 4 is above 66.20 F1 for all available languages. Although there is still a significant room for improvement in cross-lingual and zero-shot settings, the best submissions for each evaluation scenario yield remarkable results. Monolingual models outperformed the multilingual models in a few evaluation scenarios.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1787641943,"Goal":"Climate Action","Task":["Multilingual Protest News Detection - Shared Task","multilingual , cross - lingual","socio - political event information collection","Socio - political and Crisis Events Detection","national and international policy - and decision - making","data collection","fine - grained semantic classification","evaluation","multilingual protest news detection","document classification","sentence classification","event sentence coreference identification","event extraction","cross - lingual"],"Method":["text classification and information extraction systems","multi - and cross - lingual approaches","Monolingual models","multilingual models"]},{"ID":"caselli-etal-2016-nlp","title":"{NLP} and Public Engagement: The Case of the {I}talian School Reform","abstract":"In this paper we present PIERINO (PIattaforma per l{'}Estrazione e il Recupero di INformazione Online), a system that was implemented in collaboration with the Italian Ministry of Education, University and Research to analyse the citizens{'} comments given in {\\#}labuonascuola survey. The platform includes various levels of automatic analysis such as key-concept extraction and word co-occurrences. Each analysis is displayed through an intuitive view using different types of visualizations, for example radar charts and sunburst. PIERINO was effectively used to support shaping the last Italian school reform, proving the potential of NLP in the context of policy making.","year":2016,"title_abstract":"{NLP} and Public Engagement: The Case of the {I}talian School Reform In this paper we present PIERINO (PIattaforma per l{'}Estrazione e il Recupero di INformazione Online), a system that was implemented in collaboration with the Italian Ministry of Education, University and Research to analyse the citizens{'} comments given in {\\#}labuonascuola survey. The platform includes various levels of automatic analysis such as key-concept extraction and word co-occurrences. Each analysis is displayed through an intuitive view using different types of visualizations, for example radar charts and sunburst. PIERINO was effectively used to support shaping the last Italian school reform, proving the potential of NLP in the context of policy making.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1785938889,"Goal":"Sustainable Cities and Communities","Task":["Public Engagement","{I}talian School Reform","automatic analysis","key - concept extraction","word co - occurrences","policy making"],"Method":["PIERINO","visualizations","PIERINO","NLP"]},{"ID":"ramirez-orta-milios-2021-unsupervised","title":"Unsupervised document summarization using pre-trained sentence embeddings and graph centrality","abstract":"This paper describes our submission for the LongSumm task in SDP 2021. We propose a method for incorporating sentence embeddings produced by deep language models into extractive summarization techniques based on graph centrality in an unsupervised manner.The proposed method is simple, fast, can summarize any kind of document of any size and can satisfy any length constraints for the summaries produced. The method offers competitive performance to more sophisticated supervised methods and can serve as a proxy for abstractive summarization techniques","year":2021,"title_abstract":"Unsupervised document summarization using pre-trained sentence embeddings and graph centrality This paper describes our submission for the LongSumm task in SDP 2021. We propose a method for incorporating sentence embeddings produced by deep language models into extractive summarization techniques based on graph centrality in an unsupervised manner.The proposed method is simple, fast, can summarize any kind of document of any size and can satisfy any length constraints for the summaries produced. The method offers competitive performance to more sophisticated supervised methods and can serve as a proxy for abstractive summarization techniques","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1785785854,"Goal":"Partnership for the Goals","Task":["Unsupervised document summarization","LongSumm task","SDP 2021","summarization","abstractive summarization"],"Method":["pre - trained sentence embeddings","graph centrality","deep language models","graph centrality","unsupervised manner","supervised methods"]},{"ID":"liu-etal-2020-gender","title":"Does Gender Matter? Towards Fairness in Dialogue Systems","abstract":"Recently there are increasing concerns about the fairness of Artificial Intelligence (AI) in real-world applications such as computer vision and recommendations. For example, recognition algorithms in computer vision are unfair to black people such as poorly detecting their faces and inappropriately identifying them as {``}gorillas{''}. As one crucial application of AI, dialogue systems have been extensively applied in our society. They are usually built with real human conversational data; thus they could inherit some fairness issues which are held in the real world. However, the fairness of dialogue systems has not been well investigated. In this paper, we perform a pioneering study about the fairness issues in dialogue systems. In particular, we construct a benchmark dataset and propose quantitative measures to understand fairness in dialogue models. Our studies demonstrate that popular dialogue models show significant prejudice towards different genders and races. Besides, to mitigate the bias in dialogue systems, we propose two simple but effective debiasing methods. Experiments show that our methods can reduce the bias in dialogue systems significantly. The dataset and the implementation are released to foster fairness research in dialogue systems.","year":2020,"title_abstract":"Does Gender Matter? Towards Fairness in Dialogue Systems Recently there are increasing concerns about the fairness of Artificial Intelligence (AI) in real-world applications such as computer vision and recommendations. For example, recognition algorithms in computer vision are unfair to black people such as poorly detecting their faces and inappropriately identifying them as {``}gorillas{''}. As one crucial application of AI, dialogue systems have been extensively applied in our society. They are usually built with real human conversational data; thus they could inherit some fairness issues which are held in the real world. However, the fairness of dialogue systems has not been well investigated. In this paper, we perform a pioneering study about the fairness issues in dialogue systems. In particular, we construct a benchmark dataset and propose quantitative measures to understand fairness in dialogue models. Our studies demonstrate that popular dialogue models show significant prejudice towards different genders and races. Besides, to mitigate the bias in dialogue systems, we propose two simple but effective debiasing methods. Experiments show that our methods can reduce the bias in dialogue systems significantly. The dataset and the implementation are released to foster fairness research in dialogue systems.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1785497665,"Goal":"Gender Equality","Task":["Dialogue Systems","Artificial Intelligence","real - world applications","computer vision","recommendations","computer vision","AI","dialogue systems","fairness issues","dialogue systems","dialogue systems","dialogue systems","fairness research","dialogue systems"],"Method":["recognition algorithms","dialogue systems","dialogue models","dialogue models","debiasing methods"]},{"ID":"shterionov-2021-early","title":"Early-stage development of the {S}ign{ON} application and open framework {--} challenges and opportunities","abstract":"SignON is an EU Horizon 2020 Research and Innovation project, that is developing a smartphone application and an open framework to facilitate translation between different European sign, spoken and text languages. The framework will incorporate state of the art sign language recognition and presentation, speech processing technologies and, in its core, multi-modal, cross-language machine translation. The framework, dedicated to the computationally heavy tasks and distributed on the cloud powers the application {--} a lightweight app running on a standard mobile device. The application and framework are being researched, designed and developed through a co-creation user-centric approach with the European deaf and hard of hearing communities. In this session, the speakers will detail their progress, challenges and lessons learned in the early-stage development of the application and framework. They will also present their Agile DevOps approach and the next steps in the evolution of the SignON project.","year":2021,"title_abstract":"Early-stage development of the {S}ign{ON} application and open framework {--} challenges and opportunities SignON is an EU Horizon 2020 Research and Innovation project, that is developing a smartphone application and an open framework to facilitate translation between different European sign, spoken and text languages. The framework will incorporate state of the art sign language recognition and presentation, speech processing technologies and, in its core, multi-modal, cross-language machine translation. The framework, dedicated to the computationally heavy tasks and distributed on the cloud powers the application {--} a lightweight app running on a standard mobile device. The application and framework are being researched, designed and developed through a co-creation user-centric approach with the European deaf and hard of hearing communities. In this session, the speakers will detail their progress, challenges and lessons learned in the early-stage development of the application and framework. They will also present their Agile DevOps approach and the next steps in the evolution of the SignON project.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1782529503,"Goal":"Sustainable Cities and Communities","Task":["Research and Innovation project","translation","sign language recognition and presentation","multi - modal , cross - language machine translation","computationally heavy tasks","SignON project"],"Method":["SignON","smartphone application","open framework","speech processing technologies","lightweight app","co - creation user - centric approach"]},{"ID":"oostdijk-etal-2020-connection","title":"The Connection between the Text and Images of News Articles: New Insights for Multimedia Analysis","abstract":"We report on a case study of text and images that reveals the inadequacy of simplistic assumptions about their connection and interplay. The context of our work is a larger effort to create automatic systems that can extract event information from online news articles about flooding disasters. We carry out a manual analysis of 1000 articles containing a keyword related to flooding. The analysis reveals that the articles in our data set cluster into seven categories related to different topical aspects of flooding, and that the images accompanying the articles cluster into five categories related to the content they depict. The results demonstrate that flood-related news articles do not consistently report on a single, currently unfolding flooding event and we should also not assume that a flood-related image will directly relate to a flooding-event described in the corresponding article. In particular, spatiotemporal distance is important. We validate the manual analysis with an automatic classifier demonstrating the technical feasibility of multimedia analysis approaches that admit more realistic relationships between text and images. In sum, our case study confirms that closer attention to the connection between text and images has the potential to improve the collection of multimodal information from news articles.","year":2020,"title_abstract":"The Connection between the Text and Images of News Articles: New Insights for Multimedia Analysis We report on a case study of text and images that reveals the inadequacy of simplistic assumptions about their connection and interplay. The context of our work is a larger effort to create automatic systems that can extract event information from online news articles about flooding disasters. We carry out a manual analysis of 1000 articles containing a keyword related to flooding. The analysis reveals that the articles in our data set cluster into seven categories related to different topical aspects of flooding, and that the images accompanying the articles cluster into five categories related to the content they depict. The results demonstrate that flood-related news articles do not consistently report on a single, currently unfolding flooding event and we should also not assume that a flood-related image will directly relate to a flooding-event described in the corresponding article. In particular, spatiotemporal distance is important. We validate the manual analysis with an automatic classifier demonstrating the technical feasibility of multimedia analysis approaches that admit more realistic relationships between text and images. In sum, our case study confirms that closer attention to the connection between text and images has the potential to improve the collection of multimodal information from news articles.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1782437563,"Goal":"Sustainable Cities and Communities","Task":["Multimedia Analysis","manual analysis","multimodal information"],"Method":["automatic systems","manual analysis","automatic classifier","multimedia analysis approaches"]},{"ID":"phillips-etal-2017-intrinsic","title":"Intrinsic and Extrinsic Evaluation of Spatiotemporal Text Representations in {T}witter Streams","abstract":"Language in social media is a dynamic system, constantly evolving and adapting, with words and concepts rapidly emerging, disappearing, and changing their meaning. These changes can be estimated using word representations in context, over time and across locations. A number of methods have been proposed to track these spatiotemporal changes but no general method exists to evaluate the quality of these representations. Previous work largely focused on qualitative evaluation, which we improve by proposing a set of visualizations that highlight changes in text representation over both space and time. We demonstrate usefulness of novel spatiotemporal representations to explore and characterize specific aspects of the corpus of tweets collected from European countries over a two-week period centered around the terrorist attacks in Brussels in March 2016. In addition, we quantitatively evaluate spatiotemporal representations by feeding them into a downstream classification task {--} event type prediction. Thus, our work is the first to provide both intrinsic (qualitative) and extrinsic (quantitative) evaluation of text representations for spatiotemporal trends.","year":2017,"title_abstract":"Intrinsic and Extrinsic Evaluation of Spatiotemporal Text Representations in {T}witter Streams Language in social media is a dynamic system, constantly evolving and adapting, with words and concepts rapidly emerging, disappearing, and changing their meaning. These changes can be estimated using word representations in context, over time and across locations. A number of methods have been proposed to track these spatiotemporal changes but no general method exists to evaluate the quality of these representations. Previous work largely focused on qualitative evaluation, which we improve by proposing a set of visualizations that highlight changes in text representation over both space and time. We demonstrate usefulness of novel spatiotemporal representations to explore and characterize specific aspects of the corpus of tweets collected from European countries over a two-week period centered around the terrorist attacks in Brussels in March 2016. In addition, we quantitatively evaluate spatiotemporal representations by feeding them into a downstream classification task {--} event type prediction. Thus, our work is the first to provide both intrinsic (qualitative) and extrinsic (quantitative) evaluation of text representations for spatiotemporal trends.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1781305075,"Goal":"Sustainable Cities and Communities","Task":["Spatiotemporal Text Representations","qualitative evaluation","text representation","classification task","event type prediction","spatiotemporal trends"],"Method":["word representations","visualizations","spatiotemporal representations","spatiotemporal representations","text representations"]},{"ID":"molla-2017-macquarie","title":"{M}acquarie {U}niversity at {B}io{ASQ} 5b {--} Query-based Summarisation Techniques for Selecting the Ideal Answers","abstract":"Macquarie University{'}s contribution to the BioASQ challenge (Task 5b Phase B) focused on the use of query-based extractive summarisation techniques for the generation of the ideal answers. Four runs were submitted, with approaches ranging from a trivial system that selected the first $n$ snippets, to the use of deep learning approaches under a regression framework. Our experiments and the ROUGE results of the five test batches of BioASQ indicate surprisingly good results for the trivial approach. Overall, most of our runs on the first three test batches achieved the best ROUGE-SU4 results in the challenge.","year":2017,"title_abstract":"{M}acquarie {U}niversity at {B}io{ASQ} 5b {--} Query-based Summarisation Techniques for Selecting the Ideal Answers Macquarie University{'}s contribution to the BioASQ challenge (Task 5b Phase B) focused on the use of query-based extractive summarisation techniques for the generation of the ideal answers. Four runs were submitted, with approaches ranging from a trivial system that selected the first $n$ snippets, to the use of deep learning approaches under a regression framework. Our experiments and the ROUGE results of the five test batches of BioASQ indicate surprisingly good results for the trivial approach. Overall, most of our runs on the first three test batches achieved the best ROUGE-SU4 results in the challenge.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1781148612,"Goal":"Quality Education","Task":["Summarisation"],"Method":["query - based extractive summarisation techniques","trivial system","deep learning approaches","regression framework"]},{"ID":"goldfarb-tarrant-etal-2021-intrinsic","title":"Intrinsic Bias Metrics Do Not Correlate with Application Bias","abstract":"Natural Language Processing (NLP) systems learn harmful societal biases that cause them to amplify inequality as they are deployed in more and more situations. To guide efforts at debiasing these systems, the NLP community relies on a variety of metrics that quantify bias in models. Some of these metrics are intrinsic, measuring bias in word embedding spaces, and some are extrinsic, measuring bias in downstream tasks that the word embeddings enable. Do these intrinsic and extrinsic metrics correlate with each other? We compare intrinsic and extrinsic metrics across hundreds of trained models covering different tasks and experimental conditions. Our results show no reliable correlation between these metrics that holds in all scenarios across tasks and languages. We urge researchers working on debiasing to focus on extrinsic measures of bias, and to make using these measures more feasible via creation of new challenge sets and annotated test data. To aid this effort, we release code, a new intrinsic metric, and an annotated test set focused on gender bias in hate speech.","year":2021,"title_abstract":"Intrinsic Bias Metrics Do Not Correlate with Application Bias Natural Language Processing (NLP) systems learn harmful societal biases that cause them to amplify inequality as they are deployed in more and more situations. To guide efforts at debiasing these systems, the NLP community relies on a variety of metrics that quantify bias in models. Some of these metrics are intrinsic, measuring bias in word embedding spaces, and some are extrinsic, measuring bias in downstream tasks that the word embeddings enable. Do these intrinsic and extrinsic metrics correlate with each other? We compare intrinsic and extrinsic metrics across hundreds of trained models covering different tasks and experimental conditions. Our results show no reliable correlation between these metrics that holds in all scenarios across tasks and languages. We urge researchers working on debiasing to focus on extrinsic measures of bias, and to make using these measures more feasible via creation of new challenge sets and annotated test data. To aid this effort, we release code, a new intrinsic metric, and an annotated test set focused on gender bias in hate speech.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1781015098,"Goal":"Reduced Inequalities","Task":["Natural Language Processing","NLP","downstream tasks","debiasing","hate speech"],"Method":["Application Bias","bias in models"]},{"ID":"babafemi-akinfaderin-2020-predicting","title":"Predicting and Analyzing Law-Making in {K}enya","abstract":"Modelling and analyzing parliamentary legislation, roll-call votes and order of proceedings in developed countries has received significant attention in recent years. In this paper, we focused on understanding the bills introduced in a developing democracy, the Kenyan bicameral parliament. We developed and trained machine learning models on a combination of features extracted from the bills to predict the outcome - if a bill will be enacted or not. We observed that the texts in a bill are not as relevant as the year and month the bill was introduced and the category the bill belongs to.","year":2020,"title_abstract":"Predicting and Analyzing Law-Making in {K}enya Modelling and analyzing parliamentary legislation, roll-call votes and order of proceedings in developed countries has received significant attention in recent years. In this paper, we focused on understanding the bills introduced in a developing democracy, the Kenyan bicameral parliament. We developed and trained machine learning models on a combination of features extracted from the bills to predict the outcome - if a bill will be enacted or not. We observed that the texts in a bill are not as relevant as the year and month the bill was introduced and the category the bill belongs to.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.1780054122,"Goal":"Peace, Justice and Strong Institutions","Task":["Predicting and Analyzing Law - Making","Modelling and analyzing parliamentary legislation"],"Method":["machine learning models"]},{"ID":"perez-almendros-etal-2020-dont","title":"Don{'}t Patronize Me! An Annotated Dataset with Patronizing and Condescending Language towards Vulnerable Communities","abstract":"In this paper, we introduce a new annotated dataset which is aimed at supporting the development of NLP models to identify and categorize language that is patronizing or condescending towards vulnerable communities (e.g. refugees, homeless people, poor families). While the prevalence of such language in the general media has long been shown to have harmful effects, it differs from other types of harmful language, in that it is generally used unconsciously and with good intentions. We furthermore believe that the often subtle nature of patronizing and condescending language (PCL) presents an interesting technical challenge for the NLP community. Our analysis of the proposed dataset shows that identifying PCL is hard for standard NLP models, with language models such as BERT achieving the best results.","year":2020,"title_abstract":"Don{'}t Patronize Me! An Annotated Dataset with Patronizing and Condescending Language towards Vulnerable Communities In this paper, we introduce a new annotated dataset which is aimed at supporting the development of NLP models to identify and categorize language that is patronizing or condescending towards vulnerable communities (e.g. refugees, homeless people, poor families). While the prevalence of such language in the general media has long been shown to have harmful effects, it differs from other types of harmful language, in that it is generally used unconsciously and with good intentions. We furthermore believe that the often subtle nature of patronizing and condescending language (PCL) presents an interesting technical challenge for the NLP community. Our analysis of the proposed dataset shows that identifying PCL is hard for standard NLP models, with language models such as BERT achieving the best results.","social_need":"No Poverty End poverty in all its forms everywhere","cosine_similarity":0.1779529303,"Goal":"No Poverty","Task":["NLP community"],"Method":["NLP models","PCL","NLP models","language models","BERT"]},{"ID":"heo-etal-2021-quality","title":"Quality Estimation Using Dual Encoders with Transfer Learning","abstract":"This paper describes POSTECH{'}s quality estimation systems submitted to Task 2 of the WMT 2021 quality estimation shared task: Word and Sentence-Level Post-editing Effort. We notice that it is possible to improve the stability of the latest quality estimation models that have only one encoder based on the self-attention mechanism to simultaneously process both of the two input data, a source sequence and its machine translation, in that such models have neglected to take advantage of pre-trained monolingual representations, which are generally accepted as reliable representations for various natural language processing tasks. Therefore, our model uses two pre-trained monolingual encoders and then exchanges the information of two encoded representations through two additional cross attention networks. According to the official leaderboard, our systems outperform the baseline systems in terms of the Matthews correlation coefficient for machine translations{'} word-level quality estimation and in terms of the Pearson{'}s correlation coefficient for sentence-level quality estimation by 0.4126 and 0.5497 respectively.","year":2021,"title_abstract":"Quality Estimation Using Dual Encoders with Transfer Learning This paper describes POSTECH{'}s quality estimation systems submitted to Task 2 of the WMT 2021 quality estimation shared task: Word and Sentence-Level Post-editing Effort. We notice that it is possible to improve the stability of the latest quality estimation models that have only one encoder based on the self-attention mechanism to simultaneously process both of the two input data, a source sequence and its machine translation, in that such models have neglected to take advantage of pre-trained monolingual representations, which are generally accepted as reliable representations for various natural language processing tasks. Therefore, our model uses two pre-trained monolingual encoders and then exchanges the information of two encoded representations through two additional cross attention networks. According to the official leaderboard, our systems outperform the baseline systems in terms of the Matthews correlation coefficient for machine translations{'} word-level quality estimation and in terms of the Pearson{'}s correlation coefficient for sentence-level quality estimation by 0.4126 and 0.5497 respectively.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1779147089,"Goal":"Quality Education","Task":["Quality Estimation","WMT 2021 quality estimation","Word and Sentence - Level Post - editing Effort","machine translation","natural language processing tasks"],"Method":["Dual Encoders","Transfer Learning","POSTECH{'}s quality estimation systems","quality estimation models","encoder","self - attention mechanism","monolingual representations","monolingual encoders","encoded representations","cross attention networks"]},{"ID":"tsvetkov-etal-2018-socially","title":"Socially Responsible {NLP}","abstract":"As language technologies have become increasingly prevalent, there is a growing awareness that decisions we make about our data, methods, and tools are often tied up with their impact on people and societies. This tutorial will provide an overview of real-world applications of language technologies and the potential ethical implications associated with them. We will discuss philosophical foundations of ethical research along with state of the art techniques. Through this tutorial, we intend to provide the NLP researcher with an overview of tools to ensure that the data, algorithms, and models that they build are socially responsible. These tools will include a checklist of common pitfalls that one should avoid (e.g., demographic bias in data collection), as well as methods to adequately mitigate these issues (e.g., adjusting sampling rates or de-biasing through regularization). The tutorial is based on a new course on Ethics and NLP developed at Carnegie Mellon University.","year":2018,"title_abstract":"Socially Responsible {NLP} As language technologies have become increasingly prevalent, there is a growing awareness that decisions we make about our data, methods, and tools are often tied up with their impact on people and societies. This tutorial will provide an overview of real-world applications of language technologies and the potential ethical implications associated with them. We will discuss philosophical foundations of ethical research along with state of the art techniques. Through this tutorial, we intend to provide the NLP researcher with an overview of tools to ensure that the data, algorithms, and models that they build are socially responsible. These tools will include a checklist of common pitfalls that one should avoid (e.g., demographic bias in data collection), as well as methods to adequately mitigate these issues (e.g., adjusting sampling rates or de-biasing through regularization). The tutorial is based on a new course on Ethics and NLP developed at Carnegie Mellon University.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.177878201,"Goal":"Gender Equality","Task":["real - world applications","language technologies","ethical research","data collection)","Ethics","NLP"],"Method":["language technologies","NLP researcher","regularization)"]},{"ID":"das-pon-barry-2018-turn","title":"Turn-Taking Strategies for Human-Robot Peer-Learning Dialogue","abstract":"In this paper, we apply the contribution model of grounding to a corpus of human-human peer-mentoring dialogues. From this analysis, we propose effective turn-taking strategies for human-robot interaction with a teachable robot. Specifically, we focus on (1) how robots can encourage humans to present and (2) how robots can signal that they are going to begin a new presentation. We evaluate the strategies against a corpus of human-robot dialogues and offer three guidelines for teachable robots to follow to achieve more human-like collaborative dialogue.","year":2018,"title_abstract":"Turn-Taking Strategies for Human-Robot Peer-Learning Dialogue In this paper, we apply the contribution model of grounding to a corpus of human-human peer-mentoring dialogues. From this analysis, we propose effective turn-taking strategies for human-robot interaction with a teachable robot. Specifically, we focus on (1) how robots can encourage humans to present and (2) how robots can signal that they are going to begin a new presentation. We evaluate the strategies against a corpus of human-robot dialogues and offer three guidelines for teachable robots to follow to achieve more human-like collaborative dialogue.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1778599918,"Goal":"Partnership for the Goals","Task":["Human - Robot Peer - Learning Dialogue","human - robot interaction"],"Method":["Turn - Taking Strategies","contribution model","grounding","turn - taking strategies"]},{"ID":"rabinovich-etal-2017-personalized","title":"Personalized Machine Translation: Preserving Original Author Traits","abstract":"The language that we produce reflects our personality, and various personal and demographic characteristics can be detected in natural language texts. We focus on one particular personal trait of the author, gender, and study how it is manifested in original texts and in translations. We show that author{'}s gender has a powerful, clear signal in originals texts, but this signal is obfuscated in human and machine translation. We then propose simple domain-adaptation techniques that help retain the original gender traits in the translation, without harming the quality of the translation, thereby creating more personalized machine translation systems.","year":2017,"title_abstract":"Personalized Machine Translation: Preserving Original Author Traits The language that we produce reflects our personality, and various personal and demographic characteristics can be detected in natural language texts. We focus on one particular personal trait of the author, gender, and study how it is manifested in original texts and in translations. We show that author{'}s gender has a powerful, clear signal in originals texts, but this signal is obfuscated in human and machine translation. We then propose simple domain-adaptation techniques that help retain the original gender traits in the translation, without harming the quality of the translation, thereby creating more personalized machine translation systems.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1778552085,"Goal":"Gender Equality","Task":["Personalized Machine Translation","human and machine translation","translation","translation","machine translation systems"],"Method":["domain - adaptation techniques"]},{"ID":"chiao-etal-2006-evaluation","title":"Evaluation of multilingual text alignment systems: the {ARCADE} {II} project","abstract":"This paper describes the ARCADE II project, concerned with the evaluation of parallel text alignment systems. The ARCADE II project aims at exploring the techniques of multilingual text alignment through a fine evaluation of the existing techniques and the development of new alignment methods. The evaluation campaign consists of two tracks devoted to the evaluation of alignment at sentence and word level respectively. It differs from ARCADE I in the multilingual aspect and the investigation of lexical alignment.","year":2006,"title_abstract":"Evaluation of multilingual text alignment systems: the {ARCADE} {II} project This paper describes the ARCADE II project, concerned with the evaluation of parallel text alignment systems. The ARCADE II project aims at exploring the techniques of multilingual text alignment through a fine evaluation of the existing techniques and the development of new alignment methods. The evaluation campaign consists of two tracks devoted to the evaluation of alignment at sentence and word level respectively. It differs from ARCADE I in the multilingual aspect and the investigation of lexical alignment.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1778236628,"Goal":"Gender Equality","Task":["Evaluation of multilingual text alignment systems","ARCADE II project","parallel text alignment systems","ARCADE II","multilingual text alignment","alignment","multilingual aspect","lexical alignment"],"Method":["alignment methods","ARCADE I"]},{"ID":"stahlberg-kumar-2020-seq2edits","title":"{S}eq2{E}dits: Sequence Transduction Using Span-level Edit Operations","abstract":"We propose Seq2Edits, an open-vocabulary approach to sequence editing for natural language processing (NLP) tasks with a high degree of overlap between input and output texts. In this approach, each sequence-to-sequence transduction is represented as a sequence of edit operations, where each operation either replaces an entire source span with target tokens or keeps it unchanged. We evaluate our method on five NLP tasks (text normalization, sentence fusion, sentence splitting {\\&} rephrasing, text simplification, and grammatical error correction) and report competitive results across the board. For grammatical error correction, our method speeds up inference by up to 5.2x compared to full sequence models because inference time depends on the number of edits rather than the number of target tokens. For text normalization, sentence fusion, and grammatical error correction, our approach improves explainability by associating each edit operation with a human-readable tag.","year":2020,"title_abstract":"{S}eq2{E}dits: Sequence Transduction Using Span-level Edit Operations We propose Seq2Edits, an open-vocabulary approach to sequence editing for natural language processing (NLP) tasks with a high degree of overlap between input and output texts. In this approach, each sequence-to-sequence transduction is represented as a sequence of edit operations, where each operation either replaces an entire source span with target tokens or keeps it unchanged. We evaluate our method on five NLP tasks (text normalization, sentence fusion, sentence splitting {\\&} rephrasing, text simplification, and grammatical error correction) and report competitive results across the board. For grammatical error correction, our method speeds up inference by up to 5.2x compared to full sequence models because inference time depends on the number of edits rather than the number of target tokens. For text normalization, sentence fusion, and grammatical error correction, our approach improves explainability by associating each edit operation with a human-readable tag.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1776465476,"Goal":"Gender Equality","Task":["sequence editing","natural language processing","sequence - to - sequence transduction","NLP tasks","normalization","sentence fusion","sentence splitting","rephrasing","text simplification","grammatical error correction)","grammatical error correction","inference","text normalization","sentence fusion","grammatical error correction","explainability"],"Method":["Sequence Transduction","Span - level Edit Operations","Seq2Edits","open - vocabulary approach","edit operations","full sequence models","edit operation","human - readable tag"]},{"ID":"stambolieva-2019-eu","title":"{EU} 4 {U}: An educational platform for the cultural heritage of the {EU}","abstract":"The paper presents an ongoing project of the NBU Laboratory for Language Technology aiming to create a multilingual, CEFR-graded electronic didactic resource for online learning, centered on the history and cultural heritage of the EU (e-EULearn). The resource is developed within the e-Platform of the NBU Laboratory for Language Technology and re-uses the rich corpus of educational material created at the Laboratory for the needs of NBU program modules, distance and blended learning language courses and other projects. Focus being not just on foreign language tuition, but above all on people, places and events in the history and culture of the EU member states, the annotation modules of the e-Platform have been accordingly extended. Current and upcoming activities are directed at: 1\/ enriching the English corpus of didactic materials on EU history and culture, 2\/ translating the texts into (the) other official EU languages and aligning the translations with the English texts; 3\/ developing new test modules. In the process of developing this resource, a database on important people, places, objects and events in the cultural history of the EU will be created.","year":2019,"title_abstract":"{EU} 4 {U}: An educational platform for the cultural heritage of the {EU} The paper presents an ongoing project of the NBU Laboratory for Language Technology aiming to create a multilingual, CEFR-graded electronic didactic resource for online learning, centered on the history and cultural heritage of the EU (e-EULearn). The resource is developed within the e-Platform of the NBU Laboratory for Language Technology and re-uses the rich corpus of educational material created at the Laboratory for the needs of NBU program modules, distance and blended learning language courses and other projects. Focus being not just on foreign language tuition, but above all on people, places and events in the history and culture of the EU member states, the annotation modules of the e-Platform have been accordingly extended. Current and upcoming activities are directed at: 1\/ enriching the English corpus of didactic materials on EU history and culture, 2\/ translating the texts into (the) other official EU languages and aligning the translations with the English texts; 3\/ developing new test modules. In the process of developing this resource, a database on important people, places, objects and events in the cultural history of the EU will be created.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1774543226,"Goal":"Quality Education","Task":["Language Technology","online learning","Language Technology","NBU program modules","distance and blended learning language courses","foreign language tuition"],"Method":["educational platform","annotation modules","e - Platform"]},{"ID":"saha-bandyopadhyay-2005-semantics","title":"A Semantics-based {E}nglish-{B}engali {EBMT} System for Translating News Headlines","abstract":"The paper reports an Example based Machine Translation System for translating News Headlines from English to Bengali. The input headline is initially searched in the Direct Example Base. If it cannot be found, the input headline is tagged and the tagged headline is searched in the Generalized Tagged Example Base. If a match is obtained, the tagged headline in Bengali is retrieved from the example base, the output Bengali headline is generated after retrieving the Bengali equivalents of the English words from appropriate dictionaries and then applying relevant synthesis rules for generating the Bengali surface level words. If some named entities and acronyms are not present in the dictionary, transliteration scheme is applied for obtaining the Bengali equivalent. If a match is not found, the tagged input headline is analysed to identify the constituent phrase(s). The target translation is generated using English-Bengali phrasal example base, appropriate dictionaries and a set of heuristics for Bengali phrase reordering. If the headline still cannot be translated using example base strategy, a heuristic translation strategy will be applied. Any new input tagged headline along with its translation by the user will be inserted in the tagged Example base after generalization.","year":2005,"title_abstract":"A Semantics-based {E}nglish-{B}engali {EBMT} System for Translating News Headlines The paper reports an Example based Machine Translation System for translating News Headlines from English to Bengali. The input headline is initially searched in the Direct Example Base. If it cannot be found, the input headline is tagged and the tagged headline is searched in the Generalized Tagged Example Base. If a match is obtained, the tagged headline in Bengali is retrieved from the example base, the output Bengali headline is generated after retrieving the Bengali equivalents of the English words from appropriate dictionaries and then applying relevant synthesis rules for generating the Bengali surface level words. If some named entities and acronyms are not present in the dictionary, transliteration scheme is applied for obtaining the Bengali equivalent. If a match is not found, the tagged input headline is analysed to identify the constituent phrase(s). The target translation is generated using English-Bengali phrasal example base, appropriate dictionaries and a set of heuristics for Bengali phrase reordering. If the headline still cannot be translated using example base strategy, a heuristic translation strategy will be applied. Any new input tagged headline along with its translation by the user will be inserted in the tagged Example base after generalization.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1774151474,"Goal":"Gender Equality","Task":["Translating News Headlines","translating News Headlines","translation","Bengali phrase reordering"],"Method":["Semantics - based {E}nglish -","Example based Machine Translation System","synthesis rules","transliteration scheme","dictionaries","heuristics","example base strategy","heuristic translation strategy"]},{"ID":"dong-etal-2021-room","title":"Room to Grow: Understanding Personal Characteristics Behind Self Improvement Using Social Media","abstract":"Many people aim for change, but not everyone succeeds. While there are a number of social psychology theories that propose motivation-related characteristics of those who persist with change, few computational studies have explored the motivational stage of personal change. In this paper, we investigate a new dataset consisting of the writings of people who manifest intention to change, some of whom persist while others do not. Using a variety of linguistic analysis techniques, we first examine the writing patterns that distinguish the two groups of people. Persistent people tend to reference more topics related to long-term self-improvement and use a more complicated writing style. Drawing on these consistent differences, we build a classifier that can reliably identify the people more likely to persist, based on their language. Our experiments provide new insights into the motivation-related behavior of people who persist with their intention to change.","year":2021,"title_abstract":"Room to Grow: Understanding Personal Characteristics Behind Self Improvement Using Social Media Many people aim for change, but not everyone succeeds. While there are a number of social psychology theories that propose motivation-related characteristics of those who persist with change, few computational studies have explored the motivational stage of personal change. In this paper, we investigate a new dataset consisting of the writings of people who manifest intention to change, some of whom persist while others do not. Using a variety of linguistic analysis techniques, we first examine the writing patterns that distinguish the two groups of people. Persistent people tend to reference more topics related to long-term self-improvement and use a more complicated writing style. Drawing on these consistent differences, we build a classifier that can reliably identify the people more likely to persist, based on their language. Our experiments provide new insights into the motivation-related behavior of people who persist with their intention to change.","social_need":"Decent Work and Economic Growth Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all","cosine_similarity":0.1774030477,"Goal":"Decent Work and Economic Growth","Task":["Personal Characteristics","Self Improvement","motivational stage of personal change"],"Method":["social psychology theories","linguistic analysis techniques","classifier"]},{"ID":"mccarthy-etal-2021-mixed","title":"A Mixed-Methods Analysis of Western and {H}ong {K}ong{--}based Reporting on the 2019{--}2020 Protests","abstract":"We apply statistical techniques from natural language processing to Western and Hong Kong{--}based English language newspaper articles that discuss the 2019{--}2020 Hong Kong protests of the Anti-Extradition Law Amendment Bill Movement. Topic modeling detects central themes of the reporting and shows the differing agendas toward \\textit{one country, two systems}. Embedding-based usage shift (at the word level) and sentiment analysis (at the document level) both support that Hong Kong{--}based reporting is more negative and more emotionally charged. A two-way test shows that while July 1, 2019 is a turning point for media portrayal, the differences between western- and Hong Kong{--}based reporting did not magnify when the protests began; rather, they already existed. Taken together, these findings clarify how the portrayal of activism in Hong Kong evolved throughout the Movement.","year":2021,"title_abstract":"A Mixed-Methods Analysis of Western and {H}ong {K}ong{--}based Reporting on the 2019{--}2020 Protests We apply statistical techniques from natural language processing to Western and Hong Kong{--}based English language newspaper articles that discuss the 2019{--}2020 Hong Kong protests of the Anti-Extradition Law Amendment Bill Movement. Topic modeling detects central themes of the reporting and shows the differing agendas toward \\textit{one country, two systems}. Embedding-based usage shift (at the word level) and sentiment analysis (at the document level) both support that Hong Kong{--}based reporting is more negative and more emotionally charged. A two-way test shows that while July 1, 2019 is a turning point for media portrayal, the differences between western- and Hong Kong{--}based reporting did not magnify when the protests began; rather, they already existed. Taken together, these findings clarify how the portrayal of activism in Hong Kong evolved throughout the Movement.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1773892343,"Goal":"Reduced Inequalities","Task":["media portrayal","portrayal of activism"],"Method":["Mixed - Methods Analysis","statistical techniques","natural language processing","Topic modeling","Embedding - based usage shift","sentiment analysis"]},{"ID":"uksik-etal-2021-estonian","title":"{E}stonian as a Second Language Teacher{'}s Tools","abstract":"The paper presents the results of the project {``}Teacher{'}s Tools{''} (et {\\~O}petaja t{\\\"o}{\\\"o}riistad) published as a subpage of the new language portal S{\\~o}naveeb developed by the Institute of the Estonian Language. The toolbox includes four modules: vocabulary, grammar, communicative language activities and text evaluation. The tools are aimed to help teachers and specialists of Estonian as a second language plan courses, create new educational materials, exercises and tests based on CEFR level descriptions.","year":2021,"title_abstract":"{E}stonian as a Second Language Teacher{'}s Tools The paper presents the results of the project {``}Teacher{'}s Tools{''} (et {\\~O}petaja t{\\\"o}{\\\"o}riistad) published as a subpage of the new language portal S{\\~o}naveeb developed by the Institute of the Estonian Language. The toolbox includes four modules: vocabulary, grammar, communicative language activities and text evaluation. The tools are aimed to help teachers and specialists of Estonian as a second language plan courses, create new educational materials, exercises and tests based on CEFR level descriptions.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1773120761,"Goal":"Quality Education","Task":["vocabulary","grammar","communicative language activities","text evaluation"],"Method":["Second Language Teacher{'}s Tools"]},{"ID":"klang-nugues-2019-docria","title":"{D}ocria: Processing and Storing Linguistic Data with {W}ikipedia","abstract":"The availability of user-generated content has increased significantly over time. Wikipedia is one example of a corpora which spans a huge range of topics and is freely available. Storing and processing these corpora requires flexible documents models as they may contain malicious and incorrect data. Docria is a library which attempts to address this issue by providing a solution which can be used with small to large corpora, from laptops using Python interactively in a Jupyter notebook to clusters running map-reduce frameworks with optimized compiled code. Docria is available as open-source code.","year":2019,"title_abstract":"{D}ocria: Processing and Storing Linguistic Data with {W}ikipedia The availability of user-generated content has increased significantly over time. Wikipedia is one example of a corpora which spans a huge range of topics and is freely available. Storing and processing these corpora requires flexible documents models as they may contain malicious and incorrect data. Docria is a library which attempts to address this issue by providing a solution which can be used with small to large corpora, from laptops using Python interactively in a Jupyter notebook to clusters running map-reduce frameworks with optimized compiled code. Docria is available as open-source code.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.1772506833,"Goal":"Life Below Water","Task":["Processing and Storing Linguistic Data"],"Method":["{W}ikipedia","documents models","Docria","map - reduce frameworks","compiled code","Docria"]},{"ID":"rinsche-2001-managing","title":"Managing translation and localisation projects with {LTC} Organiser","abstract":"Using an invented case study, the paper describes how multilingual translation projects can be managed efficiently with an enterprise resource management tool called {``}LTC Organiser{''}, which was developed specifically for the particular requirements of the language industry. The talk will describe the most important aspects of the integrated solution, such as client and supplier management, project and finance management, managing tools used in the translation process, reporting facilities, security and user management, directory management, sort and search facilities as well as web functionality available at several levels.","year":2001,"title_abstract":"Managing translation and localisation projects with {LTC} Organiser Using an invented case study, the paper describes how multilingual translation projects can be managed efficiently with an enterprise resource management tool called {``}LTC Organiser{''}, which was developed specifically for the particular requirements of the language industry. The talk will describe the most important aspects of the integrated solution, such as client and supplier management, project and finance management, managing tools used in the translation process, reporting facilities, security and user management, directory management, sort and search facilities as well as web functionality available at several levels.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1772327572,"Goal":"Partnership for the Goals","Task":["Managing translation and localisation projects","multilingual translation projects","language industry","client and supplier management","project and finance management","translation process","reporting facilities","security","user management","directory management","sort and search facilities"],"Method":["{LTC} Organiser","enterprise resource management tool","{``}LTC Organiser{''}","managing tools"]},{"ID":"gaut-etal-2020-towards","title":"Towards Understanding Gender Bias in Relation Extraction","abstract":"Recent developments in Neural Relation Extraction (NRE) have made significant strides towards Automated Knowledge Base Construction. While much attention has been dedicated towards improvements in accuracy, there have been no attempts in the literature to evaluate social biases exhibited in NRE systems. In this paper, we create WikiGenderBias, a distantly supervised dataset composed of over 45,000 sentences including a 10{\\%} human annotated test set for the purpose of analyzing gender bias in relation extraction systems. We find that when extracting spouse-of and hypernym (i.e., occupation) relations, an NRE system performs differently when the gender of the target entity is different. However, such disparity does not appear when extracting relations such as birthDate or birthPlace. We also analyze how existing bias mitigation techniques, such as name anonymization, word embedding debiasing, and data augmentation affect the NRE system in terms of maintaining the test performance and reducing biases. Unfortunately, due to NRE models rely heavily on surface level cues, we find that existing bias mitigation approaches have a negative effect on NRE. Our analysis lays groundwork for future quantifying and mitigating bias in NRE.","year":2020,"title_abstract":"Towards Understanding Gender Bias in Relation Extraction Recent developments in Neural Relation Extraction (NRE) have made significant strides towards Automated Knowledge Base Construction. While much attention has been dedicated towards improvements in accuracy, there have been no attempts in the literature to evaluate social biases exhibited in NRE systems. In this paper, we create WikiGenderBias, a distantly supervised dataset composed of over 45,000 sentences including a 10{\\%} human annotated test set for the purpose of analyzing gender bias in relation extraction systems. We find that when extracting spouse-of and hypernym (i.e., occupation) relations, an NRE system performs differently when the gender of the target entity is different. However, such disparity does not appear when extracting relations such as birthDate or birthPlace. We also analyze how existing bias mitigation techniques, such as name anonymization, word embedding debiasing, and data augmentation affect the NRE system in terms of maintaining the test performance and reducing biases. Unfortunately, due to NRE models rely heavily on surface level cues, we find that existing bias mitigation approaches have a negative effect on NRE. Our analysis lays groundwork for future quantifying and mitigating bias in NRE.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1771308631,"Goal":"Gender Equality","Task":["Gender Bias","Relation Extraction","Neural Relation Extraction","Automated Knowledge Base Construction","NRE","gender bias","relation extraction systems","data augmentation","NRE","NRE","mitigating bias","NRE"],"Method":["NRE","bias mitigation techniques","name anonymization","word embedding debiasing","NRE","bias mitigation approaches"]},{"ID":"yang-etal-2020-semeval","title":"{S}em{E}val-2020 Task 5: Counterfactual Recognition","abstract":"We present a counterfactual recognition (CR) task, the shared Task 5 of SemEval-2020. Counterfactuals describe potential outcomes (consequents) produced by actions or circumstances that did not happen or cannot happen and are counter to the facts (antecedent). Counterfactual thinking is an important characteristic of the human cognitive system; it connects antecedents and consequent with causal relations. Our task provides a benchmark for counterfactual recognition in natural language with two subtasks. Subtask-1 aims to determine whether a given sentence is a counterfactual statement or not. Subtask-2 requires the participating systems to extract the antecedent and consequent in a given counterfactual statement. During the SemEval-2020 official evaluation period, we received 27 submissions to Subtask-1 and 11 to Subtask-2. Our data and baseline code are made publicly available at https:\/\/zenodo.org\/record\/3932442. The task website and leaderboard can be found at https:\/\/competitions.codalab.org\/competitions\/21691.","year":2020,"title_abstract":"{S}em{E}val-2020 Task 5: Counterfactual Recognition We present a counterfactual recognition (CR) task, the shared Task 5 of SemEval-2020. Counterfactuals describe potential outcomes (consequents) produced by actions or circumstances that did not happen or cannot happen and are counter to the facts (antecedent). Counterfactual thinking is an important characteristic of the human cognitive system; it connects antecedents and consequent with causal relations. Our task provides a benchmark for counterfactual recognition in natural language with two subtasks. Subtask-1 aims to determine whether a given sentence is a counterfactual statement or not. Subtask-2 requires the participating systems to extract the antecedent and consequent in a given counterfactual statement. During the SemEval-2020 official evaluation period, we received 27 submissions to Subtask-1 and 11 to Subtask-2. Our data and baseline code are made publicly available at https:\/\/zenodo.org\/record\/3932442. The task website and leaderboard can be found at https:\/\/competitions.codalab.org\/competitions\/21691.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1770283282,"Goal":"Climate Action","Task":["Counterfactual Recognition","counterfactual recognition (CR) task","SemEval - 2020","counterfactual recognition","Subtask - 1"],"Method":["Counterfactual thinking","human cognitive system;"]},{"ID":"kao-etal-2020-ntunlpl","title":"{NTUNLPL} at {F}in{C}ausal 2020, Task 2:Improving Causality Detection Using {V}iterbi Decoder","abstract":"In order to provide an explanation of machine learning models, causality detection attracts lots of attention in the artificial intelligence research community. In this paper, we explore the cause-effect detection in financial news and propose an approach, which combines the BIO scheme with the Viterbi decoder for addressing this challenge. Our approach is ranked the first in the official run of cause-effect detection (Task 2) of the FinCausal-2020 shared task. We not only report the implementation details and ablation analysis in this paper, but also publish our code for academic usage.","year":2020,"title_abstract":"{NTUNLPL} at {F}in{C}ausal 2020, Task 2:Improving Causality Detection Using {V}iterbi Decoder In order to provide an explanation of machine learning models, causality detection attracts lots of attention in the artificial intelligence research community. In this paper, we explore the cause-effect detection in financial news and propose an approach, which combines the BIO scheme with the Viterbi decoder for addressing this challenge. Our approach is ranked the first in the official run of cause-effect detection (Task 2) of the FinCausal-2020 shared task. We not only report the implementation details and ablation analysis in this paper, but also publish our code for academic usage.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1770119965,"Goal":"Climate Action","Task":["Causality Detection","causality detection","artificial intelligence research community","cause - effect detection","cause - effect detection","FinCausal - 2020 shared task","ablation analysis"],"Method":["{V}iterbi Decoder","machine learning models","BIO scheme","Viterbi decoder"]},{"ID":"singla-etal-2017-automatic","title":"Automatic Community Creation for Abstractive Spoken Conversations Summarization","abstract":"Summarization of spoken conversations is a challenging task, since it requires deep understanding of dialogs. Abstractive summarization techniques rely on linking the summary sentences to sets of original conversation sentences, i.e. communities. Unfortunately, such linking information is rarely available or requires trained annotators. We propose and experiment automatic community creation using cosine similarity on different levels of representation: raw text, WordNet SynSet IDs, and word embeddings. We show that the abstractive summarization systems with automatic communities significantly outperform previously published results on both English and Italian corpora.","year":2017,"title_abstract":"Automatic Community Creation for Abstractive Spoken Conversations Summarization Summarization of spoken conversations is a challenging task, since it requires deep understanding of dialogs. Abstractive summarization techniques rely on linking the summary sentences to sets of original conversation sentences, i.e. communities. Unfortunately, such linking information is rarely available or requires trained annotators. We propose and experiment automatic community creation using cosine similarity on different levels of representation: raw text, WordNet SynSet IDs, and word embeddings. We show that the abstractive summarization systems with automatic communities significantly outperform previously published results on both English and Italian corpora.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1769794524,"Goal":"Sustainable Cities and Communities","Task":["Automatic Community Creation","Abstractive Spoken Conversations","Summarization","Summarization of spoken conversations","deep understanding of dialogs","summarization","automatic community creation","summarization"],"Method":["WordNet SynSet IDs","word embeddings"]},{"ID":"hosseini-etal-2020-content","title":"Content analysis of {P}ersian\/{F}arsi Tweets during {COVID}-19 pandemic in {I}ran using {NLP}","abstract":"Iran, along with China, South Korea, and Italy was among the countries that were hit hard in the first wave of the COVID-19 spread. Twitter is one of the widely-used online platforms by Iranians inside and abroad for sharing their opinion, thoughts, and feelings about a wide range of issues. In this study, using more than 530,000 original tweets in Persian\/Farsi on COVID-19, we analyzed the topics discussed among users, who are mainly Iranians, to gauge and track the response to the pandemic and how it evolved over time. We applied a combination of manual annotation of a random sample of tweets and topic modeling tools to classify the contents and frequency of each category of topics. We identified the top 25 topics among which living experience under home quarantine emerged as a major talking point. We additionally categorized the broader content of tweets that shows satire, followed by news, is the dominant tweet type among Iranian users. While this framework and methodology can be used to track public response to ongoing developments related to COVID-19, a generalization of this framework can become a useful framework to gauge Iranian public reaction to ongoing policy measures or events locally and internationally.","year":2020,"title_abstract":"Content analysis of {P}ersian\/{F}arsi Tweets during {COVID}-19 pandemic in {I}ran using {NLP} Iran, along with China, South Korea, and Italy was among the countries that were hit hard in the first wave of the COVID-19 spread. Twitter is one of the widely-used online platforms by Iranians inside and abroad for sharing their opinion, thoughts, and feelings about a wide range of issues. In this study, using more than 530,000 original tweets in Persian\/Farsi on COVID-19, we analyzed the topics discussed among users, who are mainly Iranians, to gauge and track the response to the pandemic and how it evolved over time. We applied a combination of manual annotation of a random sample of tweets and topic modeling tools to classify the contents and frequency of each category of topics. We identified the top 25 topics among which living experience under home quarantine emerged as a major talking point. We additionally categorized the broader content of tweets that shows satire, followed by news, is the dominant tweet type among Iranian users. While this framework and methodology can be used to track public response to ongoing developments related to COVID-19, a generalization of this framework can become a useful framework to gauge Iranian public reaction to ongoing policy measures or events locally and internationally.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1768476367,"Goal":"Climate Action","Task":["COVID - 19","public response","COVID - 19"],"Method":["Content analysis","{P}ersian\/{F}arsi","Twitter","manual annotation","topic modeling tools"]},{"ID":"kutuzov-etal-2017-tracing","title":"Tracing armed conflicts with diachronic word embedding models","abstract":"Recent studies have shown that word embedding models can be used to trace time-related (diachronic) semantic shifts in particular words. In this paper, we evaluate some of these approaches on the new task of predicting the dynamics of global armed conflicts on a year-to-year basis, using a dataset from the conflict research field as the gold standard and the Gigaword news corpus as the training data. The results show that much work still remains in extracting {`}cultural{'} semantic shifts from diachronic word embedding models. At the same time, we present a new task complete with an evaluation set and introduce the {`}anchor words{'} method which outperforms previous approaches on this set.","year":2017,"title_abstract":"Tracing armed conflicts with diachronic word embedding models Recent studies have shown that word embedding models can be used to trace time-related (diachronic) semantic shifts in particular words. In this paper, we evaluate some of these approaches on the new task of predicting the dynamics of global armed conflicts on a year-to-year basis, using a dataset from the conflict research field as the gold standard and the Gigaword news corpus as the training data. The results show that much work still remains in extracting {`}cultural{'} semantic shifts from diachronic word embedding models. At the same time, we present a new task complete with an evaluation set and introduce the {`}anchor words{'} method which outperforms previous approaches on this set.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1766989976,"Goal":"Reduced Inequalities","Task":["Tracing armed conflicts","dynamics of global armed conflicts","conflict research field"],"Method":["diachronic word embedding models","word embedding models","diachronic word embedding models","words{'} method"]},{"ID":"hamoui-etal-2020-flodusta","title":"{F}lo{D}us{TA}: Saudi Tweets Dataset for Flood, Dust Storm, and Traffic Accident Events","abstract":"The rise of social media platforms makes it a valuable information source of recent events and users{'} perspective towards them. Twitter has been one of the most important communication platforms in recent years. Event detection, one of the information extraction aspects, involves identifying specified types of events in the text. Detecting events from tweets can help to predict real-world events precisely. A serious challenge that faces Arabic event detection is the lack of Arabic datasets that can be exploited in detecting events. This paper will describe FloDusTA, which is a dataset of tweets that we have built for the purpose of developing an event detection system. The dataset contains tweets written in both Modern Standard Arabic and Saudi dialect. The process of building the dataset starting from tweets collection to annotation by human annotators will be present. The tweets are labeled with four labels: flood, dust storm, traffic accident, and non-event. The dataset was tested for classification and the result was strongly encouraging.","year":2020,"title_abstract":"{F}lo{D}us{TA}: Saudi Tweets Dataset for Flood, Dust Storm, and Traffic Accident Events The rise of social media platforms makes it a valuable information source of recent events and users{'} perspective towards them. Twitter has been one of the most important communication platforms in recent years. Event detection, one of the information extraction aspects, involves identifying specified types of events in the text. Detecting events from tweets can help to predict real-world events precisely. A serious challenge that faces Arabic event detection is the lack of Arabic datasets that can be exploited in detecting events. This paper will describe FloDusTA, which is a dataset of tweets that we have built for the purpose of developing an event detection system. The dataset contains tweets written in both Modern Standard Arabic and Saudi dialect. The process of building the dataset starting from tweets collection to annotation by human annotators will be present. The tweets are labeled with four labels: flood, dust storm, traffic accident, and non-event. The dataset was tested for classification and the result was strongly encouraging.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1766751111,"Goal":"Sustainable Cities and Communities","Task":["Flood , Dust Storm","Traffic Accident Events","Event detection","information extraction aspects","Detecting events","real - world events","Arabic event detection","detecting events","annotation","classification"],"Method":["Twitter","FloDusTA","event detection system"]},{"ID":"jansen-ustalov-2019-textgraphs","title":"{T}ext{G}raphs 2019 Shared Task on Multi-Hop Inference for Explanation Regeneration","abstract":"While automated question answering systems are increasingly able to retrieve answers to natural language questions, their ability to generate detailed human-readable explanations for their answers is still quite limited. The Shared Task on Multi-Hop Inference for Explanation Regeneration tasks participants with regenerating detailed gold explanations for standardized elementary science exam questions by selecting facts from a knowledge base of semi-structured tables. Each explanation contains between 1 and 16 interconnected facts that form an {``}explanation graph{''} spanning core scientific knowledge and detailed world knowledge. It is expected that successfully combining these facts to generate detailed explanations will require advancing methods in multi-hop inference and information combination, and will make use of the supervised training data provided by the WorldTree explanation corpus. The top-performing system achieved a mean average precision (MAP) of 0.56, substantially advancing the state-of-the-art over a baseline information retrieval model. Detailed extended analyses of all submitted systems showed large relative improvements in accessing the most challenging multi-hop inference problems, while absolute performance remains low, highlighting the difficulty of generating detailed explanations through multi-hop reasoning.","year":2019,"title_abstract":"{T}ext{G}raphs 2019 Shared Task on Multi-Hop Inference for Explanation Regeneration While automated question answering systems are increasingly able to retrieve answers to natural language questions, their ability to generate detailed human-readable explanations for their answers is still quite limited. The Shared Task on Multi-Hop Inference for Explanation Regeneration tasks participants with regenerating detailed gold explanations for standardized elementary science exam questions by selecting facts from a knowledge base of semi-structured tables. Each explanation contains between 1 and 16 interconnected facts that form an {``}explanation graph{''} spanning core scientific knowledge and detailed world knowledge. It is expected that successfully combining these facts to generate detailed explanations will require advancing methods in multi-hop inference and information combination, and will make use of the supervised training data provided by the WorldTree explanation corpus. The top-performing system achieved a mean average precision (MAP) of 0.56, substantially advancing the state-of-the-art over a baseline information retrieval model. Detailed extended analyses of all submitted systems showed large relative improvements in accessing the most challenging multi-hop inference problems, while absolute performance remains low, highlighting the difficulty of generating detailed explanations through multi-hop reasoning.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1766473055,"Goal":"Quality Education","Task":["Multi - Hop Inference","Explanation Regeneration","automated question answering systems","Multi - Hop Inference","Explanation Regeneration tasks","regenerating detailed gold explanations","multi - hop inference","information combination","multi - hop inference problems","generating detailed explanations"],"Method":["baseline information retrieval model","multi - hop reasoning"]},{"ID":"tiedemann-2006-isa","title":"{ISA} {\\&} {ICA} - Two Web Interfaces for Interactive Alignment of Bitexts alignment of parallel texts","abstract":"ISA and ICA are two web interfaces for interactive alignment of parallel texts. ISA provides an interface for automatic and manual sentence alignment. It includes cognate filters and uses structural markup to improve automatic alignment and provides intuitive tools for editing them. Alignment results can be saved to disk or sent via e-mail. ICA provides an interface to the clue aligner from the Uplug toolbox. It allows one to set various parameters and visualizes alignment results in a two-dimensional matrix. Word alignments can be edited and saved to disk.","year":2006,"title_abstract":"{ISA} {\\&} {ICA} - Two Web Interfaces for Interactive Alignment of Bitexts alignment of parallel texts ISA and ICA are two web interfaces for interactive alignment of parallel texts. ISA provides an interface for automatic and manual sentence alignment. It includes cognate filters and uses structural markup to improve automatic alignment and provides intuitive tools for editing them. Alignment results can be saved to disk or sent via e-mail. ICA provides an interface to the clue aligner from the Uplug toolbox. It allows one to set various parameters and visualizes alignment results in a two-dimensional matrix. Word alignments can be edited and saved to disk.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.1766228229,"Goal":"Peace, Justice and Strong Institutions","Task":["Interactive Alignment of Bitexts alignment of parallel texts ISA","interactive alignment of parallel texts","automatic and manual sentence alignment","automatic alignment","Alignment"],"Method":["Web Interfaces","ICA","web interfaces","ISA","cognate filters","structural markup","ICA","clue aligner","Uplug toolbox"]},{"ID":"kaneko-etal-2018-tmu","title":"{TMU} System for {SLAM}-2018","abstract":"We introduce the TMU systems for the second language acquisition modeling shared task 2018 (Settles et al., 2018). To model learner error patterns, it is necessary to maintain a considerable amount of information regarding the type of exercises learners have been learning in the past and the manner in which they answered them. Tracking an enormous learner{'}s learning history and their correct and mistaken answers is essential to predict the learner{'}s future mistakes. Therefore, we propose a model which tracks the learner{'}s learning history efficiently. Our systems ranked fourth in the English and Spanish subtasks, and fifth in the French subtask.","year":2018,"title_abstract":"{TMU} System for {SLAM}-2018 We introduce the TMU systems for the second language acquisition modeling shared task 2018 (Settles et al., 2018). To model learner error patterns, it is necessary to maintain a considerable amount of information regarding the type of exercises learners have been learning in the past and the manner in which they answered them. Tracking an enormous learner{'}s learning history and their correct and mistaken answers is essential to predict the learner{'}s future mistakes. Therefore, we propose a model which tracks the learner{'}s learning history efficiently. Our systems ranked fourth in the English and Spanish subtasks, and fifth in the French subtask.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1765113175,"Goal":"Quality Education","Task":["second language acquisition modeling shared task","learner error patterns"],"Method":["TMU systems"]},{"ID":"arseniev-koehler-etal-2018-type","title":"What type of happiness are you looking for? - A closer look at detecting mental health from language","abstract":"Computational models to detect mental illnesses from text and speech could enhance our understanding of mental health while offering opportunities for early detection and intervention. However, these models are often disconnected from the lived experience of depression and the larger diagnostic debates in mental health. This article investigates these disconnects, primarily focusing on the labels used to diagnose depression, how these labels are computationally represented, and the performance metrics used to evaluate computational models. We also consider how medical instruments used to measure depression, such as the Patient Health Questionnaire (PHQ), contribute to these disconnects. To illustrate our points, we incorporate mixed-methods analyses of 698 interviews on emotional health, which are coupled with self-report PHQ screens for depression. We propose possible strategies to bridge these gaps between modern psychiatric understandings of depression, lay experience of depression, and computational representation.","year":2018,"title_abstract":"What type of happiness are you looking for? - A closer look at detecting mental health from language Computational models to detect mental illnesses from text and speech could enhance our understanding of mental health while offering opportunities for early detection and intervention. However, these models are often disconnected from the lived experience of depression and the larger diagnostic debates in mental health. This article investigates these disconnects, primarily focusing on the labels used to diagnose depression, how these labels are computationally represented, and the performance metrics used to evaluate computational models. We also consider how medical instruments used to measure depression, such as the Patient Health Questionnaire (PHQ), contribute to these disconnects. To illustrate our points, we incorporate mixed-methods analyses of 698 interviews on emotional health, which are coupled with self-report PHQ screens for depression. We propose possible strategies to bridge these gaps between modern psychiatric understandings of depression, lay experience of depression, and computational representation.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.1764282882,"Goal":"Good Health and Well-Being","Task":["detecting mental health","mental illnesses","mental health","early detection and intervention","mental health","depression","emotional health","depression","lay experience of depression"],"Method":["language Computational models","computational models","medical instruments","Patient Health Questionnaire","mixed - methods analyses","self - report PHQ screens","psychiatric understandings of depression","computational representation"]},{"ID":"stefanov-etal-2020-predicting","title":"Predicting the Topical Stance and Political Leaning of Media using Tweets","abstract":"Discovering the stances of media outlets and influential people on current, debatable topics is important for social statisticians and policy makers. Many supervised solutions exist for determining viewpoints, but manually annotating training data is costly. In this paper, we propose a cascaded method that uses unsupervised learning to ascertain the stance of Twitter users with respect to a polarizing topic by leveraging their retweet behavior; then, it uses supervised learning based on user labels to characterize both the general political leaning of online media and of popular Twitter users, as well as their stance with respect to the target polarizing topic. We evaluate the model by comparing its predictions to gold labels from the Media Bias\/Fact Check website, achieving 82.6{\\%} accuracy.","year":2020,"title_abstract":"Predicting the Topical Stance and Political Leaning of Media using Tweets Discovering the stances of media outlets and influential people on current, debatable topics is important for social statisticians and policy makers. Many supervised solutions exist for determining viewpoints, but manually annotating training data is costly. In this paper, we propose a cascaded method that uses unsupervised learning to ascertain the stance of Twitter users with respect to a polarizing topic by leveraging their retweet behavior; then, it uses supervised learning based on user labels to characterize both the general political leaning of online media and of popular Twitter users, as well as their stance with respect to the target polarizing topic. We evaluate the model by comparing its predictions to gold labels from the Media Bias\/Fact Check website, achieving 82.6{\\%} accuracy.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1764187813,"Goal":"Climate Action","Task":["Topical Stance and Political Leaning of Media","social statisticians","policy makers","determining viewpoints"],"Method":["supervised solutions","cascaded method","unsupervised learning","supervised learning"]},{"ID":"benotti-blackburn-2021-recipe","title":"A recipe for annotating grounded clarifications","abstract":"In order to interpret the communicative intents of an utterance, it needs to be grounded in something that is outside of language; that is, grounded in world modalities. In this paper, we argue that dialogue clarification mechanisms make explicit the process of interpreting the communicative intents of the speaker{'}s utterances by grounding them in the various modalities in which the dialogue is situated. This paper frames dialogue clarification mechanisms as an understudied research problem and a key missing piece in the giant jigsaw puzzle of natural language understanding. We discuss both the theoretical background and practical challenges posed by this problem and propose a recipe for obtaining grounding annotations. We conclude by highlighting ethical issues that need to be addressed in future work.","year":2021,"title_abstract":"A recipe for annotating grounded clarifications In order to interpret the communicative intents of an utterance, it needs to be grounded in something that is outside of language; that is, grounded in world modalities. In this paper, we argue that dialogue clarification mechanisms make explicit the process of interpreting the communicative intents of the speaker{'}s utterances by grounding them in the various modalities in which the dialogue is situated. This paper frames dialogue clarification mechanisms as an understudied research problem and a key missing piece in the giant jigsaw puzzle of natural language understanding. We discuss both the theoretical background and practical challenges posed by this problem and propose a recipe for obtaining grounding annotations. We conclude by highlighting ethical issues that need to be addressed in future work.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1761551797,"Goal":"Partnership for the Goals","Task":["annotating grounded clarifications","natural language understanding","grounding annotations"],"Method":["dialogue clarification mechanisms","dialogue clarification mechanisms"]},{"ID":"maiti-vucetic-2019-spatial","title":"Spatial Aggregation Facilitates Discovery of Spatial Topics","abstract":"Spatial aggregation refers to merging of documents created at the same spatial location. We show that by spatial aggregation of a large collection of documents and applying a traditional topic discovery algorithm on the aggregated data we can efficiently discover spatially distinct topics. By looking at topic discovery through matrix factorization lenses we show that spatial aggregation allows low rank approximation of the original document-word matrix, in which spatially distinct topics are preserved and non-spatial topics are aggregated into a single topic. Our experiments on synthetic data confirm this observation. Our experiments on 4.7 million tweets collected during the Sandy Hurricane in 2012 show that spatial and temporal aggregation allows rapid discovery of relevant spatial and temporal topics during that period. Our work indicates that different forms of document aggregation might be effective in rapid discovery of various types of distinct topics from large collections of documents.","year":2019,"title_abstract":"Spatial Aggregation Facilitates Discovery of Spatial Topics Spatial aggregation refers to merging of documents created at the same spatial location. We show that by spatial aggregation of a large collection of documents and applying a traditional topic discovery algorithm on the aggregated data we can efficiently discover spatially distinct topics. By looking at topic discovery through matrix factorization lenses we show that spatial aggregation allows low rank approximation of the original document-word matrix, in which spatially distinct topics are preserved and non-spatial topics are aggregated into a single topic. Our experiments on synthetic data confirm this observation. Our experiments on 4.7 million tweets collected during the Sandy Hurricane in 2012 show that spatial and temporal aggregation allows rapid discovery of relevant spatial and temporal topics during that period. Our work indicates that different forms of document aggregation might be effective in rapid discovery of various types of distinct topics from large collections of documents.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1760803759,"Goal":"Sustainable Cities and Communities","Task":["Spatial Aggregation","Discovery of Spatial Topics","Spatial aggregation","merging of documents","topic discovery","document aggregation","rapid discovery of various types of distinct topics"],"Method":["spatial aggregation","topic discovery algorithm","matrix factorization lenses","spatial aggregation","low rank approximation","spatial and temporal aggregation"]},{"ID":"saggion-2006-multilingual","title":"Multilingual Multidocument Summarization Tools and Evaluation","abstract":"We describe a number of experiments carried out to address the problem of creating summaries from multiple sources in multiple languages. A centroid-based sentence extraction system has been developed which decides the content of the summary using texts in different languages and uses sentences from English sources alone to create the final output. We describe the evaluation of the system in the recent Multilingual Summarization Evaluation MSE 2005 using the pyramids and ROUGE methods.","year":2006,"title_abstract":"Multilingual Multidocument Summarization Tools and Evaluation We describe a number of experiments carried out to address the problem of creating summaries from multiple sources in multiple languages. A centroid-based sentence extraction system has been developed which decides the content of the summary using texts in different languages and uses sentences from English sources alone to create the final output. We describe the evaluation of the system in the recent Multilingual Summarization Evaluation MSE 2005 using the pyramids and ROUGE methods.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1760765314,"Goal":"Partnership for the Goals","Task":["Multilingual Multidocument Summarization Tools","creating summaries","Multilingual Summarization Evaluation MSE 2005"],"Method":["centroid - based sentence extraction system","pyramids","ROUGE methods"]},{"ID":"plesco-rychtyckyi-2012-machine","title":"Machine Translation as a Global Enterprise Service at Ford","abstract":"Ford Motor Company is at the forefront of the global economy and with this comes the need for communicating with regional manufacturing staff and plant employees in their own languages. Asian employees, in particular, do not necessarily learn English as a second language as is often the case in European countries, so manufacturing systems are now mandated to support local languages. This support is required for plant floor system applications where static data (labels, menus, and messages) as well as dynamic data (user entered controlled and free text) is required to be translated from\/to English and the local languages. This facilitates commonization of business methods where best practices can be shared globally between plant and staff members. In this paper and presentation, we will describe our experiences in bringing Machine Translation technology to a large multinational corporation such as Ford and discuss the lessons we learned as well as both the successes and failures we have experienced.","year":2012,"title_abstract":"Machine Translation as a Global Enterprise Service at Ford Ford Motor Company is at the forefront of the global economy and with this comes the need for communicating with regional manufacturing staff and plant employees in their own languages. Asian employees, in particular, do not necessarily learn English as a second language as is often the case in European countries, so manufacturing systems are now mandated to support local languages. This support is required for plant floor system applications where static data (labels, menus, and messages) as well as dynamic data (user entered controlled and free text) is required to be translated from\/to English and the local languages. This facilitates commonization of business methods where best practices can be shared globally between plant and staff members. In this paper and presentation, we will describe our experiences in bringing Machine Translation technology to a large multinational corporation such as Ford and discuss the lessons we learned as well as both the successes and failures we have experienced.","social_need":"Decent Work and Economic Growth Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all","cosine_similarity":0.1760481894,"Goal":"Decent Work and Economic Growth","Task":["Machine Translation","Global Enterprise Service","Ford Ford Motor Company","plant floor system applications","Machine Translation technology"],"Method":["manufacturing systems","business methods"]},{"ID":"wang-lepage-2016-combining","title":"Combining fast{\\_}align with Hierarchical Sub-sentential Alignment for Better Word Alignments","abstract":"fast align is a simple and fast word alignment tool which is widely used in state-of-the-art machine translation systems. It yields comparable results in the end-to-end translation experiments of various language pairs. However, fast align does not perform as well as GIZA++ when applied to language pairs with distinct word orders, like English and Japanese. In this paper, given the lexical translation table output by fast align, we propose to realign words using the hierarchical sub-sentential alignment approach. Experimental results show that simple additional processing improves the performance of word alignment, which is measured by counting alignment matches in comparison with fast align. We also report the result of final machine translation in both English-Japanese and Japanese-English. We show our best system provided significant improvements over the baseline as measured by BLEU and RIBES.","year":2016,"title_abstract":"Combining fast{\\_}align with Hierarchical Sub-sentential Alignment for Better Word Alignments fast align is a simple and fast word alignment tool which is widely used in state-of-the-art machine translation systems. It yields comparable results in the end-to-end translation experiments of various language pairs. However, fast align does not perform as well as GIZA++ when applied to language pairs with distinct word orders, like English and Japanese. In this paper, given the lexical translation table output by fast align, we propose to realign words using the hierarchical sub-sentential alignment approach. Experimental results show that simple additional processing improves the performance of word alignment, which is measured by counting alignment matches in comparison with fast align. We also report the result of final machine translation in both English-Japanese and Japanese-English. We show our best system provided significant improvements over the baseline as measured by BLEU and RIBES.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1760470271,"Goal":"Gender Equality","Task":["Word Alignments","machine translation","end - to - end translation experiments","word alignment","machine translation"],"Method":["fast{\\_}align","Hierarchical Sub - sentential Alignment","fast align","word alignment tool","align","GIZA++","fast align","hierarchical sub - sentential alignment approach","additional processing","fast align"]},{"ID":"vidgen-etal-2020-detecting","title":"Detecting {E}ast {A}sian Prejudice on Social Media","abstract":"During COVID-19 concerns have heightened about the spread of aggressive and hateful language online, especially hostility directed against East Asia and East Asian people. We report on a new dataset and the creation of a machine learning classifier that categorizes social media posts from Twitter into four classes: Hostility against East Asia, Criticism of East Asia, Meta-discussions of East Asian prejudice, and a neutral class. The classifier achieves a macro-F1 score of 0.83. We then conduct an in-depth ground-up error analysis and show that the model struggles with edge cases and ambiguous content. We provide the 20,000 tweet training dataset (annotated by experienced analysts), which also contains several secondary categories and additional flags. We also provide the 40,000 original annotations (before adjudication), the full codebook, annotations for COVID-19 relevance and East Asian relevance and stance for 1,000 hashtags, and the final model.","year":2020,"title_abstract":"Detecting {E}ast {A}sian Prejudice on Social Media During COVID-19 concerns have heightened about the spread of aggressive and hateful language online, especially hostility directed against East Asia and East Asian people. We report on a new dataset and the creation of a machine learning classifier that categorizes social media posts from Twitter into four classes: Hostility against East Asia, Criticism of East Asia, Meta-discussions of East Asian prejudice, and a neutral class. The classifier achieves a macro-F1 score of 0.83. We then conduct an in-depth ground-up error analysis and show that the model struggles with edge cases and ambiguous content. We provide the 20,000 tweet training dataset (annotated by experienced analysts), which also contains several secondary categories and additional flags. We also provide the 40,000 original annotations (before adjudication), the full codebook, annotations for COVID-19 relevance and East Asian relevance and stance for 1,000 hashtags, and the final model.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1759029478,"Goal":"Climate Action","Task":["Detecting","stance"],"Method":["machine learning classifier"]},{"ID":"alhindi-etal-2018-evidence","title":"Where is Your Evidence: Improving Fact-checking by Justification Modeling","abstract":"Fact-checking is a journalistic practice that compares a claim made publicly against trusted sources of facts. Wang (2017) introduced a large dataset of validated claims from the POLITIFACT.com website (LIAR dataset), enabling the development of machine learning approaches for fact-checking. However, approaches based on this dataset have focused primarily on modeling the claim and speaker-related metadata, without considering the evidence used by humans in labeling the claims. We extend the LIAR dataset by automatically extracting the justification from the fact-checking article used by humans to label a given claim. We show that modeling the extracted justification in conjunction with the claim (and metadata) provides a significant improvement regardless of the machine learning model used (feature-based or deep learning) both in a binary classification task (true, false) and in a six-way classification task (pants on fire, false, mostly false, half true, mostly true, true).","year":2018,"title_abstract":"Where is Your Evidence: Improving Fact-checking by Justification Modeling Fact-checking is a journalistic practice that compares a claim made publicly against trusted sources of facts. Wang (2017) introduced a large dataset of validated claims from the POLITIFACT.com website (LIAR dataset), enabling the development of machine learning approaches for fact-checking. However, approaches based on this dataset have focused primarily on modeling the claim and speaker-related metadata, without considering the evidence used by humans in labeling the claims. We extend the LIAR dataset by automatically extracting the justification from the fact-checking article used by humans to label a given claim. We show that modeling the extracted justification in conjunction with the claim (and metadata) provides a significant improvement regardless of the machine learning model used (feature-based or deep learning) both in a binary classification task (true, false) and in a six-way classification task (pants on fire, false, mostly false, half true, mostly true, true).","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1758504659,"Goal":"Climate Action","Task":["Fact - checking","Justification","Fact - checking","journalistic practice","fact - checking","binary classification task","six - way classification task"],"Method":["machine learning approaches","machine learning model","deep learning)"]},{"ID":"adams-etal-2022-concrete","title":"Doing not Being: Concrete Language as a Bridge from Language Technology to Ethnically Inclusive Job Ads","abstract":"This paper makes the case for studying concreteness in language as a bridge that will allow language technology to support the understanding and improvement of ethnic inclusivity in job advertisements. We propose an annotation scheme that guides the assignment of sentences in job ads to classes that reflect concrete actions, i.e., what the employer needs people to $do$, and abstract dispositions, i.e., who the employer expects people to $be$. Using an annotated dataset of Dutch-language job ads, we demonstrate that machine learning technology is effectively able to distinguish these classes.","year":2022,"title_abstract":"Doing not Being: Concrete Language as a Bridge from Language Technology to Ethnically Inclusive Job Ads This paper makes the case for studying concreteness in language as a bridge that will allow language technology to support the understanding and improvement of ethnic inclusivity in job advertisements. We propose an annotation scheme that guides the assignment of sentences in job ads to classes that reflect concrete actions, i.e., what the employer needs people to $do$, and abstract dispositions, i.e., who the employer expects people to $be$. Using an annotated dataset of Dutch-language job ads, we demonstrate that machine learning technology is effectively able to distinguish these classes.","social_need":"Decent Work and Economic Growth Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all","cosine_similarity":0.1758349091,"Goal":"Decent Work and Economic Growth","Task":["Ethnically Inclusive Job Ads","understanding and improvement of ethnic inclusivity","job advertisements"],"Method":["Language Technology","language technology","annotation scheme","machine learning technology"]},{"ID":"petrova-2019-translation","title":"Translation Quality Assessment Tools and Processes in Relation to {CAT} Tools","abstract":"Modern translation QA tools are the latest attempt to overcome the inevitable subjective component of human revisers. This paper analyzes the current situation in the translation industry in respect to those tools and their relationship with CAT tools. The adoption of international standards has set the basic frame that defines {``}quality{''}. Because of the clear impossibility to develop a universal QA tool, all of the existing ones have in common a wide variety of settings for the user to choose from. A brief comparison is made between most popular standalone QA tools. In order to verify their results in practice, QA outputs from two of those tools have been compared. Polls that cover a period of 12 years have been collected. Their participants explained what practices they adopted in order to guarantee quality.","year":2019,"title_abstract":"Translation Quality Assessment Tools and Processes in Relation to {CAT} Tools Modern translation QA tools are the latest attempt to overcome the inevitable subjective component of human revisers. This paper analyzes the current situation in the translation industry in respect to those tools and their relationship with CAT tools. The adoption of international standards has set the basic frame that defines {``}quality{''}. Because of the clear impossibility to develop a universal QA tool, all of the existing ones have in common a wide variety of settings for the user to choose from. A brief comparison is made between most popular standalone QA tools. In order to verify their results in practice, QA outputs from two of those tools have been compared. Polls that cover a period of 12 years have been collected. Their participants explained what practices they adopted in order to guarantee quality.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1757629663,"Goal":"Quality Education","Task":["Translation","translation industry","QA","QA","QA"],"Method":["translation QA tools","CAT tools"]},{"ID":"han-etal-2019-permanent","title":"No Permanent {F}riends or Enemies: Tracking Relationships between Nations from News","abstract":"Understanding the dynamics of international politics is important yet challenging for civilians. In this work, we explore unsupervised neural models to infer relations between nations from news articles. We extend existing models by incorporating shallow linguistics information and propose a new automatic evaluation metric that aligns relationship dynamics with manually annotated key events. As understanding international relations requires carefully analyzing complex relationships, we conduct in-person human evaluations with three groups of participants. Overall, humans prefer the outputs of our model and give insightful feedback that suggests future directions for human-centered models. Furthermore, our model reveals interesting regional differences in news coverage. For instance, with respect to US-China relations, Singaporean media focus more on {``}strengthening{''} and {``}purchasing{''}, while US media focus more on {``}criticizing{''} and {``}denouncing{''}.","year":2019,"title_abstract":"No Permanent {F}riends or Enemies: Tracking Relationships between Nations from News Understanding the dynamics of international politics is important yet challenging for civilians. In this work, we explore unsupervised neural models to infer relations between nations from news articles. We extend existing models by incorporating shallow linguistics information and propose a new automatic evaluation metric that aligns relationship dynamics with manually annotated key events. As understanding international relations requires carefully analyzing complex relationships, we conduct in-person human evaluations with three groups of participants. Overall, humans prefer the outputs of our model and give insightful feedback that suggests future directions for human-centered models. Furthermore, our model reveals interesting regional differences in news coverage. For instance, with respect to US-China relations, Singaporean media focus more on {``}strengthening{''} and {``}purchasing{''}, while US media focus more on {``}criticizing{''} and {``}denouncing{''}.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1757097244,"Goal":"Reduced Inequalities","Task":["Tracking Relationships between Nations","dynamics of international politics","understanding international relations"],"Method":["unsupervised neural models","human - centered models"]},{"ID":"yu-etal-2020-expanrl","title":"{E}xpan{RL}: Hierarchical Reinforcement Learning for Course Concept Expansion in {MOOC}s","abstract":"Within the prosperity of Massive Open Online Courses (MOOCs), the education applications that automatically provide extracurricular knowledge for MOOC users become rising research topics. However, MOOC courses{'} diversity and rapid updates make it more challenging to find suitable new knowledge for students. In this paper, we present ExpanRL, an end-to-end hierarchical reinforcement learning (HRL) model for concept expansion in MOOCs. Employing a two-level HRL mechanism of seed selection and concept expansion, ExpanRL is more feasible to adjust the expansion strategy to find new concepts based on the students{'} feedback on expansion results. Our experiments on nine novel datasets from real MOOCs show that ExpanRL achieves significant improvements over existing methods and maintain competitive performance under different settings.","year":2020,"title_abstract":"{E}xpan{RL}: Hierarchical Reinforcement Learning for Course Concept Expansion in {MOOC}s Within the prosperity of Massive Open Online Courses (MOOCs), the education applications that automatically provide extracurricular knowledge for MOOC users become rising research topics. However, MOOC courses{'} diversity and rapid updates make it more challenging to find suitable new knowledge for students. In this paper, we present ExpanRL, an end-to-end hierarchical reinforcement learning (HRL) model for concept expansion in MOOCs. Employing a two-level HRL mechanism of seed selection and concept expansion, ExpanRL is more feasible to adjust the expansion strategy to find new concepts based on the students{'} feedback on expansion results. Our experiments on nine novel datasets from real MOOCs show that ExpanRL achieves significant improvements over existing methods and maintain competitive performance under different settings.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1756863892,"Goal":"Quality Education","Task":["Course Concept Expansion","education applications","concept expansion","MOOCs","expansion"],"Method":["Hierarchical Reinforcement Learning","ExpanRL","end - to - end hierarchical reinforcement learning","HRL mechanism","seed selection","concept expansion","ExpanRL","expansion strategy","MOOCs","ExpanRL"]},{"ID":"kiesel-etal-2022-identifying","title":"Identifying the Human Values behind Arguments","abstract":"This paper studies the (often implicit) human values behind natural language arguments, such as to have freedom of thought or to be broadminded. Values are commonly accepted answers to why some option is desirable in the ethical sense and are thus essential both in real-world argumentation and theoretical argumentation frameworks. However, their large variety has been a major obstacle to modeling them in argument mining. To overcome this obstacle, we contribute an operationalization of human values, namely a multi-level taxonomy with 54 values that is in line with psychological research. Moreover, we provide a dataset of 5270 arguments from four geographical cultures, manually annotated for human values. First experiments with the automatic classification of human values are promising, with F$_1$-scores up to 0.81 and 0.25 on average.","year":2022,"title_abstract":"Identifying the Human Values behind Arguments This paper studies the (often implicit) human values behind natural language arguments, such as to have freedom of thought or to be broadminded. Values are commonly accepted answers to why some option is desirable in the ethical sense and are thus essential both in real-world argumentation and theoretical argumentation frameworks. However, their large variety has been a major obstacle to modeling them in argument mining. To overcome this obstacle, we contribute an operationalization of human values, namely a multi-level taxonomy with 54 values that is in line with psychological research. Moreover, we provide a dataset of 5270 arguments from four geographical cultures, manually annotated for human values. First experiments with the automatic classification of human values are promising, with F$_1$-scores up to 0.81 and 0.25 on average.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1756051481,"Goal":"Reduced Inequalities","Task":["real - world argumentation","theoretical argumentation frameworks","argument mining","automatic classification of human values"],"Method":["multi - level taxonomy"]},{"ID":"kaushal-etal-2021-twt","title":"t{WT}{--}{WT}: A Dataset to Assert the Role of Target Entities for Detecting Stance of Tweets","abstract":"The stance detection task aims at detecting the stance of a tweet or a text for a target. These targets can be named entities or free-form sentences (claims). Though the task involves reasoning of the tweet with respect to a target, we find that it is possible to achieve high accuracy on several publicly available Twitter stance detection datasets without looking at the target sentence. Specifically, a simple tweet classification model achieved human-level performance on the WT{--}WT dataset and more than two-third accuracy on various other datasets. We investigate the existence of biases in such datasets to find the potential spurious correlations of sentiment-stance relations and lexical choice associated with the stance category. Furthermore, we propose a new large dataset free of such biases and demonstrate its aptness on the existing stance detection systems. Our empirical findings show much scope for research on the stance detection task and proposes several considerations for creating future stance detection datasets.","year":2021,"title_abstract":"t{WT}{--}{WT}: A Dataset to Assert the Role of Target Entities for Detecting Stance of Tweets The stance detection task aims at detecting the stance of a tweet or a text for a target. These targets can be named entities or free-form sentences (claims). Though the task involves reasoning of the tweet with respect to a target, we find that it is possible to achieve high accuracy on several publicly available Twitter stance detection datasets without looking at the target sentence. Specifically, a simple tweet classification model achieved human-level performance on the WT{--}WT dataset and more than two-third accuracy on various other datasets. We investigate the existence of biases in such datasets to find the potential spurious correlations of sentiment-stance relations and lexical choice associated with the stance category. Furthermore, we propose a new large dataset free of such biases and demonstrate its aptness on the existing stance detection systems. Our empirical findings show much scope for research on the stance detection task and proposes several considerations for creating future stance detection datasets.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1755858958,"Goal":"Climate Action","Task":["Detecting Stance of Tweets","stance detection task","stance detection task","stance detection"],"Method":["tweet classification model","stance detection systems"]},{"ID":"siminyu-etal-2022-corpus","title":"Corpus Development of Kiswahili Speech Recognition Test and Evaluation sets, Preemptively Mitigating Demographic Bias Through Collaboration with Linguists","abstract":"Language technologies, particularly speech technologies, are becoming more pervasive for access to digital platforms and resources. This brings to the forefront concerns of their inclusivity, first in terms of language diversity. Additionally, research shows speech recognition to be more accurate for men than for women and more accurate for individuals younger than 30 years of age than those older. In the Global South where languages are low resource, these same issues should be taken into consideration in data collection efforts to not replicate these mistakes. It is also important to note that in varying contexts within the Global South, this work presents additional nuance and potential for bias based on accents, related dialects and variants of a language. This paper documents i) the designing and execution of a Linguists Engagement for purposes of building an inclusive Kiswahili Speech Recognition dataset, representative of the diversity among speakers of the language ii) the unexpected yet key learning in terms of socio-linguistcs which demonstrate the importance of multi-disciplinarity in teams developing datasets and NLP technologies iii) the creation of a test dataset intended to be used for evaluating the performance of Speech Recognition models on demographic groups that are likely to be underrepresented.","year":2022,"title_abstract":"Corpus Development of Kiswahili Speech Recognition Test and Evaluation sets, Preemptively Mitigating Demographic Bias Through Collaboration with Linguists Language technologies, particularly speech technologies, are becoming more pervasive for access to digital platforms and resources. This brings to the forefront concerns of their inclusivity, first in terms of language diversity. Additionally, research shows speech recognition to be more accurate for men than for women and more accurate for individuals younger than 30 years of age than those older. In the Global South where languages are low resource, these same issues should be taken into consideration in data collection efforts to not replicate these mistakes. It is also important to note that in varying contexts within the Global South, this work presents additional nuance and potential for bias based on accents, related dialects and variants of a language. This paper documents i) the designing and execution of a Linguists Engagement for purposes of building an inclusive Kiswahili Speech Recognition dataset, representative of the diversity among speakers of the language ii) the unexpected yet key learning in terms of socio-linguistcs which demonstrate the importance of multi-disciplinarity in teams developing datasets and NLP technologies iii) the creation of a test dataset intended to be used for evaluating the performance of Speech Recognition models on demographic groups that are likely to be underrepresented.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1755398214,"Goal":"Gender Equality","Task":["Kiswahili Speech Recognition","Preemptively Mitigating Demographic Bias","speech recognition","data collection","Linguists Engagement","socio - linguistcs","NLP technologies"],"Method":["Language technologies","speech technologies","Speech Recognition models"]},{"ID":"tang-kageura-2017-fighting","title":"{`}Fighting{'} or {`}Conflict{'}? An Approach to Revealing Concepts of Terms in Political Discourse","abstract":"Previous work on the epistemology of fact-checking indicated the dilemma between the needs of binary answers for the public and ambiguity of political discussion. Determining concepts represented by terms in political discourse can be considered as a Word-Sense Disambiguation (WSD) task. The analysis of political discourse, however, requires identifying precise concepts of terms from relatively small data. This work attempts to provide a basic framework for revealing concepts of terms in political discourse with explicit contextual information. The framework consists of three parts: 1) extracting important terms, 2) generating concordance for each term with stipulative definitions and explanations, and 3) agglomerating similar information of the term by hierarchical clustering. Utterances made by Prime Minister Abe Shinzo in the Diet of Japan are used to examine our framework. Importantly, we revealed the conceptual inconsistency of the term Sonritsu-kiki-jitai. The framework was proved to work, but only for a small number of terms due to lack of explicit contextual information.","year":2017,"title_abstract":"{`}Fighting{'} or {`}Conflict{'}? An Approach to Revealing Concepts of Terms in Political Discourse Previous work on the epistemology of fact-checking indicated the dilemma between the needs of binary answers for the public and ambiguity of political discussion. Determining concepts represented by terms in political discourse can be considered as a Word-Sense Disambiguation (WSD) task. The analysis of political discourse, however, requires identifying precise concepts of terms from relatively small data. This work attempts to provide a basic framework for revealing concepts of terms in political discourse with explicit contextual information. The framework consists of three parts: 1) extracting important terms, 2) generating concordance for each term with stipulative definitions and explanations, and 3) agglomerating similar information of the term by hierarchical clustering. Utterances made by Prime Minister Abe Shinzo in the Diet of Japan are used to examine our framework. Importantly, we revealed the conceptual inconsistency of the term Sonritsu-kiki-jitai. The framework was proved to work, but only for a small number of terms due to lack of explicit contextual information.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1755071729,"Goal":"Reduced Inequalities","Task":["Revealing Concepts of Terms in Political Discourse","fact - checking","Determining concepts","Word - Sense Disambiguation","analysis of political discourse","revealing concepts of terms","political discourse"],"Method":["hierarchical clustering"]},{"ID":"jiang-etal-2019-massistant","title":"{MA}ssistant: A Personal Knowledge Assistant for {MOOC} Learners","abstract":"Massive Open Online Courses (MOOCs) have developed rapidly and attracted large number of learners. In this work, we present MAssistant system, a personal knowledge assistant for MOOC learners. MAssistant helps users to trace the concepts they have learned in MOOCs, and to build their own concept graphs. There are three key components in MAssistant: (i) a large-scale concept graph built from open data sources, which contains concepts in various domains and relations among them; (ii) a browser extension which interacts with learners when they are watching video lectures, and presents important concepts to them; (iii) a web application allowing users to explore their personal concept graphs, which are built based on their learning activities on MOOCs. MAssistant will facilitate the knowledge management task for MOOC learners, and make the learning on MOOCs easier.","year":2019,"title_abstract":"{MA}ssistant: A Personal Knowledge Assistant for {MOOC} Learners Massive Open Online Courses (MOOCs) have developed rapidly and attracted large number of learners. In this work, we present MAssistant system, a personal knowledge assistant for MOOC learners. MAssistant helps users to trace the concepts they have learned in MOOCs, and to build their own concept graphs. There are three key components in MAssistant: (i) a large-scale concept graph built from open data sources, which contains concepts in various domains and relations among them; (ii) a browser extension which interacts with learners when they are watching video lectures, and presents important concepts to them; (iii) a web application allowing users to explore their personal concept graphs, which are built based on their learning activities on MOOCs. MAssistant will facilitate the knowledge management task for MOOC learners, and make the learning on MOOCs easier.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1755058318,"Goal":"Quality Education","Task":["MOOC learners","large - scale concept graph","knowledge management task","MOOC learners","MOOCs"],"Method":["Personal Knowledge Assistant","MAssistant","personal knowledge assistant","MAssistant","MAssistant","browser extension","web application","MAssistant"]},{"ID":"hambardzumyan-etal-2020-yerevanns","title":"{Y}ereva{NN}{'}s Systems for {WMT}20 Biomedical Translation Task: The Effect of Fixing Misaligned Sentence Pairs","abstract":"This report describes YerevaNN{'}s neural machine translation systems and data processing pipelines developed for WMT20 biomedical translation task. We provide systems for English-Russian and English-German language pairs. For the English-Russian pair, our submissions achieve the best BLEU scores, with en$\\rightarrow$ru direction outperforming the other systems by a significant margin. We explain most of the improvements by our heavy data preprocessing pipeline which attempts to fix poorly aligned sentences in the parallel data.","year":2020,"title_abstract":"{Y}ereva{NN}{'}s Systems for {WMT}20 Biomedical Translation Task: The Effect of Fixing Misaligned Sentence Pairs This report describes YerevaNN{'}s neural machine translation systems and data processing pipelines developed for WMT20 biomedical translation task. We provide systems for English-Russian and English-German language pairs. For the English-Russian pair, our submissions achieve the best BLEU scores, with en$\\rightarrow$ru direction outperforming the other systems by a significant margin. We explain most of the improvements by our heavy data preprocessing pipeline which attempts to fix poorly aligned sentences in the parallel data.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1754322052,"Goal":"Gender Equality","Task":["Translation Task","translation","WMT20 biomedical translation task"],"Method":["data processing pipelines","data preprocessing pipeline"]},{"ID":"vesik-etal-2020-one","title":"One Model to Pronounce Them All: Multilingual Grapheme-to-Phoneme Conversion With a Transformer Ensemble","abstract":"The task of grapheme-to-phoneme (G2P) conversion is important for both speech recognition and synthesis. Similar to other speech and language processing tasks, in a scenario where only small-sized training data are available, learning G2P models is challenging. We describe a simple approach of exploiting model ensembles, based on multilingual Transformers and self-training, to develop a highly effective G2P solution for 15 languages. Our models are developed as part of our participation in the SIGMORPHON 2020 Shared Task 1 focused at G2P. Our best models achieve 14.99 word error rate (WER) and 3.30 phoneme error rate (PER), a sizeable improvement over the shared task competitive baselines.","year":2020,"title_abstract":"One Model to Pronounce Them All: Multilingual Grapheme-to-Phoneme Conversion With a Transformer Ensemble The task of grapheme-to-phoneme (G2P) conversion is important for both speech recognition and synthesis. Similar to other speech and language processing tasks, in a scenario where only small-sized training data are available, learning G2P models is challenging. We describe a simple approach of exploiting model ensembles, based on multilingual Transformers and self-training, to develop a highly effective G2P solution for 15 languages. Our models are developed as part of our participation in the SIGMORPHON 2020 Shared Task 1 focused at G2P. Our best models achieve 14.99 word error rate (WER) and 3.30 phoneme error rate (PER), a sizeable improvement over the shared task competitive baselines.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1753903627,"Goal":"Gender Equality","Task":["Multilingual Grapheme - to - Phoneme Conversion","grapheme - to - phoneme","conversion","speech recognition","synthesis","speech and language processing tasks","SIGMORPHON 2020 Shared Task","G2P"],"Method":["Transformer Ensemble","G2P models","model ensembles","multilingual Transformers","self - training","G2P solution"]},{"ID":"nikiforova-etal-2020-geo","title":"Geo-Aware Image Caption Generation","abstract":"Standard image caption generation systems produce generic descriptions of images and do not utilize any contextual information or world knowledge. In particular, they are unable to generate captions that contain references to the geographic context of an image, for example, the location where a photograph is taken or relevant geographic objects around an image location. In this paper, we develop a geo-aware image caption generation system, which incorporates geographic contextual information into a standard image captioning pipeline. We propose a way to build an image-specific representation of the geographic context and adapt the caption generation network to produce appropriate geographic names in the image descriptions. We evaluate our system on a novel captioning dataset that contains contextualized captions and geographic metadata and achieve substantial improvements in BLEU, ROUGE, METEOR and CIDEr scores. We also introduce a new metric to assess generated geographic references directly and empirically demonstrate our system{'}s ability to produce captions with relevant and factually accurate geographic referencing.","year":2020,"title_abstract":"Geo-Aware Image Caption Generation Standard image caption generation systems produce generic descriptions of images and do not utilize any contextual information or world knowledge. In particular, they are unable to generate captions that contain references to the geographic context of an image, for example, the location where a photograph is taken or relevant geographic objects around an image location. In this paper, we develop a geo-aware image caption generation system, which incorporates geographic contextual information into a standard image captioning pipeline. We propose a way to build an image-specific representation of the geographic context and adapt the caption generation network to produce appropriate geographic names in the image descriptions. We evaluate our system on a novel captioning dataset that contains contextualized captions and geographic metadata and achieve substantial improvements in BLEU, ROUGE, METEOR and CIDEr scores. We also introduce a new metric to assess generated geographic references directly and empirically demonstrate our system{'}s ability to produce captions with relevant and factually accurate geographic referencing.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1753649414,"Goal":"Sustainable Cities and Communities","Task":["Geo - Aware Image Caption Generation","image - specific representation"],"Method":["image caption generation systems","geo - aware image caption generation system","image captioning pipeline","caption generation network"]},{"ID":"deutsch-roth-2020-sacrerouge","title":"{S}acre{ROUGE}: An Open-Source Library for Using and Developing Summarization Evaluation Metrics","abstract":"We present SacreROUGE, an open-source library for using and developing summarization evaluation metrics. SacreROUGE removes many obstacles that researchers face when using or developing metrics: (1) The library provides Python wrappers around the official implementations of existing evaluation metrics so they share a common, easy-to-use interface; (2) it provides functionality to evaluate how well any metric implemented in the library correlates to human-annotated judgments, so no additional code needs to be written for a new evaluation metric; and (3) it includes scripts for loading datasets that contain human judgments so they can easily be used for evaluation. This work describes the design of the library, including the core Metric interface, the command-line API for evaluating summarization models and metrics, and the scripts to load and reformat publicly available datasets. The development of SacreROUGE is ongoing and open to contributions from the community.","year":2020,"title_abstract":"{S}acre{ROUGE}: An Open-Source Library for Using and Developing Summarization Evaluation Metrics We present SacreROUGE, an open-source library for using and developing summarization evaluation metrics. SacreROUGE removes many obstacles that researchers face when using or developing metrics: (1) The library provides Python wrappers around the official implementations of existing evaluation metrics so they share a common, easy-to-use interface; (2) it provides functionality to evaluate how well any metric implemented in the library correlates to human-annotated judgments, so no additional code needs to be written for a new evaluation metric; and (3) it includes scripts for loading datasets that contain human judgments so they can easily be used for evaluation. This work describes the design of the library, including the core Metric interface, the command-line API for evaluating summarization models and metrics, and the scripts to load and reformat publicly available datasets. The development of SacreROUGE is ongoing and open to contributions from the community.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1753482968,"Goal":"Partnership for the Goals","Task":["summarization evaluation metrics","evaluation","summarization"],"Method":["SacreROUGE","open - source library","SacreROUGE","Python wrappers","core Metric interface","command - line API"]},{"ID":"neveol-etal-2022-french","title":"{F}rench {C}row{S}-Pairs: Extending a challenge dataset for measuring social bias in masked language models to a language other than {E}nglish","abstract":"Warning: This paper contains explicit statements of offensive stereotypes which may be upsetting.Much work on biases in natural language processing has addressed biases linked to the social and cultural experience of English speaking individuals in the United States. We seek to widen the scope of bias studies by creating material to measure social bias in language models (LMs) against specific demographic groups in France. We build on the US-centered CrowS-pairs dataset to create a multilingual stereotypes dataset that allows for comparability across languages while also characterizing biases that are specific to each country and language. We introduce 1,679 sentence pairs in French that cover stereotypes in ten types of bias like gender and age. 1,467 sentence pairs are translated from CrowS-pairs and 212 are newly crowdsourced. The sentence pairs contrast stereotypes concerning underadvantaged groups with the same sentence concerning advantaged groups. We find that four widely used language models (three French, one multilingual) favor sentences that express stereotypes in most bias categories. We report on the translation process from English into French, which led to a characterization of stereotypes in CrowS-pairs including the identification of US-centric cultural traits. We offer guidelines to further extend the dataset to other languages and cultural environments.","year":2022,"title_abstract":"{F}rench {C}row{S}-Pairs: Extending a challenge dataset for measuring social bias in masked language models to a language other than {E}nglish Warning: This paper contains explicit statements of offensive stereotypes which may be upsetting.Much work on biases in natural language processing has addressed biases linked to the social and cultural experience of English speaking individuals in the United States. We seek to widen the scope of bias studies by creating material to measure social bias in language models (LMs) against specific demographic groups in France. We build on the US-centered CrowS-pairs dataset to create a multilingual stereotypes dataset that allows for comparability across languages while also characterizing biases that are specific to each country and language. We introduce 1,679 sentence pairs in French that cover stereotypes in ten types of bias like gender and age. 1,467 sentence pairs are translated from CrowS-pairs and 212 are newly crowdsourced. The sentence pairs contrast stereotypes concerning underadvantaged groups with the same sentence concerning advantaged groups. We find that four widely used language models (three French, one multilingual) favor sentences that express stereotypes in most bias categories. We report on the translation process from English into French, which led to a characterization of stereotypes in CrowS-pairs including the identification of US-centric cultural traits. We offer guidelines to further extend the dataset to other languages and cultural environments.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1751809716,"Goal":"Reduced Inequalities","Task":["measuring social bias","natural language processing","bias studies","social bias in language models","translation process","characterization of stereotypes","identification of US - centric cultural traits"],"Method":["masked language models","language models"]},{"ID":"ning-etal-2022-meta","title":"A Meta-framework for Spatiotemporal Quantity Extraction from Text","abstract":"News events are often associated with quantities (e.g., the number of COVID-19 patients or the number of arrests in a protest), and it is often important to extract their type, time, and location from unstructured text in order to analyze these quantity events. This paper thus formulates the NLP problem of spatiotemporal quantity extraction, and proposes the first meta-framework for solving it. This meta-framework contains a formalism that decomposes the problem into several information extraction tasks, a shareable crowdsourcing pipeline, and transformer-based baseline models. We demonstrate the meta-framework in three domains{---}the COVID-19 pandemic, Black Lives Matter protests, and 2020 California wildfires{---}to show that the formalism is general and extensible, the crowdsourcing pipeline facilitates fast and high-quality data annotation, and the baseline system can handle spatiotemporal quantity extraction well enough to be practically useful. We release all resources for future research on this topic at https:\/\/github.com\/steqe.","year":2022,"title_abstract":"A Meta-framework for Spatiotemporal Quantity Extraction from Text News events are often associated with quantities (e.g., the number of COVID-19 patients or the number of arrests in a protest), and it is often important to extract their type, time, and location from unstructured text in order to analyze these quantity events. This paper thus formulates the NLP problem of spatiotemporal quantity extraction, and proposes the first meta-framework for solving it. This meta-framework contains a formalism that decomposes the problem into several information extraction tasks, a shareable crowdsourcing pipeline, and transformer-based baseline models. We demonstrate the meta-framework in three domains{---}the COVID-19 pandemic, Black Lives Matter protests, and 2020 California wildfires{---}to show that the formalism is general and extensible, the crowdsourcing pipeline facilitates fast and high-quality data annotation, and the baseline system can handle spatiotemporal quantity extraction well enough to be practically useful. We release all resources for future research on this topic at https:\/\/github.com\/steqe.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1751562059,"Goal":"Sustainable Cities and Communities","Task":["Spatiotemporal Quantity Extraction","NLP","spatiotemporal quantity extraction","information extraction tasks","COVID - 19","data annotation","spatiotemporal quantity extraction"],"Method":["Meta - framework","meta - framework","meta - framework","shareable crowdsourcing pipeline","transformer - based baseline models","meta - framework","crowdsourcing pipeline"]},{"ID":"moryossef-etal-2019-filling","title":"Filling Gender {\\&} Number Gaps in Neural Machine Translation with Black-box Context Injection","abstract":"When translating from a language that does not morphologically mark information such as gender and number into a language that does, translation systems must {``}guess{''} this missing information, often leading to incorrect translations in the given context. We propose a black-box approach for injecting the missing information to a pre-trained neural machine translation system, allowing to control the morphological variations in the generated translations without changing the underlying model or training data. We evaluate our method on an English to Hebrew translation task, and show that it is effective in injecting the gender and number information and that supplying the correct information improves the translation accuracy in up to 2.3 BLEU on a female-speaker test set for a state-of-the-art online black-box system. Finally, we perform a fine-grained syntactic analysis of the generated translations that shows the effectiveness of our method.","year":2019,"title_abstract":"Filling Gender {\\&} Number Gaps in Neural Machine Translation with Black-box Context Injection When translating from a language that does not morphologically mark information such as gender and number into a language that does, translation systems must {``}guess{''} this missing information, often leading to incorrect translations in the given context. We propose a black-box approach for injecting the missing information to a pre-trained neural machine translation system, allowing to control the morphological variations in the generated translations without changing the underlying model or training data. We evaluate our method on an English to Hebrew translation task, and show that it is effective in injecting the gender and number information and that supplying the correct information improves the translation accuracy in up to 2.3 BLEU on a female-speaker test set for a state-of-the-art online black-box system. Finally, we perform a fine-grained syntactic analysis of the generated translations that shows the effectiveness of our method.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1750251949,"Goal":"Gender Equality","Task":["Filling Gender","Neural Machine Translation","translation","English to Hebrew translation task"],"Method":["Black - box Context Injection","black - box approach","neural machine translation system","online black - box system","syntactic analysis"]},{"ID":"spiliopoulou-etal-2021-novel","title":"A Novel Framework for Detecting Important Subevents from Crisis Events via Dynamic Semantic Graphs","abstract":"Social media is an essential tool to share information about crisis events, such as natural disasters. Event Detection aims at extracting information in the form of an event, but considers each event in isolation, without combining information across sentences or events. Many posts in Crisis NLP contain repetitive or complementary information which needs to be aggregated (e.g., the number of trapped people and their location) for disaster response. Although previous approaches in Crisis NLP aggregate information across posts, they only use shallow representations of the content (e.g., keywords), which cannot adequately represent the semantics of a crisis event and its sub-events. In this work, we propose a novel framework to extract critical sub-events from a large-scale crisis event by combining important information across relevant tweets. Our framework first converts all the tweets from a crisis event into a temporally-ordered set of graphs. Then it extracts sub-graphs that represent semantic relationships connecting verbs and nouns in 3 to 6 node sub-graphs. It does this by learning edge weights via Dynamic Graph Convolutional Networks (DGCNs) and extracting smaller, relevant sub-graphs. Our experiments show that our extracted structures (1) are semantically meaningful sub-events and (2) contain information important for the large crisis-event. Furthermore, we show that our approach significantly outperforms event detection baselines, highlighting the importance of aggregating information across tweets for our task.","year":2021,"title_abstract":"A Novel Framework for Detecting Important Subevents from Crisis Events via Dynamic Semantic Graphs Social media is an essential tool to share information about crisis events, such as natural disasters. Event Detection aims at extracting information in the form of an event, but considers each event in isolation, without combining information across sentences or events. Many posts in Crisis NLP contain repetitive or complementary information which needs to be aggregated (e.g., the number of trapped people and their location) for disaster response. Although previous approaches in Crisis NLP aggregate information across posts, they only use shallow representations of the content (e.g., keywords), which cannot adequately represent the semantics of a crisis event and its sub-events. In this work, we propose a novel framework to extract critical sub-events from a large-scale crisis event by combining important information across relevant tweets. Our framework first converts all the tweets from a crisis event into a temporally-ordered set of graphs. Then it extracts sub-graphs that represent semantic relationships connecting verbs and nouns in 3 to 6 node sub-graphs. It does this by learning edge weights via Dynamic Graph Convolutional Networks (DGCNs) and extracting smaller, relevant sub-graphs. Our experiments show that our extracted structures (1) are semantically meaningful sub-events and (2) contain information important for the large crisis-event. Furthermore, we show that our approach significantly outperforms event detection baselines, highlighting the importance of aggregating information across tweets for our task.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.175021261,"Goal":"Climate Action","Task":["Detecting Important Subevents","Crisis Events","Dynamic Semantic Graphs","Event Detection","Crisis NLP","disaster response","Crisis NLP","critical sub - events","large - scale crisis event"],"Method":["shallow representations","Dynamic Graph Convolutional Networks","event detection baselines"]},{"ID":"johannessen-etal-2010-enhancing","title":"Enhancing Language Resources with Maps","abstract":"We will look at how maps can be integrated in research resources, such as language databases and language corpora. By using maps, search results can be illustrated in a way that immediately gives the user information that words or numbers on their own would not give. We will illustrate with two different resources, into which we have now added a Google Maps application: The Nordic Dialect Corpus (Johannessen et al. 2009) and The Nordic Syntactic Judgments Database (Lindstad et al. 2009). We have integrated Google Maps into these applications. The database contains some hundred syntactic test sentences that have been evaluated by four speakers in more than hundred locations in Norway and Sweden. Searching for the evaluations of a particular sentence gives a list of several hundred judgments, which are difficult for a human researcher to assess. With the map option, isoglosses are immediately visible. We show in the paper that both with the maps depicting corpus hits and with the maps depicting database results, the map visualizations actually show clear geographical differences that would be very difficult to spot just by reading concordance lines or database tables.","year":2010,"title_abstract":"Enhancing Language Resources with Maps We will look at how maps can be integrated in research resources, such as language databases and language corpora. By using maps, search results can be illustrated in a way that immediately gives the user information that words or numbers on their own would not give. We will illustrate with two different resources, into which we have now added a Google Maps application: The Nordic Dialect Corpus (Johannessen et al. 2009) and The Nordic Syntactic Judgments Database (Lindstad et al. 2009). We have integrated Google Maps into these applications. The database contains some hundred syntactic test sentences that have been evaluated by four speakers in more than hundred locations in Norway and Sweden. Searching for the evaluations of a particular sentence gives a list of several hundred judgments, which are difficult for a human researcher to assess. With the map option, isoglosses are immediately visible. We show in the paper that both with the maps depicting corpus hits and with the maps depicting database results, the map visualizations actually show clear geographical differences that would be very difficult to spot just by reading concordance lines or database tables.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1749891937,"Goal":"Sustainable Cities and Communities","Task":["Enhancing Language Resources"],"Method":["Google Maps application","Google Maps","map visualizations"]},{"ID":"soldaini-etal-2018-helping","title":"Helping or Hurting? Predicting Changes in Users{'} Risk of Self-Harm Through Online Community Interactions","abstract":"In recent years, online communities have formed around suicide and self-harm prevention. While these communities offer support in moment of crisis, they can also normalize harmful behavior, discourage professional treatment, and instigate suicidal ideation. In this work, we focus on how interaction with others in such a community affects the mental state of users who are seeking support. We first build a dataset of conversation threads between users in a distressed state and community members offering support. We then show how to construct a classifier to predict whether distressed users are helped or harmed by the interactions in the thread, and we achieve a macro-F1 score of up to 0.69.","year":2018,"title_abstract":"Helping or Hurting? Predicting Changes in Users{'} Risk of Self-Harm Through Online Community Interactions In recent years, online communities have formed around suicide and self-harm prevention. While these communities offer support in moment of crisis, they can also normalize harmful behavior, discourage professional treatment, and instigate suicidal ideation. In this work, we focus on how interaction with others in such a community affects the mental state of users who are seeking support. We first build a dataset of conversation threads between users in a distressed state and community members offering support. We then show how to construct a classifier to predict whether distressed users are helped or harmed by the interactions in the thread, and we achieve a macro-F1 score of up to 0.69.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1749660224,"Goal":"Sustainable Cities and Communities","Task":["Predicting Changes","suicide and self - harm prevention"],"Method":["classifier"]},{"ID":"thompson-etal-2020-semantic","title":"Semantic Annotation for Improved Safety in Construction Work","abstract":"Risk management is a vital activity to ensure employee safety in construction projects. Various documents provide important supporting evidence, including details of previous incidents, consequences and mitigation strategies. Potential hazards may depend on a complex set of project-specific attributes, including activities undertaken, location, equipment used, etc. However, finding evidence about previous projects with similar attributes can be problematic, since information about risks and mitigations is usually hidden within and may be dispersed across a range of different free text documents. Automatic named entity recognition (NER), which identifies mentions of concepts in free text documents, is the first stage in structuring knowledge contained within them. While developing NER methods generally relies on annotated corpora, we are not aware of any such corpus targeted at concepts relevant to construction safety. In response, we have designed a novel named entity annotation scheme and associated guidelines for this domain, which covers hazards, consequences, mitigation strategies and project attributes. Four health and safety experts used the guidelines to annotate a total of 600 sentences from accident reports; an average inter-annotator agreement rate of 0.79 F-Score shows that our work constitutes an important first step towards developing tools for detailed semantic analysis of construction safety documents.","year":2020,"title_abstract":"Semantic Annotation for Improved Safety in Construction Work Risk management is a vital activity to ensure employee safety in construction projects. Various documents provide important supporting evidence, including details of previous incidents, consequences and mitigation strategies. Potential hazards may depend on a complex set of project-specific attributes, including activities undertaken, location, equipment used, etc. However, finding evidence about previous projects with similar attributes can be problematic, since information about risks and mitigations is usually hidden within and may be dispersed across a range of different free text documents. Automatic named entity recognition (NER), which identifies mentions of concepts in free text documents, is the first stage in structuring knowledge contained within them. While developing NER methods generally relies on annotated corpora, we are not aware of any such corpus targeted at concepts relevant to construction safety. In response, we have designed a novel named entity annotation scheme and associated guidelines for this domain, which covers hazards, consequences, mitigation strategies and project attributes. Four health and safety experts used the guidelines to annotate a total of 600 sentences from accident reports; an average inter-annotator agreement rate of 0.79 F-Score shows that our work constitutes an important first step towards developing tools for detailed semantic analysis of construction safety documents.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1749200225,"Goal":"Sustainable Cities and Communities","Task":["Semantic Annotation","Improved Safety in Construction Work","Risk management","employee safety","construction projects","Automatic named entity recognition","NER","construction safety","semantic analysis of construction safety documents"],"Method":["named entity annotation scheme"]},{"ID":"vazquez-bel-2012-classification","title":"A Classification of Adjectives for Polarity Lexicons Enhancement","abstract":"Subjective language detection is one of the most important challenges in Sentiment Analysis. Because of the weight and frequency in opinionated texts, adjectives are considered a key piece in the opinion extraction process. These subjective units are more and more frequently collected in polarity lexicons in which they appear annotated with their prior polarity. However, at the moment, any polarity lexicon takes into account prior polarity variations across domains. This paper proves that a majority of adjectives change their prior polarity value depending on the domain. We propose a distinction between domain dependent and domain independent adjectives. Moreover, our analysis led us to propose a further classification related to subjectivity degree: constant, mixed and highly subjective adjectives. Following this classification, polarity values will be a better support for Sentiment Analysis.","year":2012,"title_abstract":"A Classification of Adjectives for Polarity Lexicons Enhancement Subjective language detection is one of the most important challenges in Sentiment Analysis. Because of the weight and frequency in opinionated texts, adjectives are considered a key piece in the opinion extraction process. These subjective units are more and more frequently collected in polarity lexicons in which they appear annotated with their prior polarity. However, at the moment, any polarity lexicon takes into account prior polarity variations across domains. This paper proves that a majority of adjectives change their prior polarity value depending on the domain. We propose a distinction between domain dependent and domain independent adjectives. Moreover, our analysis led us to propose a further classification related to subjectivity degree: constant, mixed and highly subjective adjectives. Following this classification, polarity values will be a better support for Sentiment Analysis.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1748096943,"Goal":"Reduced Inequalities","Task":["Classification of Adjectives","Polarity Lexicons","Subjective language detection","Sentiment Analysis","opinion extraction process","classification","Sentiment Analysis"],"Method":["polarity lexicon"]},{"ID":"bosco-etal-2016-tweeting","title":"Tweeting and Being Ironic in the Debate about a Political Reform: the {F}rench Annotated Corpus {TW}itter-{M}ariage{P}our{T}ous","abstract":"The paper introduces a new annotated French data set for Sentiment Analysis, which is a currently missing resource. It focuses on the collection from Twitter of data related to the socio-political debate about the reform of the bill for wedding in France. The design of the annotation scheme is described, which extends a polarity label set by making available tags for marking target semantic areas and figurative language devices. The annotation process is presented and the disagreement discussed, in particular, in the perspective of figurative language use and in that of the semantic oriented annotation, which are open challenges for NLP systems.","year":2016,"title_abstract":"Tweeting and Being Ironic in the Debate about a Political Reform: the {F}rench Annotated Corpus {TW}itter-{M}ariage{P}our{T}ous The paper introduces a new annotated French data set for Sentiment Analysis, which is a currently missing resource. It focuses on the collection from Twitter of data related to the socio-political debate about the reform of the bill for wedding in France. The design of the annotation scheme is described, which extends a polarity label set by making available tags for marking target semantic areas and figurative language devices. The annotation process is presented and the disagreement discussed, in particular, in the perspective of figurative language use and in that of the semantic oriented annotation, which are open challenges for NLP systems.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1744412333,"Goal":"Reduced Inequalities","Task":["Political Reform","Sentiment Analysis","annotation process","figurative language use","semantic oriented annotation","NLP"],"Method":["annotation scheme"]},{"ID":"mubarak-hassan-2021-arcorona","title":"{A}r{C}orona: Analyzing {A}rabic Tweets in the Early Days of Coronavirus ({COVID}-19) Pandemic","abstract":"Over the past few months, there were huge numbers of circulating tweets and discussions about Coronavirus (COVID-19) in the Arab region. It is important for policy makers and many people to identify types of shared tweets to better understand public behavior, topics of interest, requests from governments, sources of tweets, etc. It is also crucial to prevent spreading of rumors and misinformation about the virus or bad cures. To this end, we present the largest manually annotated dataset of Arabic tweets related to COVID-19. We describe annotation guidelines, analyze our dataset and build effective machine learning and transformer based models for classification.","year":2021,"title_abstract":"{A}r{C}orona: Analyzing {A}rabic Tweets in the Early Days of Coronavirus ({COVID}-19) Pandemic Over the past few months, there were huge numbers of circulating tweets and discussions about Coronavirus (COVID-19) in the Arab region. It is important for policy makers and many people to identify types of shared tweets to better understand public behavior, topics of interest, requests from governments, sources of tweets, etc. It is also crucial to prevent spreading of rumors and misinformation about the virus or bad cures. To this end, we present the largest manually annotated dataset of Arabic tweets related to COVID-19. We describe annotation guidelines, analyze our dataset and build effective machine learning and transformer based models for classification.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1744391918,"Goal":"Climate Action","Task":["Coronavirus ({COVID} - 19)","classification"],"Method":["machine learning","transformer based models"]},{"ID":"paz-argaman-tsarfaty-2019-run","title":"{RUN} through the Streets: A New Dataset and Baseline Models for Realistic Urban Navigation","abstract":"Following navigation instructions in natural language (NL) requires a composition of language, action, and knowledge of the environment. Knowledge of the environment may be provided via visual sensors or as a symbolic world representation referred to as a map. Previous work on map-based NL navigation relied on small artificial worlds with a fixed set of entities known in advance. Here we introduce the Realistic Urban Navigation (RUN) task, aimed at interpreting NL navigation instructions based on a real, dense, urban map. Using Amazon Mechanical Turk, we collected a dataset of 2515 instructions aligned with actual routes over three regions of Manhattan. We then empirically study which aspects of a neural architecture are important for the RUN success, and empirically show that entity abstraction, attention over words and worlds, and a constantly updating world-state, significantly contribute to task accuracy.","year":2019,"title_abstract":"{RUN} through the Streets: A New Dataset and Baseline Models for Realistic Urban Navigation Following navigation instructions in natural language (NL) requires a composition of language, action, and knowledge of the environment. Knowledge of the environment may be provided via visual sensors or as a symbolic world representation referred to as a map. Previous work on map-based NL navigation relied on small artificial worlds with a fixed set of entities known in advance. Here we introduce the Realistic Urban Navigation (RUN) task, aimed at interpreting NL navigation instructions based on a real, dense, urban map. Using Amazon Mechanical Turk, we collected a dataset of 2515 instructions aligned with actual routes over three regions of Manhattan. We then empirically study which aspects of a neural architecture are important for the RUN success, and empirically show that entity abstraction, attention over words and worlds, and a constantly updating world-state, significantly contribute to task accuracy.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.174252674,"Goal":"Sustainable Cities and Communities","Task":["Realistic Urban Navigation","navigation instructions","map - based NL navigation","Realistic Urban Navigation","NL navigation instructions","RUN"],"Method":["symbolic world representation","neural architecture"]},{"ID":"kotamraju-blanco-2021-written-justifications","title":"Written Justifications are Key to Aggregate Crowdsourced Forecasts","abstract":"This paper demonstrates that aggregating crowdsourced forecasts benefits from modeling the written justifications provided by forecasters. Our experiments show that the majority and weighted vote baselines are competitive, and that the written justifications are beneficial to call a question throughout its life except in the last quarter. We also conduct an error analysis shedding light into the characteristics that make a justification unreliable.","year":2021,"title_abstract":"Written Justifications are Key to Aggregate Crowdsourced Forecasts This paper demonstrates that aggregating crowdsourced forecasts benefits from modeling the written justifications provided by forecasters. Our experiments show that the majority and weighted vote baselines are competitive, and that the written justifications are beneficial to call a question throughout its life except in the last quarter. We also conduct an error analysis shedding light into the characteristics that make a justification unreliable.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1742447168,"Goal":"Climate Action","Task":["Aggregate Crowdsourced Forecasts","aggregating crowdsourced forecasts"],"Method":["majority and weighted vote baselines","error analysis"]},{"ID":"xiang-etal-2021-three","title":"A Three-step Method for Multi-Hop Inference Explanation Regeneration","abstract":"Multi-hop inference for explanation generation is to combine two or more facts to make an inference. The task focuses on generating explanations for elementary science questions. In the task, the relevance between the explanations and the QA pairs is of vital importance. To address the task, a three-step framework is proposed. Firstly, vector distance between two texts is utilized to recall the top-K relevant explanations for each question, reducing the calculation consumption. Then, a selection module is employed to choose those most relative facts in an autoregressive manner, giving a preliminary order for the retrieved facts. Thirdly, we adopt a re-ranking module to re-rank the retrieved candidate explanations with relevance between each fact and the QA pairs. Experimental results illustrate the effectiveness of the proposed framework with an improvement of 39.78{\\%} in NDCG over the official baseline.","year":2021,"title_abstract":"A Three-step Method for Multi-Hop Inference Explanation Regeneration Multi-hop inference for explanation generation is to combine two or more facts to make an inference. The task focuses on generating explanations for elementary science questions. In the task, the relevance between the explanations and the QA pairs is of vital importance. To address the task, a three-step framework is proposed. Firstly, vector distance between two texts is utilized to recall the top-K relevant explanations for each question, reducing the calculation consumption. Then, a selection module is employed to choose those most relative facts in an autoregressive manner, giving a preliminary order for the retrieved facts. Thirdly, we adopt a re-ranking module to re-rank the retrieved candidate explanations with relevance between each fact and the QA pairs. Experimental results illustrate the effectiveness of the proposed framework with an improvement of 39.78{\\%} in NDCG over the official baseline.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1741409749,"Goal":"Quality Education","Task":["Multi - Hop Inference Explanation","Multi - hop inference","explanation generation","inference","explanations","NDCG"],"Method":["Three - step Method","selection module","autoregressive manner","re - ranking module"]},{"ID":"saito-2018-curriculum","title":"Curriculum Learning Based on Reward Sparseness for Deep Reinforcement Learning of Task Completion Dialogue Management","abstract":"Learning from sparse and delayed reward is a central issue in reinforcement learning. In this paper, to tackle reward sparseness problem of task oriented dialogue management, we propose a curriculum based approach on the number of slots of user goals. This curriculum makes it possible to learn dialogue management for sets of user goals with large number of slots. We also propose a dialogue policy based on progressive neural networks whose modules with parameters are appended with previous parameters fixed as the curriculum proceeds, and this policy improves performances over the one with single set of parameters.","year":2018,"title_abstract":"Curriculum Learning Based on Reward Sparseness for Deep Reinforcement Learning of Task Completion Dialogue Management Learning from sparse and delayed reward is a central issue in reinforcement learning. In this paper, to tackle reward sparseness problem of task oriented dialogue management, we propose a curriculum based approach on the number of slots of user goals. This curriculum makes it possible to learn dialogue management for sets of user goals with large number of slots. We also propose a dialogue policy based on progressive neural networks whose modules with parameters are appended with previous parameters fixed as the curriculum proceeds, and this policy improves performances over the one with single set of parameters.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1741354316,"Goal":"Quality Education","Task":["Deep Reinforcement Learning of Task Completion","Dialogue Management","sparse and delayed reward","reinforcement learning","reward sparseness problem of task oriented dialogue management","dialogue management"],"Method":["Curriculum Learning","Reward Sparseness","curriculum based approach","dialogue policy","progressive neural networks"]},{"ID":"krishna-etal-2022-measuring","title":"Measuring Fairness of Text Classifiers via Prediction Sensitivity","abstract":"With the rapid growth in language processing applications, fairness has emerged as an important consideration in data-driven solutions. Although various fairness definitions have been explored in the recent literature, there is lack of consensus on which metrics most accurately reflect the fairness of a system. In this work, we propose a new formulation {--} accumulated prediction sensitivity, which measures fairness in machine learning models based on the model{'}s prediction sensitivity to perturbations in input features. The metric attempts to quantify the extent to which a single prediction depends on a protected attribute, where the protected attribute encodes the membership status of an individual in a protected group. We show that the metric can be theoretically linked with a specific notion of group fairness (statistical parity) and individual fairness. It also correlates well with humans{'} perception of fairness. We conduct experiments on two text classification datasets {--} Jigsaw Toxicity, and Bias in Bios, and evaluate the correlations between metrics and manual annotations on whether the model produced a fair outcome. We observe that the proposed fairness metric based on prediction sensitivity is statistically significantly more correlated with human annotation than the existing counterfactual fairness metric.","year":2022,"title_abstract":"Measuring Fairness of Text Classifiers via Prediction Sensitivity With the rapid growth in language processing applications, fairness has emerged as an important consideration in data-driven solutions. Although various fairness definitions have been explored in the recent literature, there is lack of consensus on which metrics most accurately reflect the fairness of a system. In this work, we propose a new formulation {--} accumulated prediction sensitivity, which measures fairness in machine learning models based on the model{'}s prediction sensitivity to perturbations in input features. The metric attempts to quantify the extent to which a single prediction depends on a protected attribute, where the protected attribute encodes the membership status of an individual in a protected group. We show that the metric can be theoretically linked with a specific notion of group fairness (statistical parity) and individual fairness. It also correlates well with humans{'} perception of fairness. We conduct experiments on two text classification datasets {--} Jigsaw Toxicity, and Bias in Bios, and evaluate the correlations between metrics and manual annotations on whether the model produced a fair outcome. We observe that the proposed fairness metric based on prediction sensitivity is statistically significantly more correlated with human annotation than the existing counterfactual fairness metric.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1741179079,"Goal":"Reduced Inequalities","Task":["Measuring Fairness","language processing applications","fairness","data - driven solutions"],"Method":["Text Classifiers","Prediction Sensitivity","machine learning models"]},{"ID":"xie-etal-2020-exploring","title":"Exploring Question-Specific Rewards for Generating Deep Questions","abstract":"Recent question generation (QG) approaches often utilize the sequence-to-sequence framework (Seq2Seq) to optimize the log likelihood of ground-truth questions using teacher forcing. However, this training objective is inconsistent with actual question quality, which is often reflected by certain global properties such as whether the question can be answered by the document. As such, we directly optimize for QG-specific objectives via reinforcement learning to improve question quality. We design three different rewards that target to improve the fluency, relevance, and answerability of generated questions. We conduct both automatic and human evaluations in addition to thorough analysis to explore the effect of each QG-specific reward. We find that optimizing on question-specific rewards generally leads to better performance in automatic evaluation metrics. However, only the rewards that correlate well with human judgement (e.g., relevance) lead to real improvement in question quality. Optimizing for the others, especially answerability, introduces incorrect bias to the model, resulting in poorer question quality. The code is publicly available at https:\/\/github.com\/YuxiXie\/RL-for-Question-Generation.","year":2020,"title_abstract":"Exploring Question-Specific Rewards for Generating Deep Questions Recent question generation (QG) approaches often utilize the sequence-to-sequence framework (Seq2Seq) to optimize the log likelihood of ground-truth questions using teacher forcing. However, this training objective is inconsistent with actual question quality, which is often reflected by certain global properties such as whether the question can be answered by the document. As such, we directly optimize for QG-specific objectives via reinforcement learning to improve question quality. We design three different rewards that target to improve the fluency, relevance, and answerability of generated questions. We conduct both automatic and human evaluations in addition to thorough analysis to explore the effect of each QG-specific reward. We find that optimizing on question-specific rewards generally leads to better performance in automatic evaluation metrics. However, only the rewards that correlate well with human judgement (e.g., relevance) lead to real improvement in question quality. Optimizing for the others, especially answerability, introduces incorrect bias to the model, resulting in poorer question quality. The code is publicly available at https:\/\/github.com\/YuxiXie\/RL-for-Question-Generation.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1740233898,"Goal":"Quality Education","Task":["Question - Specific Rewards","Generating Deep Questions","generation","QG - specific objectives","answerability of generated questions","question - specific rewards","Question - Generation"],"Method":["sequence - to - sequence framework","reinforcement learning"]},{"ID":"eisenstein-2019-measuring","title":"Measuring and Modeling Language Change","abstract":"This tutorial is designed to help researchers answer the following sorts of questions: - Are people happier on the weekend? - What was 1861{'}s word of the year? - Are Democrats and Republicans more different than ever? - When did {``}gay{''} stop meaning {``}happy{''}? - Are gender stereotypes getting weaker, stronger, or just different? - Who is a linguistic leader? - How can we get internet users to be more polite and objective? Such questions are fundamental to the social sciences and humanities, and scholars in these disciplines are increasingly turning to computational techniques for answers. Meanwhile, the ACL community is increasingly engaged with data that varies across time, and with the social insights that can be offered by analyzing temporal patterns and trends. The purpose of this tutorial is to facilitate this convergence in two main ways: 1. By synthesizing recent computational techniques for handling and modeling temporal data, such as dynamic word embeddings, the tutorial will provide a starting point for future computational research. It will also identify useful tools for social scientists and digital humanities scholars. 2. The tutorial will provide an overview of techniques and datasets from the quantitative social sciences and the digital humanities, which are not well-known in the computational linguistics community. These techniques include vector autoregressive models, multiple comparisons corrections for hypothesis testing, and causal inference. Datasets include historical newspaper archives and corpora of contemporary political speech.","year":2019,"title_abstract":"Measuring and Modeling Language Change This tutorial is designed to help researchers answer the following sorts of questions: - Are people happier on the weekend? - What was 1861{'}s word of the year? - Are Democrats and Republicans more different than ever? - When did {``}gay{''} stop meaning {``}happy{''}? - Are gender stereotypes getting weaker, stronger, or just different? - Who is a linguistic leader? - How can we get internet users to be more polite and objective? Such questions are fundamental to the social sciences and humanities, and scholars in these disciplines are increasingly turning to computational techniques for answers. Meanwhile, the ACL community is increasingly engaged with data that varies across time, and with the social insights that can be offered by analyzing temporal patterns and trends. The purpose of this tutorial is to facilitate this convergence in two main ways: 1. By synthesizing recent computational techniques for handling and modeling temporal data, such as dynamic word embeddings, the tutorial will provide a starting point for future computational research. It will also identify useful tools for social scientists and digital humanities scholars. 2. The tutorial will provide an overview of techniques and datasets from the quantitative social sciences and the digital humanities, which are not well-known in the computational linguistics community. These techniques include vector autoregressive models, multiple comparisons corrections for hypothesis testing, and causal inference. Datasets include historical newspaper archives and corpora of contemporary political speech.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1740055382,"Goal":"Reduced Inequalities","Task":["Measuring and Modeling Language Change","social sciences and humanities","ACL community","modeling temporal data","dynamic word embeddings","computational research","social scientists","digital humanities scholars","quantitative social sciences","digital humanities","computational linguistics community","hypothesis testing","causal inference"],"Method":["computational techniques","computational techniques","vector autoregressive models","multiple comparisons corrections"]},{"ID":"su-etal-2021-discussion","title":"Discussion on the relationship between elders{'} daily conversations and cognitive executive function: using word vectors and regression models","abstract":"As the average life expectancy of Chinese people rises, the health care problems of the elderly are becoming more diverse, and the demand for long-term care is also increasing. Therefore, how to help the elderly have a good quality of life and maintain their dignity is what we need to think about. This research intends to explore the characteristics of natural language of normal aging people through a deep model. First, we collect information through focus groups so that the elders can naturally interact with other participants in the process. Then, through the word vector model and regression model, an executive function prediction model based on dialogue data is established to help understand the degradation trajectory of executive function and establish an early warning.","year":2021,"title_abstract":"Discussion on the relationship between elders{'} daily conversations and cognitive executive function: using word vectors and regression models As the average life expectancy of Chinese people rises, the health care problems of the elderly are becoming more diverse, and the demand for long-term care is also increasing. Therefore, how to help the elderly have a good quality of life and maintain their dignity is what we need to think about. This research intends to explore the characteristics of natural language of normal aging people through a deep model. First, we collect information through focus groups so that the elders can naturally interact with other participants in the process. Then, through the word vector model and regression model, an executive function prediction model based on dialogue data is established to help understand the degradation trajectory of executive function and establish an early warning.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.17370148,"Goal":"Good Health and Well-Being","Task":["health care problems","early warning"],"Method":["word vectors","regression models","deep model","word vector model","regression model","executive function prediction model"]},{"ID":"sharifirad-etal-2018-boosting","title":"Boosting Text Classification Performance on Sexist Tweets by Text Augmentation and Text Generation Using a Combination of Knowledge Graphs","abstract":"Text classification models have been heavily utilized for a slew of interesting natural language processing problems. Like any other machine learning model, these classifiers are very dependent on the size and quality of the training dataset. Insufficient and imbalanced datasets will lead to poor performance. An interesting solution to poor datasets is to take advantage of the world knowledge in the form of knowledge graphs to improve our training data. In this paper, we use ConceptNet and Wikidata to improve sexist tweet classification by two methods (1) text augmentation and (2) text generation. In our text generation approach, we generate new tweets by replacing words using data acquired from ConceptNet relations in order to increase the size of our training set, this method is very helpful with frustratingly small datasets, preserves the label and increases diversity. In our text augmentation approach, the number of tweets remains the same but their words are augmented (concatenation) with words extracted from their ConceptNet relations and their description extracted from Wikidata. In our text augmentation approach, the number of tweets in each class remains the same but the range of each tweet increases. Our experiments show that our approach improves sexist tweet classification significantly in our entire machine learning models. Our approach can be readily applied to any other small dataset size like hate speech or abusive language and text classification problem using any machine learning model.","year":2018,"title_abstract":"Boosting Text Classification Performance on Sexist Tweets by Text Augmentation and Text Generation Using a Combination of Knowledge Graphs Text classification models have been heavily utilized for a slew of interesting natural language processing problems. Like any other machine learning model, these classifiers are very dependent on the size and quality of the training dataset. Insufficient and imbalanced datasets will lead to poor performance. An interesting solution to poor datasets is to take advantage of the world knowledge in the form of knowledge graphs to improve our training data. In this paper, we use ConceptNet and Wikidata to improve sexist tweet classification by two methods (1) text augmentation and (2) text generation. In our text generation approach, we generate new tweets by replacing words using data acquired from ConceptNet relations in order to increase the size of our training set, this method is very helpful with frustratingly small datasets, preserves the label and increases diversity. In our text augmentation approach, the number of tweets remains the same but their words are augmented (concatenation) with words extracted from their ConceptNet relations and their description extracted from Wikidata. In our text augmentation approach, the number of tweets in each class remains the same but the range of each tweet increases. Our experiments show that our approach improves sexist tweet classification significantly in our entire machine learning models. Our approach can be readily applied to any other small dataset size like hate speech or abusive language and text classification problem using any machine learning model.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1735483408,"Goal":"Gender Equality","Task":["Boosting Text Classification","Text Augmentation","Text Generation","natural language processing problems","sexist tweet classification","text augmentation","generation","sexist tweet classification","text classification problem"],"Method":["Combination of Knowledge Graphs","Text classification models","machine learning model","text generation approach","text augmentation approach","text augmentation approach","machine learning models","machine learning model"]},{"ID":"barbosa-etal-2019-enabling","title":"Enabling Search and Collaborative Assembly of Causal Interactions Extracted from Multilingual and Multi-domain Free Text","abstract":"Many of the most pressing current research problems (e.g., public health, food security, or climate change) require multi-disciplinary collaborations. In order to facilitate this process, we propose a system that incorporates multi-domain extractions of causal interactions into a single searchable knowledge graph. Our system enables users to search iteratively over direct and indirect connections in this knowledge graph, and collaboratively build causal models in real time. To enable the aggregation of causal information from multiple languages, we extend an open-domain machine reader to Portuguese. The new Portuguese reader extracts over 600 thousand causal statements from 120 thousand Portuguese publications with a precision of 62{\\%}, which demonstrates the value of mining multilingual scientific information.","year":2019,"title_abstract":"Enabling Search and Collaborative Assembly of Causal Interactions Extracted from Multilingual and Multi-domain Free Text Many of the most pressing current research problems (e.g., public health, food security, or climate change) require multi-disciplinary collaborations. In order to facilitate this process, we propose a system that incorporates multi-domain extractions of causal interactions into a single searchable knowledge graph. Our system enables users to search iteratively over direct and indirect connections in this knowledge graph, and collaboratively build causal models in real time. To enable the aggregation of causal information from multiple languages, we extend an open-domain machine reader to Portuguese. The new Portuguese reader extracts over 600 thousand causal statements from 120 thousand Portuguese publications with a precision of 62{\\%}, which demonstrates the value of mining multilingual scientific information.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1734264344,"Goal":"Climate Action","Task":["Search","Collaborative Assembly of Causal Interactions","public health","food security","climate change)","multi - domain extractions of causal interactions","aggregation of causal information","mining multilingual scientific information"],"Method":["searchable knowledge graph","causal models","open - domain machine reader","Portuguese reader"]},{"ID":"conforti-etal-2018-towards","title":"Towards Automatic Fake News Detection: Cross-Level Stance Detection in News Articles","abstract":"In this paper, we propose to adapt the four-staged pipeline proposed by Zubiaga et al. (2018) for the Rumor Verification task to the problem of Fake News Detection. We show that the recently released FNC-1 corpus covers two of its steps, namely the \\textit{Tracking} and the \\textit{Stance Detection} task. We identify asymmetry in length in the input to be a key characteristic of the latter step, when adapted to the framework of Fake News Detection, and propose to handle it as a specific type of \\textit{Cross-Level Stance Detection}. Inspired by theories from the field of Journalism Studies, we implement and test two architectures to successfully model the internal structure of an article and its interactions with a claim.","year":2018,"title_abstract":"Towards Automatic Fake News Detection: Cross-Level Stance Detection in News Articles In this paper, we propose to adapt the four-staged pipeline proposed by Zubiaga et al. (2018) for the Rumor Verification task to the problem of Fake News Detection. We show that the recently released FNC-1 corpus covers two of its steps, namely the \\textit{Tracking} and the \\textit{Stance Detection} task. We identify asymmetry in length in the input to be a key characteristic of the latter step, when adapted to the framework of Fake News Detection, and propose to handle it as a specific type of \\textit{Cross-Level Stance Detection}. Inspired by theories from the field of Journalism Studies, we implement and test two architectures to successfully model the internal structure of an article and its interactions with a claim.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1732918024,"Goal":"Climate Action","Task":["Automatic Fake News Detection","Cross - Level Stance Detection","Rumor Verification task","Fake News Detection","\\textit{Stance Detection} task","Fake News Detection","\\textit{Cross - Level Stance Detection}","Journalism Studies"],"Method":["four - staged pipeline","\\textit{Tracking}"]},{"ID":"rikters-etal-2020-university","title":"The {U}niversity of {T}okyo{'}s Submissions to the {WAT} 2020 Shared Task","abstract":"The paper describes the development process of the The University of Tokyo{'}s NMT systems that were submitted to the WAT 2020 Document-level Business Scene Dialogue Translation sub-task. We describe the data processing workflow, NMT system training architectures, and automatic evaluation results. For the WAT 2020 shared task, we submitted 12 systems (both constrained and unconstrained) for English-Japanese and Japanese-English translation directions. The submitted systems were trained using Transformer models and one was a SMT baseline.","year":2020,"title_abstract":"The {U}niversity of {T}okyo{'}s Submissions to the {WAT} 2020 Shared Task The paper describes the development process of the The University of Tokyo{'}s NMT systems that were submitted to the WAT 2020 Document-level Business Scene Dialogue Translation sub-task. We describe the data processing workflow, NMT system training architectures, and automatic evaluation results. For the WAT 2020 shared task, we submitted 12 systems (both constrained and unconstrained) for English-Japanese and Japanese-English translation directions. The submitted systems were trained using Transformer models and one was a SMT baseline.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1732787639,"Goal":"Partnership for the Goals","Task":["NMT","WAT 2020 Document - level Business Scene Dialogue Translation sub - task","NMT","automatic evaluation","WAT 2020 shared task","English - Japanese","Japanese - English translation directions"],"Method":["data processing workflow","Transformer models","SMT baseline"]},{"ID":"kuhn-2016-flexible","title":"Flexible and Reliable Text Analytics in the Digital Humanities {--} Some Methodological Considerations","abstract":"The availability of Language Technology Resources and Tools generates a considerable methodological potential in the Digital Humanities: aspects of research questions from the Humanities and Social Sciences can be addressed on text collections in ways that were unavailable to traditional approaches. I start this talk by sketching some sample scenarios of Digital Humanities projects which involve various Humanities and Social Science disciplines, noting that the potential for a meaningful contribution to higher-level questions is highest when the employed language technological models are carefully tailored both (a) to characteristics of the given target corpus, and (b) to relevant analytical subtasks feeding the discipline-specific research questions. Keeping up a multidisciplinary perspective, I then point out a recurrent dilemma in Digital Humanities projects that follow the conventional set-up of collaboration: to build high-quality computational models for the data, fixed analytical targets should be specified as early as possible {--} but to be able to respond to Humanities questions as they evolve over the course of analysis, the analytical machinery should be kept maximally flexible. To reach both, I argue for a novel collaborative culture that rests on a more interleaved, continuous dialogue. (Re-)Specification of analytical targets should be an ongoing process in which the Humanities Scholars and Social Scientists play a role that is as important as the Computational Scientists{'} role. A promising approach lies in the identification of re-occurring types of analytical subtasks, beyond linguistic standard tasks, which can form building blocks for text analysis across disciplines, and for which corpus-based characterizations (viz. annotations) can be collected, compared and revised. On such grounds, computational modeling is more directly tied to the evolving research questions, and hence the seemingly opposing needs of reliable target specifications vs. {``}malleable{''} frameworks of analysis can be reconciled. Experimental work following this approach is under way in the Center for Reflected Text Analytics (CRETA) in Stuttgart.","year":2016,"title_abstract":"Flexible and Reliable Text Analytics in the Digital Humanities {--} Some Methodological Considerations The availability of Language Technology Resources and Tools generates a considerable methodological potential in the Digital Humanities: aspects of research questions from the Humanities and Social Sciences can be addressed on text collections in ways that were unavailable to traditional approaches. I start this talk by sketching some sample scenarios of Digital Humanities projects which involve various Humanities and Social Science disciplines, noting that the potential for a meaningful contribution to higher-level questions is highest when the employed language technological models are carefully tailored both (a) to characteristics of the given target corpus, and (b) to relevant analytical subtasks feeding the discipline-specific research questions. Keeping up a multidisciplinary perspective, I then point out a recurrent dilemma in Digital Humanities projects that follow the conventional set-up of collaboration: to build high-quality computational models for the data, fixed analytical targets should be specified as early as possible {--} but to be able to respond to Humanities questions as they evolve over the course of analysis, the analytical machinery should be kept maximally flexible. To reach both, I argue for a novel collaborative culture that rests on a more interleaved, continuous dialogue. (Re-)Specification of analytical targets should be an ongoing process in which the Humanities Scholars and Social Scientists play a role that is as important as the Computational Scientists{'} role. A promising approach lies in the identification of re-occurring types of analytical subtasks, beyond linguistic standard tasks, which can form building blocks for text analysis across disciplines, and for which corpus-based characterizations (viz. annotations) can be collected, compared and revised. On such grounds, computational modeling is more directly tied to the evolving research questions, and hence the seemingly opposing needs of reliable target specifications vs. {``}malleable{''} frameworks of analysis can be reconciled. Experimental work following this approach is under way in the Center for Reflected Text Analytics (CRETA) in Stuttgart.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1732451916,"Goal":"Sustainable Cities and Communities","Task":["Flexible and Reliable Text Analytics","Digital Humanities projects","Digital Humanities projects","analytical subtasks","linguistic standard tasks","text analysis","analysis","Reflected Text Analytics"],"Method":["language technological models","computational models","analytical machinery","collaborative culture","computational modeling"]},{"ID":"santos-paraboni-2019-moral","title":"Moral Stance Recognition and Polarity Classification from {T}witter and Elicited Text","abstract":"We introduce a labelled corpus of stances about moral issues for the Brazilian Portuguese language, and present reference results for both the stance recognition and polarity classification tasks. The corpus is built from Twitter and further expanded with data elicited through crowd sourcing and labelled by their own authors. Put together, the corpus and reference results are expected to be taken as a baseline for further studies in the field of stance recognition and polarity classification from text.","year":2019,"title_abstract":"Moral Stance Recognition and Polarity Classification from {T}witter and Elicited Text We introduce a labelled corpus of stances about moral issues for the Brazilian Portuguese language, and present reference results for both the stance recognition and polarity classification tasks. The corpus is built from Twitter and further expanded with data elicited through crowd sourcing and labelled by their own authors. Put together, the corpus and reference results are expected to be taken as a baseline for further studies in the field of stance recognition and polarity classification from text.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.173151195,"Goal":"Climate Action","Task":["Moral Stance Recognition","Polarity Classification","stance recognition","polarity classification tasks","stance recognition","polarity classification"],"Method":["{T}witter"]},{"ID":"voita-etal-2018-context","title":"Context-Aware Neural Machine Translation Learns Anaphora Resolution","abstract":"Standard machine translation systems process sentences in isolation and hence ignore extra-sentential information, even though extended context can both prevent mistakes in ambiguous cases and improve translation coherence. We introduce a context-aware neural machine translation model designed in such way that the flow of information from the extended context to the translation model can be controlled and analyzed. We experiment with an English-Russian subtitles dataset, and observe that much of what is captured by our model deals with improving pronoun translation. We measure correspondences between induced attention distributions and coreference relations and observe that the model implicitly captures anaphora. It is consistent with gains for sentences where pronouns need to be gendered in translation. Beside improvements in anaphoric cases, the model also improves in overall BLEU, both over its context-agnostic version (+0.7) and over simple concatenation of the context and source sentences (+0.6).","year":2018,"title_abstract":"Context-Aware Neural Machine Translation Learns Anaphora Resolution Standard machine translation systems process sentences in isolation and hence ignore extra-sentential information, even though extended context can both prevent mistakes in ambiguous cases and improve translation coherence. We introduce a context-aware neural machine translation model designed in such way that the flow of information from the extended context to the translation model can be controlled and analyzed. We experiment with an English-Russian subtitles dataset, and observe that much of what is captured by our model deals with improving pronoun translation. We measure correspondences between induced attention distributions and coreference relations and observe that the model implicitly captures anaphora. It is consistent with gains for sentences where pronouns need to be gendered in translation. Beside improvements in anaphoric cases, the model also improves in overall BLEU, both over its context-agnostic version (+0.7) and over simple concatenation of the context and source sentences (+0.6).","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1730675101,"Goal":"Gender Equality","Task":["Context - Aware Neural Machine Translation","Anaphora Resolution","machine translation","translation","pronoun translation","translation"],"Method":["context - aware neural machine translation model","translation model","context - agnostic version"]},{"ID":"mccready-henderson-2020-social","title":"Social Meaning in Repeated Interactions","abstract":"Judgements about communicative agents evolve over the course of interactions both in how individuals are judged for testimonial reliability and for (ideological) trustworthiness. This paper combines a theory of social meaning and persona with a theory of reliability within a game-theoretic view of communication, giving a formal model involving interactional histories, repeated game models and ways of evaluating social meaning and trustworthiness.","year":2020,"title_abstract":"Social Meaning in Repeated Interactions Judgements about communicative agents evolve over the course of interactions both in how individuals are judged for testimonial reliability and for (ideological) trustworthiness. This paper combines a theory of social meaning and persona with a theory of reliability within a game-theoretic view of communication, giving a formal model involving interactional histories, repeated game models and ways of evaluating social meaning and trustworthiness.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1730210781,"Goal":"Sustainable Cities and Communities","Task":["Social Meaning","evaluating social meaning","trustworthiness"],"Method":["social meaning","persona","reliability","game - theoretic view of communication","repeated game models"]},{"ID":"hinrichs-etal-2010-weblicht","title":"{W}eb{L}icht: Web-based {LRT} Services in a Distributed e{S}cience Infrastructure","abstract":"eScience - enhanced science - is a new paradigm of scientific work and research. In the humanities, eScience environments can be helpful in establishing new workflows and lifecycles of scientific data. WebLicht is such an eScience environment for linguistic analysis, making linguistic tools and resources available network-wide. Today, most digital language resources and tools (LRT) are available by download only. This is inconvenient for someone who wants to use and combine several tools because these tools are normally not compatible with each other. To overcome this restriction, WebLicht makes the functionality of linguistic tools and the resources themselves available via the internet as web services. In WebLicht, several kinds of linguistic tools are available which cover the basic functionality of automatic and incremental creation of annotated text corpora. To make use of the more than 70 tools and resources currently available, the end user needs nothing more than just a common web browser.","year":2010,"title_abstract":"{W}eb{L}icht: Web-based {LRT} Services in a Distributed e{S}cience Infrastructure eScience - enhanced science - is a new paradigm of scientific work and research. In the humanities, eScience environments can be helpful in establishing new workflows and lifecycles of scientific data. WebLicht is such an eScience environment for linguistic analysis, making linguistic tools and resources available network-wide. Today, most digital language resources and tools (LRT) are available by download only. This is inconvenient for someone who wants to use and combine several tools because these tools are normally not compatible with each other. To overcome this restriction, WebLicht makes the functionality of linguistic tools and the resources themselves available via the internet as web services. In WebLicht, several kinds of linguistic tools are available which cover the basic functionality of automatic and incremental creation of annotated text corpora. To make use of the more than 70 tools and resources currently available, the end user needs nothing more than just a common web browser.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1728764474,"Goal":"Life on Land","Task":["eScience - enhanced science","scientific work and research","eScience environments","linguistic analysis","automatic and incremental creation of annotated text corpora"],"Method":["Web - based {LRT} Services","WebLicht","eScience environment","WebLicht","WebLicht","linguistic tools","web browser"]},{"ID":"wittenburg-etal-2004-architecture","title":"Architecture for Distributed Language Resource Management and Archiving","abstract":"An architecture is presented that provides an integrated framework for managing, archiving and accessing language resources. This architecture was discussed in the DELAMAN network {--} a world-wide network of archives holding material about endangered languages. Such a framework will be built upon a metadata infrastructure, a mechanism to resolve unique resource identifiers, user and access rights management components. These components are closely related and have to be based on redundant and distributed services. For all these components existing middleware seems to be available, however, it has to be checked how they can interact with each other.","year":2004,"title_abstract":"Architecture for Distributed Language Resource Management and Archiving An architecture is presented that provides an integrated framework for managing, archiving and accessing language resources. This architecture was discussed in the DELAMAN network {--} a world-wide network of archives holding material about endangered languages. Such a framework will be built upon a metadata infrastructure, a mechanism to resolve unique resource identifiers, user and access rights management components. These components are closely related and have to be based on redundant and distributed services. For all these components existing middleware seems to be available, however, it has to be checked how they can interact with each other.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.1728682816,"Goal":"Life Below Water","Task":["Distributed Language Resource Management","Archiving","managing , archiving and accessing language resources"],"Method":["metadata infrastructure","user and access rights management components"]},{"ID":"bakharia-2018-selecting","title":"Selecting {NLP} Techniques to Evaluate Learning Design Objectives in Collaborative Multi-perspective Elaboration Activities","abstract":"PerspectivesX is a multi-perspective elaboration tool designed to encourage learner submission and curation across a range of collaborative learning activities. In this paper, it is shown that the learning design objectives of collaborative learning activities can be evaluated using NLP techniques, but that careful analysis of learner impact and pedagogical intent are required in order to select appropriate techniques. In particular, this paper focuses on the NLP techniques required to deliver an instructor dashboard, personalized learner feedback and content recommendation within multi-perspective elaboration activities. Key NLP techniques considered for inclusion include summarization, topic modeling, paraphrase detection and diversified content recommendation.","year":2018,"title_abstract":"Selecting {NLP} Techniques to Evaluate Learning Design Objectives in Collaborative Multi-perspective Elaboration Activities PerspectivesX is a multi-perspective elaboration tool designed to encourage learner submission and curation across a range of collaborative learning activities. In this paper, it is shown that the learning design objectives of collaborative learning activities can be evaluated using NLP techniques, but that careful analysis of learner impact and pedagogical intent are required in order to select appropriate techniques. In particular, this paper focuses on the NLP techniques required to deliver an instructor dashboard, personalized learner feedback and content recommendation within multi-perspective elaboration activities. Key NLP techniques considered for inclusion include summarization, topic modeling, paraphrase detection and diversified content recommendation.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.172790736,"Goal":"Quality Education","Task":["Learning Design Objectives","Collaborative Multi - perspective Elaboration Activities","PerspectivesX","learner submission and curation","collaborative learning activities","collaborative learning activities","instructor dashboard","personalized learner feedback","content recommendation","multi - perspective elaboration activities","summarization","paraphrase detection","diversified content recommendation"],"Method":["multi - perspective elaboration tool","NLP techniques","NLP techniques","NLP techniques","topic modeling"]},{"ID":"anastasiou-etal-2020-move","title":"{``}You move {THIS}!{''}: Annotation of Pointing Gestures on Tabletop Interfaces in Low Awareness Situations","abstract":"This paper analyses pointing gestures during low awareness situations occurring in a collaborative problem-solving activity implemented on an interactive tabletop interface. Awareness is considered as crucial requirement to support fluid and natural collaboration. We focus on pointing gestures as strategy to maintain awareness. We describe the results from a user study with five groups, each group consisting of three participants, who were asked to solve a task collaboratively on a tabletop interface. The ideal problem-solving solution would have been, if the three participants had been fully aware of what their personal area is depicting and had communicated this properly to the peers. However, often some participants are hesitant due to lack of awareness, some other want to take the lead work or expedite the process, and therefore pointing gestures to others{'} personal areas arise. Our results from analyzing a multimodal corpus of 168.68 minutes showed that in 95{\\%} of the cases, one user pointed to the personal area of the other, while in a few cases (3{\\%}) a user not only pointed, but also performed a touch gesture on the personal area of another user. In our study, the mean for such pointing gestures in low awareness situations per minute and for all groups was M=1.96, SD=0.58.","year":2020,"title_abstract":"{``}You move {THIS}!{''}: Annotation of Pointing Gestures on Tabletop Interfaces in Low Awareness Situations This paper analyses pointing gestures during low awareness situations occurring in a collaborative problem-solving activity implemented on an interactive tabletop interface. Awareness is considered as crucial requirement to support fluid and natural collaboration. We focus on pointing gestures as strategy to maintain awareness. We describe the results from a user study with five groups, each group consisting of three participants, who were asked to solve a task collaboratively on a tabletop interface. The ideal problem-solving solution would have been, if the three participants had been fully aware of what their personal area is depicting and had communicated this properly to the peers. However, often some participants are hesitant due to lack of awareness, some other want to take the lead work or expedite the process, and therefore pointing gestures to others{'} personal areas arise. Our results from analyzing a multimodal corpus of 168.68 minutes showed that in 95{\\%} of the cases, one user pointed to the personal area of the other, while in a few cases (3{\\%}) a user not only pointed, but also performed a touch gesture on the personal area of another user. In our study, the mean for such pointing gestures in low awareness situations per minute and for all groups was M=1.96, SD=0.58.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1727082133,"Goal":"Partnership for the Goals","Task":["Annotation of Pointing Gestures","Low Awareness Situations","pointing gestures","low awareness situations","collaborative problem - solving activity","Awareness","fluid and natural collaboration","pointing gestures","problem - solving solution","pointing gestures"],"Method":["Tabletop Interfaces"]},{"ID":"ethayarajh-jurafsky-2020-utility","title":"Utility is in the Eye of the User: A Critique of {NLP} Leaderboards","abstract":"Benchmarks such as GLUE have helped drive advances in NLP by incentivizing the creation of more accurate models. While this leaderboard paradigm has been remarkably successful, a historical focus on performance-based evaluation has been at the expense of other qualities that the NLP community values in models, such as compactness, fairness, and energy efficiency. In this opinion paper, we study the divergence between what is incentivized by leaderboards and what is useful in practice through the lens of microeconomic theory. We frame both the leaderboard and NLP practitioners as consumers and the benefit they get from a model as its utility to them. With this framing, we formalize how leaderboards {--} in their current form {--} can be poor proxies for the NLP community at large. For example, a highly inefficient model would provide less utility to practitioners but not to a leaderboard, since it is a cost that only the former must bear. To allow practitioners to better estimate a model{'}s utility to them, we advocate for more transparency on leaderboards, such as the reporting of statistics that are of practical concern (e.g., model size, energy efficiency, and inference latency).","year":2020,"title_abstract":"Utility is in the Eye of the User: A Critique of {NLP} Leaderboards Benchmarks such as GLUE have helped drive advances in NLP by incentivizing the creation of more accurate models. While this leaderboard paradigm has been remarkably successful, a historical focus on performance-based evaluation has been at the expense of other qualities that the NLP community values in models, such as compactness, fairness, and energy efficiency. In this opinion paper, we study the divergence between what is incentivized by leaderboards and what is useful in practice through the lens of microeconomic theory. We frame both the leaderboard and NLP practitioners as consumers and the benefit they get from a model as its utility to them. With this framing, we formalize how leaderboards {--} in their current form {--} can be poor proxies for the NLP community at large. For example, a highly inefficient model would provide less utility to practitioners but not to a leaderboard, since it is a cost that only the former must bear. To allow practitioners to better estimate a model{'}s utility to them, we advocate for more transparency on leaderboards, such as the reporting of statistics that are of practical concern (e.g., model size, energy efficiency, and inference latency).","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1726906896,"Goal":"Reduced Inequalities","Task":["Utility","NLP","evaluation","NLP","NLP community"],"Method":["Leaderboards Benchmarks","GLUE","leaderboard paradigm","microeconomic theory","NLP practitioners"]},{"ID":"nakaguchi-etal-2016-combining","title":"Combining Human Inputters and Language Services to provide Multi-language support system for International Symposiums","abstract":"In this research, we introduce and implement a method that combines human inputters and machine translators. When the languages of the participants vary widely, the cost of simultaneous translation becomes very high. However, the results of simply applying machine translation to speech text do not have the quality that is needed for real use. Thus, we propose a method that people who understand the language of the speaker cooperate with a machine translation service in support of multilingualization by the co-creation of value. We implement a system with this method and apply it to actual presentations. While the quality of direct machine translations is 1.84 (fluency) and 2.89 (adequacy), the system has corresponding values of 3.76 and 3.85.","year":2016,"title_abstract":"Combining Human Inputters and Language Services to provide Multi-language support system for International Symposiums In this research, we introduce and implement a method that combines human inputters and machine translators. When the languages of the participants vary widely, the cost of simultaneous translation becomes very high. However, the results of simply applying machine translation to speech text do not have the quality that is needed for real use. Thus, we propose a method that people who understand the language of the speaker cooperate with a machine translation service in support of multilingualization by the co-creation of value. We implement a system with this method and apply it to actual presentations. While the quality of direct machine translations is 1.84 (fluency) and 2.89 (adequacy), the system has corresponding values of 3.76 and 3.85.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1724785566,"Goal":"Partnership for the Goals","Task":["machine translators","simultaneous translation","machine translation","multilingualization","co - creation of value","direct machine translations"],"Method":["Language Services","Multi - language support system","machine translation service"]},{"ID":"kim-etal-2016-open","title":"The Open Framework for Developing Knowledge Base And Question Answering System","abstract":"Developing a question answering (QA) system is a task of implementing and integrating modules of different technologies and evaluating an integrated whole system, which inevitably goes with a collaboration among experts of different domains. For supporting a easy collaboration, this demonstration presents the open framework that aims to support developing a QA system in collaborative and intuitive ways. The demonstration also shows the QA system developed by our novel framework.","year":2016,"title_abstract":"The Open Framework for Developing Knowledge Base And Question Answering System Developing a question answering (QA) system is a task of implementing and integrating modules of different technologies and evaluating an integrated whole system, which inevitably goes with a collaboration among experts of different domains. For supporting a easy collaboration, this demonstration presents the open framework that aims to support developing a QA system in collaborative and intuitive ways. The demonstration also shows the QA system developed by our novel framework.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1724601984,"Goal":"Quality Education","Task":["Knowledge Base And Question Answering System","question answering (QA) system","QA","QA"],"Method":["Open Framework","open framework"]},{"ID":"wang-etal-2020-cuhk","title":"{CUHK} at {S}em{E}val-2020 Task 4: {C}ommon{S}ense Explanation, Reasoning and Prediction with Multi-task Learning","abstract":"This paper describes our system submitted to task 4 of SemEval 2020: Commonsense Validation and Explanation (ComVE) which consists of three sub-tasks. The task is to directly validate the given sentence whether or not to make sense and require the model to explain it. Based on BERT architecture with the multi-task setting, we propose an effective and interpretable {``}Explain, Reason and Predict{''} (ERP) system to solve the three sub-tasks about commonsense: (a) Validation, (b) Reasoning, and (c) Explanation. Inspired by cognitive studies of common sense, our system first generates a reason or understanding of the sentences and then choose which one statement makes sense, which is achieved by multi-task learning. During the post-evaluation, our system has reached 92.9{\\%} accuracy in subtask A (rank 11), 89.7{\\%} accuracy in subtask B (rank 9), and BLEU score of 12.9 in subtask C (rank 8).","year":2020,"title_abstract":"{CUHK} at {S}em{E}val-2020 Task 4: {C}ommon{S}ense Explanation, Reasoning and Prediction with Multi-task Learning This paper describes our system submitted to task 4 of SemEval 2020: Commonsense Validation and Explanation (ComVE) which consists of three sub-tasks. The task is to directly validate the given sentence whether or not to make sense and require the model to explain it. Based on BERT architecture with the multi-task setting, we propose an effective and interpretable {``}Explain, Reason and Predict{''} (ERP) system to solve the three sub-tasks about commonsense: (a) Validation, (b) Reasoning, and (c) Explanation. Inspired by cognitive studies of common sense, our system first generates a reason or understanding of the sentences and then choose which one statement makes sense, which is achieved by multi-task learning. During the post-evaluation, our system has reached 92.9{\\%} accuracy in subtask A (rank 11), 89.7{\\%} accuracy in subtask B (rank 9), and BLEU score of 12.9 in subtask C (rank 8).","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1723944545,"Goal":"Quality Education","Task":["Reasoning and Prediction","Multi - task Learning","SemEval 2020","Commonsense Validation and Explanation","multi - task setting","commonsense","Validation","Reasoning","Explanation","cognitive studies of common sense"],"Method":["BERT","Reason and Predict{''} (ERP) system","multi - task learning"]},{"ID":"joshi-etal-2020-state","title":"The State and Fate of Linguistic Diversity and Inclusion in the {NLP} World","abstract":"Language technologies contribute to promoting multilingualism and linguistic diversity around the world. However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications. In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time. Our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the {``}language agnostic{''} status of current models and systems. Through this paper, we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind.","year":2020,"title_abstract":"The State and Fate of Linguistic Diversity and Inclusion in the {NLP} World Language technologies contribute to promoting multilingualism and linguistic diversity around the world. However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications. In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time. Our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the {``}language agnostic{''} status of current models and systems. Through this paper, we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1723317951,"Goal":"Partnership for the Goals","Task":["NLP","ACL community"],"Method":["World Language technologies","language technologies"]},{"ID":"wilson-2008-annotating","title":"Annotating Subjective Content in Meetings","abstract":"This paper presents an annotation scheme for marking subjective content in meetings, specifically the opinions and sentiments that participants express as part of their discussion. The scheme adapts concepts from the Multi-perspective Question Answering (MPQA) Annotation Scheme, an annotation scheme for marking opinions and attributions in the news. The adaptations reflect the differences in multiparty conversation as compared to text, as well as the overall goals of our project.","year":2008,"title_abstract":"Annotating Subjective Content in Meetings This paper presents an annotation scheme for marking subjective content in meetings, specifically the opinions and sentiments that participants express as part of their discussion. The scheme adapts concepts from the Multi-perspective Question Answering (MPQA) Annotation Scheme, an annotation scheme for marking opinions and attributions in the news. The adaptations reflect the differences in multiparty conversation as compared to text, as well as the overall goals of our project.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1722700596,"Goal":"Partnership for the Goals","Task":["Annotating Subjective Content","marking subjective content","marking opinions","attributions"],"Method":["annotation scheme","Multi - perspective Question Answering (MPQA) Annotation Scheme","annotation scheme"]},{"ID":"nikulasdottir-etal-2020-language","title":"Language Technology Programme for {I}celandic 2019-2023","abstract":"In this paper, we describe a new national language technology programme for Icelandic. The programme, which spans a period of five years, aims at making Icelandic usable in communication and interactions in the digital world, by developing accessible, open-source language resources and software. The research and development work within the programme is carried out by a consortium of universities, institutions, and private companies, with a strong emphasis on cooperation between academia and industries. Five core projects will be the main content of the programme: language resources, speech recognition, speech synthesis, machine translation, and spell and grammar checking. We also describe other national language technology programmes and give an overview over the history of language technology in Iceland.","year":2020,"title_abstract":"Language Technology Programme for {I}celandic 2019-2023 In this paper, we describe a new national language technology programme for Icelandic. The programme, which spans a period of five years, aims at making Icelandic usable in communication and interactions in the digital world, by developing accessible, open-source language resources and software. The research and development work within the programme is carried out by a consortium of universities, institutions, and private companies, with a strong emphasis on cooperation between academia and industries. Five core projects will be the main content of the programme: language resources, speech recognition, speech synthesis, machine translation, and spell and grammar checking. We also describe other national language technology programmes and give an overview over the history of language technology in Iceland.","social_need":"Industry, Innovation and Infrastrucure Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation","cosine_similarity":0.1722435653,"Goal":"Industry, Innovation and Infrastrucure","Task":["national language technology programme","communication","language resources","speech recognition","speech synthesis","machine translation","spell and grammar checking","national language technology programmes","language technology"],"Method":["Language Technology Programme"]},{"ID":"gibbon-etal-2004-wala","title":"{WALA}: A Multilingual Resource Repository for {W}est {A}frican Languages","abstract":"The West African Language Archive (WALA) initiative has emerged from a number of concurrent projects, and aims to encourage local scholars to create high quality decentralised repositories documenting West African languages, and to make these repositories available to language communities, language planners, educationalists and scientists via an internet metadata portal such as OLAC (Open Language Archive Community). A wide range of criteria has to be met in designing and implementing this kind of archive. We discuss these criteria with reference to experiences in documentation work in three very different ongoing language documentation projects, on designing an encyclopaedia, on documenting an endangered language, and on creating a speech synthesiser. We pay special attention to the provision of metadata, a formal variety of catalogue or housekeeping information, without which resources are doomed to remain inaccessible.","year":2004,"title_abstract":"{WALA}: A Multilingual Resource Repository for {W}est {A}frican Languages The West African Language Archive (WALA) initiative has emerged from a number of concurrent projects, and aims to encourage local scholars to create high quality decentralised repositories documenting West African languages, and to make these repositories available to language communities, language planners, educationalists and scientists via an internet metadata portal such as OLAC (Open Language Archive Community). A wide range of criteria has to be met in designing and implementing this kind of archive. We discuss these criteria with reference to experiences in documentation work in three very different ongoing language documentation projects, on designing an encyclopaedia, on documenting an endangered language, and on creating a speech synthesiser. We pay special attention to the provision of metadata, a formal variety of catalogue or housekeeping information, without which resources are doomed to remain inaccessible.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.1721634865,"Goal":"Life Below Water","Task":["language planners","archive","language documentation projects"],"Method":["Multilingual Resource Repository","speech synthesiser"]},{"ID":"shaar-etal-2021-findings","title":"Findings of the {NLP}4{IF}-2021 Shared Tasks on Fighting the {COVID}-19 Infodemic and Censorship Detection","abstract":"We present the results and the main findings of the NLP4IF-2021 shared tasks. Task 1 focused on fighting the COVID-19 infodemic in social media, and it was offered in Arabic, Bulgarian, and English. Given a tweet, it asked to predict whether that tweet contains a verifiable claim, and if so, whether it is likely to be false, is of general interest, is likely to be harmful, and is worthy of manual fact-checking; also, whether it is harmful to society, and whether it requires the attention of policy makers. Task 2 focused on censorship detection, and was offered in Chinese. A total of ten teams submitted systems for task 1, and one team participated in task 2; nine teams also submitted a system description paper. Here, we present the tasks, analyze the results, and discuss the system submissions and the methods they used. Most submissions achieved sizable improvements over several baselines, and the best systems used pre-trained Transformers and ensembles. The data, the scorers and the leaderboards for the tasks are available at http:\/\/gitlab.com\/NLP4IF\/nlp4if-2021.","year":2021,"title_abstract":"Findings of the {NLP}4{IF}-2021 Shared Tasks on Fighting the {COVID}-19 Infodemic and Censorship Detection We present the results and the main findings of the NLP4IF-2021 shared tasks. Task 1 focused on fighting the COVID-19 infodemic in social media, and it was offered in Arabic, Bulgarian, and English. Given a tweet, it asked to predict whether that tweet contains a verifiable claim, and if so, whether it is likely to be false, is of general interest, is likely to be harmful, and is worthy of manual fact-checking; also, whether it is harmful to society, and whether it requires the attention of policy makers. Task 2 focused on censorship detection, and was offered in Chinese. A total of ten teams submitted systems for task 1, and one team participated in task 2; nine teams also submitted a system description paper. Here, we present the tasks, analyze the results, and discuss the system submissions and the methods they used. Most submissions achieved sizable improvements over several baselines, and the best systems used pre-trained Transformers and ensembles. The data, the scorers and the leaderboards for the tasks are available at http:\/\/gitlab.com\/NLP4IF\/nlp4if-2021.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1721050888,"Goal":"Climate Action","Task":["Censorship Detection","NLP4IF - 2021 shared tasks","manual fact - checking;","censorship detection"],"Method":["Transformers","ensembles"]},{"ID":"klenner-etal-2017-stance","title":"Stance Detection in {F}acebook Posts of a {G}erman Right-wing Party","abstract":"We argue that in order to detect stance, not only the explicit attitudes of the stance holder towards the targets are crucial. It is the whole narrative the writer drafts that counts, including the way he hypostasizes the discourse referents: as benefactors or villains, as victims or beneficiaries. We exemplify the ability of our system to identify targets and detect the writer{'}s stance towards them on the basis of about 100 000 Facebook posts of a German right-wing party. A reader and writer model on top of our verb-based attitude extraction directly reveal stance conflicts.","year":2017,"title_abstract":"Stance Detection in {F}acebook Posts of a {G}erman Right-wing Party We argue that in order to detect stance, not only the explicit attitudes of the stance holder towards the targets are crucial. It is the whole narrative the writer drafts that counts, including the way he hypostasizes the discourse referents: as benefactors or villains, as victims or beneficiaries. We exemplify the ability of our system to identify targets and detect the writer{'}s stance towards them on the basis of about 100 000 Facebook posts of a German right-wing party. A reader and writer model on top of our verb-based attitude extraction directly reveal stance conflicts.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1720487773,"Goal":"Climate Action","Task":["Stance Detection"],"Method":["reader and writer model","verb - based attitude extraction"]},{"ID":"lietard-etal-2021-language","title":"Do Language Models Know the Way to {R}ome?","abstract":"The global geometry of language models is important for a range of applications, but language model probes tend to evaluate rather local relations, for which ground truths are easily obtained. In this paper we exploit the fact that in geography, ground truths are available beyond local relations. In a series of experiments, we evaluate the extent to which language model representations of city and country names are isomorphic to real-world geography, e.g., if you tell a language model where Paris and Berlin are, does it know the way to Rome? We find that language models generally encode limited geographic information, but with larger models performing the best, suggesting that geographic knowledge \\textit{can} be induced from higher-order co-occurrence statistics.","year":2021,"title_abstract":"Do Language Models Know the Way to {R}ome? The global geometry of language models is important for a range of applications, but language model probes tend to evaluate rather local relations, for which ground truths are easily obtained. In this paper we exploit the fact that in geography, ground truths are available beyond local relations. In a series of experiments, we evaluate the extent to which language model representations of city and country names are isomorphic to real-world geography, e.g., if you tell a language model where Paris and Berlin are, does it know the way to Rome? We find that language models generally encode limited geographic information, but with larger models performing the best, suggesting that geographic knowledge \\textit{can} be induced from higher-order co-occurrence statistics.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1720293462,"Goal":"Sustainable Cities and Communities","Task":["geography"],"Method":["Language Models","global geometry of language models","language model probes","language model representations","language model","language models"]},{"ID":"fine-etal-2020-assessing","title":"Assessing population-level symptoms of anxiety, depression, and suicide risk in real time using {NLP} applied to social media data","abstract":"Prevailing methods for assessing population-level mental health require costly collection of large samples of data through instruments such as surveys, and are thus slow to reflect current, rapidly changing social conditions. This constrains how easily population-level mental health data can be integrated into health and policy decision-making. Here, we demonstrate that natural language processing applied to publicly-available social media data can provide real-time estimates of psychological distress in the population (specifically, English-speaking Twitter users in the US). We examine population-level changes in linguistic correlates of mental health symptoms in response to the COVID-19 pandemic and to the killing of George Floyd. As a case study, we focus on social media data from healthcare providers, compared to a control sample. Our results provide a concrete demonstration of how the tools of computational social science can be applied to provide real-time or near-real-time insight into the impact of public events on mental health.","year":2020,"title_abstract":"Assessing population-level symptoms of anxiety, depression, and suicide risk in real time using {NLP} applied to social media data Prevailing methods for assessing population-level mental health require costly collection of large samples of data through instruments such as surveys, and are thus slow to reflect current, rapidly changing social conditions. This constrains how easily population-level mental health data can be integrated into health and policy decision-making. Here, we demonstrate that natural language processing applied to publicly-available social media data can provide real-time estimates of psychological distress in the population (specifically, English-speaking Twitter users in the US). We examine population-level changes in linguistic correlates of mental health symptoms in response to the COVID-19 pandemic and to the killing of George Floyd. As a case study, we focus on social media data from healthcare providers, compared to a control sample. Our results provide a concrete demonstration of how the tools of computational social science can be applied to provide real-time or near-real-time insight into the impact of public events on mental health.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1720092148,"Goal":"Climate Action","Task":["Assessing population - level symptoms of anxiety","suicide risk","population - level mental health","health and policy decision - making","real - time estimates of psychological distress","computational social science","real - time or near - real - time insight","mental health"],"Method":["{NLP}","natural language processing"]},{"ID":"burkhardt-etal-2010-database","title":"A Database of Age and Gender Annotated Telephone Speech","abstract":"This article describes an age-annotated database of German telephone speech. All in all 47 hours of prompted and free text was recorded, uttered by 954 paid participants in a style typical for automated voice services. The participants were selected based on an equal distribution of males and females within four age cluster groups; children, youth, adults and seniors. Within the children, gender is not distinguished, because it doesn\u0092t have a strong enough effect on the voice. The textual content was designed to be typical for automated voice services and consists mainly of short commands, single words and numbers. An additional database consists of 659 speakers (368 female and 291 male) that called an automated voice portal server and answered freely on one of the two questions \u0093What is your favourite dish?\u0094 and \u0093What would you take to an island?\u0094 (island set, 422 speakers). This data might be used for out-of domain testing. The data will be used to tune an age-detecting automated voice service and might be released to research institutes under controlled conditions as part of an open age and gender detection challenge.","year":2010,"title_abstract":"A Database of Age and Gender Annotated Telephone Speech This article describes an age-annotated database of German telephone speech. All in all 47 hours of prompted and free text was recorded, uttered by 954 paid participants in a style typical for automated voice services. The participants were selected based on an equal distribution of males and females within four age cluster groups; children, youth, adults and seniors. Within the children, gender is not distinguished, because it doesn\u0092t have a strong enough effect on the voice. The textual content was designed to be typical for automated voice services and consists mainly of short commands, single words and numbers. An additional database consists of 659 speakers (368 female and 291 male) that called an automated voice portal server and answered freely on one of the two questions \u0093What is your favourite dish?\u0094 and \u0093What would you take to an island?\u0094 (island set, 422 speakers). This data might be used for out-of domain testing. The data will be used to tune an age-detecting automated voice service and might be released to research institutes under controlled conditions as part of an open age and gender detection challenge.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1719748378,"Goal":"Gender Equality","Task":["automated voice services","automated voice services","out - of domain testing","age - detecting automated voice service","age and gender detection challenge"],"Method":["voice portal server"]},{"ID":"parida-etal-2021-nlphuts","title":"{NLPH}ut{'}s Participation at {WAT}2021","abstract":"This paper provides the description of shared tasks to the WAT 2021 by our team {``}NLPHut{''}. We have participated in the English\u2192Hindi Multimodal translation task, English\u2192Malayalam Multimodal translation task, and Indic Multi-lingual translation task. We have used the state-of-the-art Transformer model with language tags in different settings for the translation task and proposed a novel {``}region-specific{''} caption generation approach using a combination of image CNN and LSTM for the Hindi and Malayalam image captioning. Our submission tops in English\u2192Malayalam Multimodal translation task (text-only translation, and Malayalam caption), and ranks second-best in English\u2192Hindi Multimodal translation task (text-only translation, and Hindi caption). Our submissions have also performed well in the Indic Multilingual translation tasks.","year":2021,"title_abstract":"{NLPH}ut{'}s Participation at {WAT}2021 This paper provides the description of shared tasks to the WAT 2021 by our team {``}NLPHut{''}. We have participated in the English\u2192Hindi Multimodal translation task, English\u2192Malayalam Multimodal translation task, and Indic Multi-lingual translation task. We have used the state-of-the-art Transformer model with language tags in different settings for the translation task and proposed a novel {``}region-specific{''} caption generation approach using a combination of image CNN and LSTM for the Hindi and Malayalam image captioning. Our submission tops in English\u2192Malayalam Multimodal translation task (text-only translation, and Malayalam caption), and ranks second-best in English\u2192Hindi Multimodal translation task (text-only translation, and Hindi caption). Our submissions have also performed well in the Indic Multilingual translation tasks.","social_need":"Clean Water and Sanitation Ensure availability and sustainable management of water and sanitation for all","cosine_similarity":0.171860069,"Goal":"Clean Water and Sanitation","Task":["English\u2192Hindi Multimodal translation task","English\u2192Malayalam Multimodal translation task","Indic Multi - lingual translation task","translation task","English\u2192Malayalam Multimodal translation task","translation","English\u2192Hindi Multimodal translation task","translation","Hindi caption)","Indic Multilingual translation tasks"],"Method":["Transformer model","{``}region - specific{''} caption generation approach","image CNN","LSTM"]},{"ID":"lin-etal-2018-global","title":"Global Encoding for Abstractive Summarization","abstract":"In neural abstractive summarization, the conventional sequence-to-sequence (seq2seq) model often suffers from repetition and semantic irrelevance. To tackle the problem, we propose a global encoding framework, which controls the information flow from the encoder to the decoder based on the global information of the source context. It consists of a convolutional gated unit to perform global encoding to improve the representations of the source-side information. Evaluations on the LCSTS and the English Gigaword both demonstrate that our model outperforms the baseline models, and the analysis shows that our model is capable of generating summary of higher quality and reducing repetition.","year":2018,"title_abstract":"Global Encoding for Abstractive Summarization In neural abstractive summarization, the conventional sequence-to-sequence (seq2seq) model often suffers from repetition and semantic irrelevance. To tackle the problem, we propose a global encoding framework, which controls the information flow from the encoder to the decoder based on the global information of the source context. It consists of a convolutional gated unit to perform global encoding to improve the representations of the source-side information. Evaluations on the LCSTS and the English Gigaword both demonstrate that our model outperforms the baseline models, and the analysis shows that our model is capable of generating summary of higher quality and reducing repetition.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1718245447,"Goal":"Partnership for the Goals","Task":["Abstractive Summarization","neural abstractive summarization"],"Method":["Global Encoding","sequence - to - sequence (seq2seq) model","global encoding framework","encoder","decoder","convolutional gated unit","global encoding","LCSTS"]},{"ID":"ntoutsi-2020-bias","title":"Bias in {AI}-systems: A multi-step approach","abstract":"Algorithmic-based decision making powered via AI and (big) data has already penetrated into almost all spheres of human life, from content recommendation and healthcare to predictive policing and autonomous driving, deeply affecting everyone, anywhere, anytime. While technology allows previously unthinkable optimizations in the automation of expensive human decision making, the risks that the technology can pose are also high, leading to an ever increasing public concern about the impact of the technology in our lives. The area of responsible AI has recently emerged in an attempt to put humans at the center of AI-based systems by considering aspects, such as fairness, reliability and privacy of decision-making systems. In this talk, we will focus on the fairness aspect. We will start with understanding the many sources of bias and how biases can enter at each step of the learning process and even get propagated\/amplified from previous steps. We will continue with methods for mitigating bias which typically focus on some step of the pipeline (data, algorithms or results) and why it is important to target bias in each step and collectively, in the whole (machine) learning pipeline. We will conclude this talk by discussing accountability issues in connection to bias and in particular, proactive consideration via bias-aware data collection, processing and algorithmic selection and retroactive consideration via explanations.","year":2020,"title_abstract":"Bias in {AI}-systems: A multi-step approach Algorithmic-based decision making powered via AI and (big) data has already penetrated into almost all spheres of human life, from content recommendation and healthcare to predictive policing and autonomous driving, deeply affecting everyone, anywhere, anytime. While technology allows previously unthinkable optimizations in the automation of expensive human decision making, the risks that the technology can pose are also high, leading to an ever increasing public concern about the impact of the technology in our lives. The area of responsible AI has recently emerged in an attempt to put humans at the center of AI-based systems by considering aspects, such as fairness, reliability and privacy of decision-making systems. In this talk, we will focus on the fairness aspect. We will start with understanding the many sources of bias and how biases can enter at each step of the learning process and even get propagated\/amplified from previous steps. We will continue with methods for mitigating bias which typically focus on some step of the pipeline (data, algorithms or results) and why it is important to target bias in each step and collectively, in the whole (machine) learning pipeline. We will conclude this talk by discussing accountability issues in connection to bias and in particular, proactive consideration via bias-aware data collection, processing and algorithmic selection and retroactive consideration via explanations.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1717759967,"Goal":"Quality Education","Task":["content recommendation","healthcare","predictive policing","autonomous driving","expensive human decision making","responsible AI","mitigating bias","accountability issues","bias","proactive consideration","processing","retroactive consideration"],"Method":["{AI} - systems","multi - step approach","Algorithmic - based decision making","AI","AI - based systems","decision - making systems","learning process","learning pipeline","bias - aware data collection","algorithmic selection"]},{"ID":"temnikova-etal-2012-clcm","title":"{CLCM} - A Linguistic Resource for Effective Simplification of Instructions in the Crisis Management Domain and its Evaluations","abstract":"Due to the increasing number of emergency situations which can have substantial consequences, both financially and fatally, the Crisis Management (CM) domain is developing at an exponential speed. The efficient management of emergency situations relies on clear communication between all of the participants in a crisis situation. For these reasons the Text Complexity (TC) of the CM domain needed to be investigated and showed that CM domain texts exhibit high TC levels. This article presents a new linguistic resource in the form of Controlled Language (CL) guidelines for manual text simplification in the CM domain which aims to address high TC in the CM domain and produce clear messages to be used in crisis situations. The effectiveness of the resource has been tested via evaluation from several different perspectives important for the domain. The overall results show that the CLCM simplification has a positive impact on TC, reading comprehension, manual translation and machine translation. Additionally, an investigation of the cognitive difficulty in applying manual simplification operations led to interesting discoveries. This article provides details of the evaluation methods, the conducted experiments, their results and indications about future work.","year":2012,"title_abstract":"{CLCM} - A Linguistic Resource for Effective Simplification of Instructions in the Crisis Management Domain and its Evaluations Due to the increasing number of emergency situations which can have substantial consequences, both financially and fatally, the Crisis Management (CM) domain is developing at an exponential speed. The efficient management of emergency situations relies on clear communication between all of the participants in a crisis situation. For these reasons the Text Complexity (TC) of the CM domain needed to be investigated and showed that CM domain texts exhibit high TC levels. This article presents a new linguistic resource in the form of Controlled Language (CL) guidelines for manual text simplification in the CM domain which aims to address high TC in the CM domain and produce clear messages to be used in crisis situations. The effectiveness of the resource has been tested via evaluation from several different perspectives important for the domain. The overall results show that the CLCM simplification has a positive impact on TC, reading comprehension, manual translation and machine translation. Additionally, an investigation of the cognitive difficulty in applying manual simplification operations led to interesting discoveries. This article provides details of the evaluation methods, the conducted experiments, their results and indications about future work.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.171772033,"Goal":"Climate Action","Task":["Effective Simplification of Instructions","Crisis Management Domain","Crisis Management","management of emergency situations","manual text simplification","reading comprehension","manual translation","machine translation","manual simplification operations"],"Method":["Linguistic Resource","CM","CM","linguistic resource","CM","CM","CLCM"]},{"ID":"priyanshu-etal-2021-something","title":"{``}Something Something Hota Hai!{''} An Explainable Approach towards Sentiment Analysis on {I}ndian Code-Mixed Data","abstract":"The increasing use of social media sites in countries like India has given rise to large volumes of code-mixed data. Sentiment analysis of this data can provide integral insights into people{'}s perspectives and opinions. Code-mixed data is often noisy in nature due to multiple spellings for the same word, lack of definite order of words in a sentence, and random abbreviations. Thus, working with code-mixed data is more challenging than monolingual data. Interpreting a model{'}s predictions allows us to determine the robustness of the model against different forms of noise. In this paper, we propose a methodology to integrate explainable approaches into code-mixed sentiment analysis. By interpreting the predictions of sentiment analysis models we evaluate how well the model is able to adapt to the implicit noises present in code-mixed data.","year":2021,"title_abstract":"{``}Something Something Hota Hai!{''} An Explainable Approach towards Sentiment Analysis on {I}ndian Code-Mixed Data The increasing use of social media sites in countries like India has given rise to large volumes of code-mixed data. Sentiment analysis of this data can provide integral insights into people{'}s perspectives and opinions. Code-mixed data is often noisy in nature due to multiple spellings for the same word, lack of definite order of words in a sentence, and random abbreviations. Thus, working with code-mixed data is more challenging than monolingual data. Interpreting a model{'}s predictions allows us to determine the robustness of the model against different forms of noise. In this paper, we propose a methodology to integrate explainable approaches into code-mixed sentiment analysis. By interpreting the predictions of sentiment analysis models we evaluate how well the model is able to adapt to the implicit noises present in code-mixed data.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1717117131,"Goal":"Reduced Inequalities","Task":["Sentiment Analysis","Sentiment analysis","code - mixed sentiment analysis"],"Method":["Explainable Approach","explainable approaches","sentiment analysis models"]},{"ID":"ranasinghe-etal-2020-transquest-wmt2020","title":"{T}rans{Q}uest at {WMT}2020: Sentence-Level Direct Assessment","abstract":"This paper presents the team TransQuest{'}s participation in Sentence-Level Direct Assessment shared task in WMT 2020. We introduce a simple QE framework based on cross-lingual transformers, and we use it to implement and evaluate two different neural architectures. The proposed methods achieve state-of-the-art results surpassing the results obtained by OpenKiwi, the baseline used in the shared task. We further fine tune the QE framework by performing ensemble and data augmentation. Our approach is the winning solution in all of the language pairs according to the WMT 2020 official results.","year":2020,"title_abstract":"{T}rans{Q}uest at {WMT}2020: Sentence-Level Direct Assessment This paper presents the team TransQuest{'}s participation in Sentence-Level Direct Assessment shared task in WMT 2020. We introduce a simple QE framework based on cross-lingual transformers, and we use it to implement and evaluate two different neural architectures. The proposed methods achieve state-of-the-art results surpassing the results obtained by OpenKiwi, the baseline used in the shared task. We further fine tune the QE framework by performing ensemble and data augmentation. Our approach is the winning solution in all of the language pairs according to the WMT 2020 official results.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1716853976,"Goal":"Gender Equality","Task":["Sentence - Level Direct Assessment","Sentence - Level Direct Assessment shared task","WMT 2020"],"Method":["QE framework","cross - lingual transformers","neural architectures","OpenKiwi","QE framework","ensemble","data augmentation"]},{"ID":"htait-2018-adapted","title":"Adapted Sentiment Similarity Seed Words For {F}rench Tweets{'} Polarity Classification","abstract":"We present, in this paper, our contribution in DEFT 2018 task 2 : {``}Global polarity{''}, determining the overall polarity (Positive, Negative, Neutral or MixPosNeg) of tweets regarding public transport, in French language. Our system is based on a list of sentiment seed-words adapted for French public transport tweets. These seed-words are extracted from DEFT{'}s training annotated dataset, and the sentiment relations between seed-words and other terms are captured by cosine measure of their word embeddings representations, using a French language word embeddings model of 683k words. Our semi-supervised system achieved an F1-measure equals to 0.64.","year":2018,"title_abstract":"Adapted Sentiment Similarity Seed Words For {F}rench Tweets{'} Polarity Classification We present, in this paper, our contribution in DEFT 2018 task 2 : {``}Global polarity{''}, determining the overall polarity (Positive, Negative, Neutral or MixPosNeg) of tweets regarding public transport, in French language. Our system is based on a list of sentiment seed-words adapted for French public transport tweets. These seed-words are extracted from DEFT{'}s training annotated dataset, and the sentiment relations between seed-words and other terms are captured by cosine measure of their word embeddings representations, using a French language word embeddings model of 683k words. Our semi-supervised system achieved an F1-measure equals to 0.64.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1716734469,"Goal":"Reduced Inequalities","Task":["Polarity Classification","DEFT 2018","polarity{''}","public transport","DEFT{'}s"],"Method":["word embeddings representations","French language word embeddings model","semi - supervised system"]},{"ID":"doyle-etal-2017-alignment","title":"Alignment at Work: Using Language to Distinguish the Internalization and Self-Regulation Components of Cultural Fit in Organizations","abstract":"Cultural fit is widely believed to affect the success of individuals and the groups to which they belong. Yet it remains an elusive, poorly measured construct. Recent research draws on computational linguistics to measure cultural fit but overlooks asymmetries in cultural adaptation. By contrast, we develop a directed, dynamic measure of cultural fit based on linguistic alignment, which estimates the influence of one person{'}s word use on another{'}s and distinguishes between two enculturation mechanisms: internalization and self-regulation. We use this measure to trace employees{'} enculturation trajectories over a large, multi-year corpus of corporate emails and find that patterns of alignment in the first six months of employment are predictive of individuals{'} downstream outcomes, especially involuntary exit. Further predictive analyses suggest referential alignment plays an overlooked role in linguistic alignment.","year":2017,"title_abstract":"Alignment at Work: Using Language to Distinguish the Internalization and Self-Regulation Components of Cultural Fit in Organizations Cultural fit is widely believed to affect the success of individuals and the groups to which they belong. Yet it remains an elusive, poorly measured construct. Recent research draws on computational linguistics to measure cultural fit but overlooks asymmetries in cultural adaptation. By contrast, we develop a directed, dynamic measure of cultural fit based on linguistic alignment, which estimates the influence of one person{'}s word use on another{'}s and distinguishes between two enculturation mechanisms: internalization and self-regulation. We use this measure to trace employees{'} enculturation trajectories over a large, multi-year corpus of corporate emails and find that patterns of alignment in the first six months of employment are predictive of individuals{'} downstream outcomes, especially involuntary exit. Further predictive analyses suggest referential alignment plays an overlooked role in linguistic alignment.","social_need":"Decent Work and Economic Growth Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all","cosine_similarity":0.1715122461,"Goal":"Decent Work and Economic Growth","Task":["Internalization and Self - Regulation Components of Cultural Fit","Cultural fit","cultural fit","cultural adaptation","linguistic alignment","referential alignment","linguistic alignment"],"Method":["computational linguistics","directed , dynamic measure of cultural fit","enculturation mechanisms","self - regulation"]},{"ID":"huynh-etal-2020-banana","title":"{BANANA} at {WNUT}-2020 Task 2: Identifying {COVID}-19 Information on {T}witter by Combining Deep Learning and Transfer Learning Models","abstract":"The outbreak COVID-19 virus caused a significant impact on the health of people all over the world. Therefore, it is essential to have a piece of constant and accurate information about the disease with everyone. This paper describes our prediction system for WNUT-2020 Task 2: Identification of Informative COVID-19 English Tweets. The dataset for this task contains size 10,000 tweets in English labeled by humans. The ensemble model from our three transformer and deep learning models is used for the final prediction. The experimental result indicates that we have achieved F1 for the INFORMATIVE label on our systems at 88.81{\\%} on the test set.","year":2020,"title_abstract":"{BANANA} at {WNUT}-2020 Task 2: Identifying {COVID}-19 Information on {T}witter by Combining Deep Learning and Transfer Learning Models The outbreak COVID-19 virus caused a significant impact on the health of people all over the world. Therefore, it is essential to have a piece of constant and accurate information about the disease with everyone. This paper describes our prediction system for WNUT-2020 Task 2: Identification of Informative COVID-19 English Tweets. The dataset for this task contains size 10,000 tweets in English labeled by humans. The ensemble model from our three transformer and deep learning models is used for the final prediction. The experimental result indicates that we have achieved F1 for the INFORMATIVE label on our systems at 88.81{\\%} on the test set.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1714344919,"Goal":"Climate Action","Task":["WNUT - 2020 Task 2","Identification of Informative COVID - 19","prediction"],"Method":["Deep Learning","Transfer Learning Models","prediction system","ensemble model","transformer and deep learning models"]},{"ID":"kim-etal-2021-pipeline","title":"The Pipeline Model for Resolution of Anaphoric Reference and Resolution of Entity Reference","abstract":"The objective of anaphora resolution in dialogue shared-task is to go above and beyond the simple cases of coreference resolution in written text on which NLP has mostly focused so far, which arguably overestimate the performance of current SOTA models. The anaphora resolution in dialogue shared-task consists of three subtasks; subtask1, resolution of anaphoric identity and non-referring expression identification, subtask2, resolution of bridging references, and subtask3, resolution of discourse deixis\/abstract anaphora. In this paper, we propose the pipelined model (i.e., a resolution of anaphoric identity and a resolution of bridging references) for the subtask1 and the subtask2. In the subtask1, our model detects mention via the parentheses prediction. Then, we yield mention representation using the token representation constituting the mention. Mention representation is fed to the coreference resolution model for clustering. In the subtask2, our model resolves bridging references via the MRC framework. We construct query for each entity as {``}What is related of ENTITY?{''}. The input of our model is query and documents(i.e., all utterances of dialogue). Then, our model predicts entity span that is answer for query.","year":2021,"title_abstract":"The Pipeline Model for Resolution of Anaphoric Reference and Resolution of Entity Reference The objective of anaphora resolution in dialogue shared-task is to go above and beyond the simple cases of coreference resolution in written text on which NLP has mostly focused so far, which arguably overestimate the performance of current SOTA models. The anaphora resolution in dialogue shared-task consists of three subtasks; subtask1, resolution of anaphoric identity and non-referring expression identification, subtask2, resolution of bridging references, and subtask3, resolution of discourse deixis\/abstract anaphora. In this paper, we propose the pipelined model (i.e., a resolution of anaphoric identity and a resolution of bridging references) for the subtask1 and the subtask2. In the subtask1, our model detects mention via the parentheses prediction. Then, we yield mention representation using the token representation constituting the mention. Mention representation is fed to the coreference resolution model for clustering. In the subtask2, our model resolves bridging references via the MRC framework. We construct query for each entity as {``}What is related of ENTITY?{''}. The input of our model is query and documents(i.e., all utterances of dialogue). Then, our model predicts entity span that is answer for query.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1713552475,"Goal":"Partnership for the Goals","Task":["Resolution of Anaphoric Reference","Resolution of Entity Reference","anaphora resolution","dialogue shared - task","coreference resolution","NLP","anaphora resolution","dialogue shared - task","resolution of anaphoric identity","non - referring expression identification","resolution of bridging references","resolution of discourse deixis\/abstract anaphora","resolution of anaphoric identity","resolution of bridging references)","clustering","bridging references"],"Method":["Pipeline Model","SOTA models","pipelined model","parentheses prediction","mention representation","token representation","Mention representation","coreference resolution model","MRC framework"]},{"ID":"robertson-etal-2021-covid","title":"A {COVID}-19 news coverage mood map of {E}urope","abstract":"We present a COVID-19 news dashboard which visualizes sentiment in pandemic news coverage in different languages across Europe. The dashboard shows analyses for positive\/neutral\/negative sentiment and moral sentiment for news articles across countries and languages. First we extract news articles from news-crawl. Then we use a pre-trained multilingual BERT model for sentiment analysis of news article headlines and a dictionary and word vectors -based method for moral sentiment analysis of news articles. The resulting dashboard gives a unified overview of news events on COVID-19 news overall sentiment, and the region and language of publication from the period starting from the beginning of January 2020 to the end of January 2021.","year":2021,"title_abstract":"A {COVID}-19 news coverage mood map of {E}urope We present a COVID-19 news dashboard which visualizes sentiment in pandemic news coverage in different languages across Europe. The dashboard shows analyses for positive\/neutral\/negative sentiment and moral sentiment for news articles across countries and languages. First we extract news articles from news-crawl. Then we use a pre-trained multilingual BERT model for sentiment analysis of news article headlines and a dictionary and word vectors -based method for moral sentiment analysis of news articles. The resulting dashboard gives a unified overview of news events on COVID-19 news overall sentiment, and the region and language of publication from the period starting from the beginning of January 2020 to the end of January 2021.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1713529825,"Goal":"Climate Action","Task":["sentiment","sentiment analysis of news article headlines","moral sentiment analysis of news articles"],"Method":["multilingual BERT model","dictionary and word vectors - based method"]},{"ID":"mostafazadeh-davani-etal-2021-improving","title":"Improving Counterfactual Generation for Fair Hate Speech Detection","abstract":"Bias mitigation approaches reduce models{'} dependence on sensitive features of data, such as social group tokens (SGTs), resulting in equal predictions across the sensitive features. In hate speech detection, however, equalizing model predictions may ignore important differences among targeted social groups, as hate speech can contain stereotypical language specific to each SGT. Here, to take the specific language about each SGT into account, we rely on counterfactual fairness and equalize predictions among counterfactuals, generated by changing the SGTs. Our method evaluates the similarity in sentence likelihoods (via pre-trained language models) among counterfactuals, to treat SGTs equally only within interchangeable contexts. By applying logit pairing to equalize outcomes on the restricted set of counterfactuals for each instance, we improve fairness metrics while preserving model performance on hate speech detection.","year":2021,"title_abstract":"Improving Counterfactual Generation for Fair Hate Speech Detection Bias mitigation approaches reduce models{'} dependence on sensitive features of data, such as social group tokens (SGTs), resulting in equal predictions across the sensitive features. In hate speech detection, however, equalizing model predictions may ignore important differences among targeted social groups, as hate speech can contain stereotypical language specific to each SGT. Here, to take the specific language about each SGT into account, we rely on counterfactual fairness and equalize predictions among counterfactuals, generated by changing the SGTs. Our method evaluates the similarity in sentence likelihoods (via pre-trained language models) among counterfactuals, to treat SGTs equally only within interchangeable contexts. By applying logit pairing to equalize outcomes on the restricted set of counterfactuals for each instance, we improve fairness metrics while preserving model performance on hate speech detection.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1713106781,"Goal":"Gender Equality","Task":["Counterfactual Generation","Fair Hate Speech Detection","hate speech detection","hate speech detection"],"Method":["Bias mitigation approaches","equalizing model predictions","language models)","logit pairing"]},{"ID":"hossain-etal-2020-covidlies","title":"{COVIDL}ies: Detecting {COVID}-19 Misinformation on Social Media","abstract":"The ongoing pandemic has heightened the need for developing tools to flag COVID-19-related misinformation on the internet, specifically on social media such as Twitter. However, due to novel language and the rapid change of information, existing misinformation detection datasets are not effective for evaluating systems designed to detect misinformation on this topic. Misinformation detection can be divided into two sub-tasks: (i) retrieval of misconceptions relevant to posts being checked for veracity, and (ii) stance detection to identify whether the posts Agree, Disagree, or express No Stance towards the retrieved misconceptions. To facilitate research on this task, we release COVIDLies (https:\/\/ucinlp.github.io\/covid19 ), a dataset of 6761 expert-annotated tweets to evaluate the performance of misinformation detection systems on 86 different pieces of COVID-19 related misinformation. We evaluate existing NLP systems on this dataset, providing initial benchmarks and identifying key challenges for future models to improve upon.","year":2020,"title_abstract":"{COVIDL}ies: Detecting {COVID}-19 Misinformation on Social Media The ongoing pandemic has heightened the need for developing tools to flag COVID-19-related misinformation on the internet, specifically on social media such as Twitter. However, due to novel language and the rapid change of information, existing misinformation detection datasets are not effective for evaluating systems designed to detect misinformation on this topic. Misinformation detection can be divided into two sub-tasks: (i) retrieval of misconceptions relevant to posts being checked for veracity, and (ii) stance detection to identify whether the posts Agree, Disagree, or express No Stance towards the retrieved misconceptions. To facilitate research on this task, we release COVIDLies (https:\/\/ucinlp.github.io\/covid19 ), a dataset of 6761 expert-annotated tweets to evaluate the performance of misinformation detection systems on 86 different pieces of COVID-19 related misinformation. We evaluate existing NLP systems on this dataset, providing initial benchmarks and identifying key challenges for future models to improve upon.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1712411344,"Goal":"Climate Action","Task":["COVID - 19","misinformation detection","Misinformation detection","retrieval of misconceptions","stance detection"],"Method":["misinformation detection systems","NLP systems"]},{"ID":"zamani-etal-2020-understanding","title":"Understanding Weekly {COVID}-19 Concerns through Dynamic Content-Specific {LDA} Topic Modeling","abstract":"The novelty and global scale of the COVID-19 pandemic has lead to rapid societal changes in a short span of time. As government policy and health measures shift, public perceptions and concerns also change, an evolution documented within discourse on social media.We propose a dynamic content-specific LDA topic modeling technique that can help to identify different domains of COVID-specific discourse that can be used to track societal shifts in concerns or views. Our experiments show that these model-derived topics are more coherent than standard LDA topics, and also provide new features that are more helpful in prediction of COVID-19 related outcomes including social mobility and unemployment rate.","year":2020,"title_abstract":"Understanding Weekly {COVID}-19 Concerns through Dynamic Content-Specific {LDA} Topic Modeling The novelty and global scale of the COVID-19 pandemic has lead to rapid societal changes in a short span of time. As government policy and health measures shift, public perceptions and concerns also change, an evolution documented within discourse on social media.We propose a dynamic content-specific LDA topic modeling technique that can help to identify different domains of COVID-specific discourse that can be used to track societal shifts in concerns or views. Our experiments show that these model-derived topics are more coherent than standard LDA topics, and also provide new features that are more helpful in prediction of COVID-19 related outcomes including social mobility and unemployment rate.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1712172627,"Goal":"Climate Action","Task":["COVID - 19","prediction of COVID - 19 related outcomes","social mobility","unemployment rate"],"Method":["Dynamic Content - Specific","Topic Modeling","dynamic content - specific LDA topic modeling technique","LDA topics"]},{"ID":"han-etal-2019-level","title":"Level-Up: Learning to Improve Proficiency Level of Essays","abstract":"We introduce a method for generating suggestions on a given sentence for improving the proficiency level. In our approach, the sentence is transformed into a sequence of grammatical elements aimed at providing suggestions of more advanced grammar elements based on originals. The method involves parsing the sentence, identifying grammatical elements, and ranking related elements to recommend a higher level of grammatical element. We present a prototype tutoring system, Level-Up, that applies the method to English learners{'} essays in order to assist them in writing and reading. Evaluation on a set of essays shows that our method does assist user in writing.","year":2019,"title_abstract":"Level-Up: Learning to Improve Proficiency Level of Essays We introduce a method for generating suggestions on a given sentence for improving the proficiency level. In our approach, the sentence is transformed into a sequence of grammatical elements aimed at providing suggestions of more advanced grammar elements based on originals. The method involves parsing the sentence, identifying grammatical elements, and ranking related elements to recommend a higher level of grammatical element. We present a prototype tutoring system, Level-Up, that applies the method to English learners{'} essays in order to assist them in writing and reading. Evaluation on a set of essays shows that our method does assist user in writing.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1711012125,"Goal":"Quality Education","Task":["generating suggestions","writing","reading"],"Method":["Level - Up","tutoring system","Level - Up"]},{"ID":"zhang-van-genabith-2021-bidirectional","title":"A Bidirectional Transformer Based Alignment Model for Unsupervised Word Alignment","abstract":"Word alignment and machine translation are two closely related tasks. Neural translation models, such as RNN-based and Transformer models, employ a target-to-source attention mechanism which can provide rough word alignments, but with a rather low accuracy. High-quality word alignment can help neural machine translation in many different ways, such as missing word detection, annotation transfer and lexicon injection. Existing methods for learning word alignment include statistical word aligners (e.g. GIZA++) and recently neural word alignment models. This paper presents a bidirectional Transformer based alignment (BTBA) model for unsupervised learning of the word alignment task. Our BTBA model predicts the current target word by attending the source context and both left-side and right-side target context to produce accurate target-to-source attention (alignment). We further fine-tune the target-to-source attention in the BTBA model to obtain better alignments using a full context based optimization method and self-supervised training. We test our method on three word alignment tasks and show that our method outperforms both previous neural word alignment approaches and the popular statistical word aligner GIZA++.","year":2021,"title_abstract":"A Bidirectional Transformer Based Alignment Model for Unsupervised Word Alignment Word alignment and machine translation are two closely related tasks. Neural translation models, such as RNN-based and Transformer models, employ a target-to-source attention mechanism which can provide rough word alignments, but with a rather low accuracy. High-quality word alignment can help neural machine translation in many different ways, such as missing word detection, annotation transfer and lexicon injection. Existing methods for learning word alignment include statistical word aligners (e.g. GIZA++) and recently neural word alignment models. This paper presents a bidirectional Transformer based alignment (BTBA) model for unsupervised learning of the word alignment task. Our BTBA model predicts the current target word by attending the source context and both left-side and right-side target context to produce accurate target-to-source attention (alignment). We further fine-tune the target-to-source attention in the BTBA model to obtain better alignments using a full context based optimization method and self-supervised training. We test our method on three word alignment tasks and show that our method outperforms both previous neural word alignment approaches and the popular statistical word aligner GIZA++.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1710956246,"Goal":"Gender Equality","Task":["Unsupervised Word Alignment","Word alignment","machine translation","word alignment","neural machine translation","missing word detection","annotation transfer","lexicon injection","learning word alignment","unsupervised learning","word alignment task","word alignment tasks"],"Method":["Bidirectional Transformer Based Alignment Model","Neural translation models","RNN - based and Transformer models","target - to - source attention mechanism","statistical word aligners","neural word alignment models","bidirectional Transformer based alignment","BTBA model","BTBA model","full context based optimization method","self - supervised training","neural word alignment approaches","statistical word aligner"]},{"ID":"rehm-etal-2020-towards","title":"Towards an Interoperable Ecosystem of {AI} and {LT} Platforms: A Roadmap for the Implementation of Different Levels of Interoperability","abstract":"With regard to the wider area of AI\/LT platform interoperability, we concentrate on two core aspects: (1) cross-platform search and discovery of resources and services; (2) composition of cross-platform service workflows. We devise five different levels (of increasing complexity) of platform interoperability that we suggest to implement in a wider federation of AI\/LT platforms. We illustrate the approach using the five emerging AI\/LT platforms AI4EU, ELG, Lynx, QURATOR and SPEAKER.","year":2020,"title_abstract":"Towards an Interoperable Ecosystem of {AI} and {LT} Platforms: A Roadmap for the Implementation of Different Levels of Interoperability With regard to the wider area of AI\/LT platform interoperability, we concentrate on two core aspects: (1) cross-platform search and discovery of resources and services; (2) composition of cross-platform service workflows. We devise five different levels (of increasing complexity) of platform interoperability that we suggest to implement in a wider federation of AI\/LT platforms. We illustrate the approach using the five emerging AI\/LT platforms AI4EU, ELG, Lynx, QURATOR and SPEAKER.","social_need":"Industry, Innovation and Infrastrucure Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation","cosine_similarity":0.1710497141,"Goal":"Industry, Innovation and Infrastrucure","Task":["AI\/LT platform interoperability","cross - platform search and discovery of resources and services; (2) composition of cross - platform service workflows"],"Method":["{AI}","{LT} Platforms","AI\/LT platforms","AI\/LT","ELG","Lynx","QURATOR"]},{"ID":"muller-etal-2016-evaluation","title":"Evaluation of the {KIT} Lecture Translation System","abstract":"To attract foreign students is among the goals of the Karlsruhe Institute of Technology (KIT). One obstacle to achieving this goal is that lectures at KIT are usually held in German which many foreign students are not sufficiently proficient in, as, e.g., opposed to English. While the students from abroad are learning German during their stay at KIT, it is challenging to become proficient enough in it in order to follow a lecture. As a solution to this problem we offer our automatic simultaneous lecture translation. It translates German lectures into English in real time. While not as good as human interpreters, the system is available at a price that KIT can afford in order to offer it in potentially all lectures. In order to assess whether the quality of the system we have conducted a user study. In this paper we present this study, the way it was conducted and its results. The results indicate that the quality of the system has passed a threshold as to be able to support students in their studies. The study has helped to identify the most crucial weaknesses of the systems and has guided us which steps to take next.","year":2016,"title_abstract":"Evaluation of the {KIT} Lecture Translation System To attract foreign students is among the goals of the Karlsruhe Institute of Technology (KIT). One obstacle to achieving this goal is that lectures at KIT are usually held in German which many foreign students are not sufficiently proficient in, as, e.g., opposed to English. While the students from abroad are learning German during their stay at KIT, it is challenging to become proficient enough in it in order to follow a lecture. As a solution to this problem we offer our automatic simultaneous lecture translation. It translates German lectures into English in real time. While not as good as human interpreters, the system is available at a price that KIT can afford in order to offer it in potentially all lectures. In order to assess whether the quality of the system we have conducted a user study. In this paper we present this study, the way it was conducted and its results. The results indicate that the quality of the system has passed a threshold as to be able to support students in their studies. The study has helped to identify the most crucial weaknesses of the systems and has guided us which steps to take next.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1709401608,"Goal":"Quality Education","Task":["automatic simultaneous lecture translation"],"Method":["{KIT} Lecture Translation System","human interpreters"]},{"ID":"yates-etal-2016-effects","title":"Effects of Sampling on {T}witter Trend Detection","abstract":"Much research has focused on detecting trends on Twitter, including health-related trends such as mentions of Influenza-like illnesses or their symptoms. The majority of this research has been conducted using Twitter{'}s public feed, which includes only about 1{\\%} of all public tweets. It is unclear if, when, and how using Twitter{'}s 1{\\%} feed has affected the evaluation of trend detection methods. In this work we use a larger feed to investigate the effects of sampling on Twitter trend detection. We focus on using health-related trends to estimate the prevalence of Influenza-like illnesses based on tweets. We use ground truth obtained from the CDC and Google Flu Trends to explore how the prevalence estimates degrade when moving from a 100{\\%} to a 1{\\%} sample. We find that using the 1{\\%} sample is unlikely to substantially harm ILI estimates made at the national level, but can cause poor performance when estimates are made at the city level.","year":2016,"title_abstract":"Effects of Sampling on {T}witter Trend Detection Much research has focused on detecting trends on Twitter, including health-related trends such as mentions of Influenza-like illnesses or their symptoms. The majority of this research has been conducted using Twitter{'}s public feed, which includes only about 1{\\%} of all public tweets. It is unclear if, when, and how using Twitter{'}s 1{\\%} feed has affected the evaluation of trend detection methods. In this work we use a larger feed to investigate the effects of sampling on Twitter trend detection. We focus on using health-related trends to estimate the prevalence of Influenza-like illnesses based on tweets. We use ground truth obtained from the CDC and Google Flu Trends to explore how the prevalence estimates degrade when moving from a 100{\\%} to a 1{\\%} sample. We find that using the 1{\\%} sample is unlikely to substantially harm ILI estimates made at the national level, but can cause poor performance when estimates are made at the city level.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1707658172,"Goal":"Climate Action","Task":["{T}witter Trend Detection","detecting trends","sampling","Twitter trend detection","prevalence of Influenza - like illnesses","prevalence estimates","ILI"],"Method":["trend detection methods"]},{"ID":"kaushal-vaidhya-2020-winners","title":"Winners at {W}-{NUT} 2020 Shared Task-3: Leveraging Event Specific and Chunk Span information for Extracting {COVID} Entities from Tweets","abstract":"Twitter has acted as an important source of information during disasters and pandemic, especially during the times of COVID-19. In this paper, we describe our system entry for WNUT 2020 Shared Task-3. The task was aimed at automating the extraction of a variety of COVID-19 related events from Twitter, such as individuals who recently contracted the virus, someone with symptoms who were denied testing and believed remedies against the infection. The system consists of separate multi-task models for slot-filling subtasks and sentence-classification subtasks, while leveraging the useful sentence-level information for the corresponding event. The system uses COVID-Twitter-BERT with attention-weighted pooling of candidate slot-chunk features to capture the useful information chunks. The system ranks 1st at the leaderboard with F1 of 0.6598, without using any ensembles or additional datasets.","year":2020,"title_abstract":"Winners at {W}-{NUT} 2020 Shared Task-3: Leveraging Event Specific and Chunk Span information for Extracting {COVID} Entities from Tweets Twitter has acted as an important source of information during disasters and pandemic, especially during the times of COVID-19. In this paper, we describe our system entry for WNUT 2020 Shared Task-3. The task was aimed at automating the extraction of a variety of COVID-19 related events from Twitter, such as individuals who recently contracted the virus, someone with symptoms who were denied testing and believed remedies against the infection. The system consists of separate multi-task models for slot-filling subtasks and sentence-classification subtasks, while leveraging the useful sentence-level information for the corresponding event. The system uses COVID-Twitter-BERT with attention-weighted pooling of candidate slot-chunk features to capture the useful information chunks. The system ranks 1st at the leaderboard with F1 of 0.6598, without using any ensembles or additional datasets.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.170630008,"Goal":"Climate Action","Task":["Extracting {COVID} Entities","COVID - 19","WNUT","extraction","COVID - 19","slot - filling subtasks","sentence - classification subtasks"],"Method":["multi - task models","COVID - Twitter - BERT","attention - weighted pooling of candidate slot - chunk features"]},{"ID":"stasaski-hearst-2017-multiple","title":"Multiple Choice Question Generation Utilizing An Ontology","abstract":"Ontologies provide a structured representation of concepts and the relationships which connect them. This work investigates how a pre-existing educational Biology ontology can be used to generate useful practice questions for students by using the connectivity structure in a novel way. It also introduces a novel way to generate multiple-choice distractors from the ontology, and compares this to a baseline of using embedding representations of nodes. An assessment by an experienced science teacher shows a significant advantage over a baseline when using the ontology for distractor generation. A subsequent study with three science teachers on the results of a modified question generation algorithm finds significant improvements. An in-depth analysis of the teachers{'} comments yields useful insights for any researcher working on automated question generation for educational applications.","year":2017,"title_abstract":"Multiple Choice Question Generation Utilizing An Ontology Ontologies provide a structured representation of concepts and the relationships which connect them. This work investigates how a pre-existing educational Biology ontology can be used to generate useful practice questions for students by using the connectivity structure in a novel way. It also introduces a novel way to generate multiple-choice distractors from the ontology, and compares this to a baseline of using embedding representations of nodes. An assessment by an experienced science teacher shows a significant advantage over a baseline when using the ontology for distractor generation. A subsequent study with three science teachers on the results of a modified question generation algorithm finds significant improvements. An in-depth analysis of the teachers{'} comments yields useful insights for any researcher working on automated question generation for educational applications.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1703692377,"Goal":"Quality Education","Task":["Multiple Choice Question Generation","distractor generation","automated question generation","educational applications"],"Method":["Ontology Ontologies","structured representation","embedding representations of nodes","question generation algorithm"]},{"ID":"pathak-etal-2020-self","title":"Self-Supervised Claim Identification for Automated Fact Checking","abstract":"We propose a novel, attention-based self-supervised approach to identify {``}claim-worthy{''} sentences in a fake news article, an important first step in automated fact-checking. We leverage \\textit{aboutness} of headline and content using attention mechanism for this task. The identified claims can be used for downstream task of claim verification for which we are releasing a benchmark dataset of manually selected compelling articles with veracity labels and associated evidence. This work goes beyond stylistic analysis to identifying content that influences reader belief. Experiments with three datasets show the strength of our model.","year":2020,"title_abstract":"Self-Supervised Claim Identification for Automated Fact Checking We propose a novel, attention-based self-supervised approach to identify {``}claim-worthy{''} sentences in a fake news article, an important first step in automated fact-checking. We leverage \\textit{aboutness} of headline and content using attention mechanism for this task. The identified claims can be used for downstream task of claim verification for which we are releasing a benchmark dataset of manually selected compelling articles with veracity labels and associated evidence. This work goes beyond stylistic analysis to identifying content that influences reader belief. Experiments with three datasets show the strength of our model.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1703274399,"Goal":"Climate Action","Task":["Self - Supervised Claim Identification","Automated Fact Checking","automated fact - checking","claim verification","content"],"Method":["attention - based self - supervised approach","attention mechanism","stylistic analysis"]},{"ID":"lauscher-glavas-2019-consistently","title":"Are We Consistently Biased? Multidimensional Analysis of Biases in Distributional Word Vectors","abstract":"Word embeddings have recently been shown to reflect many of the pronounced societal biases (e.g., gender bias or racial bias). Existing studies are, however, limited in scope and do not investigate the consistency of biases across relevant dimensions like embedding models, types of texts, and different languages. In this work, we present a systematic study of biases encoded in distributional word vector spaces: we analyze how consistent the bias effects are across languages, corpora, and embedding models. Furthermore, we analyze the cross-lingual biases encoded in bilingual embedding spaces, indicative of the effects of bias transfer encompassed in cross-lingual transfer of NLP models. Our study yields some unexpected findings, e.g., that biases can be emphasized or downplayed by different embedding models or that user-generated content may be less biased than encyclopedic text. We hope our work catalyzes bias research in NLP and informs the development of bias reduction techniques.","year":2019,"title_abstract":"Are We Consistently Biased? Multidimensional Analysis of Biases in Distributional Word Vectors Word embeddings have recently been shown to reflect many of the pronounced societal biases (e.g., gender bias or racial bias). Existing studies are, however, limited in scope and do not investigate the consistency of biases across relevant dimensions like embedding models, types of texts, and different languages. In this work, we present a systematic study of biases encoded in distributional word vector spaces: we analyze how consistent the bias effects are across languages, corpora, and embedding models. Furthermore, we analyze the cross-lingual biases encoded in bilingual embedding spaces, indicative of the effects of bias transfer encompassed in cross-lingual transfer of NLP models. Our study yields some unexpected findings, e.g., that biases can be emphasized or downplayed by different embedding models or that user-generated content may be less biased than encyclopedic text. We hope our work catalyzes bias research in NLP and informs the development of bias reduction techniques.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1702926159,"Goal":"Reduced Inequalities","Task":["Multidimensional Analysis of Biases","Word embeddings","NLP","bias research","NLP"],"Method":["embedding models","embedding models","bias transfer","cross - lingual transfer","embedding models","bias reduction techniques"]},{"ID":"lapesa-etal-2020-debatenet","title":"{DE}bate{N}et-mig15:Tracing the 2015 Immigration Debate in {G}ermany Over Time","abstract":"DEbateNet-migr15 is a manually annotated dataset for German which covers the public debate on immigration in 2015. The building block of our annotation is the political science notion of a claim, i.e., a statement made by a political actor (a politician, a party, or a group of citizens) that a specific action should be taken (e.g., vacant flats should be assigned to refugees). We identify claims in newspaper articles, assign them to actors and fine-grained categories and annotate their polarity and date. The aim of this paper is two-fold: first, we release the full DEbateNet-mig15 corpus and document it by means of a quantitative and qualitative analysis; second, we demonstrate its application in a discourse network analysis framework, which enables us to capture the temporal dynamics of the political debate","year":2020,"title_abstract":"{DE}bate{N}et-mig15:Tracing the 2015 Immigration Debate in {G}ermany Over Time DEbateNet-migr15 is a manually annotated dataset for German which covers the public debate on immigration in 2015. The building block of our annotation is the political science notion of a claim, i.e., a statement made by a political actor (a politician, a party, or a group of citizens) that a specific action should be taken (e.g., vacant flats should be assigned to refugees). We identify claims in newspaper articles, assign them to actors and fine-grained categories and annotate their polarity and date. The aim of this paper is two-fold: first, we release the full DEbateNet-mig15 corpus and document it by means of a quantitative and qualitative analysis; second, we demonstrate its application in a discourse network analysis framework, which enables us to capture the temporal dynamics of the political debate","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1702264249,"Goal":"Sustainable Cities and Communities","Task":["political science notion","quantitative and qualitative analysis;"],"Method":["discourse network analysis framework"]},{"ID":"jalili-sabet-etal-2016-improving","title":"Improving Word Alignment of Rare Words with Word Embeddings","abstract":"We address the problem of inducing word alignment for language pairs by developing an unsupervised model with the capability of getting applied to other generative alignment models. We approach the task by: i)proposing a new alignment model based on the IBM alignment model 1 that uses vector representation of words, and ii)examining the use of similar source words to overcome the problem of rare source words and improving the alignments. We apply our method to English-French corpora and run the experiments with different sizes of sentence pairs. Our results show competitive performance against the baseline and in some cases improve the results up to 6.9{\\%} in terms of precision.","year":2016,"title_abstract":"Improving Word Alignment of Rare Words with Word Embeddings We address the problem of inducing word alignment for language pairs by developing an unsupervised model with the capability of getting applied to other generative alignment models. We approach the task by: i)proposing a new alignment model based on the IBM alignment model 1 that uses vector representation of words, and ii)examining the use of similar source words to overcome the problem of rare source words and improving the alignments. We apply our method to English-French corpora and run the experiments with different sizes of sentence pairs. Our results show competitive performance against the baseline and in some cases improve the results up to 6.9{\\%} in terms of precision.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1702217758,"Goal":"Gender Equality","Task":["Word Alignment of Rare Words","inducing word alignment"],"Method":["Word Embeddings","unsupervised model","generative alignment models","alignment model","IBM alignment model","vector representation of words"]},{"ID":"morell-etal-2012-iula2standoff","title":"{I}ula2{S}tandoff: a tool for creating standoff documents for the {IULACT}","abstract":"Due to the increase in the number and depth of analyses required over the text, like entity recognition, POS tagging, syntactic analysis, etc. the annotation in-line has become unpractical. In Natural Language Processing (NLP) some emphasis has been placed in finding an annotation method to solve this problem. A possibility is the standoff annotation. With this annotation style it is possible to add new levels of annotation without disturbing exiting ones, with minimal knock on effects. This annotation will increase the possibility of adding more linguistic information as well as more possibilities for sharing textual resources. In this paper we present a tool developed in the framework of the European Metanet4u (Enhancing the European Linguistic Infrastructure, GA 270893) for creating a multi-layered XML annotation scheme, based on the GrAF proposal for standoff annotations.","year":2012,"title_abstract":"{I}ula2{S}tandoff: a tool for creating standoff documents for the {IULACT} Due to the increase in the number and depth of analyses required over the text, like entity recognition, POS tagging, syntactic analysis, etc. the annotation in-line has become unpractical. In Natural Language Processing (NLP) some emphasis has been placed in finding an annotation method to solve this problem. A possibility is the standoff annotation. With this annotation style it is possible to add new levels of annotation without disturbing exiting ones, with minimal knock on effects. This annotation will increase the possibility of adding more linguistic information as well as more possibilities for sharing textual resources. In this paper we present a tool developed in the framework of the European Metanet4u (Enhancing the European Linguistic Infrastructure, GA 270893) for creating a multi-layered XML annotation scheme, based on the GrAF proposal for standoff annotations.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1701962501,"Goal":"Life on Land","Task":["syntactic analysis","annotation","Natural Language Processing","standoff annotations"],"Method":["entity recognition","POS tagging","annotation method","standoff annotation","annotation style","multi - layered XML annotation scheme","GrAF proposal"]},{"ID":"yuste-rodrigo-2001-making","title":"Making {MT} commonplace in translation training curricula {\\mbox{$\\bullet$}} too many misconceptions, so much potential!","abstract":"This paper tackles the issue of how to teach Machine Translation (MT) to future translators enrolled in a university translation-training course. Teaching MT to trainee translators usually entails two main difficulties: first, a misunderstanding of what MT is really useful for, which normally leads to the misconception that MT output{'}s quality always equals zero; second, a widespread fear that machines are to replace human translators, consequently leaving them out of work. In order to fight these generalised prejudices on MT among (future) translators, translation instruction should be primarily practical and realistic, as well as learner-centred. It thus ought to highlight the fact that: 1) MT systems and applications are essential components of today{'}s global multilingual documentation production; 2) the way in which MT is employed in large multilingual organisations and international companies opens up new work avenues for translators. This will be illustrated by two activities, one using commercial MT systems for quick translations, whose process outcome is improved through the trainees{'} interaction with the system; the other focusing on MT output comprehensibility by speakers of target language only. MT is thus a mainstream component of a translation-training framework delineated in Yuste (2000) that, by placing the trainee in workplace-like situations, also echoes Kiraly (1999).","year":2001,"title_abstract":"Making {MT} commonplace in translation training curricula {\\mbox{$\\bullet$}} too many misconceptions, so much potential! This paper tackles the issue of how to teach Machine Translation (MT) to future translators enrolled in a university translation-training course. Teaching MT to trainee translators usually entails two main difficulties: first, a misunderstanding of what MT is really useful for, which normally leads to the misconception that MT output{'}s quality always equals zero; second, a widespread fear that machines are to replace human translators, consequently leaving them out of work. In order to fight these generalised prejudices on MT among (future) translators, translation instruction should be primarily practical and realistic, as well as learner-centred. It thus ought to highlight the fact that: 1) MT systems and applications are essential components of today{'}s global multilingual documentation production; 2) the way in which MT is employed in large multilingual organisations and international companies opens up new work avenues for translators. This will be illustrated by two activities, one using commercial MT systems for quick translations, whose process outcome is improved through the trainees{'} interaction with the system; the other focusing on MT output comprehensibility by speakers of target language only. MT is thus a mainstream component of a translation-training framework delineated in Yuste (2000) that, by placing the trainee in workplace-like situations, also echoes Kiraly (1999).","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.170178324,"Goal":"Quality Education","Task":["translation training curricula","Machine Translation","university translation - training course","MT","translators","MT","translators","translation instruction","MT","global multilingual documentation production;","translators","MT","quick translations","MT"],"Method":["MT","MT","MT","translation - training framework"]},{"ID":"mendelsohn-etal-2021-modeling","title":"Modeling Framing in Immigration Discourse on Social Media","abstract":"The framing of political issues can influence policy and public opinion. Even though the public plays a key role in creating and spreading frames, little is known about how ordinary people on social media frame political issues. By creating a new dataset of immigration-related tweets labeled for multiple framing typologies from political communication theory, we develop supervised models to detect frames. We demonstrate how users{'} ideology and region impact framing choices, and how a message{'}s framing influences audience responses. We find that the more commonly-used issue-generic frames obscure important ideological and regional patterns that are only revealed by immigration-specific frames. Furthermore, frames oriented towards human interests, culture, and politics are associated with higher user engagement. This large-scale analysis of a complex social and linguistic phenomenon contributes to both NLP and social science research.","year":2021,"title_abstract":"Modeling Framing in Immigration Discourse on Social Media The framing of political issues can influence policy and public opinion. Even though the public plays a key role in creating and spreading frames, little is known about how ordinary people on social media frame political issues. By creating a new dataset of immigration-related tweets labeled for multiple framing typologies from political communication theory, we develop supervised models to detect frames. We demonstrate how users{'} ideology and region impact framing choices, and how a message{'}s framing influences audience responses. We find that the more commonly-used issue-generic frames obscure important ideological and regional patterns that are only revealed by immigration-specific frames. Furthermore, frames oriented towards human interests, culture, and politics are associated with higher user engagement. This large-scale analysis of a complex social and linguistic phenomenon contributes to both NLP and social science research.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1701680869,"Goal":"Climate Action","Task":["Modeling Framing","Immigration Discourse","framing of political issues","spreading frames","political communication theory","large - scale analysis of a complex social and linguistic phenomenon","NLP and social science research"],"Method":["framing typologies","supervised models"]},{"ID":"aji-etal-2022-one","title":"One Country, 700+ Languages: {NLP} Challenges for Underrepresented Languages and Dialects in {I}ndonesia","abstract":"NLP research is impeded by a lack of resources and awareness of the challenges presented by underrepresented languages and dialects. Focusing on the languages spoken in Indonesia, the second most linguistically diverse and the fourth most populous nation of the world, we provide an overview of the current state of NLP research for Indonesia{'}s 700+ languages. We highlight challenges in Indonesian NLP and how these affect the performance of current NLP systems. Finally, we provide general recommendations to help develop NLP technology not only for languages of Indonesia but also other underrepresented languages.","year":2022,"title_abstract":"One Country, 700+ Languages: {NLP} Challenges for Underrepresented Languages and Dialects in {I}ndonesia NLP research is impeded by a lack of resources and awareness of the challenges presented by underrepresented languages and dialects. Focusing on the languages spoken in Indonesia, the second most linguistically diverse and the fourth most populous nation of the world, we provide an overview of the current state of NLP research for Indonesia{'}s 700+ languages. We highlight challenges in Indonesian NLP and how these affect the performance of current NLP systems. Finally, we provide general recommendations to help develop NLP technology not only for languages of Indonesia but also other underrepresented languages.","social_need":"No Poverty End poverty in all its forms everywhere","cosine_similarity":0.1699780375,"Goal":"No Poverty","Task":["NLP","NLP","NLP"],"Method":["NLP technology"]},{"ID":"evert-etal-2020-corpus","title":"{C}orpus {Q}uery {L}ingua {F}ranca part {II}: Ontology","abstract":"The present paper outlines the projected second part of the Corpus Query Lingua Franca (CQLF) family of standards: CQLF Ontology, which is currently in the process of standardization at the International Standards Organization (ISO), in its Technical Committee 37, Subcommittee 4 (TC37SC4) and its national mirrors. The first part of the family, ISO 24623-1 (henceforth CQLF Metamodel), was successfully adopted as an international standard at the beginning of 2018. The present paper reflects the state of the CQLF Ontology at the moment of submission for the Committee Draft ballot. We provide a brief overview of the CQLF Metamodel, present the assumptions and aims of the CQLF Ontology, its basic structure, and its potential extended applications. The full ontology is expected to emerge from a community process, starting from an initial version created by the authors of the present paper.","year":2020,"title_abstract":"{C}orpus {Q}uery {L}ingua {F}ranca part {II}: Ontology The present paper outlines the projected second part of the Corpus Query Lingua Franca (CQLF) family of standards: CQLF Ontology, which is currently in the process of standardization at the International Standards Organization (ISO), in its Technical Committee 37, Subcommittee 4 (TC37SC4) and its national mirrors. The first part of the family, ISO 24623-1 (henceforth CQLF Metamodel), was successfully adopted as an international standard at the beginning of 2018. The present paper reflects the state of the CQLF Ontology at the moment of submission for the Committee Draft ballot. We provide a brief overview of the CQLF Metamodel, present the assumptions and aims of the CQLF Ontology, its basic structure, and its potential extended applications. The full ontology is expected to emerge from a community process, starting from an initial version created by the authors of the present paper.","social_need":"Clean Water and Sanitation Ensure availability and sustainable management of water and sanitation for all","cosine_similarity":0.1698155403,"Goal":"Clean Water and Sanitation","Task":["Corpus Query Lingua Franca"],"Method":["CQLF Ontology","CQLF Metamodel)","CQLF Ontology","CQLF","CQLF Ontology"]},{"ID":"el-khatib-etal-2016-topotext","title":"{T}opo{T}ext: Interactive Digital Mapping of Literary Text","abstract":"We demonstrate TopoText, an interactive tool for digital mapping of literary text. TopoText takes as input a literary piece of text such as a novel or a biography article and automatically extracts all place names in the text. The identified places are then geoparsed and displayed on an interactive map. TopoText calculates the number of times a place was mentioned in the text, which is then reflected on the map allowing the end-user to grasp the importance of the different places within the text. It also displays the most frequent words mentioned within a specified proximity of a place name in context or across the entire text. This can also be faceted according to part of speech tags. Finally, TopoText keeps the human in the loop by allowing the end-user to disambiguate places and to provide specific place annotations. All extracted information such as geolocations, place frequencies, as well as all user-provided annotations can be automatically exported as a CSV file that can be imported later by the same user or other users.","year":2016,"title_abstract":"{T}opo{T}ext: Interactive Digital Mapping of Literary Text We demonstrate TopoText, an interactive tool for digital mapping of literary text. TopoText takes as input a literary piece of text such as a novel or a biography article and automatically extracts all place names in the text. The identified places are then geoparsed and displayed on an interactive map. TopoText calculates the number of times a place was mentioned in the text, which is then reflected on the map allowing the end-user to grasp the importance of the different places within the text. It also displays the most frequent words mentioned within a specified proximity of a place name in context or across the entire text. This can also be faceted according to part of speech tags. Finally, TopoText keeps the human in the loop by allowing the end-user to disambiguate places and to provide specific place annotations. All extracted information such as geolocations, place frequencies, as well as all user-provided annotations can be automatically exported as a CSV file that can be imported later by the same user or other users.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1697865725,"Goal":"Sustainable Cities and Communities","Task":["Interactive Digital Mapping of Literary Text","digital mapping of literary text"],"Method":["TopoText","interactive tool","TopoText","TopoText","TopoText"]},{"ID":"reed-cutsuridis-2020-demonstration","title":"Demonstration of a Literature Based Discovery System based on Ontologies, Semantic Filters and Word Embeddings for the Raynaud Disease-Fish Oil Rediscovery","abstract":"A novel literature-based discovery system based on UMLS Ontologies, Semantic Filters, Statistics, and Word Embed-dings was developed and validated against the well-established Raynaud{'}s disease {--} Fish Oil discovery by min-ing different size and specificity corpora of Pubmed titles and abstracts. Results show an {`}inverse effect{'} between open ver-sus closed discovery search modes. In open discovery, a more general and bigger corpus (Vascular disease or Peri-vascular disease) produces better results than a more specific and smaller in size corpus (Raynaud disease), whereas in closed discovery, the exact opposite is true.","year":2020,"title_abstract":"Demonstration of a Literature Based Discovery System based on Ontologies, Semantic Filters and Word Embeddings for the Raynaud Disease-Fish Oil Rediscovery A novel literature-based discovery system based on UMLS Ontologies, Semantic Filters, Statistics, and Word Embed-dings was developed and validated against the well-established Raynaud{'}s disease {--} Fish Oil discovery by min-ing different size and specificity corpora of Pubmed titles and abstracts. Results show an {`}inverse effect{'} between open ver-sus closed discovery search modes. In open discovery, a more general and bigger corpus (Vascular disease or Peri-vascular disease) produces better results than a more specific and smaller in size corpus (Raynaud disease), whereas in closed discovery, the exact opposite is true.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.1697396934,"Goal":"Life Below Water","Task":["Raynaud Disease - Fish Oil Rediscovery","Fish Oil discovery","open discovery","closed discovery"],"Method":["Literature Based Discovery System","Semantic Filters","Word Embeddings","literature - based discovery system","UMLS Ontologies","Semantic Filters","Statistics","Word Embed - dings","Raynaud{'}s disease","sus closed discovery search modes"]},{"ID":"konkol-etal-2017-geographical","title":"Geographical Evaluation of Word Embeddings","abstract":"Word embeddings are commonly compared either with human-annotated word similarities or through improvements in natural language processing tasks. We propose a novel principle which compares the information from word embeddings with reality. We implement this principle by comparing the information in the word embeddings with geographical positions of cities. Our evaluation linearly transforms the semantic space to optimally fit the real positions of cities and measures the deviation between the position given by word embeddings and the real position. A set of well-known word embeddings with state-of-the-art results were evaluated. We also introduce a visualization that helps with error analysis.","year":2017,"title_abstract":"Geographical Evaluation of Word Embeddings Word embeddings are commonly compared either with human-annotated word similarities or through improvements in natural language processing tasks. We propose a novel principle which compares the information from word embeddings with reality. We implement this principle by comparing the information in the word embeddings with geographical positions of cities. Our evaluation linearly transforms the semantic space to optimally fit the real positions of cities and measures the deviation between the position given by word embeddings and the real position. A set of well-known word embeddings with state-of-the-art results were evaluated. We also introduce a visualization that helps with error analysis.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1696617007,"Goal":"Sustainable Cities and Communities","Task":["Geographical Evaluation of Word Embeddings","Word embeddings","natural language processing tasks","error analysis"],"Method":["word embeddings","visualization"]},{"ID":"lee-etal-2021-mnlp","title":"{MNLP} at {MEDIQA} 2021: Fine-Tuning {PEGASUS} for Consumer Health Question Summarization","abstract":"This paper details a Consumer Health Question (CHQ) summarization model submitted to MEDIQA 2021 for shared task 1: Question Summarization. Many CHQs are composed of multiple sentences with typos or unnecessary information, which can interfere with automated question answering systems. Question summarization mitigates this issue by removing this unnecessary information, aiding automated systems in generating a more accurate summary. Our summarization approach focuses on applying multiple pre-processing techniques, including question focus identification on the input and the development of an ensemble method to combine question focus with an abstractive summarization method. We use the state-of-art abstractive summarization model, PEGASUS (Pre-training with Extracted Gap-sentences for Abstractive Summarization), to generate abstractive summaries. Our experiments show that using our ensemble method, which combines abstractive summarization with question focus identification, improves performance over using summarization alone. Our model shows a ROUGE-2 F-measure of 11.14{\\%} against the official test dataset.","year":2021,"title_abstract":"{MNLP} at {MEDIQA} 2021: Fine-Tuning {PEGASUS} for Consumer Health Question Summarization This paper details a Consumer Health Question (CHQ) summarization model submitted to MEDIQA 2021 for shared task 1: Question Summarization. Many CHQs are composed of multiple sentences with typos or unnecessary information, which can interfere with automated question answering systems. Question summarization mitigates this issue by removing this unnecessary information, aiding automated systems in generating a more accurate summary. Our summarization approach focuses on applying multiple pre-processing techniques, including question focus identification on the input and the development of an ensemble method to combine question focus with an abstractive summarization method. We use the state-of-art abstractive summarization model, PEGASUS (Pre-training with Extracted Gap-sentences for Abstractive Summarization), to generate abstractive summaries. Our experiments show that using our ensemble method, which combines abstractive summarization with question focus identification, improves performance over using summarization alone. Our model shows a ROUGE-2 F-measure of 11.14{\\%} against the official test dataset.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.1696296334,"Goal":"Good Health and Well-Being","Task":["Fine - Tuning","Consumer Health Question Summarization","summarization","Question Summarization","CHQs","automated question answering systems","summarization","summarization","summarization","summarization","Abstractive Summarization)","abstractive summarization","summarization"],"Method":["Consumer Health Question","pre - processing techniques","question focus identification","ensemble method","PEGASUS (Pre - training","ensemble method","question focus identification"]},{"ID":"datta-etal-2021-virtual","title":"Virtual Pre-Service Teacher Assessment and Feedback via Conversational Agents","abstract":"Conversational agents and assistants have been used for decades to facilitate learning. There are many examples of conversational agents used for educational and training purposes in K-12, higher education, healthcare, the military, and private industry settings. The most common forms of conversational agents in education are teaching agents that directly teach and support learning, peer agents that serve as knowledgeable learning companions to guide learners in the learning process, and teachable agents that function as a novice or less-knowledgeable student trained and taught by a learner who learns by teaching. The Instructional Quality Assessment (IQA) provides a robust framework to evaluate reading comprehension and mathematics instruction. We developed a system for pre-service teachers, individuals in a teacher preparation program, to evaluate teaching instruction quality based on a modified interpretation of IQA metrics. Our demonstration and approach take advantage of recent advances in Natural Language Processing (NLP) and deep learning for each dialogue system component. We built an open-source conversational agent system to engage pre-service teachers in a specific mathematical scenario focused on scale factor with the aim to provide feedback on pre-service teachers{'} questioning strategies. We believe our system is not only practical for teacher education programs but can also enable other researchers to build new educational scenarios with minimal effort.","year":2021,"title_abstract":"Virtual Pre-Service Teacher Assessment and Feedback via Conversational Agents Conversational agents and assistants have been used for decades to facilitate learning. There are many examples of conversational agents used for educational and training purposes in K-12, higher education, healthcare, the military, and private industry settings. The most common forms of conversational agents in education are teaching agents that directly teach and support learning, peer agents that serve as knowledgeable learning companions to guide learners in the learning process, and teachable agents that function as a novice or less-knowledgeable student trained and taught by a learner who learns by teaching. The Instructional Quality Assessment (IQA) provides a robust framework to evaluate reading comprehension and mathematics instruction. We developed a system for pre-service teachers, individuals in a teacher preparation program, to evaluate teaching instruction quality based on a modified interpretation of IQA metrics. Our demonstration and approach take advantage of recent advances in Natural Language Processing (NLP) and deep learning for each dialogue system component. We built an open-source conversational agent system to engage pre-service teachers in a specific mathematical scenario focused on scale factor with the aim to provide feedback on pre-service teachers{'} questioning strategies. We believe our system is not only practical for teacher education programs but can also enable other researchers to build new educational scenarios with minimal effort.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1695950478,"Goal":"Quality Education","Task":["Virtual Pre - Service Teacher Assessment and Feedback","learning","educational and training purposes","K - 12","higher education","healthcare","private industry settings","education","learning","learning process","reading comprehension and mathematics instruction","pre - service teachers","teacher preparation program","Natural Language Processing","deep learning","teacher education programs"],"Method":["Conversational Agents","Conversational agents","assistants","conversational agents","conversational agents","teaching agents","peer agents","teachable agents","dialogue system component","open - source conversational agent system","questioning strategies"]},{"ID":"alnajjar-etal-2020-verdd","title":"Ve{'}rdd. Narrowing the Gap between Paper Dictionaries, Low-Resource {NLP} and Community Involvement","abstract":"We present an open-source online dictionary editing system, Ve\u2032rdd, that offers a chance to re-evaluate and edit grassroots dictionaries that have been exposed to multiple amateur editors. The idea is to incorporate community activities into a state-of-the-art finite-state language description of a seriously endangered minority language, Skolt Sami. Problems involve getting the community to take part in things above the pencil-and-paper level. At times, it seems that the native speakers and the dictionary oriented are lacking technical understanding to utilize the infrastructures which might make their work more meaningful in the future, i.e. multiple reuse of all of their input. Therefore, our system integrates with the existing tools and infrastructures for Uralic language masking the technical complexities behind a user-friendly UI.","year":2020,"title_abstract":"Ve{'}rdd. Narrowing the Gap between Paper Dictionaries, Low-Resource {NLP} and Community Involvement We present an open-source online dictionary editing system, Ve\u2032rdd, that offers a chance to re-evaluate and edit grassroots dictionaries that have been exposed to multiple amateur editors. The idea is to incorporate community activities into a state-of-the-art finite-state language description of a seriously endangered minority language, Skolt Sami. Problems involve getting the community to take part in things above the pencil-and-paper level. At times, it seems that the native speakers and the dictionary oriented are lacking technical understanding to utilize the infrastructures which might make their work more meaningful in the future, i.e. multiple reuse of all of their input. Therefore, our system integrates with the existing tools and infrastructures for Uralic language masking the technical complexities behind a user-friendly UI.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1694724113,"Goal":"Sustainable Cities and Communities","Task":["Uralic language"],"Method":["online dictionary editing system","Ve\u2032rdd","finite - state language description"]},{"ID":"celik-etal-2021-su","title":"{SU}-{NLP} at {CASE} 2021 Task 1: Protest News Detection for {E}nglish","abstract":"This paper summarizes our group{'}s efforts in the multilingual protest news detection shared task, which is organized as a part of the Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE) Workshop. We participated in all four subtasks in English. Especially in the identification of event containing sentences task, our proposed ensemble approach using RoBERTa and multichannel CNN-LexStem model yields higher performance. Similarly in the event extraction task, our transformer-LSTM-CRF architecture outperforms regular transformers significantly.","year":2021,"title_abstract":"{SU}-{NLP} at {CASE} 2021 Task 1: Protest News Detection for {E}nglish This paper summarizes our group{'}s efforts in the multilingual protest news detection shared task, which is organized as a part of the Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE) Workshop. We participated in all four subtasks in English. Especially in the identification of event containing sentences task, our proposed ensemble approach using RoBERTa and multichannel CNN-LexStem model yields higher performance. Similarly in the event extraction task, our transformer-LSTM-CRF architecture outperforms regular transformers significantly.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1694681644,"Goal":"Climate Action","Task":["Protest News Detection","multilingual protest news detection shared task","Automated Extraction of Socio - political Events","identification of event containing sentences task","event extraction task"],"Method":["ensemble approach","RoBERTa","multichannel CNN - LexStem model","transformer - LSTM","CRF","regular transformers"]},{"ID":"tardy-etal-2020-align","title":"Align then Summarize: Automatic Alignment Methods for Summarization Corpus Creation","abstract":"Summarizing texts is not a straightforward task. Before even considering text summarization, one should determine what kind of summary is expected. How much should the information be compressed? Is it relevant to reformulate or should the summary stick to the original phrasing? State-of-the-art on automatic text summarization mostly revolves around news articles. We suggest that considering a wider variety of tasks would lead to an improvement in the field, in terms of generalization and robustness. We explore meeting summarization: generating reports from automatic transcriptions. Our work consists in segmenting and aligning transcriptions with respect to reports, to get a suitable dataset for neural summarization. Using a bootstrapping approach, we provide pre-alignments that are corrected by human annotators, making a validation set against which we evaluate automatic models. This consistently reduces annotators{'} efforts by providing iteratively better pre-alignment and maximizes the corpus size by using annotations from our automatic alignment models. Evaluation is conducted on publicmeetings, a novel corpus of aligned public meetings. We report automatic alignment and summarization performances on this corpus and show that automatic alignment is relevant for data annotation since it leads to large improvement of almost +4 on all ROUGE scores on the summarization task.","year":2020,"title_abstract":"Align then Summarize: Automatic Alignment Methods for Summarization Corpus Creation Summarizing texts is not a straightforward task. Before even considering text summarization, one should determine what kind of summary is expected. How much should the information be compressed? Is it relevant to reformulate or should the summary stick to the original phrasing? State-of-the-art on automatic text summarization mostly revolves around news articles. We suggest that considering a wider variety of tasks would lead to an improvement in the field, in terms of generalization and robustness. We explore meeting summarization: generating reports from automatic transcriptions. Our work consists in segmenting and aligning transcriptions with respect to reports, to get a suitable dataset for neural summarization. Using a bootstrapping approach, we provide pre-alignments that are corrected by human annotators, making a validation set against which we evaluate automatic models. This consistently reduces annotators{'} efforts by providing iteratively better pre-alignment and maximizes the corpus size by using annotations from our automatic alignment models. Evaluation is conducted on publicmeetings, a novel corpus of aligned public meetings. We report automatic alignment and summarization performances on this corpus and show that automatic alignment is relevant for data annotation since it leads to large improvement of almost +4 on all ROUGE scores on the summarization task.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1693778485,"Goal":"Gender Equality","Task":["Summarization","Summarizing texts","text summarization","automatic text summarization","meeting summarization","generating reports","automatic transcriptions","aligning transcriptions","summarization","automatic alignment","summarization","automatic alignment","data annotation","summarization task"],"Method":["Automatic Alignment Methods","bootstrapping approach","automatic models","automatic alignment models"]},{"ID":"cromieres-kurohashi-2019-kyoto","title":"{K}yoto {U}niversity Participation to the {WMT} 2019 News Shared Task","abstract":"We describe here the experiments we did for the the news translation shared task of WMT 2019. We focused on the new German-to-French language direction, and mostly used current standard approaches to develop a Neural Machine Translation system. We make use of the Tensor2Tensor implementation of the Transformer model. After carefully cleaning the data and noting the importance of the good use of recent monolingual data for the task, we obtain our final result by combining the output of a diverse set of trained models through the use of their {``}checkpoint agreement{''}.","year":2019,"title_abstract":"{K}yoto {U}niversity Participation to the {WMT} 2019 News Shared Task We describe here the experiments we did for the the news translation shared task of WMT 2019. We focused on the new German-to-French language direction, and mostly used current standard approaches to develop a Neural Machine Translation system. We make use of the Tensor2Tensor implementation of the Transformer model. After carefully cleaning the data and noting the importance of the good use of recent monolingual data for the task, we obtain our final result by combining the output of a diverse set of trained models through the use of their {``}checkpoint agreement{''}.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1692524552,"Goal":"Gender Equality","Task":["news translation shared task","WMT 2019"],"Method":["Neural Machine Translation system","Tensor2Tensor implementation","Transformer model"]},{"ID":"guo-etal-2022-auto","title":"Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts","abstract":"Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task. In this paper, we propose an automatic method to mitigate the biases in pretrained language models. Different from previous debiasing work that uses external corpora to fine-tune the pretrained models, we instead directly probe the biases encoded in pretrained models through prompts. Specifically, we propose a variant of the beam search method to automatically search for \\textit{biased prompts} such that the cloze-style completions are the most different with respect to different demographic groups. Given the identified biased prompts, we then propose a distribution alignment loss to mitigate the biases. Experiment results on standard datasets and metrics show that our proposed \\textbf{Auto-Debias} approach can significantly reduce biases, including gender and racial bias, in pretrained language models such as BERT, RoBERTa and ALBERT. Moreover, the improvement in fairness does not decrease the language models{'} understanding abilities, as shown using the GLUE benchmark.","year":2022,"title_abstract":"Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task. In this paper, we propose an automatic method to mitigate the biases in pretrained language models. Different from previous debiasing work that uses external corpora to fine-tune the pretrained models, we instead directly probe the biases encoded in pretrained models through prompts. Specifically, we propose a variant of the beam search method to automatically search for \\textit{biased prompts} such that the cloze-style completions are the most different with respect to different demographic groups. Given the identified biased prompts, we then propose a distribution alignment loss to mitigate the biases. Experiment results on standard datasets and metrics show that our proposed \\textbf{Auto-Debias} approach can significantly reduce biases, including gender and racial bias, in pretrained language models such as BERT, RoBERTa and ALBERT. Moreover, the improvement in fairness does not decrease the language models{'} understanding abilities, as shown using the GLUE benchmark.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1691842973,"Goal":"Gender Equality","Task":["real - world applications"],"Method":["Auto - Debias","Debiasing","Masked Language Models","pretrained language models","automatic method","pretrained language models","debiasing","pretrained models","pretrained models","beam search method","distribution alignment loss","\\textbf{Auto - Debias} approach","pretrained language models","BERT","ALBERT","language models{'}"]},{"ID":"dawkins-2021-second","title":"Second Order {W}ino{B}ias ({S}o{W}ino{B}ias) Test Set for Latent Gender Bias Detection in Coreference Resolution","abstract":"We observe an instance of gender-induced bias in a downstream application, despite the absence of explicit gender words in the test cases. We provide a test set, SoWinoBias, for the purpose of measuring such latent gender bias in coreference resolution systems. We evaluate the performance of current debiasing methods on the SoWinoBias test set, especially in reference to the method{'}s design and altered embedding space properties. See https:\/\/github.com\/hillary-dawkins\/SoWinoBias.","year":2021,"title_abstract":"Second Order {W}ino{B}ias ({S}o{W}ino{B}ias) Test Set for Latent Gender Bias Detection in Coreference Resolution We observe an instance of gender-induced bias in a downstream application, despite the absence of explicit gender words in the test cases. We provide a test set, SoWinoBias, for the purpose of measuring such latent gender bias in coreference resolution systems. We evaluate the performance of current debiasing methods on the SoWinoBias test set, especially in reference to the method{'}s design and altered embedding space properties. See https:\/\/github.com\/hillary-dawkins\/SoWinoBias.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1691316664,"Goal":"Gender Equality","Task":["Latent Gender Bias Detection","Coreference Resolution","downstream application","latent gender bias","coreference resolution systems"],"Method":["debiasing methods"]},{"ID":"saini-etal-2021-pedagogical","title":"Pedagogical Principles in the Online Teaching of Text Mining: A Retrospection","abstract":"The ongoing COVID-19 pandemic has brought online education to the forefront of pedagogical discussions. To make this increased interest sustainable in a post-pandemic era, online courses must be built on strong pedagogical foundations. With a long history of pedagogic research, there are many principles, frameworks, and models available to help teachers in doing so. These models cover different teaching perspectives, such as constructive alignment, feedback, and the learning environment. In this paper, we discuss how we designed and implemented our online Natural Language Processing (NLP) course following constructive alignment and adhering to the pedagogical principles of LTU. By examining our course and analyzing student evaluation forms, we show that we have met our goal and successfully delivered the course. Furthermore, we discuss the additional benefits resulting from the current mode of delivery, including the increased reusability of course content and increased potential for collaboration between universities. Lastly, we also discuss where we can and will further improve the current course design.","year":2021,"title_abstract":"Pedagogical Principles in the Online Teaching of Text Mining: A Retrospection The ongoing COVID-19 pandemic has brought online education to the forefront of pedagogical discussions. To make this increased interest sustainable in a post-pandemic era, online courses must be built on strong pedagogical foundations. With a long history of pedagogic research, there are many principles, frameworks, and models available to help teachers in doing so. These models cover different teaching perspectives, such as constructive alignment, feedback, and the learning environment. In this paper, we discuss how we designed and implemented our online Natural Language Processing (NLP) course following constructive alignment and adhering to the pedagogical principles of LTU. By examining our course and analyzing student evaluation forms, we show that we have met our goal and successfully delivered the course. Furthermore, we discuss the additional benefits resulting from the current mode of delivery, including the increased reusability of course content and increased potential for collaboration between universities. Lastly, we also discuss where we can and will further improve the current course design.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1690760255,"Goal":"Quality Education","Task":["Online Teaching of Text Mining","online education","pedagogical discussions","online courses","pedagogic research","online Natural Language Processing","constructive alignment","delivery"],"Method":["Pedagogical Principles","pedagogical foundations","pedagogical principles","LTU"]},{"ID":"marchand-2002-extraction","title":"Extraction et classification automatique de mat{\\'e}riaux textuels pour la cr{\\'e}ation de tests de langue","abstract":"Nous pr{\\'e}sentons l{'}{\\'e}tat de d{\\'e}veloppement d{'}un outil d{'}extraction et de classification automatique de phrases pour la cr{\\'e}ation de tests de langue. Cet outil de TAL est con{\\c{c}}u pour, dans un premier temps, localiser et extraire de larges corpus en ligne du mat{\\'e}riel textuel (phrases) poss{\\'e}dant des propri{\\'e}t{\\'e}s linguistiques bien sp{\\'e}cifiques. Il permet, dans un deuxi{\\`e}me temps, de classifier automatiquement ces phrases-candidates d{'}apr{\\`e}s le type d{'}erreurs qu{'}elles sont en mesure de contenir. Le d{\\'e}veloppement de cet outil s{'}inscrit dans un contexte d{'}optimalisation du processus de production d{'}items pour les tests d{'}{\\'e}valuation. Pour r{\\'e}pondre aux exigences croissantes de production, les industries de d{\\'e}veloppement de tests de comp{\\'e}tences doivent {\\^e}tre capable de d{\\'e}velopper rapidement de grandes quantit{\\'e}s de tests. De plus, pour des raisons de s{\\'e}curit{\\'e}, les items doivent {\\^e}tre continuellement remplac{\\'e}s, ce qui cr{\\'e}e un besoin d{'}approvisionnement constant. Ces exigences de production et r{\\'e}vision sont, pour ces organisations, co{\\^u}teuses en temps et en personnel. Les b{\\'e}n{\\'e}fices {\\`a} retirer du d{\\'e}veloppement et de l{'}implantation d{'}un outil capable d{'}automatiser la majeure partie du processus de production de ces items sont par cons{\\'e}quents consid{\\'e}rables.","year":2002,"title_abstract":"Extraction et classification automatique de mat{\\'e}riaux textuels pour la cr{\\'e}ation de tests de langue Nous pr{\\'e}sentons l{'}{\\'e}tat de d{\\'e}veloppement d{'}un outil d{'}extraction et de classification automatique de phrases pour la cr{\\'e}ation de tests de langue. Cet outil de TAL est con{\\c{c}}u pour, dans un premier temps, localiser et extraire de larges corpus en ligne du mat{\\'e}riel textuel (phrases) poss{\\'e}dant des propri{\\'e}t{\\'e}s linguistiques bien sp{\\'e}cifiques. Il permet, dans un deuxi{\\`e}me temps, de classifier automatiquement ces phrases-candidates d{'}apr{\\`e}s le type d{'}erreurs qu{'}elles sont en mesure de contenir. Le d{\\'e}veloppement de cet outil s{'}inscrit dans un contexte d{'}optimalisation du processus de production d{'}items pour les tests d{'}{\\'e}valuation. Pour r{\\'e}pondre aux exigences croissantes de production, les industries de d{\\'e}veloppement de tests de comp{\\'e}tences doivent {\\^e}tre capable de d{\\'e}velopper rapidement de grandes quantit{\\'e}s de tests. De plus, pour des raisons de s{\\'e}curit{\\'e}, les items doivent {\\^e}tre continuellement remplac{\\'e}s, ce qui cr{\\'e}e un besoin d{'}approvisionnement constant. Ces exigences de production et r{\\'e}vision sont, pour ces organisations, co{\\^u}teuses en temps et en personnel. Les b{\\'e}n{\\'e}fices {\\`a} retirer du d{\\'e}veloppement et de l{'}implantation d{'}un outil capable d{'}automatiser la majeure partie du processus de production de ces items sont par cons{\\'e}quents consid{\\'e}rables.","social_need":"Responsible Consumption and Production Ensure sustainable consumption and production patterns","cosine_similarity":0.1690161526,"Goal":"Responsible Consumption and Production","Task":["classification","production"],"Method":["classification automatique de phrases","classifier"]},{"ID":"choukri-etal-2016-elra","title":"{ELRA} Activities and Services","abstract":"After celebrating its 20th anniversary in 2015, ELRA is carrying on its strong involvement in the HLT field. To share ELRA{'}s expertise of those 21 past years, this article begins with a presentation of ELRA{'}s strategic Data and LR Management Plan for a wide use by the language communities. Then, we further report on ELRA{'}s activities and services provided since LREC 2014. When looking at the cataloguing and licensing activities, we can see that ELRA has been active at making the Meta-Share repository move toward new developments steps, supporting Europe to obtain accurate LRs within the Connecting Europe Facility programme, promoting the use of LR citation, creating the ELRA License Wizard web portal.The article further elaborates on the recent LR production activities of various written, speech and video resources, commissioned by public and private customers. In parallel, ELDA has also worked on several EU-funded projects centred on strategic issues related to the European Digital Single Market. The last part gives an overview of the latest dissemination activities, with a special focus on the celebration of its 20th anniversary organised in Dubrovnik (Croatia) and the following up of LREC, as well as the launching of the new ELRA portal.","year":2016,"title_abstract":"{ELRA} Activities and Services After celebrating its 20th anniversary in 2015, ELRA is carrying on its strong involvement in the HLT field. To share ELRA{'}s expertise of those 21 past years, this article begins with a presentation of ELRA{'}s strategic Data and LR Management Plan for a wide use by the language communities. Then, we further report on ELRA{'}s activities and services provided since LREC 2014. When looking at the cataloguing and licensing activities, we can see that ELRA has been active at making the Meta-Share repository move toward new developments steps, supporting Europe to obtain accurate LRs within the Connecting Europe Facility programme, promoting the use of LR citation, creating the ELRA License Wizard web portal.The article further elaborates on the recent LR production activities of various written, speech and video resources, commissioned by public and private customers. In parallel, ELDA has also worked on several EU-funded projects centred on strategic issues related to the European Digital Single Market. The last part gives an overview of the latest dissemination activities, with a special focus on the celebration of its 20th anniversary organised in Dubrovnik (Croatia) and the following up of LREC, as well as the launching of the new ELRA portal.","social_need":"Industry, Innovation and Infrastrucure Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation","cosine_similarity":0.168983385,"Goal":"Industry, Innovation and Infrastrucure","Task":["HLT field","cataloguing and licensing activities","Connecting Europe Facility programme","LR","LR production activities","dissemination activities","LREC"],"Method":["ELRA","ELRA{'}s strategic Data","LR Management Plan","ELRA","Meta - Share repository","ELRA portal"]},{"ID":"wei-etal-2020-people","title":"What Are People Asking About {COVID-19}? A Question Classification Dataset","abstract":"We present COVID-Q, a set of 1,690 questions about COVID-19 from 13 sources, which we annotate into 15 question categories and 207 question clusters. The most common questions in our dataset asked about transmission, prevention, and societal effects of COVID, and we found that many questions that appeared in multiple sources were not answered by any FAQ websites of reputable organizations such as the CDC and FDA. We post our dataset publicly at https:\/\/github.com\/JerryWei03\/COVID-Q. For classifying questions into 15 categories, a BERT baseline scored 58.1{\\%} accuracy when trained on 20 examples per category, and for a question clustering task, a BERT + triplet loss baseline achieved 49.5{\\%} accuracy. We hope COVID-Q can help either for direct use in developing applied systems or as a domain-specific resource for model evaluation.","year":2020,"title_abstract":"What Are People Asking About {COVID-19}? A Question Classification Dataset We present COVID-Q, a set of 1,690 questions about COVID-19 from 13 sources, which we annotate into 15 question categories and 207 question clusters. The most common questions in our dataset asked about transmission, prevention, and societal effects of COVID, and we found that many questions that appeared in multiple sources were not answered by any FAQ websites of reputable organizations such as the CDC and FDA. We post our dataset publicly at https:\/\/github.com\/JerryWei03\/COVID-Q. For classifying questions into 15 categories, a BERT baseline scored 58.1{\\%} accuracy when trained on 20 examples per category, and for a question clustering task, a BERT + triplet loss baseline achieved 49.5{\\%} accuracy. We hope COVID-Q can help either for direct use in developing applied systems or as a domain-specific resource for model evaluation.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1689679921,"Goal":"Climate Action","Task":["transmission","prevention","COVID","classifying questions","question clustering task","model evaluation"],"Method":["BERT","BERT + triplet loss baseline"]},{"ID":"fukutomi-1999-experiment","title":"Experiment report of a commercial machine translation in a manufacturing industry domain","abstract":"The aim of this paper is to provide a report of an experiment using a Commercial Machine Translation (CMT) software in a manufacturing company in the UK, with particular reference to Japanese \/ English Machine translation. It presents the main difficulties involved in the translation of industrial documents from Japanese to English, and discusses how the productivity and quality of translation can be improved through the use of commercial Machine Translation (MT) software. This is an empirical data and is proposed translators' point of view in a manufacturing factory. The survey focuses on a manufacturing organisation which does not have the resources needed to develop their own MT system. The globalization of the Japanese manufacturing industry makes it necessary for the translation of manuals and other documents to be as rapid as possible. In this paper, linguistic features of both English and Japanese are discussed from the evaluation experiment in order to make up writing rules for members of staff at Makita Manufacturing Europe. It also discusses viewpoint of British engineers when translated manuals are read.","year":1999,"title_abstract":"Experiment report of a commercial machine translation in a manufacturing industry domain The aim of this paper is to provide a report of an experiment using a Commercial Machine Translation (CMT) software in a manufacturing company in the UK, with particular reference to Japanese \/ English Machine translation. It presents the main difficulties involved in the translation of industrial documents from Japanese to English, and discusses how the productivity and quality of translation can be improved through the use of commercial Machine Translation (MT) software. This is an empirical data and is proposed translators' point of view in a manufacturing factory. The survey focuses on a manufacturing organisation which does not have the resources needed to develop their own MT system. The globalization of the Japanese manufacturing industry makes it necessary for the translation of manuals and other documents to be as rapid as possible. In this paper, linguistic features of both English and Japanese are discussed from the evaluation experiment in order to make up writing rules for members of staff at Makita Manufacturing Europe. It also discusses viewpoint of British engineers when translated manuals are read.","social_need":"Decent Work and Economic Growth Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all","cosine_similarity":0.1688448191,"Goal":"Decent Work and Economic Growth","Task":["machine translation","manufacturing industry domain","Machine translation","translation of industrial documents","translation","manufacturing factory","manufacturing organisation","MT","translation"],"Method":["Commercial Machine Translation","commercial Machine Translation (MT) software"]},{"ID":"el-asri-etal-2014-nastia","title":"{NASTIA}: Negotiating Appointment Setting Interface","abstract":"This paper describes a French Spoken Dialogue System (SDS) named NASTIA (Negotiating Appointment SeTting InterfAce). Appointment scheduling is a hybrid task halfway between slot-filling and negotiation. NASTIA implements three different negotiation strategies. These strategies were tested on 1734 dialogues with 385 users who interacted at most 5 times with the SDS and gave a rating on a scale of 1 to 10 for each dialogue. Previous appointment scheduling systems were evaluated with the same experimental protocol. NASTIA is different from these systems in that it can adapt its strategy during the dialogue. The highest system task completion rate with these systems was 81{\\%} whereas NASTIA had an 88{\\%} average and its best performing strategy even reached 92{\\%}. This strategy also significantly outperformed previous systems in terms of overall user rating with an average of 8.28 against 7.40. The experiment also enabled highlighting global recommendations for building spoken dialogue systems.","year":2014,"title_abstract":"{NASTIA}: Negotiating Appointment Setting Interface This paper describes a French Spoken Dialogue System (SDS) named NASTIA (Negotiating Appointment SeTting InterfAce). Appointment scheduling is a hybrid task halfway between slot-filling and negotiation. NASTIA implements three different negotiation strategies. These strategies were tested on 1734 dialogues with 385 users who interacted at most 5 times with the SDS and gave a rating on a scale of 1 to 10 for each dialogue. Previous appointment scheduling systems were evaluated with the same experimental protocol. NASTIA is different from these systems in that it can adapt its strategy during the dialogue. The highest system task completion rate with these systems was 81{\\%} whereas NASTIA had an 88{\\%} average and its best performing strategy even reached 92{\\%}. This strategy also significantly outperformed previous systems in terms of overall user rating with an average of 8.28 against 7.40. The experiment also enabled highlighting global recommendations for building spoken dialogue systems.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.168767482,"Goal":"Partnership for the Goals","Task":["Negotiating Appointment Setting Interface","Appointment scheduling","hybrid task","slot - filling","negotiation","global recommendations","spoken dialogue systems"],"Method":["French Spoken Dialogue System","NASTIA","Appointment SeTting InterfAce)","NASTIA","negotiation strategies","appointment scheduling systems","NASTIA","NASTIA"]},{"ID":"marinelli-etal-2006-using","title":"Using Core Ontology for Domain Lexicon Structuring","abstract":"The users\u0092 demand has determined the need to manage the growing new technical maritime terminology which includes very different domains such as the juridical or commercial ones. A terminological database was built by exploiting the computational tools of ItalWordNet (IWN) and its lexical-semantic model (EuroWordNet).This paper concerns the development of database structure and data coding, relevance of the concepts of \u0091term\u0092 and \u0091domain\u0092, information potential of the terms, complexity of this domain and detailed ontology structuring recently undertaken and still in progress. Our domain structure is described defining a core set of terms representing the two main sub-domains specified in \u0091technical-nautical\u0092 and \u0091maritime transport\u0092 terminology. These terms are sufficiently general to be the root nodes of the core ontology we are developing. They are mostly domain-dependent, but the link with the Top Ontology of IWN remains, endorsing either general and \u0091foundation\u0092 information, or detailed description directly connected with the specific domain. Through the semantic relations linking the synsets, every term \u0091inherits\u0092 the top ontology definitions and becomes itself an integral part of the structure. While codifying a term in the maritime database, the reference is at the same time allowed to the Base Concepts of the terminological ontology embedding the term in the semantic network, showing that upper and core ontologies make it possible for the framework to integrate different views on the same domain in a meaningful way.","year":2006,"title_abstract":"Using Core Ontology for Domain Lexicon Structuring The users\u0092 demand has determined the need to manage the growing new technical maritime terminology which includes very different domains such as the juridical or commercial ones. A terminological database was built by exploiting the computational tools of ItalWordNet (IWN) and its lexical-semantic model (EuroWordNet).This paper concerns the development of database structure and data coding, relevance of the concepts of \u0091term\u0092 and \u0091domain\u0092, information potential of the terms, complexity of this domain and detailed ontology structuring recently undertaken and still in progress. Our domain structure is described defining a core set of terms representing the two main sub-domains specified in \u0091technical-nautical\u0092 and \u0091maritime transport\u0092 terminology. These terms are sufficiently general to be the root nodes of the core ontology we are developing. They are mostly domain-dependent, but the link with the Top Ontology of IWN remains, endorsing either general and \u0091foundation\u0092 information, or detailed description directly connected with the specific domain. Through the semantic relations linking the synsets, every term \u0091inherits\u0092 the top ontology definitions and becomes itself an integral part of the structure. While codifying a term in the maritime database, the reference is at the same time allowed to the Base Concepts of the terminological ontology embedding the term in the semantic network, showing that upper and core ontologies make it possible for the framework to integrate different views on the same domain in a meaningful way.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.1686272919,"Goal":"Life Below Water","Task":["Domain Lexicon Structuring","database structure","data coding"],"Method":["Core Ontology","computational tools","ItalWordNet","lexical - semantic model","ontology structuring","IWN","terminological ontology"]},{"ID":"wilks-1994-notes","title":"Some notes on the state of the art: Where are we now in {MT}: what works and what doesn{'}t?","abstract":"The paper examines briefly the impact of the {``}statistical turn{''} in machine translation (MT) R{\\&}D in the last decade, and particularly the way in which it has made large scale language resources (lexicons, text corpora etc.) more important than ever before and reinforced the role of evaluation in the development of the field. But resources mean, almost by definition, co-operation between groups and, in the case of MT, specifically co-operation between language groups and states. The paper then considers what alternatives there are now for MT R{\\&}D. One is to continue with interlingual methods of translation, even though those are not normally thought of as close to statistical methods. The reason is that statistical methods, taken alone, have almost certainly reached a ceiling in terms of the proportion of sentences and linguistic phenomena they can translate successfully. Interlingual methods remain popular within large electronics companies in Japan, and in a large US Government funded project (PANGLOSS). The question then discussed is what role there can be for interlinguas and interlingual methods in co-operation in MT across linguistic and national boundaries. The paper then turns to evaluation and asks whether, across national and continental boundaries, it can become a co-operative or a {``}hegemonic{''} enterprise. Finally the paper turns to resources themselves and asks why co-operation on resources is proving so hard, even though there are bright spots of real co-operation.","year":1994,"title_abstract":"Some notes on the state of the art: Where are we now in {MT}: what works and what doesn{'}t? The paper examines briefly the impact of the {``}statistical turn{''} in machine translation (MT) R{\\&}D in the last decade, and particularly the way in which it has made large scale language resources (lexicons, text corpora etc.) more important than ever before and reinforced the role of evaluation in the development of the field. But resources mean, almost by definition, co-operation between groups and, in the case of MT, specifically co-operation between language groups and states. The paper then considers what alternatives there are now for MT R{\\&}D. One is to continue with interlingual methods of translation, even though those are not normally thought of as close to statistical methods. The reason is that statistical methods, taken alone, have almost certainly reached a ceiling in terms of the proportion of sentences and linguistic phenomena they can translate successfully. Interlingual methods remain popular within large electronics companies in Japan, and in a large US Government funded project (PANGLOSS). The question then discussed is what role there can be for interlinguas and interlingual methods in co-operation in MT across linguistic and national boundaries. The paper then turns to evaluation and asks whether, across national and continental boundaries, it can become a co-operative or a {``}hegemonic{''} enterprise. Finally the paper turns to resources themselves and asks why co-operation on resources is proving so hard, even though there are bright spots of real co-operation.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1686198115,"Goal":"Partnership for the Goals","Task":["machine translation","evaluation","MT","MT R{\\&}D","translation","co - operation","MT","evaluation"],"Method":["interlingual methods","statistical methods","statistical methods","Interlingual methods","interlingual methods"]},{"ID":"del-gratta-etal-2012-language","title":"The Language Library: supporting community effort for collective resource production","abstract":"Relations among phenomena at different linguistic levels are at the essence of language properties but today we focus mostly on one specific linguistic layer at a time, without (having the possibility of) paying attention to the relations among the different layers. At the same time our efforts are too much scattered without much possibility of exploiting other people's achievements. To address the complexities hidden in multilayer interrelations even small amounts of processed data can be useful, improving the performance of complex systems. Exploiting the current trend towards sharing we want to initiate a collective movement that works towards creating synergies and harmonisation among different annotation efforts that are now dispersed. In this paper we present the general architecture of the Language Library, an initiative which is conceived as a facility for gathering and making available through simple functionalities the linguistic knowledge the field is able to produce, putting in place new ways of collaboration within the LRT community. In order to reach this goal, a first population round of the Language Library has started around a core of parallel\/comparable texts that have been annotated by several contributors submitting a paper for LREC2012. The Language Library has also an ancillary aim related to language documentation and archiving and it is conceived as a theory-neutral space which allows for several language processing philosophies to coexist.","year":2012,"title_abstract":"The Language Library: supporting community effort for collective resource production Relations among phenomena at different linguistic levels are at the essence of language properties but today we focus mostly on one specific linguistic layer at a time, without (having the possibility of) paying attention to the relations among the different layers. At the same time our efforts are too much scattered without much possibility of exploiting other people's achievements. To address the complexities hidden in multilayer interrelations even small amounts of processed data can be useful, improving the performance of complex systems. Exploiting the current trend towards sharing we want to initiate a collective movement that works towards creating synergies and harmonisation among different annotation efforts that are now dispersed. In this paper we present the general architecture of the Language Library, an initiative which is conceived as a facility for gathering and making available through simple functionalities the linguistic knowledge the field is able to produce, putting in place new ways of collaboration within the LRT community. In order to reach this goal, a first population round of the Language Library has started around a core of parallel\/comparable texts that have been annotated by several contributors submitting a paper for LREC2012. The Language Library has also an ancillary aim related to language documentation and archiving and it is conceived as a theory-neutral space which allows for several language processing philosophies to coexist.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1686186194,"Goal":"Sustainable Cities and Communities","Task":["community effort","collective resource production","complex systems","annotation","LRT community","LREC2012","language documentation","archiving"],"Method":["Language Library","Language Library","Language Library","Language Library","language processing philosophies"]},{"ID":"lin-etal-2012-revealing","title":"Revealing Contentious Concepts Across Social Groups","abstract":"In this paper, a computational model based on concept polarity is proposed to investigate the influence of communications across the diacultural groups. The hypothesis of this work is that there are communities or groups which can be characterized by a network of concepts and the corresponding valuations of those concepts that are agreed upon by the members of the community. We apply an existing research tool, ECO, to generate text representative of each community and create community specific Valuation Concept Networks (VCN). We then compare VCNs across the communities, to attempt to find contentious concepts, which could subsequently be the focus of further exploration as points of contention between the two communities. A prototype, CPAM (Changing Positions, Altering Minds), was implemented as a proof of concept for this approach. The experiment was conducted using blog data from pro-Palestinian and pro-Israeli communities. A potential application of this method and future work are discussed as well.","year":2012,"title_abstract":"Revealing Contentious Concepts Across Social Groups In this paper, a computational model based on concept polarity is proposed to investigate the influence of communications across the diacultural groups. The hypothesis of this work is that there are communities or groups which can be characterized by a network of concepts and the corresponding valuations of those concepts that are agreed upon by the members of the community. We apply an existing research tool, ECO, to generate text representative of each community and create community specific Valuation Concept Networks (VCN). We then compare VCNs across the communities, to attempt to find contentious concepts, which could subsequently be the focus of further exploration as points of contention between the two communities. A prototype, CPAM (Changing Positions, Altering Minds), was implemented as a proof of concept for this approach. The experiment was conducted using blog data from pro-Palestinian and pro-Israeli communities. A potential application of this method and future work are discussed as well.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1685966998,"Goal":"Reduced Inequalities","Task":["Revealing Contentious Concepts"],"Method":["computational model","concept polarity","ECO","community specific Valuation Concept Networks","VCNs","CPAM"]},{"ID":"rooshenas-etal-2018-training","title":"Training Structured Prediction Energy Networks with Indirect Supervision","abstract":"This paper introduces rank-based training of structured prediction energy networks (SPENs). Our method samples from output structures using gradient descent and minimizes the ranking violation of the sampled structures with respect to a scalar scoring function defined with domain knowledge. We have successfully trained SPEN for citation field extraction without any labeled data instances, where the only source of supervision is a simple human-written scoring function. Such scoring functions are often easy to provide; the SPEN then furnishes an efficient structured prediction inference procedure.","year":2018,"title_abstract":"Training Structured Prediction Energy Networks with Indirect Supervision This paper introduces rank-based training of structured prediction energy networks (SPENs). Our method samples from output structures using gradient descent and minimizes the ranking violation of the sampled structures with respect to a scalar scoring function defined with domain knowledge. We have successfully trained SPEN for citation field extraction without any labeled data instances, where the only source of supervision is a simple human-written scoring function. Such scoring functions are often easy to provide; the SPEN then furnishes an efficient structured prediction inference procedure.","social_need":"Affordable and Clean Energy Ensure access to affordable, reliable, sustainable and modern energy for all","cosine_similarity":0.1685346663,"Goal":"Affordable and Clean Energy","Task":["citation field extraction"],"Method":["Structured Prediction Energy Networks","Indirect Supervision","rank - based training of structured prediction energy networks","gradient descent","SPEN","scoring functions","SPEN","structured prediction inference procedure"]},{"ID":"vossen-etal-2008-kyoto","title":"{KYOTO}: a System for Mining, Structuring and Distributing Knowledge across Languages and Cultures","abstract":"We outline work performed within the framework of a current EC project. The goal is to construct a language-independent information system for a specific domain (environment\/ecology\/biodiversity) anchored in a language-independent ontology that is linked to wordnets in seven languages. For each language, information extraction and identification of lexicalized concepts with ontological entries is carried out by text miners (\u0093Kybots\u0094). The mapping of language-specific lexemes to the ontology allows for crosslinguistic identification and translation of equivalent terms. The infrastructure developed within this project enables long-range knowledge sharing and transfer across many languages and cultures, addressing the need for global and uniform transition of knowledge beyond the specific domains addressed here.","year":2008,"title_abstract":"{KYOTO}: a System for Mining, Structuring and Distributing Knowledge across Languages and Cultures We outline work performed within the framework of a current EC project. The goal is to construct a language-independent information system for a specific domain (environment\/ecology\/biodiversity) anchored in a language-independent ontology that is linked to wordnets in seven languages. For each language, information extraction and identification of lexicalized concepts with ontological entries is carried out by text miners (\u0093Kybots\u0094). The mapping of language-specific lexemes to the ontology allows for crosslinguistic identification and translation of equivalent terms. The infrastructure developed within this project enables long-range knowledge sharing and transfer across many languages and cultures, addressing the need for global and uniform transition of knowledge beyond the specific domains addressed here.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.1684611291,"Goal":"Life Below Water","Task":["Structuring and Distributing Knowledge","information extraction","identification of lexicalized concepts","mapping of language - specific lexemes","crosslinguistic identification","translation","long - range knowledge sharing","global and uniform transition of knowledge"],"Method":["language - independent information system","text miners"]},{"ID":"vasiljevs-etal-2012-creation","title":"Creation of an Open Shared Language Resource Repository in the Nordic and Baltic Countries","abstract":"The META-NORD project has contributed to an open infrastructure for language resources (data and tools) under the META-NET umbrella. This paper presents the key objectives of META-NORD and reports on the results achieved in the first year of the project. META-NORD has mapped and described the national language technology landscape in the Nordic and Baltic countries in terms of language use, language technology and resources, main actors in the academy, industry, government and society; identified and collected the first batch of language resources in the Nordic and Baltic countries; documented, processed, linked, and upgraded the identified language resources to agreed standards and guidelines. The three horizontal multilingual actions in META-NORD are overviewed in this paper: linking and validating Nordic and Baltic wordnets, the harmonisation of multilingual Nordic and Baltic treebanks, and consolidating multilingual terminology resources across European countries. This paper also touches upon intellectual property rights for the sharing of language resources.","year":2012,"title_abstract":"Creation of an Open Shared Language Resource Repository in the Nordic and Baltic Countries The META-NORD project has contributed to an open infrastructure for language resources (data and tools) under the META-NET umbrella. This paper presents the key objectives of META-NORD and reports on the results achieved in the first year of the project. META-NORD has mapped and described the national language technology landscape in the Nordic and Baltic countries in terms of language use, language technology and resources, main actors in the academy, industry, government and society; identified and collected the first batch of language resources in the Nordic and Baltic countries; documented, processed, linked, and upgraded the identified language resources to agreed standards and guidelines. The three horizontal multilingual actions in META-NORD are overviewed in this paper: linking and validating Nordic and Baltic wordnets, the harmonisation of multilingual Nordic and Baltic treebanks, and consolidating multilingual terminology resources across European countries. This paper also touches upon intellectual property rights for the sharing of language resources.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1684547961,"Goal":"Partnership for the Goals","Task":["Open Shared Language Resource Repository","horizontal multilingual actions","validating Nordic and Baltic wordnets","sharing of language resources"],"Method":["META - NORD","META - NORD","META - NORD","META - NORD"]},{"ID":"labropoulou-etal-2014-developing","title":"Developing a Framework for Describing Relations among Language Resources","abstract":"In this paper, we study relations holding between language resources as implemented in activities concerned with their documentation. We envision the term \u0093language resources\u0094 with an inclusive definition covering datasets (corpora, lexica, ontologies, grammars, etc.), tools (including web services, workflows, platforms etc.), related publications and documentation, specifications and guidelines. However, the scope of the paper is limited to relations holding for datasets and tools. The study fosuses on the META-SHARE infrastructure and the Linguistic Data Consortium and takes into account the ISOcat DCR relations. Based on this study, we propose a taxonomy of relations, discuss their semantics and provide specifications for their use in order to cater for semantic interoperability. Issues of granularity, redundancy in codification, naming conventions and semantics of the relations are presented.","year":2014,"title_abstract":"Developing a Framework for Describing Relations among Language Resources In this paper, we study relations holding between language resources as implemented in activities concerned with their documentation. We envision the term \u0093language resources\u0094 with an inclusive definition covering datasets (corpora, lexica, ontologies, grammars, etc.), tools (including web services, workflows, platforms etc.), related publications and documentation, specifications and guidelines. However, the scope of the paper is limited to relations holding for datasets and tools. The study fosuses on the META-SHARE infrastructure and the Linguistic Data Consortium and takes into account the ISOcat DCR relations. Based on this study, we propose a taxonomy of relations, discuss their semantics and provide specifications for their use in order to cater for semantic interoperability. Issues of granularity, redundancy in codification, naming conventions and semantics of the relations are presented.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.168397665,"Goal":"Partnership for the Goals","Task":["Describing Relations among Language Resources","semantic interoperability"],"Method":["META - SHARE infrastructure"]},{"ID":"okumura-etal-1991-multi","title":"Multi-lingual Sentence Generation from the {PIVOT} interlingua","abstract":"We wrote this report in Japanese and translated it by NEC's machine translation system PIVOT\/JE.) IBS (International Business Service) is the company which does the documentation service which contains translation business. We introduced a machine translation system into translation business in earnest last year. The introduction of a machine translation system changed the form of our translation work. The translation work was divided into some steps and the person who isn't experienced became able to take it of the work of each of translation steps. As a result, a total translation cost reduced. In this paper, first, we report on the usage of our machine translation system. Next, we report on translation quality and the translation cost with a machine translation system. Lastly, we report on the merit which was gotten by introducing machine translation.","year":1991,"title_abstract":"Multi-lingual Sentence Generation from the {PIVOT} interlingua We wrote this report in Japanese and translated it by NEC's machine translation system PIVOT\/JE.) IBS (International Business Service) is the company which does the documentation service which contains translation business. We introduced a machine translation system into translation business in earnest last year. The introduction of a machine translation system changed the form of our translation work. The translation work was divided into some steps and the person who isn't experienced became able to take it of the work of each of translation steps. As a result, a total translation cost reduced. In this paper, first, we report on the usage of our machine translation system. Next, we report on translation quality and the translation cost with a machine translation system. Lastly, we report on the merit which was gotten by introducing machine translation.","social_need":"Decent Work and Economic Growth Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all","cosine_similarity":0.1683502048,"Goal":"Decent Work and Economic Growth","Task":["Multi - lingual Sentence Generation","translation","IBS (International Business Service)","translation business","translation business","translation work","translation work","machine translation"],"Method":["machine translation system","machine translation system","machine translation system","machine translation system"]},{"ID":"kalyan-etal-2021-iiitt","title":"{IIITT} at {CASE} 2021 Task 1: Leveraging Pretrained Language Models for Multilingual Protest Detection","abstract":"In a world abounding in constant protests resulting from events like a global pandemic, climate change, religious or political conflicts, there has always been a need to detect events\/protests before getting amplified by news media or social media. This paper demonstrates our work on the sentence classification subtask of multilingual protest detection in CASE@ACL-IJCNLP 2021. We approached this task by employing various multilingual pre-trained transformer models to classify if any sentence contains information about an event that has transpired or not. We performed soft voting over the models, achieving the best results among the models, accomplishing a macro F1-Score of 0.8291, 0.7578, and 0.7951 in English, Spanish, and Portuguese, respectively.","year":2021,"title_abstract":"{IIITT} at {CASE} 2021 Task 1: Leveraging Pretrained Language Models for Multilingual Protest Detection In a world abounding in constant protests resulting from events like a global pandemic, climate change, religious or political conflicts, there has always been a need to detect events\/protests before getting amplified by news media or social media. This paper demonstrates our work on the sentence classification subtask of multilingual protest detection in CASE@ACL-IJCNLP 2021. We approached this task by employing various multilingual pre-trained transformer models to classify if any sentence contains information about an event that has transpired or not. We performed soft voting over the models, achieving the best results among the models, accomplishing a macro F1-Score of 0.8291, 0.7578, and 0.7951 in English, Spanish, and Portuguese, respectively.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1683172882,"Goal":"Climate Action","Task":["Multilingual Protest Detection","sentence classification subtask","multilingual protest detection"],"Method":["Pretrained Language Models","transformer models","soft voting"]},{"ID":"spyns-dhalleweyn-2012-smooth","title":"Smooth Sailing for {STEVIN}","abstract":"In this paper we report on the past evaluation of the STEVIN programme in the field of Human Language Technology for Dutch (HLTD). STEVIN was a 11.4 M euro programme that was jointly organised and financed by the Flemish and Dutch governments. The aim was to provide academia and industry with basic building blocks for a linguistic infrastructure for the Dutch language. An independent evaluation has been carried out. The evaluators concluded that the most important targets of the STEVIN programme have been achieved to a very high extent. In this paper, we summarise the context, the evaluation method, the resulting resources and the highlights of the STEVIN final evaluation.","year":2012,"title_abstract":"Smooth Sailing for {STEVIN} In this paper we report on the past evaluation of the STEVIN programme in the field of Human Language Technology for Dutch (HLTD). STEVIN was a 11.4 M euro programme that was jointly organised and financed by the Flemish and Dutch governments. The aim was to provide academia and industry with basic building blocks for a linguistic infrastructure for the Dutch language. An independent evaluation has been carried out. The evaluators concluded that the most important targets of the STEVIN programme have been achieved to a very high extent. In this paper, we summarise the context, the evaluation method, the resulting resources and the highlights of the STEVIN final evaluation.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.168303743,"Goal":"Partnership for the Goals","Task":["Smooth Sailing","Human Language Technology","linguistic infrastructure"],"Method":["STEVIN programme","STEVIN","STEVIN programme","STEVIN"]},{"ID":"verma-etal-2020-identifying","title":"Identifying Worry in {T}witter: Beyond Emotion Analysis","abstract":"Identifying the worries of individuals and societies plays a crucial role in providing social support and enhancing policy decision-making. Due to the popularity of social media platforms such as Twitter, users share worries about personal issues (e.g., health, finances, relationships) and broader issues (e.g., changes in society, environmental concerns, terrorism) freely. In this paper, we explore and evaluate a wide range of machine learning models to predict worry on Twitter. While this task has been closely associated with emotion prediction, we argue and show that identifying worry needs to be addressed as a separate task given the unique challenges associated with it. We conduct a user study to provide evidence that social media posts express two basic kinds of worry {--} normative and pathological {--} as stated in psychology literature. In addition, we show that existing emotion detection techniques underperform, especially while capturing normative worry. Finally, we discuss the current limitations of our approach and propose future applications of the worry identification system.","year":2020,"title_abstract":"Identifying Worry in {T}witter: Beyond Emotion Analysis Identifying the worries of individuals and societies plays a crucial role in providing social support and enhancing policy decision-making. Due to the popularity of social media platforms such as Twitter, users share worries about personal issues (e.g., health, finances, relationships) and broader issues (e.g., changes in society, environmental concerns, terrorism) freely. In this paper, we explore and evaluate a wide range of machine learning models to predict worry on Twitter. While this task has been closely associated with emotion prediction, we argue and show that identifying worry needs to be addressed as a separate task given the unique challenges associated with it. We conduct a user study to provide evidence that social media posts express two basic kinds of worry {--} normative and pathological {--} as stated in psychology literature. In addition, we show that existing emotion detection techniques underperform, especially while capturing normative worry. Finally, we discuss the current limitations of our approach and propose future applications of the worry identification system.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1682287455,"Goal":"Climate Action","Task":["Identifying Worry","social support","policy decision - making","worry","emotion prediction","identifying worry","normative worry"],"Method":["Emotion Analysis","machine learning models","emotion detection techniques","worry identification system"]},{"ID":"lu-etal-2021-sf","title":"{SF}-{QA}: Simple and Fair Evaluation Library for Open-domain Question Answering","abstract":"Although open-domain question answering (QA) draws great attention in recent years, it requires large amounts of resources for building the full system and it is often difficult to reproduce previous results due to complex configurations. In this paper, we introduce SF-QA: simple and fair evaluation framework for open-domain QA. SF-QA framework modularizes the pipeline open-domain QA system, which makes the task itself easily accessible and reproducible to research groups without enough computing resources. The proposed evaluation framework is publicly available and anyone can contribute to the code and evaluations.","year":2021,"title_abstract":"{SF}-{QA}: Simple and Fair Evaluation Library for Open-domain Question Answering Although open-domain question answering (QA) draws great attention in recent years, it requires large amounts of resources for building the full system and it is often difficult to reproduce previous results due to complex configurations. In this paper, we introduce SF-QA: simple and fair evaluation framework for open-domain QA. SF-QA framework modularizes the pipeline open-domain QA system, which makes the task itself easily accessible and reproducible to research groups without enough computing resources. The proposed evaluation framework is publicly available and anyone can contribute to the code and evaluations.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1681974977,"Goal":"Quality Education","Task":["Open - domain Question Answering","open - domain question answering","open - domain QA","QA"],"Method":["and Fair Evaluation Library","SF - QA","and fair evaluation framework","SF - QA framework","evaluation framework"]},{"ID":"kellert-mahmud-uz-zaman-2022-using","title":"Using neural topic models to track context shifts of words: a case study of {COVID}-related terms before and after the lockdown in {A}pril 2020","abstract":"This paper explores lexical meaning changes in a new dataset, which includes tweets from before and after the COVID-related lockdown in April 2020. We use this dataset to evaluate traditional and more recent unsupervised approaches to lexical semantic change that make use of contextualized word representations based on the BERT neural language model to obtain representations of word usages. We argue that previous models that encode local representations of words cannot capture global context shifts such as the context shift of face masks since the pandemic outbreak. We experiment with neural topic models to track context shifts of words. We show that this approach can reveal textual associations of words that go beyond their lexical meaning representation. We discuss future work and how to proceed capturing the pragmatic aspect of meaning change as opposed to lexical semantic change.","year":2022,"title_abstract":"Using neural topic models to track context shifts of words: a case study of {COVID}-related terms before and after the lockdown in {A}pril 2020 This paper explores lexical meaning changes in a new dataset, which includes tweets from before and after the COVID-related lockdown in April 2020. We use this dataset to evaluate traditional and more recent unsupervised approaches to lexical semantic change that make use of contextualized word representations based on the BERT neural language model to obtain representations of word usages. We argue that previous models that encode local representations of words cannot capture global context shifts such as the context shift of face masks since the pandemic outbreak. We experiment with neural topic models to track context shifts of words. We show that this approach can reveal textual associations of words that go beyond their lexical meaning representation. We discuss future work and how to proceed capturing the pragmatic aspect of meaning change as opposed to lexical semantic change.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.167978242,"Goal":"Climate Action","Task":["lexical meaning changes","lexical semantic change","representations of word usages","lexical semantic change"],"Method":["neural topic models","unsupervised approaches","contextualized word representations","BERT neural language model","local representations of words","neural topic models","lexical meaning representation"]},{"ID":"yang-etal-2020-efficient","title":"Efficient Transfer Learning for Quality Estimation with Bottleneck Adapter Layer","abstract":"The Predictor-Estimator framework for quality estimation (QE) is commonly used for its strong performance. Where the predictor and estimator works on feature extraction and quality evaluation, respectively. However, training the predictor from scratch is computationally expensive. In this paper, we propose an efficient transfer learning framework to transfer knowledge from NMT dataset into QE models. A Predictor-Estimator alike model named BAL-QE is also proposed, aiming to extract high quality features with pre-trained NMT model, and make classification with a fine-tuned Bottleneck Adapter Layer (BAL). The experiment shows that BAL-QE achieves 97{\\%} of the SOTA performance in WMT19 En-De and En-Ru QE tasks by only training 3{\\%} of parameters within 4 hours on 4 Titan XP GPUs. Compared with the commonly used NuQE baseline, BAL-QE achieves 47{\\%} (En-Ru) and 75{\\%} (En-De) of performance promotions.","year":2020,"title_abstract":"Efficient Transfer Learning for Quality Estimation with Bottleneck Adapter Layer The Predictor-Estimator framework for quality estimation (QE) is commonly used for its strong performance. Where the predictor and estimator works on feature extraction and quality evaluation, respectively. However, training the predictor from scratch is computationally expensive. In this paper, we propose an efficient transfer learning framework to transfer knowledge from NMT dataset into QE models. A Predictor-Estimator alike model named BAL-QE is also proposed, aiming to extract high quality features with pre-trained NMT model, and make classification with a fine-tuned Bottleneck Adapter Layer (BAL). The experiment shows that BAL-QE achieves 97{\\%} of the SOTA performance in WMT19 En-De and En-Ru QE tasks by only training 3{\\%} of parameters within 4 hours on 4 Titan XP GPUs. Compared with the commonly used NuQE baseline, BAL-QE achieves 47{\\%} (En-Ru) and 75{\\%} (En-De) of performance promotions.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.167862922,"Goal":"Quality Education","Task":["Quality Estimation","quality estimation","feature extraction","quality evaluation","classification"],"Method":["Transfer Learning","Bottleneck Adapter Layer","Predictor - Estimator framework","predictor and estimator","transfer learning framework","QE models","Predictor - Estimator alike model","BAL - QE","NMT model","fine - tuned Bottleneck Adapter Layer","BAL - QE","WMT19 En - De","NuQE baseline","BAL - QE"]},{"ID":"yu-etal-2019-course","title":"Course Concept Expansion in {MOOC}s with External Knowledge and Interactive Game","abstract":"As Massive Open Online Courses (MOOCs) become increasingly popular, it is promising to automatically provide extracurricular knowledge for MOOC users. Suffering from semantic drifts and lack of knowledge guidance, existing methods can not effectively expand course concepts in complex MOOC environments. In this paper, we first build a novel boundary during searching for new concepts via external knowledge base and then utilize heterogeneous features to verify the high-quality results. In addition, to involve human efforts in our model, we design an interactive optimization mechanism based on a game. Our experiments on the four datasets from Coursera and XuetangX show that the proposed method achieves significant improvements(+0.19 by MAP) over existing methods.","year":2019,"title_abstract":"Course Concept Expansion in {MOOC}s with External Knowledge and Interactive Game As Massive Open Online Courses (MOOCs) become increasingly popular, it is promising to automatically provide extracurricular knowledge for MOOC users. Suffering from semantic drifts and lack of knowledge guidance, existing methods can not effectively expand course concepts in complex MOOC environments. In this paper, we first build a novel boundary during searching for new concepts via external knowledge base and then utilize heterogeneous features to verify the high-quality results. In addition, to involve human efforts in our model, we design an interactive optimization mechanism based on a game. Our experiments on the four datasets from Coursera and XuetangX show that the proposed method achieves significant improvements(+0.19 by MAP) over existing methods.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1675331593,"Goal":"Quality Education","Task":["Course Concept Expansion"],"Method":["{MOOC}s","Interactive Game","interactive optimization mechanism"]},{"ID":"yang-etal-2019-lets","title":"Let{'}s Make Your Request More Persuasive: Modeling Persuasive Strategies via Semi-Supervised Neural Nets on Crowdfunding Platforms","abstract":"Modeling what makes a request persuasive - eliciting the desired response from a reader - is critical to the study of propaganda, behavioral economics, and advertising. Yet current models can{'}t quantify the persuasiveness of requests or extract successful persuasive strategies. Building on theories of persuasion, we propose a neural network to quantify persuasiveness and identify the persuasive strategies in advocacy requests. Our semi-supervised hierarchical neural network model is supervised by the number of people persuaded to take actions and partially supervised at the sentence level with human-labeled rhetorical strategies. Our method outperforms several baselines, uncovers persuasive strategies - offering increased interpretability of persuasive speech - and has applications for other situations with document-level supervision but only partial sentence supervision.","year":2019,"title_abstract":"Let{'}s Make Your Request More Persuasive: Modeling Persuasive Strategies via Semi-Supervised Neural Nets on Crowdfunding Platforms Modeling what makes a request persuasive - eliciting the desired response from a reader - is critical to the study of propaganda, behavioral economics, and advertising. Yet current models can{'}t quantify the persuasiveness of requests or extract successful persuasive strategies. Building on theories of persuasion, we propose a neural network to quantify persuasiveness and identify the persuasive strategies in advocacy requests. Our semi-supervised hierarchical neural network model is supervised by the number of people persuaded to take actions and partially supervised at the sentence level with human-labeled rhetorical strategies. Our method outperforms several baselines, uncovers persuasive strategies - offering increased interpretability of persuasive speech - and has applications for other situations with document-level supervision but only partial sentence supervision.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1675129086,"Goal":"Climate Action","Task":["propaganda","behavioral economics","advertising","persuasion","persuasiveness","document - level supervision"],"Method":["Modeling Persuasive Strategies","Semi - Supervised Neural Nets","Crowdfunding Platforms","persuasive strategies","neural network","persuasive strategies","semi - supervised hierarchical neural network model","persuasive strategies"]},{"ID":"oda-etal-2017-simple","title":"A Simple and Strong Baseline: {NAIST}-{NICT} Neural Machine Translation System for {WAT}2017 {E}nglish-{J}apanese Translation Task","abstract":"This paper describes the details about the NAIST-NICT machine translation system for WAT2017 English-Japanese Scientific Paper Translation Task. The system consists of a language-independent tokenizer and an attentional encoder-decoder style neural machine translation model. According to the official results, our system achieves higher translation accuracy than any systems submitted previous campaigns despite simple model architecture.","year":2017,"title_abstract":"A Simple and Strong Baseline: {NAIST}-{NICT} Neural Machine Translation System for {WAT}2017 {E}nglish-{J}apanese Translation Task This paper describes the details about the NAIST-NICT machine translation system for WAT2017 English-Japanese Scientific Paper Translation Task. The system consists of a language-independent tokenizer and an attentional encoder-decoder style neural machine translation model. According to the official results, our system achieves higher translation accuracy than any systems submitted previous campaigns despite simple model architecture.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1674276888,"Goal":"Gender Equality","Task":["Translation Task","translation","Translation Task"],"Method":["Neural Machine Translation System","NAIST - NICT","language - independent tokenizer","encoder - decoder style neural machine translation model","model architecture"]},{"ID":"brunila-etal-2021-bridging","title":"Bridging the gap between supervised classification and unsupervised topic modelling for social-media assisted crisis management","abstract":"Social media such as Twitter provide valuable information to crisis managers and affected people during natural disasters. Machine learning can help structure and extract information from the large volume of messages shared during a crisis; however, the constantly evolving nature of crises makes effective domain adaptation essential. Supervised classification is limited by unchangeable class labels that may not be relevant to new events, and unsupervised topic modelling by insufficient prior knowledge. In this paper, we bridge the gap between the two and show that BERT embeddings finetuned on crisis-related tweet classification can effectively be used to adapt to a new crisis, discovering novel topics while preserving relevant classes from supervised training, and leveraging bidirectional self-attention to extract topic keywords. We create a dataset of tweets from a snowstorm to evaluate our method{'}s transferability to new crises, and find that it outperforms traditional topic models in both automatic, and human evaluations grounded in the needs of crisis managers. More broadly, our method can be used for textual domain adaptation where the latent classes are unknown but overlap with known classes from other domains.","year":2021,"title_abstract":"Bridging the gap between supervised classification and unsupervised topic modelling for social-media assisted crisis management Social media such as Twitter provide valuable information to crisis managers and affected people during natural disasters. Machine learning can help structure and extract information from the large volume of messages shared during a crisis; however, the constantly evolving nature of crises makes effective domain adaptation essential. Supervised classification is limited by unchangeable class labels that may not be relevant to new events, and unsupervised topic modelling by insufficient prior knowledge. In this paper, we bridge the gap between the two and show that BERT embeddings finetuned on crisis-related tweet classification can effectively be used to adapt to a new crisis, discovering novel topics while preserving relevant classes from supervised training, and leveraging bidirectional self-attention to extract topic keywords. We create a dataset of tweets from a snowstorm to evaluate our method{'}s transferability to new crises, and find that it outperforms traditional topic models in both automatic, and human evaluations grounded in the needs of crisis managers. More broadly, our method can be used for textual domain adaptation where the latent classes are unknown but overlap with known classes from other domains.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1673935205,"Goal":"Climate Action","Task":["social - media assisted crisis management","Supervised classification","crisis - related tweet classification","supervised training","transferability","automatic","crisis managers","textual domain adaptation"],"Method":["supervised classification","unsupervised topic modelling","Machine learning","domain adaptation","unsupervised topic modelling","BERT","bidirectional self - attention","topic models"]},{"ID":"scott-beaton-2010-evaluating","title":"Evaluating vendors for {MT} and post-editing at Avaya","abstract":"Avaya identified machine translation and post-editing as the next step in their strategy for global information management to deliver against the ever-present business objectives of {``}Increased Efficiency and Additional Localized Content{''}. Avaya shares how they assessed the market and selected their chosen vendor.","year":2010,"title_abstract":"Evaluating vendors for {MT} and post-editing at Avaya Avaya identified machine translation and post-editing as the next step in their strategy for global information management to deliver against the ever-present business objectives of {``}Increased Efficiency and Additional Localized Content{''}. Avaya shares how they assessed the market and selected their chosen vendor.","social_need":"Decent Work and Economic Growth Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all","cosine_similarity":0.16735816,"Goal":"Decent Work and Economic Growth","Task":["post - editing","machine translation","post - editing","global information management"],"Method":["{MT}","Avaya","Avaya"]},{"ID":"xu-etal-2022-eag","title":"{EAG}: Extract and Generate Multi-way Aligned Corpus for Complete Multi-lingual Neural Machine Translation","abstract":"Complete Multi-lingual Neural Machine Translation (C-MNMT) achieves superior performance against the conventional MNMT by constructing multi-way aligned corpus, i.e., aligning bilingual training examples from different language pairs when either their source or target sides are identical. However, since exactly identical sentences from different language pairs are scarce, the power of the multi-way aligned corpus is limited by its scale. To handle this problem, this paper proposes {``}Extract and Generate{''} (EAG), a two-step approach to construct large-scale and high-quality multi-way aligned corpus from bilingual data. Specifically, we first extract candidate aligned examples by pairing the bilingual examples from different language pairs with highly similar source or target sentences; and then generate the final aligned examples from the candidates with a well-trained generation model. With this two-step pipeline, EAG can construct a large-scale and multi-way aligned corpus whose diversity is almost identical to the original bilingual corpus. Experiments on two publicly available datasets i.e., WMT-5 and OPUS-100, show that the proposed method achieves significant improvements over strong baselines, with +1.1 and +1.4 BLEU points improvements on the two datasets respectively.","year":2022,"title_abstract":"{EAG}: Extract and Generate Multi-way Aligned Corpus for Complete Multi-lingual Neural Machine Translation Complete Multi-lingual Neural Machine Translation (C-MNMT) achieves superior performance against the conventional MNMT by constructing multi-way aligned corpus, i.e., aligning bilingual training examples from different language pairs when either their source or target sides are identical. However, since exactly identical sentences from different language pairs are scarce, the power of the multi-way aligned corpus is limited by its scale. To handle this problem, this paper proposes {``}Extract and Generate{''} (EAG), a two-step approach to construct large-scale and high-quality multi-way aligned corpus from bilingual data. Specifically, we first extract candidate aligned examples by pairing the bilingual examples from different language pairs with highly similar source or target sentences; and then generate the final aligned examples from the candidates with a well-trained generation model. With this two-step pipeline, EAG can construct a large-scale and multi-way aligned corpus whose diversity is almost identical to the original bilingual corpus. Experiments on two publicly available datasets i.e., WMT-5 and OPUS-100, show that the proposed method achieves significant improvements over strong baselines, with +1.1 and +1.4 BLEU points improvements on the two datasets respectively.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1672622859,"Goal":"Gender Equality","Task":["Complete Multi - lingual Neural Machine Translation","Multi - lingual Neural Machine Translation"],"Method":["MNMT)","MNMT","generation model","EAG"]},{"ID":"caminero-etal-2012-serenoa","title":"The {SERENOA} Project: Multidimensional Context-Aware Adaptation of Service Front-Ends","abstract":"The SERENOA project is aimed at developing a novel, open platform for enabling the creation of context-sensitive Service Front-Ends (SFEs). A context-sensitive SFE provides a user interface (UI) that allows users to interact with remote services, and which exhibits some capability to be aware of the context and to react to changes of this context in a continuous way. As a result, such UI will be adapted to e.g. a person's devices, tasks, preferences, abilities, and social relationships, as well as the conditions of the surrounding physical environment, thus improving people's satisfaction and performance compared to traditional SFEs based on manually designed UIs. The final aim is to support humans in a more effective, personalized and consistent way, thus improving the quality of life for citizens. In this scenario, we envisage SERENOA as the reference implementation of a SFE adaptation platform for the 'Future Internet'.","year":2012,"title_abstract":"The {SERENOA} Project: Multidimensional Context-Aware Adaptation of Service Front-Ends The SERENOA project is aimed at developing a novel, open platform for enabling the creation of context-sensitive Service Front-Ends (SFEs). A context-sensitive SFE provides a user interface (UI) that allows users to interact with remote services, and which exhibits some capability to be aware of the context and to react to changes of this context in a continuous way. As a result, such UI will be adapted to e.g. a person's devices, tasks, preferences, abilities, and social relationships, as well as the conditions of the surrounding physical environment, thus improving people's satisfaction and performance compared to traditional SFEs based on manually designed UIs. The final aim is to support humans in a more effective, personalized and consistent way, thus improving the quality of life for citizens. In this scenario, we envisage SERENOA as the reference implementation of a SFE adaptation platform for the 'Future Internet'.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1672334373,"Goal":"Sustainable Cities and Communities","Task":["Multidimensional Context - Aware Adaptation of Service Front - Ends","context - sensitive Service Front - Ends"],"Method":["SERENOA project","SFE","SFEs","SERENOA","SFE"]},{"ID":"van-den-heuvel-2020-crossing","title":"Crossing the {SSH} Bridge with Interview Data","abstract":"Spoken audio data, such as interview data, is a scientific instrument used by researchers in various disciplines crossing the boundaries of social sciences and humanities. In this paper, we will have a closer look at a portal designed to perform speech-to-text conversion on audio recordings through Automatic Speech Recognition (ASR) in the CLARIN infrastructure. Within the cluster cross-domain EU project SSHOC the potential value of such a linguistic tool kit for processing spoken language recording has found uptake in a webinar about the topic, and in a task addressing audio analysis of panel survey data. The objective of this contribution is to show that the processing of interviews as a research instrument has opened up a fascinating and fruitful area of collaboration between Social Sciences and Humanities (SSH).","year":2020,"title_abstract":"Crossing the {SSH} Bridge with Interview Data Spoken audio data, such as interview data, is a scientific instrument used by researchers in various disciplines crossing the boundaries of social sciences and humanities. In this paper, we will have a closer look at a portal designed to perform speech-to-text conversion on audio recordings through Automatic Speech Recognition (ASR) in the CLARIN infrastructure. Within the cluster cross-domain EU project SSHOC the potential value of such a linguistic tool kit for processing spoken language recording has found uptake in a webinar about the topic, and in a task addressing audio analysis of panel survey data. The objective of this contribution is to show that the processing of interviews as a research instrument has opened up a fascinating and fruitful area of collaboration between Social Sciences and Humanities (SSH).","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1672150195,"Goal":"Sustainable Cities and Communities","Task":["speech - to - text conversion","processing spoken language recording","audio analysis of panel survey data","processing of interviews","Social Sciences"],"Method":["Automatic Speech Recognition","CLARIN infrastructure","linguistic tool kit"]},{"ID":"rudnick-etal-2014-guampa","title":"{G}uampa: a Toolkit for Collaborative Translation","abstract":"Here we present Guampa, a new software package for online collaborative translation. This system grows out of our discussions with Guarani-language activists and educators in Paraguay, and attempts to address problems faced by machine translation researchers and by members of any community speaking an under-represented language. Guampa enables volunteers and students to work together to translate documents into heritage languages, both to make more materials available in those languages, and also to generate bitext suitable for training machine translation systems. While many approaches to crowdsourcing bitext corpora focus on Mechanical Turk and temporarily engaging anonymous workers, Guampa is intended to foster an online community in which discussions can take place, language learners can practice their translation skills, and complete documents can be translated. This approach is appropriate for the Spanish-Guarani language pair as there are many speakers of both languages, and Guarani has a dedicated activist community. Our goal is to make it easy for anyone to set up their own instance of Guampa and populate it with documents -- such as automatically imported Wikipedia articles -- to be translated for their particular language pair. Guampa is freely available and relatively easy to use.","year":2014,"title_abstract":"{G}uampa: a Toolkit for Collaborative Translation Here we present Guampa, a new software package for online collaborative translation. This system grows out of our discussions with Guarani-language activists and educators in Paraguay, and attempts to address problems faced by machine translation researchers and by members of any community speaking an under-represented language. Guampa enables volunteers and students to work together to translate documents into heritage languages, both to make more materials available in those languages, and also to generate bitext suitable for training machine translation systems. While many approaches to crowdsourcing bitext corpora focus on Mechanical Turk and temporarily engaging anonymous workers, Guampa is intended to foster an online community in which discussions can take place, language learners can practice their translation skills, and complete documents can be translated. This approach is appropriate for the Spanish-Guarani language pair as there are many speakers of both languages, and Guarani has a dedicated activist community. Our goal is to make it easy for anyone to set up their own instance of Guampa and populate it with documents -- such as automatically imported Wikipedia articles -- to be translated for their particular language pair. Guampa is freely available and relatively easy to use.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1670656204,"Goal":"Sustainable Cities and Communities","Task":["Collaborative Translation","online collaborative translation","machine translation researchers","machine translation systems","crowdsourcing bitext corpora","Mechanical Turk","translation"],"Method":["{G}uampa","Guampa","software package","Guampa","Guampa","Guampa","Guampa"]},{"ID":"rehbein-ruppenhofer-2020-new","title":"A New Resource for {G}erman Causal Language","abstract":"We present a new resource for German causal language, with annotations in context for verbs, nouns and prepositions. Our dataset includes 4,390 annotated instances for more than 150 different triggers. The annotation scheme distinguishes three different types of causal events (CONSEQUENCE , MOTIVATION, PURPOSE). We also provide annotations for semantic roles, i.e. of the cause and effect for the causal event as well as the actor and affected party, if present. In the paper, we present inter-annotator agreement scores for our dataset and discuss problems for annotating causal language. Finally, we present experiments where we frame causal annotation as a sequence labelling problem and report baseline results for the prediciton of causal arguments and for predicting different types of causation.","year":2020,"title_abstract":"A New Resource for {G}erman Causal Language We present a new resource for German causal language, with annotations in context for verbs, nouns and prepositions. Our dataset includes 4,390 annotated instances for more than 150 different triggers. The annotation scheme distinguishes three different types of causal events (CONSEQUENCE , MOTIVATION, PURPOSE). We also provide annotations for semantic roles, i.e. of the cause and effect for the causal event as well as the actor and affected party, if present. In the paper, we present inter-annotator agreement scores for our dataset and discuss problems for annotating causal language. Finally, we present experiments where we frame causal annotation as a sequence labelling problem and report baseline results for the prediciton of causal arguments and for predicting different types of causation.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1669716239,"Goal":"Climate Action","Task":["{G}erman Causal Language","annotating causal language","causal annotation","sequence labelling problem","prediciton of causal arguments","predicting different types of causation"],"Method":["annotation scheme"]},{"ID":"bengoetxea-etal-2020-laguntest","title":"{L}agun{T}est: A {NLP} Based Application to Enhance Reading Comprehension","abstract":"The ability to read and understand written texts plays an important role in education, above all in the last years of primary education. This is especially pertinent in language immersion educational programmes, where some students have low linguistic competence in the languages of instruction. In this context, adapting the texts to the individual needs of each student requires a considerable effort by education professionals. However, language technologies can facilitate the laborious adaptation of materials in order to enhance reading comprehension. In this paper, we present LagunTest, a NLP based application that takes as input a text in Basque or English, and offers synonyms, definitions, examples of the words in different contexts and presents some linguistic characteristics as well as visualizations. LagunTest is based on reusable and open multilingual and multimodal tools, and it is also distributed with an open license. LagunTest is intended to ease the burden of education professionals in the task of adapting materials, and the output should always be supervised by them.","year":2020,"title_abstract":"{L}agun{T}est: A {NLP} Based Application to Enhance Reading Comprehension The ability to read and understand written texts plays an important role in education, above all in the last years of primary education. This is especially pertinent in language immersion educational programmes, where some students have low linguistic competence in the languages of instruction. In this context, adapting the texts to the individual needs of each student requires a considerable effort by education professionals. However, language technologies can facilitate the laborious adaptation of materials in order to enhance reading comprehension. In this paper, we present LagunTest, a NLP based application that takes as input a text in Basque or English, and offers synonyms, definitions, examples of the words in different contexts and presents some linguistic characteristics as well as visualizations. LagunTest is based on reusable and open multilingual and multimodal tools, and it is also distributed with an open license. LagunTest is intended to ease the burden of education professionals in the task of adapting materials, and the output should always be supervised by them.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.166964218,"Goal":"Quality Education","Task":["Reading Comprehension","education","primary education","language immersion educational programmes","adaptation of materials","reading comprehension","adapting materials"],"Method":["{NLP} Based Application","language technologies","LagunTest","NLP based application","LagunTest","LagunTest"]},{"ID":"sweed-shahaf-2021-catchphrase","title":"Catchphrase: Automatic Detection of Cultural References","abstract":"A snowclone is a customizable phrasal template that can be realized in multiple, instantly recognized variants. For example, {``}* is the new *'' (Orange is the new black, 40 is the new 30). Snowclones are extensively used in social media. In this paper, we study snowclones originating from pop-culture quotes; our goal is to automatically detect cultural references in text. We introduce a new, publicly available data set of pop-culture quotes and their corresponding snowclone usages and train models on them. We publish code for Catchphrase, an internet browser plugin to automatically detect and mark references in real-time, and examine its performance via a user study. Aside from assisting people to better comprehend cultural references, we hope that detecting snowclones can complement work on paraphrasing and help tackling long-standing questions in social science about the dynamics of information propagation.","year":2021,"title_abstract":"Catchphrase: Automatic Detection of Cultural References A snowclone is a customizable phrasal template that can be realized in multiple, instantly recognized variants. For example, {``}* is the new *'' (Orange is the new black, 40 is the new 30). Snowclones are extensively used in social media. In this paper, we study snowclones originating from pop-culture quotes; our goal is to automatically detect cultural references in text. We introduce a new, publicly available data set of pop-culture quotes and their corresponding snowclone usages and train models on them. We publish code for Catchphrase, an internet browser plugin to automatically detect and mark references in real-time, and examine its performance via a user study. Aside from assisting people to better comprehend cultural references, we hope that detecting snowclones can complement work on paraphrasing and help tackling long-standing questions in social science about the dynamics of information propagation.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1669141352,"Goal":"Climate Action","Task":["Automatic Detection of Cultural References","cultural references","mark references","detecting snowclones","paraphrasing","social science","dynamics of information propagation"],"Method":["Catchphrase","snowclone","Snowclones","snowclones","Catchphrase","internet browser plugin"]},{"ID":"scarton-etal-2017-musst","title":"{MUSST}: A Multilingual Syntactic Simplification Tool","abstract":"We describe MUSST, a multilingual syntactic simplification tool. The tool supports sentence simplifications for English, Italian and Spanish, and can be easily extended to other languages. Our implementation includes a set of general-purpose simplification rules, as well as a sentence selection module (to select sentences to be simplified) and a confidence model (to select only promising simplifications). The tool was implemented in the context of the European project SIMPATICO on text simplification for Public Administration (PA) texts. Our evaluation on sentences in the PA domain shows that we obtain correct simplifications for 76{\\%} of the simplified cases in English, 71{\\%} of the cases in Spanish. For Italian, the results are lower (38{\\%}) but the tool is still under development.","year":2017,"title_abstract":"{MUSST}: A Multilingual Syntactic Simplification Tool We describe MUSST, a multilingual syntactic simplification tool. The tool supports sentence simplifications for English, Italian and Spanish, and can be easily extended to other languages. Our implementation includes a set of general-purpose simplification rules, as well as a sentence selection module (to select sentences to be simplified) and a confidence model (to select only promising simplifications). The tool was implemented in the context of the European project SIMPATICO on text simplification for Public Administration (PA) texts. Our evaluation on sentences in the PA domain shows that we obtain correct simplifications for 76{\\%} of the simplified cases in English, 71{\\%} of the cases in Spanish. For Italian, the results are lower (38{\\%}) but the tool is still under development.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1668293625,"Goal":"Reduced Inequalities","Task":["sentence simplifications","text simplification","Public Administration"],"Method":["Multilingual Syntactic Simplification Tool","MUSST","multilingual syntactic simplification tool","general - purpose simplification rules","sentence selection module","confidence model"]},{"ID":"lepage-etal-2008-greyc","title":"The {GREYC} machine translation system for the {IWSLT} 2008 evaluation campaign.","abstract":"This year's GREYC machine translation (MT) system presents three major changes relative to the system presented during the previous campaign, while, of course, remaining a pure example-based MT system that exploits proportional analogies. Firstly, the analogy solver has been replaced with a truly non-deterministic one. Secondly, the engine has been re-engineered and a better control has been introduced. Thirdly, the data used for translation were the data provided by the organizers plus alignments obtained using a new alignment method. This year we chose to have the engine run with the word as the processing unit on the contrary to previous years where the processing unit used to be the character. The tracks the system participated in are all classic BTEC tracks (Arabic-English, Chinese-English and Chinese-Spanish) plus the so-called PIVOT task, where the test set had to be translated from Chinese into Spanish by way of English.","year":2008,"title_abstract":"The {GREYC} machine translation system for the {IWSLT} 2008 evaluation campaign. This year's GREYC machine translation (MT) system presents three major changes relative to the system presented during the previous campaign, while, of course, remaining a pure example-based MT system that exploits proportional analogies. Firstly, the analogy solver has been replaced with a truly non-deterministic one. Secondly, the engine has been re-engineered and a better control has been introduced. Thirdly, the data used for translation were the data provided by the organizers plus alignments obtained using a new alignment method. This year we chose to have the engine run with the word as the processing unit on the contrary to previous years where the processing unit used to be the character. The tracks the system participated in are all classic BTEC tracks (Arabic-English, Chinese-English and Chinese-Spanish) plus the so-called PIVOT task, where the test set had to be translated from Chinese into Spanish by way of English.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1668259948,"Goal":"Gender Equality","Task":["machine translation system","GREYC machine translation","translation","PIVOT task"],"Method":["example - based MT system","analogy solver","alignment method","processing unit"]},{"ID":"omelianchuk-etal-2020-gector","title":"{GECT}o{R} {--} Grammatical Error Correction: Tag, Not Rewrite","abstract":"In this paper, we present a simple and efficient GEC sequence tagger using a Transformer encoder. Our system is pre-trained on synthetic data and then fine-tuned in two stages: first on errorful corpora, and second on a combination of errorful and error-free parallel corpora. We design custom token-level transformations to map input tokens to target corrections. Our best single-model\/ensemble GEC tagger achieves an F{\\_}0.5 of 65.3\/66.5 on CONLL-2014 (test) and F{\\_}0.5 of 72.4\/73.6 on BEA-2019 (test). Its inference speed is up to 10 times as fast as a Transformer-based seq2seq GEC system.","year":2020,"title_abstract":"{GECT}o{R} {--} Grammatical Error Correction: Tag, Not Rewrite In this paper, we present a simple and efficient GEC sequence tagger using a Transformer encoder. Our system is pre-trained on synthetic data and then fine-tuned in two stages: first on errorful corpora, and second on a combination of errorful and error-free parallel corpora. We design custom token-level transformations to map input tokens to target corrections. Our best single-model\/ensemble GEC tagger achieves an F{\\_}0.5 of 65.3\/66.5 on CONLL-2014 (test) and F{\\_}0.5 of 72.4\/73.6 on BEA-2019 (test). Its inference speed is up to 10 times as fast as a Transformer-based seq2seq GEC system.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1667299271,"Goal":"Gender Equality","Task":["Grammatical Error Correction","GEC"],"Method":["GEC sequence tagger","Transformer encoder","token - level transformations","GEC","seq2seq"]},{"ID":"potthast-etal-2018-crowdsourcing","title":"Crowdsourcing a Large Corpus of Clickbait on {T}witter","abstract":"Clickbait has become a nuisance on social media. To address the urging task of clickbait detection, we constructed a new corpus of 38,517 annotated Twitter tweets, the Webis Clickbait Corpus 2017. To avoid biases in terms of publisher and topic, tweets were sampled from the top 27 most retweeted news publishers, covering a period of 150 days. Each tweet has been annotated on 4-point scale by five annotators recruited at Amazon{'}s Mechanical Turk. The corpus has been employed to evaluate 12 clickbait detectors submitted to the Clickbait Challenge 2017. Download: https:\/\/webis.de\/data\/webis-clickbait-17.html Challenge: https:\/\/clickbait-challenge.org","year":2018,"title_abstract":"Crowdsourcing a Large Corpus of Clickbait on {T}witter Clickbait has become a nuisance on social media. To address the urging task of clickbait detection, we constructed a new corpus of 38,517 annotated Twitter tweets, the Webis Clickbait Corpus 2017. To avoid biases in terms of publisher and topic, tweets were sampled from the top 27 most retweeted news publishers, covering a period of 150 days. Each tweet has been annotated on 4-point scale by five annotators recruited at Amazon{'}s Mechanical Turk. The corpus has been employed to evaluate 12 clickbait detectors submitted to the Clickbait Challenge 2017. Download: https:\/\/webis.de\/data\/webis-clickbait-17.html Challenge: https:\/\/clickbait-challenge.org","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1666857004,"Goal":"Climate Action","Task":["clickbait detection","Clickbait Challenge"],"Method":["clickbait detectors"]},{"ID":"xenouleas-etal-2019-sum","title":"{SUM}-{QE}: a {BERT}-based Summary Quality Estimation Model","abstract":"We propose SUM-QE, a novel Quality Estimation model for summarization based on BERT. The model addresses linguistic quality aspects that are only indirectly captured by content-based approaches to summary evaluation, without involving comparison with human references. SUM-QE achieves very high correlations with human ratings, outperforming simpler models addressing these linguistic aspects. Predictions of the SUM-QE model can be used for system development, and to inform users of the quality of automatically produced summaries and other types of generated text.","year":2019,"title_abstract":"{SUM}-{QE}: a {BERT}-based Summary Quality Estimation Model We propose SUM-QE, a novel Quality Estimation model for summarization based on BERT. The model addresses linguistic quality aspects that are only indirectly captured by content-based approaches to summary evaluation, without involving comparison with human references. SUM-QE achieves very high correlations with human ratings, outperforming simpler models addressing these linguistic aspects. Predictions of the SUM-QE model can be used for system development, and to inform users of the quality of automatically produced summaries and other types of generated text.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1666205227,"Goal":"Quality Education","Task":["summarization","BERT","summary evaluation","system development"],"Method":["{BERT} - based Summary Quality Estimation Model","SUM - QE","Quality Estimation model","content - based approaches","SUM - QE","SUM - QE model"]},{"ID":"milde-etal-2016-ambient","title":"Ambient Search: A Document Retrieval System for Speech Streams","abstract":"We present Ambient Search, an open source system for displaying and retrieving relevant documents in real time for speech input. The system works ambiently, that is, it unobstructively listens to speech streams in the background, identifies keywords and keyphrases for query construction and continuously serves relevant documents from its index. Query terms are ranked with Word2Vec and TF-IDF and are continuously updated to allow for ongoing querying of a document collection. The retrieved documents, in our case Wikipedia articles, are visualized in real time in a browser interface. Our evaluation shows that Ambient Search compares favorably to another implicit information retrieval system on speech streams. Furthermore, we extrinsically evaluate multiword keyphrase generation, showing positive impact for manual transcriptions.","year":2016,"title_abstract":"Ambient Search: A Document Retrieval System for Speech Streams We present Ambient Search, an open source system for displaying and retrieving relevant documents in real time for speech input. The system works ambiently, that is, it unobstructively listens to speech streams in the background, identifies keywords and keyphrases for query construction and continuously serves relevant documents from its index. Query terms are ranked with Word2Vec and TF-IDF and are continuously updated to allow for ongoing querying of a document collection. The retrieved documents, in our case Wikipedia articles, are visualized in real time in a browser interface. Our evaluation shows that Ambient Search compares favorably to another implicit information retrieval system on speech streams. Furthermore, we extrinsically evaluate multiword keyphrase generation, showing positive impact for manual transcriptions.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.1666190624,"Goal":"Life Below Water","Task":["Speech Streams","displaying and retrieving relevant documents","speech input","query construction","ongoing querying of a document collection","Ambient Search","multiword keyphrase generation","manual transcriptions"],"Method":["Ambient Search","Document Retrieval System","Ambient Search","TF - IDF","implicit information retrieval system"]},{"ID":"imamura-etal-2018-enhancement","title":"Enhancement of Encoder and Attention Using Target Monolingual Corpora in Neural Machine Translation","abstract":"A large-scale parallel corpus is required to train encoder-decoder neural machine translation. The method of using synthetic parallel texts, in which target monolingual corpora are automatically translated into source sentences, is effective in improving the decoder, but is unreliable for enhancing the encoder. In this paper, we propose a method that enhances the encoder and attention using target monolingual corpora by generating multiple source sentences via sampling. By using multiple source sentences, diversity close to that of humans is achieved. Our experimental results show that the translation quality is improved by increasing the number of synthetic source sentences for each given target sentence, and quality close to that using a manually created parallel corpus was achieved.","year":2018,"title_abstract":"Enhancement of Encoder and Attention Using Target Monolingual Corpora in Neural Machine Translation A large-scale parallel corpus is required to train encoder-decoder neural machine translation. The method of using synthetic parallel texts, in which target monolingual corpora are automatically translated into source sentences, is effective in improving the decoder, but is unreliable for enhancing the encoder. In this paper, we propose a method that enhances the encoder and attention using target monolingual corpora by generating multiple source sentences via sampling. By using multiple source sentences, diversity close to that of humans is achieved. Our experimental results show that the translation quality is improved by increasing the number of synthetic source sentences for each given target sentence, and quality close to that using a manually created parallel corpus was achieved.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1664981246,"Goal":"Gender Equality","Task":["Encoder and Attention","Neural Machine Translation","encoder and attention"],"Method":["encoder - decoder","decoder","encoder"]},{"ID":"tebbifakhr-etal-2019-effort","title":"Effort-Aware Neural Automatic Post-Editing","abstract":"For this round of the WMT 2019 APE shared task, our submission focuses on addressing the {``}over-correction{''} problem in APE. Over-correction occurs when the APE system tends to rephrase an already correct MT output, and the resulting sentence is penalized by a reference-based evaluation against human post-edits. Our intuition is that this problem can be prevented by informing the system about the predicted quality of the MT output or, in other terms, the expected amount of needed corrections. For this purpose, following the common approach in multilingual NMT, we prepend a special token to the beginning of both the source text and the MT output indicating the required amount of post-editing. Following the best submissions to the WMT 2018 APE shared task, our backbone architecture is based on multi-source Transformer to encode both the MT output and the corresponding source text. We participated both in the English-German and English-Russian subtasks. In the first subtask, our best submission improved the original MT output quality up to +0.98 BLEU and -0.47 TER. In the second subtask, where the higher quality of the MT output increases the risk of over-correction, none of our submitted runs was able to improve the MT output.","year":2019,"title_abstract":"Effort-Aware Neural Automatic Post-Editing For this round of the WMT 2019 APE shared task, our submission focuses on addressing the {``}over-correction{''} problem in APE. Over-correction occurs when the APE system tends to rephrase an already correct MT output, and the resulting sentence is penalized by a reference-based evaluation against human post-edits. Our intuition is that this problem can be prevented by informing the system about the predicted quality of the MT output or, in other terms, the expected amount of needed corrections. For this purpose, following the common approach in multilingual NMT, we prepend a special token to the beginning of both the source text and the MT output indicating the required amount of post-editing. Following the best submissions to the WMT 2018 APE shared task, our backbone architecture is based on multi-source Transformer to encode both the MT output and the corresponding source text. We participated both in the English-German and English-Russian subtasks. In the first subtask, our best submission improved the original MT output quality up to +0.98 BLEU and -0.47 TER. In the second subtask, where the higher quality of the MT output increases the risk of over-correction, none of our submitted runs was able to improve the MT output.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1663845778,"Goal":"Gender Equality","Task":["Effort - Aware Neural Automatic Post - Editing","{``}over - correction{''} problem","APE","Over - correction","multilingual NMT","over - correction"],"Method":["APE system","reference - based evaluation","backbone architecture","multi - source Transformer"]},{"ID":"caselli-etal-2021-guiding","title":"Guiding Principles for Participatory Design-inspired Natural Language Processing","abstract":"We introduce 9 guiding principles to integrate Participatory Design (PD) methods in the development of Natural Language Processing (NLP) systems. The adoption of PD methods by NLP will help to alleviate issues concerning the development of more democratic, fairer, less-biased technologies to process natural language data. This short paper is the outcome of an ongoing dialogue between designers and NLP experts and adopts a non-standard format following previous work by Traum (2000); Bender (2013); Abzianidze and Bos (2019). Every section is a guiding principle. While principles 1{--}3 illustrate assumptions and methods that inform community-based PD practices, we used two fictional design scenarios (Encinas and Blythe, 2018), which build on top of situations familiar to the authors, to elicit the identification of the other 6. Principles 4{--}6 describes the impact of PD methods on the design of NLP systems, targeting two critical aspects: data collection {\\&} annotation, and the deployment {\\&} evaluation. Finally, principles 7{--}9 guide a new reflexivity of the NLP research with respect to its context, actors and participants, and aims. We hope this guide will offer inspiration and a road-map to develop a new generation of PD-inspired NLP.","year":2021,"title_abstract":"Guiding Principles for Participatory Design-inspired Natural Language Processing We introduce 9 guiding principles to integrate Participatory Design (PD) methods in the development of Natural Language Processing (NLP) systems. The adoption of PD methods by NLP will help to alleviate issues concerning the development of more democratic, fairer, less-biased technologies to process natural language data. This short paper is the outcome of an ongoing dialogue between designers and NLP experts and adopts a non-standard format following previous work by Traum (2000); Bender (2013); Abzianidze and Bos (2019). Every section is a guiding principle. While principles 1{--}3 illustrate assumptions and methods that inform community-based PD practices, we used two fictional design scenarios (Encinas and Blythe, 2018), which build on top of situations familiar to the authors, to elicit the identification of the other 6. Principles 4{--}6 describes the impact of PD methods on the design of NLP systems, targeting two critical aspects: data collection {\\&} annotation, and the deployment {\\&} evaluation. Finally, principles 7{--}9 guide a new reflexivity of the NLP research with respect to its context, actors and participants, and aims. We hope this guide will offer inspiration and a road-map to develop a new generation of PD-inspired NLP.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1663838029,"Goal":"Sustainable Cities and Communities","Task":["Participatory Design - inspired Natural Language Processing","Natural Language Processing","NLP","data collection","annotation","deployment","evaluation","NLP"],"Method":["Guiding Principles","guiding principles","Participatory Design (PD) methods","PD methods","NLP","biased technologies","NLP","community - based PD practices","PD methods","PD - inspired NLP"]},{"ID":"stahlberg-etal-2018-operation","title":"An Operation Sequence Model for Explainable Neural Machine Translation","abstract":"We propose to achieve explainable neural machine translation (NMT) by changing the output representation to explain itself. We present a novel approach to NMT which generates the target sentence by monotonically walking through the source sentence. Word reordering is modeled by operations which allow setting markers in the target sentence and move a target-side write head between those markers. In contrast to many modern neural models, our system emits explicit word alignment information which is often crucial to practical machine translation as it improves explainability. Our technique can outperform a plain text system in terms of BLEU score under the recent Transformer architecture on Japanese-English and Portuguese-English, and is within 0.5 BLEU difference on Spanish-English.","year":2018,"title_abstract":"An Operation Sequence Model for Explainable Neural Machine Translation We propose to achieve explainable neural machine translation (NMT) by changing the output representation to explain itself. We present a novel approach to NMT which generates the target sentence by monotonically walking through the source sentence. Word reordering is modeled by operations which allow setting markers in the target sentence and move a target-side write head between those markers. In contrast to many modern neural models, our system emits explicit word alignment information which is often crucial to practical machine translation as it improves explainability. Our technique can outperform a plain text system in terms of BLEU score under the recent Transformer architecture on Japanese-English and Portuguese-English, and is within 0.5 BLEU difference on Spanish-English.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1663554609,"Goal":"Gender Equality","Task":["Explainable Neural Machine Translation","explainable neural machine translation","NMT","Word reordering","machine translation"],"Method":["Operation Sequence Model","neural models","plain text system","Transformer architecture"]},{"ID":"fukuda-etal-2022-naist","title":"{NAIST} Simultaneous Speech-to-Text Translation System for {IWSLT} 2022","abstract":"This paper describes NAIST{'}s simultaneous speech translation systems developed for IWSLT 2022 Evaluation Campaign. We participated the speech-to-speech track for English-to-German and English-to-Japanese. Our primary submissions were end-to-end systems using adaptive segmentation policies based on Prefix Alignment.","year":2022,"title_abstract":"{NAIST} Simultaneous Speech-to-Text Translation System for {IWSLT} 2022 This paper describes NAIST{'}s simultaneous speech translation systems developed for IWSLT 2022 Evaluation Campaign. We participated the speech-to-speech track for English-to-German and English-to-Japanese. Our primary submissions were end-to-end systems using adaptive segmentation policies based on Prefix Alignment.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1662974656,"Goal":"Gender Equality","Task":["Simultaneous Speech - to - Text Translation System","NAIST{'}s simultaneous speech translation","IWSLT 2022 Evaluation Campaign","speech - to - speech track"],"Method":["end systems","adaptive segmentation policies","Prefix Alignment"]},{"ID":"bond-bond-2019-geonames","title":"{G}eo{N}ames {W}ordnet (geown): extracting wordnets from {G}eo{N}ames","abstract":"This paper introduces a new multilingual lexicon of geographical place names. The names are based on (and linked to) the GeoNames collection. Each location is treated as a new synset, which is linked by instance{\\_}hypernym to a small set of supertypes. These supertypes are linked to the collaborative interlingual index, based on mappings from GeoDomainWordnet. If a location is already in the interlingual index, then it is also linked to the entry, using mappings from the Geo-Wordnet. Finally, if GeoNames places the location in a larger location, this is linked using the mero{\\_}location link. Wordnets can be built for any language in GeoNames, we give results for those wordnets in the Open Multilingual Wordnet. We discuss how it is mapped and the characteristics of the extracted wordnets.","year":2019,"title_abstract":"{G}eo{N}ames {W}ordnet (geown): extracting wordnets from {G}eo{N}ames This paper introduces a new multilingual lexicon of geographical place names. The names are based on (and linked to) the GeoNames collection. Each location is treated as a new synset, which is linked by instance{\\_}hypernym to a small set of supertypes. These supertypes are linked to the collaborative interlingual index, based on mappings from GeoDomainWordnet. If a location is already in the interlingual index, then it is also linked to the entry, using mappings from the Geo-Wordnet. Finally, if GeoNames places the location in a larger location, this is linked using the mero{\\_}location link. Wordnets can be built for any language in GeoNames, we give results for those wordnets in the Open Multilingual Wordnet. We discuss how it is mapped and the characteristics of the extracted wordnets.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1662197113,"Goal":"Sustainable Cities and Communities","Task":["extracting wordnets"],"Method":["GeoDomainWordnet","mero{\\_}location link","Wordnets"]},{"ID":"teodorescu-etal-2022-cree","title":"{C}ree Corpus: A Collection of n{\\^e}hiyaw{\\^e}win Resources","abstract":"Plains Cree (n{\\^e}hiyaw{\\^e}win) is an Indigenous language that is spoken in Canada and the USA. It is the most widely spoken dialect of Cree and a morphologically complex language that is polysynthetic, highly inflective, and agglutinative. It is an extremely low resource language, with no existing corpus that is both available and prepared for supporting the development of language technologies. To support n{\\^e}hiyaw{\\^e}win revitalization and preservation, we developed a corpus covering diverse genres, time periods, and texts for a variety of intended audiences. The data has been verified and cleaned; it is ready for use in developing language technologies for n{\\^e}hiyaw{\\^e}win. The corpus includes the corresponding English phrases or audio files where available. We demonstrate the utility of the corpus through its community use and its use to build language technologies that can provide the types of support that community members have expressed are desirable. The corpus is available for public use.","year":2022,"title_abstract":"{C}ree Corpus: A Collection of n{\\^e}hiyaw{\\^e}win Resources Plains Cree (n{\\^e}hiyaw{\\^e}win) is an Indigenous language that is spoken in Canada and the USA. It is the most widely spoken dialect of Cree and a morphologically complex language that is polysynthetic, highly inflective, and agglutinative. It is an extremely low resource language, with no existing corpus that is both available and prepared for supporting the development of language technologies. To support n{\\^e}hiyaw{\\^e}win revitalization and preservation, we developed a corpus covering diverse genres, time periods, and texts for a variety of intended audiences. The data has been verified and cleaned; it is ready for use in developing language technologies for n{\\^e}hiyaw{\\^e}win. The corpus includes the corresponding English phrases or audio files where available. We demonstrate the utility of the corpus through its community use and its use to build language technologies that can provide the types of support that community members have expressed are desirable. The corpus is available for public use.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1662136614,"Goal":"Sustainable Cities and Communities","Task":["language technologies","n{\\^e}hiyaw{\\^e}win revitalization","preservation"],"Method":["language technologies","language technologies"]},{"ID":"balouchzahi-etal-2021-mucs-lt","title":"{MUCS}@{LT}-{EDI}-{EACL}2021:{C}o{H}ope-Hope Speech Detection for Equality, Diversity, and Inclusion in Code-Mixed Texts","abstract":"This paper describes the models submitted by the team MUCS for {``}Hope Speech Detection for Equality, Diversity, and Inclusion-EACL 2021{''} shared task that aims at classifying a comment \/ post in English and code-mixed texts in two language pairs, namely, Tamil-English (Ta-En) and Malayalam-English (Ma-En) into one of the three predefined categories, namely, {``}Hope{\\_}speech{''}, {``}Non{\\_}hope{\\_}speech{''}, and {``}other{\\_}languages{''}. Three models namely, CoHope-ML, CoHope-NN, and CoHope-TL based on Ensemble of classifiers, Keras Neural Network (NN) and BiLSTM with Conv1d model respectively are proposed for the shared task. CoHope-ML, CoHope-NN models are trained on a feature set comprised of char sequences extracted from sentences combined with words for Ma-En and Ta-En code-mixed texts and a combination of word and char ngrams along with syntactic word ngrams for English text. CoHope-TL model consists of three major parts: training tokenizer, BERT Language Model (LM) training and then using pre-trained BERT LM as weights in BiLSTM-Conv1d model. Out of three proposed models, CoHope-ML model (best among our models) obtained 1st, 2nd, and 3rd ranks with weighted F1-scores of 0.85, 0.92, and 0.59 for Ma-En, English and Ta-En texts respectively.","year":2021,"title_abstract":"{MUCS}@{LT}-{EDI}-{EACL}2021:{C}o{H}ope-Hope Speech Detection for Equality, Diversity, and Inclusion in Code-Mixed Texts This paper describes the models submitted by the team MUCS for {``}Hope Speech Detection for Equality, Diversity, and Inclusion-EACL 2021{''} shared task that aims at classifying a comment \/ post in English and code-mixed texts in two language pairs, namely, Tamil-English (Ta-En) and Malayalam-English (Ma-En) into one of the three predefined categories, namely, {``}Hope{\\_}speech{''}, {``}Non{\\_}hope{\\_}speech{''}, and {``}other{\\_}languages{''}. Three models namely, CoHope-ML, CoHope-NN, and CoHope-TL based on Ensemble of classifiers, Keras Neural Network (NN) and BiLSTM with Conv1d model respectively are proposed for the shared task. CoHope-ML, CoHope-NN models are trained on a feature set comprised of char sequences extracted from sentences combined with words for Ma-En and Ta-En code-mixed texts and a combination of word and char ngrams along with syntactic word ngrams for English text. CoHope-TL model consists of three major parts: training tokenizer, BERT Language Model (LM) training and then using pre-trained BERT LM as weights in BiLSTM-Conv1d model. Out of three proposed models, CoHope-ML model (best among our models) obtained 1st, 2nd, and 3rd ranks with weighted F1-scores of 0.85, 0.92, and 0.59 for Ma-En, English and Ta-En texts respectively.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.166183576,"Goal":"Gender Equality","Task":["Equality","Speech Detection","Equality","Diversity","Inclusion - EACL 2021{''} shared task"],"Method":["MUCS","CoHope - ML","CoHope - NN","CoHope - TL","Ensemble of classifiers","Keras Neural Network","BiLSTM with Conv1d model","CoHope - ML","CoHope - NN","word and char ngrams","syntactic word ngrams","CoHope - TL","tokenizer","BERT Language Model","BERT LM","BiLSTM - Conv1d model","CoHope - ML model"]},{"ID":"pal-2020-wt","title":"{WT}: Wipro {AI} Submissions to the {WAT} 2020","abstract":"In this paper we present an English{--}Hindi and Hindi{--}English neural machine translation (NMT) system, submitted to the Translation shared Task organized at WAT 2020. We trained a multilingual NMT system based on transformer architecture. In this paper we show: (i) how effective pre-processing helps to improve performance, (ii) how synthetic data through back-translation from available monolingual data can help in overall translation performance, (iii) how language similarity can aid more onto it. Our submissions ranked 1st in both English to Hindi and Hindi to English translation achieving BLEU 20.80 and 29.59 respectively.","year":2020,"title_abstract":"{WT}: Wipro {AI} Submissions to the {WAT} 2020 In this paper we present an English{--}Hindi and Hindi{--}English neural machine translation (NMT) system, submitted to the Translation shared Task organized at WAT 2020. We trained a multilingual NMT system based on transformer architecture. In this paper we show: (i) how effective pre-processing helps to improve performance, (ii) how synthetic data through back-translation from available monolingual data can help in overall translation performance, (iii) how language similarity can aid more onto it. Our submissions ranked 1st in both English to Hindi and Hindi to English translation achieving BLEU 20.80 and 29.59 respectively.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.166158855,"Goal":"Gender Equality","Task":["translation","Translation shared Task","WAT 2020","translation","translation"],"Method":["NMT","transformer architecture","pre - processing","back - translation"]},{"ID":"si-etal-2021-topic","title":"Topic-Aware Evidence Reasoning and Stance-Aware Aggregation for Fact Verification","abstract":"Fact verification is a challenging task that requires simultaneously reasoning and aggregating over multiple retrieved pieces of evidence to evaluate the truthfulness of a claim. Existing approaches typically (i) explore the semantic interaction between the claim and evidence at different granularity levels but fail to capture their topical consistency during the reasoning process, which we believe is crucial for verification; (ii) aggregate multiple pieces of evidence equally without considering their implicit stances to the claim, thereby introducing spurious information. To alleviate the above issues, we propose a novel topic-aware evidence reasoning and stance-aware aggregation model for more accurate fact verification, with the following four key properties: 1) checking topical consistency between the claim and evidence; 2) maintaining topical coherence among multiple pieces of evidence; 3) ensuring semantic similarity between the global topic information and the semantic representation of evidence; 4) aggregating evidence based on their implicit stances to the claim. Extensive experiments conducted on the two benchmark datasets demonstrate the superiority of the proposed model over several state-of-the-art approaches for fact verification. The source code can be obtained from https:\/\/github.com\/jasenchn\/TARSA.","year":2021,"title_abstract":"Topic-Aware Evidence Reasoning and Stance-Aware Aggregation for Fact Verification Fact verification is a challenging task that requires simultaneously reasoning and aggregating over multiple retrieved pieces of evidence to evaluate the truthfulness of a claim. Existing approaches typically (i) explore the semantic interaction between the claim and evidence at different granularity levels but fail to capture their topical consistency during the reasoning process, which we believe is crucial for verification; (ii) aggregate multiple pieces of evidence equally without considering their implicit stances to the claim, thereby introducing spurious information. To alleviate the above issues, we propose a novel topic-aware evidence reasoning and stance-aware aggregation model for more accurate fact verification, with the following four key properties: 1) checking topical consistency between the claim and evidence; 2) maintaining topical coherence among multiple pieces of evidence; 3) ensuring semantic similarity between the global topic information and the semantic representation of evidence; 4) aggregating evidence based on their implicit stances to the claim. Extensive experiments conducted on the two benchmark datasets demonstrate the superiority of the proposed model over several state-of-the-art approaches for fact verification. The source code can be obtained from https:\/\/github.com\/jasenchn\/TARSA.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1661515832,"Goal":"Climate Action","Task":["Fact Verification","Fact verification","verification;","fact verification","fact verification"],"Method":["Topic - Aware Evidence Reasoning","Stance - Aware Aggregation","reasoning process","topic - aware evidence reasoning","stance - aware aggregation model","semantic representation"]},{"ID":"hansen-etal-2021-automatic","title":"Automatic Fake News Detection: Are Models Learning to Reason?","abstract":"Most fact checking models for automatic fake news detection are based on reasoning: given a claim with associated evidence, the models aim to estimate the claim veracity based on the supporting or refuting content within the evidence. When these models perform well, it is generally assumed to be due to the models having learned to reason over the evidence with regards to the claim. In this paper, we investigate this assumption of reasoning, by exploring the relationship and importance of both claim and evidence. Surprisingly, we find on political fact checking datasets that most often the highest effectiveness is obtained by utilizing only the evidence, as the impact of including the claim is either negligible or harmful to the effectiveness. This highlights an important problem in what constitutes evidence in existing approaches for automatic fake news detection.","year":2021,"title_abstract":"Automatic Fake News Detection: Are Models Learning to Reason? Most fact checking models for automatic fake news detection are based on reasoning: given a claim with associated evidence, the models aim to estimate the claim veracity based on the supporting or refuting content within the evidence. When these models perform well, it is generally assumed to be due to the models having learned to reason over the evidence with regards to the claim. In this paper, we investigate this assumption of reasoning, by exploring the relationship and importance of both claim and evidence. Surprisingly, we find on political fact checking datasets that most often the highest effectiveness is obtained by utilizing only the evidence, as the impact of including the claim is either negligible or harmful to the effectiveness. This highlights an important problem in what constitutes evidence in existing approaches for automatic fake news detection.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1661475003,"Goal":"Climate Action","Task":["Automatic Fake News Detection","automatic fake news detection","reasoning","fact checking datasets","automatic fake news detection"],"Method":["fact checking models","reasoning"]},{"ID":"safi-samghabadi-etal-2020-aggression","title":"Aggression and Misogyny Detection using {BERT}: A Multi-Task Approach","abstract":"In recent times, the focus of the NLP community has increased towards offensive language, aggression, and hate-speech detection.This paper presents our system for TRAC-2 shared task on {``}Aggression Identification{''} (sub-task A) and {``}Misogynistic Aggression Identification{''} (sub-task B). The data for this shared task is provided in three different languages - English, Hindi, and Bengali. Each data instance is annotated into one of the three aggression classes - Not Aggressive, Covertly Aggressive, Overtly Aggressive, as well as one of the two misogyny classes - Gendered and Non-Gendered. We propose an end-to-end neural model using attention on top of BERT that incorporates a multi-task learning paradigm to address both the sub-tasks simultaneously. Our team, {``}na14{''}, scored 0.8579 weighted F1-measure on the English sub-task B and secured 3rd rank out of 15 teams for the task. The code and the model weights are publicly available at https:\/\/github.com\/NiloofarSafi\/TRAC-2. Keywords: Aggression, Misogyny, Abusive Language, Hate-Speech Detection, BERT, NLP, Neural Networks, Social Media","year":2020,"title_abstract":"Aggression and Misogyny Detection using {BERT}: A Multi-Task Approach In recent times, the focus of the NLP community has increased towards offensive language, aggression, and hate-speech detection.This paper presents our system for TRAC-2 shared task on {``}Aggression Identification{''} (sub-task A) and {``}Misogynistic Aggression Identification{''} (sub-task B). The data for this shared task is provided in three different languages - English, Hindi, and Bengali. Each data instance is annotated into one of the three aggression classes - Not Aggressive, Covertly Aggressive, Overtly Aggressive, as well as one of the two misogyny classes - Gendered and Non-Gendered. We propose an end-to-end neural model using attention on top of BERT that incorporates a multi-task learning paradigm to address both the sub-tasks simultaneously. Our team, {``}na14{''}, scored 0.8579 weighted F1-measure on the English sub-task B and secured 3rd rank out of 15 teams for the task. The code and the model weights are publicly available at https:\/\/github.com\/NiloofarSafi\/TRAC-2. Keywords: Aggression, Misogyny, Abusive Language, Hate-Speech Detection, BERT, NLP, Neural Networks, Social Media","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1660988182,"Goal":"Gender Equality","Task":["Aggression and Misogyny Detection","NLP community","aggression","hate - speech detection","TRAC - 2 shared task","Identification{''}","Aggression Identification{''}","Aggression","Abusive Language","Hate - Speech Detection","BERT"],"Method":["{BERT}","Multi - Task Approach","end - to - end neural model","attention","BERT","multi - task learning paradigm","NLP","Neural Networks"]},{"ID":"vo-lee-2021-hierarchical","title":"Hierarchical Multi-head Attentive Network for Evidence-aware Fake News Detection","abstract":"The widespread of fake news and misinformation in various domains ranging from politics, economics to public health has posed an urgent need to automatically fact-check information. A recent trend in fake news detection is to utilize evidence from external sources. However, existing evidence-aware fake news detection methods focused on either only word-level attention or evidence-level attention, which may result in suboptimal performance. In this paper, we propose a Hierarchical Multi-head Attentive Network to fact-check textual claims. Our model jointly combines multi-head word-level attention and multi-head document-level attention, which aid explanation in both word-level and evidence-level. Experiments on two real-word datasets show that our model outperforms seven state-of-the-art baselines. Improvements over baselines are from 6{\\%} to 18{\\%}. Our source code and datasets are released at https:\/\/github.com\/nguyenvo09\/EACL2021.","year":2021,"title_abstract":"Hierarchical Multi-head Attentive Network for Evidence-aware Fake News Detection The widespread of fake news and misinformation in various domains ranging from politics, economics to public health has posed an urgent need to automatically fact-check information. A recent trend in fake news detection is to utilize evidence from external sources. However, existing evidence-aware fake news detection methods focused on either only word-level attention or evidence-level attention, which may result in suboptimal performance. In this paper, we propose a Hierarchical Multi-head Attentive Network to fact-check textual claims. Our model jointly combines multi-head word-level attention and multi-head document-level attention, which aid explanation in both word-level and evidence-level. Experiments on two real-word datasets show that our model outperforms seven state-of-the-art baselines. Improvements over baselines are from 6{\\%} to 18{\\%}. Our source code and datasets are released at https:\/\/github.com\/nguyenvo09\/EACL2021.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1660497785,"Goal":"Climate Action","Task":["Evidence - aware Fake News Detection","public health","fake news detection","fact - check textual"],"Method":["Hierarchical Multi - head Attentive Network","evidence - aware fake news detection methods","Hierarchical Multi - head Attentive Network","multi - head word - level attention"]},{"ID":"murgu-2021-approaching","title":"Approaching Stress and Performance in {RSI}: Proposal for Action to Take Back Control","abstract":"The relationship between stress and performance and Remote Interpreting (RI)\/Remote Simultaneous Interpreting (RSI) has been widely studied in academic, professional and corporate research during the past fifty years. Most of such research has attempted to correlate RI\/RSI with changes in stress levels and performance, with little to no relevant results to suggest causality. While no significant clinical causality has been found between RI\/RSI and stress, self-perceived stress during RI and especially RSI among practicing conference interpreters is consistently high and recent studies suggest a tendency on the increase. Similar results have been observed with performance, which has been and is consistently self-assessed as poorer during RI\/RSI by practicing interpreters compared to in-person interpreting, how-ever no significant decrease in performance was observed by independent reviewers. Several scholars have suggested a correlation between such low self-perceived performance \/ high self-perceived stress and a lack of control which might result from being exposed to unknown factors during RI\/RSI, prominently technological elements, the performance of which no longer re-lies on third parties but lies with the interpreters themselves. This paper is centered on the same hypothesis and suggests a proposal for action that interpreters can undertake to help regain control and thus improve their attitude toward RI\/RSI.","year":2021,"title_abstract":"Approaching Stress and Performance in {RSI}: Proposal for Action to Take Back Control The relationship between stress and performance and Remote Interpreting (RI)\/Remote Simultaneous Interpreting (RSI) has been widely studied in academic, professional and corporate research during the past fifty years. Most of such research has attempted to correlate RI\/RSI with changes in stress levels and performance, with little to no relevant results to suggest causality. While no significant clinical causality has been found between RI\/RSI and stress, self-perceived stress during RI and especially RSI among practicing conference interpreters is consistently high and recent studies suggest a tendency on the increase. Similar results have been observed with performance, which has been and is consistently self-assessed as poorer during RI\/RSI by practicing interpreters compared to in-person interpreting, how-ever no significant decrease in performance was observed by independent reviewers. Several scholars have suggested a correlation between such low self-perceived performance \/ high self-perceived stress and a lack of control which might result from being exposed to unknown factors during RI\/RSI, prominently technological elements, the performance of which no longer re-lies on third parties but lies with the interpreters themselves. This paper is centered on the same hypothesis and suggests a proposal for action that interpreters can undertake to help regain control and thus improve their attitude toward RI\/RSI.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1659359336,"Goal":"Reduced Inequalities","Task":["Approaching Stress and Performance","Remote Interpreting (RI)\/Remote Simultaneous Interpreting","corporate research","RSI","RI\/RSI","interpreting","RI\/RSI"],"Method":["RI\/RSI","conference interpreters","interpreters"]},{"ID":"maimaitituoheti-2022-ablimet","title":"{ABLIMET} @{LT}-{EDI}-{ACL}2022: A Roberta based Approach for Homophobia\/Transphobia Detection in Social Media","abstract":"This paper describes our system that participated in LT-EDI-ACL2022- Homophobia\/Transphobia Detection in Social Media. Sexual minorities face a lot of unfair treatment and discrimination in our world. This creates enormous stress and many psychological problems for sexual minorities. There is a lot of hate speech on the internet, and Homophobia\/Transphobia is the one against sexual minorities. Identifying and processing Homophobia\/ Transphobia through natural language processing technology can improve the efficiency of processing Homophobia\/ Transphobia, and can quickly screen out Homophobia\/Transphobia on the Internet. The organizer of LT-EDI-ACL2022- Homophobia\/Transphobia Detection in Social Media constructs a Homophobia\/ Transphobia detection dataset based on YouTube comments for English and Tamil. We use a Roberta -based approach to conduct Homophobia\/ Transphobia detection experiments on the dataset of the competition, and get better results.","year":2022,"title_abstract":"{ABLIMET} @{LT}-{EDI}-{ACL}2022: A Roberta based Approach for Homophobia\/Transphobia Detection in Social Media This paper describes our system that participated in LT-EDI-ACL2022- Homophobia\/Transphobia Detection in Social Media. Sexual minorities face a lot of unfair treatment and discrimination in our world. This creates enormous stress and many psychological problems for sexual minorities. There is a lot of hate speech on the internet, and Homophobia\/Transphobia is the one against sexual minorities. Identifying and processing Homophobia\/ Transphobia through natural language processing technology can improve the efficiency of processing Homophobia\/ Transphobia, and can quickly screen out Homophobia\/Transphobia on the Internet. The organizer of LT-EDI-ACL2022- Homophobia\/Transphobia Detection in Social Media constructs a Homophobia\/ Transphobia detection dataset based on YouTube comments for English and Tamil. We use a Roberta -based approach to conduct Homophobia\/ Transphobia detection experiments on the dataset of the competition, and get better results.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1658941507,"Goal":"Gender Equality","Task":["Homophobia\/Transphobia Detection","processing Homophobia\/ Transphobia","processing Homophobia\/ Transphobia","LT - EDI - ACL2022 - Homophobia\/Transphobia Detection","Homophobia\/ Transphobia detection"],"Method":["Roberta based Approach","LT - EDI","natural language processing technology","Roberta - based approach"]},{"ID":"vegi-etal-2021-anvita","title":"{ANVITA} Machine Translation System for {WAT} 2021 {M}ulti{I}ndic{MT} Shared Task","abstract":"This paper describes ANVITA-1.0 MT system, architected for submission to WAT2021 MultiIndicMT shared task by mcairt team, where the team participated in 20 translation directions: English\u2192Indic and Indic\u2192English; Indic set comprised of 10 Indian languages. ANVITA-1.0 MT system comprised of two multi-lingual NMT models one for the English\u2192Indic directions and other for the Indic\u2192English directions with shared encoder-decoder, catering 10 language pairs and twenty translation directions. The base models were built based on Transformer architecture and trained over MultiIndicMT WAT 2021 corpora and further employed back translation and transliteration for selective data augmentation, and model ensemble for better generalization. Additionally, MultiIndicMT WAT 2021 corpora was distilled using a series of filtering operations before putting up for training. ANVITA-1.0 achieved highest AM-FM score for English\u2192Bengali, 2nd for English\u2192Tamil and 3rd for English\u2192Hindi, Bengali\u2192English directions on official test set. In general, performance achieved by ANVITA for the Indic\u2192English directions are relatively better than that of English\u2192Indic directions for all the 10 language pairs when evaluated using BLEU and RIBES, although the same trend is not observed consistently when AM-FM based evaluation was carried out. As compared to BLEU, RIBES and AM-FM based scoring placed ANVITA relatively better among all the task participants.","year":2021,"title_abstract":"{ANVITA} Machine Translation System for {WAT} 2021 {M}ulti{I}ndic{MT} Shared Task This paper describes ANVITA-1.0 MT system, architected for submission to WAT2021 MultiIndicMT shared task by mcairt team, where the team participated in 20 translation directions: English\u2192Indic and Indic\u2192English; Indic set comprised of 10 Indian languages. ANVITA-1.0 MT system comprised of two multi-lingual NMT models one for the English\u2192Indic directions and other for the Indic\u2192English directions with shared encoder-decoder, catering 10 language pairs and twenty translation directions. The base models were built based on Transformer architecture and trained over MultiIndicMT WAT 2021 corpora and further employed back translation and transliteration for selective data augmentation, and model ensemble for better generalization. Additionally, MultiIndicMT WAT 2021 corpora was distilled using a series of filtering operations before putting up for training. ANVITA-1.0 achieved highest AM-FM score for English\u2192Bengali, 2nd for English\u2192Tamil and 3rd for English\u2192Hindi, Bengali\u2192English directions on official test set. In general, performance achieved by ANVITA for the Indic\u2192English directions are relatively better than that of English\u2192Indic directions for all the 10 language pairs when evaluated using BLEU and RIBES, although the same trend is not observed consistently when AM-FM based evaluation was carried out. As compared to BLEU, RIBES and AM-FM based scoring placed ANVITA relatively better among all the task participants.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1658568084,"Goal":"Gender Equality","Task":["Machine Translation System","MT","WAT2021 MultiIndicMT shared task","MT","transliteration","selective data augmentation","generalization"],"Method":["ANVITA - 1","multi - lingual NMT models","encoder - decoder","Transformer architecture","back translation","model ensemble","filtering operations","ANVITA - 1","ANVITA","RIBES","AM - FM based evaluation","RIBES","ANVITA"]},{"ID":"declerck-2006-synaf","title":"{S}yn{AF}: Towards a Standard for Syntactic Annotation","abstract":"In the paper we present the actual state of development of an international standard for syntactic annotation, called SynAF. This standard is being prepared by the Technical Committee ISO\/TC 37 (Terminology and Other Language Resources), Subcommittee SC 4 (Language Resource Management), in collaboration with the European eContent Project \u0093LIRICS\u0094 (Linguistic Infrastructure for Interoperable Resources and Systems).","year":2006,"title_abstract":"{S}yn{AF}: Towards a Standard for Syntactic Annotation In the paper we present the actual state of development of an international standard for syntactic annotation, called SynAF. This standard is being prepared by the Technical Committee ISO\/TC 37 (Terminology and Other Language Resources), Subcommittee SC 4 (Language Resource Management), in collaboration with the European eContent Project \u0093LIRICS\u0094 (Linguistic Infrastructure for Interoperable Resources and Systems).","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1658029854,"Goal":"Partnership for the Goals","Task":["Syntactic Annotation","syntactic annotation","Resource Management)"],"Method":["SynAF"]},{"ID":"belam-2001-transferable","title":"Transferable skills in an {MT} course","abstract":"The paper describes the process of designing a new MT course for final-year undergraduates. It explains the skills to be acquired as part of the module. The course will include a practical and a theoretical component, and in addition to subject-specific knowledge the course should enable students to gain competence in analysis of language and appreciation of the nature of communication. It is hoped that some of these skills will be transferable from the specific context of MT to wider areas of application. Discrete profiling and evaluation is not envisaged. The paper also defines areas where an MT course can provide opportunities not necessarily offered on conventional translation courses.","year":2001,"title_abstract":"Transferable skills in an {MT} course The paper describes the process of designing a new MT course for final-year undergraduates. It explains the skills to be acquired as part of the module. The course will include a practical and a theoretical component, and in addition to subject-specific knowledge the course should enable students to gain competence in analysis of language and appreciation of the nature of communication. It is hoped that some of these skills will be transferable from the specific context of MT to wider areas of application. Discrete profiling and evaluation is not envisaged. The paper also defines areas where an MT course can provide opportunities not necessarily offered on conventional translation courses.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1657731235,"Goal":"Quality Education","Task":["MT","analysis of language","communication","MT","Discrete profiling","evaluation","MT","translation courses"],"Method":["theoretical component"]},{"ID":"puntikov-1999-mt","title":"{MT} and {TM} technologies in localization industry: the challenge of integration","abstract":"The objective of this paper is to clarify certain technological aspects of the localization business process. An introduction to the Translation Memory (TM) technology is provided, followed by an analysis of how TM and Machine Translation (MT), when used together, can increase productivity in software localization workflow applications. A special section is devoted to the issue of standard exchange mechanisms to represent translation memory data so that they can be shared among users of different TM and MT tools.","year":1999,"title_abstract":"{MT} and {TM} technologies in localization industry: the challenge of integration The objective of this paper is to clarify certain technological aspects of the localization business process. An introduction to the Translation Memory (TM) technology is provided, followed by an analysis of how TM and Machine Translation (MT), when used together, can increase productivity in software localization workflow applications. A special section is devoted to the issue of standard exchange mechanisms to represent translation memory data so that they can be shared among users of different TM and MT tools.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1657133102,"Goal":"Partnership for the Goals","Task":["localization industry","localization business process","Machine Translation","software localization workflow applications"],"Method":["Translation Memory","TM","exchange mechanisms","TM","MT tools"]},{"ID":"sudhakar-etal-2019-transforming","title":"{``}Transforming{''} Delete, Retrieve, Generate Approach for Controlled Text Style Transfer","abstract":"Text style transfer is the task of transferring the style of text having certain stylistic attributes, while preserving non-stylistic or content information. In this work we introduce the Generative Style Transformer (GST) - a new approach to rewriting sentences to a target style in the absence of parallel style corpora. GST leverages the power of both, large unsupervised pre-trained language models as well as the Transformer. GST is a part of a larger {`}Delete Retrieve Generate{'} framework, in which we also propose a novel method of deleting style attributes from the source sentence by exploiting the inner workings of the Transformer. Our models outperform state-of-art systems across 5 datasets on sentiment, gender and political slant transfer. We also propose the use of the GLEU metric as an automatic metric of evaluation of style transfer, which we found to compare better with human ratings than the predominantly used BLEU score.","year":2019,"title_abstract":"{``}Transforming{''} Delete, Retrieve, Generate Approach for Controlled Text Style Transfer Text style transfer is the task of transferring the style of text having certain stylistic attributes, while preserving non-stylistic or content information. In this work we introduce the Generative Style Transformer (GST) - a new approach to rewriting sentences to a target style in the absence of parallel style corpora. GST leverages the power of both, large unsupervised pre-trained language models as well as the Transformer. GST is a part of a larger {`}Delete Retrieve Generate{'} framework, in which we also propose a novel method of deleting style attributes from the source sentence by exploiting the inner workings of the Transformer. Our models outperform state-of-art systems across 5 datasets on sentiment, gender and political slant transfer. We also propose the use of the GLEU metric as an automatic metric of evaluation of style transfer, which we found to compare better with human ratings than the predominantly used BLEU score.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.165656209,"Goal":"Gender Equality","Task":["Controlled Text Style Transfer","Text style transfer","rewriting sentences","deleting style attributes","sentiment","political slant transfer","evaluation","style transfer"],"Method":["Delete","Retrieve","Generate Approach","Generative Style Transformer","GST","unsupervised pre - trained language models","Transformer","GST","Retrieve Generate{'} framework","Transformer"]},{"ID":"mansouri-bigvand-etal-2017-joint","title":"Joint Prediction of Word Alignment with Alignment Types","abstract":"Current word alignment models do not distinguish between different types of alignment links. In this paper, we provide a new probabilistic model for word alignment where word alignments are associated with linguistically motivated alignment types. We propose a novel task of joint prediction of word alignment and alignment types and propose novel semi-supervised learning algorithms for this task. We also solve a sub-task of predicting the alignment type given an aligned word pair. In our experimental results, the generative models we introduce to model alignment types significantly outperform the models without alignment types.","year":2017,"title_abstract":"Joint Prediction of Word Alignment with Alignment Types Current word alignment models do not distinguish between different types of alignment links. In this paper, we provide a new probabilistic model for word alignment where word alignments are associated with linguistically motivated alignment types. We propose a novel task of joint prediction of word alignment and alignment types and propose novel semi-supervised learning algorithms for this task. We also solve a sub-task of predicting the alignment type given an aligned word pair. In our experimental results, the generative models we introduce to model alignment types significantly outperform the models without alignment types.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1656232625,"Goal":"Gender Equality","Task":["Joint Prediction of Word Alignment","Alignment Types","word alignment","joint prediction of word alignment","alignment types","sub - task","alignment type"],"Method":["word alignment models","probabilistic model","semi - supervised learning algorithms","generative models"]},{"ID":"gibbon-etal-2010-medefaidrin","title":"{M}edefaidrin: Resources Documenting the Birth and Death Language Life-cycle","abstract":"Language resources are typically defined and created for application in speech technology contexts, but the documentation of languages which are unlikely ever to be provided with enabling technologies nevertheless plays an important role in defining the heritage of a speech community and in the provision of basic insights into the language oriented components of human cognition. This is particularly true of endangered languages. The present case study concerns the documentation both of the birth and of the endangerment within a rather short space of time of a \u0091spirit language\u0092, Medefaidrin, created and used as a vehicular language by a religious community in South-Eastern Nigeria. The documentation shows phonological, orthographic, morphological, syntactic and textual typological features of Medefaidrin which indicate that typological properties of English were a model for the creation of the language, rather than typological properties of the enclaving language, Ibibio. The documentation is designed as part of the West African Language Archive (WALA), following OLAC metadata standards.","year":2010,"title_abstract":"{M}edefaidrin: Resources Documenting the Birth and Death Language Life-cycle Language resources are typically defined and created for application in speech technology contexts, but the documentation of languages which are unlikely ever to be provided with enabling technologies nevertheless plays an important role in defining the heritage of a speech community and in the provision of basic insights into the language oriented components of human cognition. This is particularly true of endangered languages. The present case study concerns the documentation both of the birth and of the endangerment within a rather short space of time of a \u0091spirit language\u0092, Medefaidrin, created and used as a vehicular language by a religious community in South-Eastern Nigeria. The documentation shows phonological, orthographic, morphological, syntactic and textual typological features of Medefaidrin which indicate that typological properties of English were a model for the creation of the language, rather than typological properties of the enclaving language, Ibibio. The documentation is designed as part of the West African Language Archive (WALA), following OLAC metadata standards.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1655709445,"Goal":"Sustainable Cities and Communities","Task":["speech technology contexts","speech community","language oriented components of human cognition"],"Method":["OLAC metadata standards"]},{"ID":"mehta-etal-2020-retouchdown","title":"Retouchdown: Releasing Touchdown on {S}treet{L}earn as a Public Resource for Language Grounding Tasks in Street View","abstract":"The Touchdown dataset (Chen et al., 2019) provides instructions by human annotators for navigation through New York City streets and for resolving spatial descriptions at a given location. To enable the wider research community to work effectively with the Touchdown tasks, we are publicly releasing the 29k raw Street View panoramas needed for Touchdown. We follow the process used for the StreetLearn data release (Mirowski et al., 2019) to check panoramas for personally identifiable information and blur them as necessary. These have been added to the StreetLearn dataset and can be obtained via the same process as used previously for StreetLearn. We also provide a reference implementation for both Touchdown tasks: vision and language navigation (VLN) and spatial description resolution (SDR). We compare our model results to those given in (Chen et al., 2019) and show that the panoramas we have added to StreetLearn support both Touchdown tasks and can be used effectively for further research and comparison.","year":2020,"title_abstract":"Retouchdown: Releasing Touchdown on {S}treet{L}earn as a Public Resource for Language Grounding Tasks in Street View The Touchdown dataset (Chen et al., 2019) provides instructions by human annotators for navigation through New York City streets and for resolving spatial descriptions at a given location. To enable the wider research community to work effectively with the Touchdown tasks, we are publicly releasing the 29k raw Street View panoramas needed for Touchdown. We follow the process used for the StreetLearn data release (Mirowski et al., 2019) to check panoramas for personally identifiable information and blur them as necessary. These have been added to the StreetLearn dataset and can be obtained via the same process as used previously for StreetLearn. We also provide a reference implementation for both Touchdown tasks: vision and language navigation (VLN) and spatial description resolution (SDR). We compare our model results to those given in (Chen et al., 2019) and show that the panoramas we have added to StreetLearn support both Touchdown tasks and can be used effectively for further research and comparison.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1654438078,"Goal":"Sustainable Cities and Communities","Task":["Language Grounding Tasks","resolving spatial descriptions","Touchdown tasks","Touchdown","Touchdown tasks","vision and language navigation","spatial description resolution","Touchdown tasks"],"Method":["Retouchdown"]},{"ID":"maks-etal-2014-generating","title":"Generating Polarity Lexicons with {W}ord{N}et propagation in 5 languages","abstract":"In this paper we focus on the creation of general-purpose (as opposed to domain-specific) polarity lexicons in five languages: French, Italian, Dutch, English and Spanish using WordNet propagation. WordNet propagation is a commonly used method to generate these lexicons as it gives high coverage of general purpose language and the semantically rich WordNets where concepts are organised in synonym , antonym and hyperonym\/hyponym structures seem to be well suited to the identification of positive and negative words. However, WordNets of different languages may vary in many ways such as the way they are compiled, the number of synsets, number of synonyms and number of semantic relations they include. In this study we investigate whether this variability translates into differences of performance when these WordNets are used for polarity propagation. Although many variants of the propagation method are developed for English, little is known about how they perform with WordNets of other languages. We implemented a propagation algorithm and designed a method to obtain seed lists similar with respect to quality and size, for each of the five languages. We evaluated the results against gold standards also developed according to a common method in order to achieve as less variance as possible between the different languages.","year":2014,"title_abstract":"Generating Polarity Lexicons with {W}ord{N}et propagation in 5 languages In this paper we focus on the creation of general-purpose (as opposed to domain-specific) polarity lexicons in five languages: French, Italian, Dutch, English and Spanish using WordNet propagation. WordNet propagation is a commonly used method to generate these lexicons as it gives high coverage of general purpose language and the semantically rich WordNets where concepts are organised in synonym , antonym and hyperonym\/hyponym structures seem to be well suited to the identification of positive and negative words. However, WordNets of different languages may vary in many ways such as the way they are compiled, the number of synsets, number of synonyms and number of semantic relations they include. In this study we investigate whether this variability translates into differences of performance when these WordNets are used for polarity propagation. Although many variants of the propagation method are developed for English, little is known about how they perform with WordNets of other languages. We implemented a propagation algorithm and designed a method to obtain seed lists similar with respect to quality and size, for each of the five languages. We evaluated the results against gold standards also developed according to a common method in order to achieve as less variance as possible between the different languages.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1654208153,"Goal":"Reduced Inequalities","Task":["Generating Polarity Lexicons","identification of positive and negative words","polarity propagation"],"Method":["{W}ord{N}et propagation","WordNet propagation","WordNet propagation","propagation method","propagation algorithm"]},{"ID":"hulpus-etal-2020-knowledge","title":"Knowledge Graphs meet Moral Values","abstract":"Operationalizing morality is crucial for understanding multiple aspects of society that have moral values at their core {--} such as riots, mobilizing movements, public debates, etc. Moral Foundations Theory (MFT) has become one of the most adopted theories of morality partly due to its accompanying lexicon, the Moral Foundation Dictionary (MFD), which offers a base for computationally dealing with morality. In this work, we exploit the MFD in a novel direction by investigating how well moral values are captured by KGs. We explore three widely used KGs, and provide concept-level analogues for the MFD. Furthermore, we propose several Personalized PageRank variations in order to score all the concepts and entities in the KGs with respect to their relevance to the different moral values. Our promising results help to progress the operationalization of morality in both NLP and KG communities.","year":2020,"title_abstract":"Knowledge Graphs meet Moral Values Operationalizing morality is crucial for understanding multiple aspects of society that have moral values at their core {--} such as riots, mobilizing movements, public debates, etc. Moral Foundations Theory (MFT) has become one of the most adopted theories of morality partly due to its accompanying lexicon, the Moral Foundation Dictionary (MFD), which offers a base for computationally dealing with morality. In this work, we exploit the MFD in a novel direction by investigating how well moral values are captured by KGs. We explore three widely used KGs, and provide concept-level analogues for the MFD. Furthermore, we propose several Personalized PageRank variations in order to score all the concepts and entities in the KGs with respect to their relevance to the different moral values. Our promising results help to progress the operationalization of morality in both NLP and KG communities.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.1653957963,"Goal":"Peace, Justice and Strong Institutions","Task":["Operationalizing morality","morality","computationally","morality","operationalization of morality","NLP","KG communities"],"Method":["Knowledge Graphs","Moral Foundations Theory","MFD","KGs","KGs","concept - level analogues","MFD","Personalized PageRank variations","KGs"]},{"ID":"zinin-xu-2020-corpus","title":"Corpus of {C}hinese Dynastic Histories: Gender Analysis over Two Millennia","abstract":"Chinese dynastic histories form a large continuous linguistic space of approximately 2000 years, from the 3rd century BCE to the 18th century CE. The histories are documented in Classical (Literary) Chinese in a corpus of over 20 million characters, suitable for the computational analysis of historical lexicon and semantic change. However, there is no freely available open-source corpus of these histories, making Classical Chinese low-resource. This project introduces a new open-source corpus of twenty-four dynastic histories covered by Creative Commons license. An original list of Classical Chinese gender-specific terms was developed as a case study for analyzing the historical linguistic use of male and female terms. The study demonstrates considerable stability in the usage of these terms, with dominance of male terms. Exploration of word meanings uses keyword analysis of focus corpora created for gender-specific terms. This method yields meaningful semantic representations that can be used for future studies of diachronic semantics.","year":2020,"title_abstract":"Corpus of {C}hinese Dynastic Histories: Gender Analysis over Two Millennia Chinese dynastic histories form a large continuous linguistic space of approximately 2000 years, from the 3rd century BCE to the 18th century CE. The histories are documented in Classical (Literary) Chinese in a corpus of over 20 million characters, suitable for the computational analysis of historical lexicon and semantic change. However, there is no freely available open-source corpus of these histories, making Classical Chinese low-resource. This project introduces a new open-source corpus of twenty-four dynastic histories covered by Creative Commons license. An original list of Classical Chinese gender-specific terms was developed as a case study for analyzing the historical linguistic use of male and female terms. The study demonstrates considerable stability in the usage of these terms, with dominance of male terms. Exploration of word meanings uses keyword analysis of focus corpora created for gender-specific terms. This method yields meaningful semantic representations that can be used for future studies of diachronic semantics.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.165391311,"Goal":"Gender Equality","Task":["Gender Analysis","computational analysis of historical lexicon and semantic change","Exploration of word meanings","diachronic semantics"],"Method":["keyword analysis","semantic representations"]},{"ID":"rehm-etal-2017-event","title":"Event Detection and Semantic Storytelling: Generating a Travelogue from a large Collection of Personal Letters","abstract":"We present an approach at identifying a specific class of events, movement action events (MAEs), in a data set that consists of ca. 2,800 personal letters exchanged by the German architect Erich Mendelsohn and his wife, Luise. A backend system uses these and other semantic analysis results as input for an authoring environment that digital curators can use to produce new pieces of digital content. In our example case, the human expert will receive recommendations from the system with the goal of putting together a travelogue, i.e., a description of the trips and journeys undertaken by the couple. We describe the components and architecture and also apply the system to news data.","year":2017,"title_abstract":"Event Detection and Semantic Storytelling: Generating a Travelogue from a large Collection of Personal Letters We present an approach at identifying a specific class of events, movement action events (MAEs), in a data set that consists of ca. 2,800 personal letters exchanged by the German architect Erich Mendelsohn and his wife, Luise. A backend system uses these and other semantic analysis results as input for an authoring environment that digital curators can use to produce new pieces of digital content. In our example case, the human expert will receive recommendations from the system with the goal of putting together a travelogue, i.e., a description of the trips and journeys undertaken by the couple. We describe the components and architecture and also apply the system to news data.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1652962565,"Goal":"Sustainable Cities and Communities","Task":["Event Detection","Semantic Storytelling","Travelogue","authoring environment","digital curators"],"Method":["backend system"]},{"ID":"alva-manchego-etal-2021-deepquest","title":"deep{Q}uest-py: {L}arge and Distilled Models for Quality Estimation","abstract":"We introduce deepQuest-py, a framework for training and evaluation of large and light-weight models for Quality Estimation (QE). deepQuest-py provides access to (1) state-of-the-art models based on pre-trained Transformers for sentence-level and word-level QE; (2) light-weight and efficient sentence-level models implemented via knowledge distillation; and (3) a web interface for testing models and visualising their predictions. deepQuest-py is available at \\url{https:\/\/github.com\/sheffieldnlp\/deepQuest-py} under a CC BY-NC-SA licence.","year":2021,"title_abstract":"deep{Q}uest-py: {L}arge and Distilled Models for Quality Estimation We introduce deepQuest-py, a framework for training and evaluation of large and light-weight models for Quality Estimation (QE). deepQuest-py provides access to (1) state-of-the-art models based on pre-trained Transformers for sentence-level and word-level QE; (2) light-weight and efficient sentence-level models implemented via knowledge distillation; and (3) a web interface for testing models and visualising their predictions. deepQuest-py is available at \\url{https:\/\/github.com\/sheffieldnlp\/deepQuest-py} under a CC BY-NC-SA licence.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1652469784,"Goal":"Quality Education","Task":["Quality Estimation","training and evaluation of large and light - weight models","Quality Estimation","sentence - level and word - level QE;"],"Method":["deep{Q}uest - py","Distilled Models","deepQuest - py","deepQuest - py","Transformers","sentence - level models","knowledge distillation;","deepQuest - py"]},{"ID":"goyal-2017-learningtoquestion","title":"{L}earning{T}o{Q}uestion at {S}em{E}val 2017 Task 3: Ranking Similar Questions by Learning to Rank Using Rich Features","abstract":"This paper describes our official entry LearningToQuestion for SemEval 2017 task 3 community question answer, subtask B. The objective is to rerank questions obtained in web forum as per their similarity to original question. Our system uses pairwise learning to rank methods on rich set of hand designed and representation learning features. We use various semantic features that help our system to achieve promising results on the task. The system achieved second highest results on official metrics MAP and good results on other search metrics.","year":2017,"title_abstract":"{L}earning{T}o{Q}uestion at {S}em{E}val 2017 Task 3: Ranking Similar Questions by Learning to Rank Using Rich Features This paper describes our official entry LearningToQuestion for SemEval 2017 task 3 community question answer, subtask B. The objective is to rerank questions obtained in web forum as per their similarity to original question. Our system uses pairwise learning to rank methods on rich set of hand designed and representation learning features. We use various semantic features that help our system to achieve promising results on the task. The system achieved second highest results on official metrics MAP and good results on other search metrics.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1652263105,"Goal":"Quality Education","Task":["Ranking Similar Questions","Learning to Rank","SemEval 2017"],"Method":["pairwise learning","rank methods"]},{"ID":"drude-etal-2012-language","title":"The Language Archive {---} a new hub for language resources","abstract":"This contribution presents \u0093The Language Archive\u0094 (TLA), a new unit at the MPI for Psycholinguistics, discussing the current developments in management of scientific data, considering the need for new data research infrastructures. Although several initiatives worldwide in the realm of language resources aim at the integration, preservation and mobilization of research data, the state of such scientific data is still often problematic. Data are often not well organized and archived and not described by metadata \u2015 even unique data such as field-work observational data on endangered languages is still mostly on perishable carriers. New data centres are needed that provide trusted, quality-reviewed, persistent services and suitable tools and that take legal and ethical issues seriously. The CLARIN initiative has established criteria for suitable centres. TLA is in a good position to be one of such centres. It is based on three essential pillars: (1) A data archive; (2) management, access and annotation tools; (3) archiving and software expertise for collaborative projects. The archive hosts mostly observational data on small languages worldwide and language acquisition data, but also data resulting from experiments.","year":2012,"title_abstract":"The Language Archive {---} a new hub for language resources This contribution presents \u0093The Language Archive\u0094 (TLA), a new unit at the MPI for Psycholinguistics, discussing the current developments in management of scientific data, considering the need for new data research infrastructures. Although several initiatives worldwide in the realm of language resources aim at the integration, preservation and mobilization of research data, the state of such scientific data is still often problematic. Data are often not well organized and archived and not described by metadata \u2015 even unique data such as field-work observational data on endangered languages is still mostly on perishable carriers. New data centres are needed that provide trusted, quality-reviewed, persistent services and suitable tools and that take legal and ethical issues seriously. The CLARIN initiative has established criteria for suitable centres. TLA is in a good position to be one of such centres. It is based on three essential pillars: (1) A data archive; (2) management, access and annotation tools; (3) archiving and software expertise for collaborative projects. The archive hosts mostly observational data on small languages worldwide and language acquisition data, but also data resulting from experiments.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.165215537,"Goal":"Life on Land","Task":["language resources","Psycholinguistics","management of scientific data","data research infrastructures","language resources","preservation and mobilization of research data","data archive;","access and annotation tools;","archiving","software expertise","collaborative projects"],"Method":["Language Archive\u0094","TLA"]},{"ID":"sierra-martinez-etal-2020-enhancing","title":"Enhancing Job Searches in {M}exico City with Language Technologies","abstract":"In this paper, we show the enhancing of the Demanded Skills Diagnosis (DiCoDe: Diagn{\\'o}stico de Competencias Demandadas), a system developed by Mexico City{'}s Ministry of Labor and Employment Promotion (STyFE: Secretar{\\'\\i}a de Trabajo y Fomento del Empleo de la Ciudad de M{\\'e}xico) that seeks to reduce information asymmetries between job seekers and employers. The project uses webscraping techniques to retrieve job vacancies posted on private job portals on a daily basis and with the purpose of informing training and individual case management policies as well as labor market monitoring. For this purpose, a collaboration project between STyFE and the Language Engineering Group (GIL: Grupo de Ingenier{\\'\\i}a Ling{\\\"u}{\\'\\i}stica) was established in order to enhance DiCoDe by applying NLP models and semantic analysis. By this collaboration, DiCoDe{'}s job vacancies system{'}s macro-structure and its geographic referencing at the city hall (municipality) level were improved. More specifically, dictionaries were created to identify demanded competencies, skills and abilities (CSA) and algorithms were developed for dynamic classifying of vacancies and identifying terms for searches on free text, in order to improve the results and processing time of queries.","year":2020,"title_abstract":"Enhancing Job Searches in {M}exico City with Language Technologies In this paper, we show the enhancing of the Demanded Skills Diagnosis (DiCoDe: Diagn{\\'o}stico de Competencias Demandadas), a system developed by Mexico City{'}s Ministry of Labor and Employment Promotion (STyFE: Secretar{\\'\\i}a de Trabajo y Fomento del Empleo de la Ciudad de M{\\'e}xico) that seeks to reduce information asymmetries between job seekers and employers. The project uses webscraping techniques to retrieve job vacancies posted on private job portals on a daily basis and with the purpose of informing training and individual case management policies as well as labor market monitoring. For this purpose, a collaboration project between STyFE and the Language Engineering Group (GIL: Grupo de Ingenier{\\'\\i}a Ling{\\\"u}{\\'\\i}stica) was established in order to enhance DiCoDe by applying NLP models and semantic analysis. By this collaboration, DiCoDe{'}s job vacancies system{'}s macro-structure and its geographic referencing at the city hall (municipality) level were improved. More specifically, dictionaries were created to identify demanded competencies, skills and abilities (CSA) and algorithms were developed for dynamic classifying of vacancies and identifying terms for searches on free text, in order to improve the results and processing time of queries.","social_need":"Decent Work and Economic Growth Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all","cosine_similarity":0.165127337,"Goal":"Decent Work and Economic Growth","Task":["Enhancing Job Searches","Demanded Skills Diagnosis","Diagn{\\'o}stico de Competencias Demandadas)","Employment Promotion","training","individual case management policies","labor market monitoring","dynamic classifying of vacancies","searches","queries"],"Method":["Language Technologies","webscraping techniques","Language Engineering Group","DiCoDe","NLP models","semantic analysis","DiCoDe{'}s job vacancies system{'}s","geographic referencing","dictionaries"]},{"ID":"yihan-2021-meaningfulness","title":"Meaningfulness and unit of {Z}ipf{'}s law: evidence from danmu comments","abstract":"Zipf{'}s law is a succinct yet powerful mathematical law in linguistics. However the mean-ingfulness and units of the law have remained controversial. The current study usesonline video comments call {``}danmu comment{''} to investigate these two questions. Theresults are consistent with previous studies arguing Zipf{'}s law is subject to topical coher-ence. Specifically it is found that danmu comments sampled from a single video followZipf{'}s law better than danmu comments sampled from a collection of videos. The resultsalso suggest the existence of multiple units of Zipf{'}s law. When different units includingwords n-grams and danmu comments are compared both words and danmu commentsobey Zipf{'}s law and words may be a better fit. The issues of combined n-grams in the literature are also discussed.","year":2021,"title_abstract":"Meaningfulness and unit of {Z}ipf{'}s law: evidence from danmu comments Zipf{'}s law is a succinct yet powerful mathematical law in linguistics. However the mean-ingfulness and units of the law have remained controversial. The current study usesonline video comments call {``}danmu comment{''} to investigate these two questions. Theresults are consistent with previous studies arguing Zipf{'}s law is subject to topical coher-ence. Specifically it is found that danmu comments sampled from a single video followZipf{'}s law better than danmu comments sampled from a collection of videos. The resultsalso suggest the existence of multiple units of Zipf{'}s law. When different units includingwords n-grams and danmu comments are compared both words and danmu commentsobey Zipf{'}s law and words may be a better fit. The issues of combined n-grams in the literature are also discussed.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1651131362,"Goal":"Reduced Inequalities","Task":["linguistics"],"Method":["Zipf{'}s law","Zipf{'}s law"]},{"ID":"monachesi-markus-2010-socially","title":"Socially Driven Ontology Enrichment for e{L}earning","abstract":"One of the objectives of the Language Technologies for Life-Long Learning (LTfLL) project, is to develop a knowledge sharing system that connects learners to resources and learners to other learners. To this end, we complement the formal knowledge represented by existing domain ontologies with the informal knowledge emerging from social tagging. More specifically, we crawl data from social media applications such as Delicious, Slideshare and YouTube. Similarity measures are employed to select possible lexicalizations of concepts that are related to the ones present in the given ontology and which are assumed to be socially relevant with respect to the input lexicalisation. In order to identify the appropriate relationships which exist between the extracted related terms and the existing domain ontology, we employ several heuristics that rely on the use of a large background knowledge base, such as DBpedia. An evaluation of the resulting ontology has been carried out. The methodology proposed allows for an appropriate enrichment process and produces a complementary vocabulary to that of a domain expert.","year":2010,"title_abstract":"Socially Driven Ontology Enrichment for e{L}earning One of the objectives of the Language Technologies for Life-Long Learning (LTfLL) project, is to develop a knowledge sharing system that connects learners to resources and learners to other learners. To this end, we complement the formal knowledge represented by existing domain ontologies with the informal knowledge emerging from social tagging. More specifically, we crawl data from social media applications such as Delicious, Slideshare and YouTube. Similarity measures are employed to select possible lexicalizations of concepts that are related to the ones present in the given ontology and which are assumed to be socially relevant with respect to the input lexicalisation. In order to identify the appropriate relationships which exist between the extracted related terms and the existing domain ontology, we employ several heuristics that rely on the use of a large background knowledge base, such as DBpedia. An evaluation of the resulting ontology has been carried out. The methodology proposed allows for an appropriate enrichment process and produces a complementary vocabulary to that of a domain expert.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.1650559604,"Goal":"Life Below Water","Task":["Socially Driven Ontology Enrichment","Language Technologies for Life - Long Learning","enrichment process"],"Method":["knowledge sharing system","social tagging","DBpedia"]},{"ID":"jiao-etal-2020-senser","title":"{S}e{N}s{ER}: Learning Cross-Building Sensor Metadata Tagger","abstract":"Sensor metadata tagging, akin to the named entity recognition task, provides key contextual information (e.g., measurement type and location) about sensors for running smart building applications. Unfortunately, sensor metadata in different buildings often follows distinct naming conventions. Therefore, learning a tagger currently requires extensive annotations on a per building basis. In this work, we propose a novel framework, SeNsER, which learns a sensor metadata tagger for a new building based on its raw metadata and some existing fully annotated building. It leverages the commonality between different buildings: At the character level, it employs bidirectional neural language models to capture the shared underlying patterns between two buildings and thus regularizes the feature learning process; At the word level, it leverages as features the k-mers existing in the fully annotated building. During inference, we further incorporate the information obtained from sources such as Wikipedia as prior knowledge. As a result, SeNsER shows promising results in extensive experiments on multiple real-world buildings.","year":2020,"title_abstract":"{S}e{N}s{ER}: Learning Cross-Building Sensor Metadata Tagger Sensor metadata tagging, akin to the named entity recognition task, provides key contextual information (e.g., measurement type and location) about sensors for running smart building applications. Unfortunately, sensor metadata in different buildings often follows distinct naming conventions. Therefore, learning a tagger currently requires extensive annotations on a per building basis. In this work, we propose a novel framework, SeNsER, which learns a sensor metadata tagger for a new building based on its raw metadata and some existing fully annotated building. It leverages the commonality between different buildings: At the character level, it employs bidirectional neural language models to capture the shared underlying patterns between two buildings and thus regularizes the feature learning process; At the word level, it leverages as features the k-mers existing in the fully annotated building. During inference, we further incorporate the information obtained from sources such as Wikipedia as prior knowledge. As a result, SeNsER shows promising results in extensive experiments on multiple real-world buildings.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1650273949,"Goal":"Sustainable Cities and Communities","Task":["Sensor metadata tagging","named entity recognition task","smart building applications","inference"],"Method":["Cross - Building Sensor Metadata Tagger","tagger","SeNsER","sensor metadata tagger","bidirectional neural language models","feature learning process;","SeNsER"]},{"ID":"wang-etal-2020-hw-tscs","title":"{HW}-{TSC}{'}s Participation at {WMT} 2020 Quality Estimation Shared Task","abstract":"This paper presents our work in the WMT 2020 Word and Sentence-Level Post-Editing Quality Estimation (QE) Shared Task. Our system follows standard Predictor-Estimator architecture, with a pre-trained Transformer as the Predictor, and specific classifiers and regressors as Estimators. We integrate Bottleneck Adapter Layers in the Predictor to improve the transfer learning efficiency and prevent from over-fitting. At the same time, we jointly train the word- and sentence-level tasks with a unified model with multitask learning. Pseudo-PE assisted QE (PEAQE) is proposed, resulting in significant improvements on the performance. Our submissions achieve competitive result in word\/sentence-level sub-tasks for both of En-De\/Zh language pairs.","year":2020,"title_abstract":"{HW}-{TSC}{'}s Participation at {WMT} 2020 Quality Estimation Shared Task This paper presents our work in the WMT 2020 Word and Sentence-Level Post-Editing Quality Estimation (QE) Shared Task. Our system follows standard Predictor-Estimator architecture, with a pre-trained Transformer as the Predictor, and specific classifiers and regressors as Estimators. We integrate Bottleneck Adapter Layers in the Predictor to improve the transfer learning efficiency and prevent from over-fitting. At the same time, we jointly train the word- and sentence-level tasks with a unified model with multitask learning. Pseudo-PE assisted QE (PEAQE) is proposed, resulting in significant improvements on the performance. Our submissions achieve competitive result in word\/sentence-level sub-tasks for both of En-De\/Zh language pairs.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1649752855,"Goal":"Quality Education","Task":["WMT 2020 Word and Sentence - Level Post - Editing Quality Estimation","over - fitting","word - and sentence - level tasks","word\/sentence - level sub - tasks"],"Method":["Predictor - Estimator architecture","pre - trained Transformer","Predictor","classifiers","regressors","Estimators","Bottleneck Adapter Layers","Predictor","unified model","multitask learning","Pseudo - PE assisted QE"]},{"ID":"partanen-etal-2018-first","title":"The First {K}omi-{Z}yrian {U}niversal {D}ependencies Treebanks","abstract":"Two Komi-Zyrian treebanks were included in the Universal Dependencies 2.2 release. This article contextualizes the treebanks, discusses the process through which they were created, and outlines the future plans and timeline for the next improvements. Special attention is paid to the possibilities of using UD in the documentation and description of endangered languages.","year":2018,"title_abstract":"The First {K}omi-{Z}yrian {U}niversal {D}ependencies Treebanks Two Komi-Zyrian treebanks were included in the Universal Dependencies 2.2 release. This article contextualizes the treebanks, discusses the process through which they were created, and outlines the future plans and timeline for the next improvements. Special attention is paid to the possibilities of using UD in the documentation and description of endangered languages.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.1648242027,"Goal":"Life Below Water","Task":["documentation and description of endangered languages"],"Method":["UD"]},{"ID":"luken-etal-2018-qed","title":"{QED}: A fact verification system for the {FEVER} shared task","abstract":"This paper describes our system submission to the 2018 Fact Extraction and VERification (FEVER) shared task. The system uses a heuristics-based approach for evidence extraction and a modified version of the inference model by Parikh et al. (2016) for classification. Our process is broken down into three modules: potentially relevant documents are gathered based on key phrases in the claim, then any possible evidence sentences inside those documents are extracted, and finally our classifier discards any evidence deemed irrelevant and uses the remaining to classify the claim{'}s veracity. Our system beats the shared task baseline by 12{\\%} and is successful at finding correct evidence (evidence retrieval F1 of 62.5{\\%} on the development set).","year":2018,"title_abstract":"{QED}: A fact verification system for the {FEVER} shared task This paper describes our system submission to the 2018 Fact Extraction and VERification (FEVER) shared task. The system uses a heuristics-based approach for evidence extraction and a modified version of the inference model by Parikh et al. (2016) for classification. Our process is broken down into three modules: potentially relevant documents are gathered based on key phrases in the claim, then any possible evidence sentences inside those documents are extracted, and finally our classifier discards any evidence deemed irrelevant and uses the remaining to classify the claim{'}s veracity. Our system beats the shared task baseline by 12{\\%} and is successful at finding correct evidence (evidence retrieval F1 of 62.5{\\%} on the development set).","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1647926867,"Goal":"Climate Action","Task":["Fact Extraction and VERification (FEVER) shared task","evidence extraction","classification"],"Method":["fact verification system","heuristics - based approach","inference model","classifier"]},{"ID":"halkia-etal-2020-conflict","title":"Conflict Event Modelling: Research Experiment and Event Data Limitations","abstract":"This paper presents the conflict event modelling experiment, conducted at the Joint Research Centre of the European Commission, particularly focusing on the limitations of the input data. This model is under evaluation as to potentially complement the Global Conflict Risk Index (GCRI), a conflict risk model supporting the design of European Union{'}s conflict prevention strategies. The model aims at estimating the occurrence of material conflict events, under the assumption that an increase in material conflict events goes along with a decrease in material and verbal cooperation. It adopts a Long-Short Term Memory Cell Recurrent Neural Network on country-level actor-based event datasets that indicate potential triggers to violent conflict such as demonstrations, strikes, or elections-related violence. The observed data and the outcome of the model predictions consecutively, consolidate an early warning alarm system that signals abnormal social unrest upheavals, and appears promising as an approach towards a conflict trigger model. However, event-based systems still require overcoming certain obstacles related to the quality of the input data and the event classification method.","year":2020,"title_abstract":"Conflict Event Modelling: Research Experiment and Event Data Limitations This paper presents the conflict event modelling experiment, conducted at the Joint Research Centre of the European Commission, particularly focusing on the limitations of the input data. This model is under evaluation as to potentially complement the Global Conflict Risk Index (GCRI), a conflict risk model supporting the design of European Union{'}s conflict prevention strategies. The model aims at estimating the occurrence of material conflict events, under the assumption that an increase in material conflict events goes along with a decrease in material and verbal cooperation. It adopts a Long-Short Term Memory Cell Recurrent Neural Network on country-level actor-based event datasets that indicate potential triggers to violent conflict such as demonstrations, strikes, or elections-related violence. The observed data and the outcome of the model predictions consecutively, consolidate an early warning alarm system that signals abnormal social unrest upheavals, and appears promising as an approach towards a conflict trigger model. However, event-based systems still require overcoming certain obstacles related to the quality of the input data and the event classification method.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.164776504,"Goal":"Peace, Justice and Strong Institutions","Task":["conflict event modelling experiment","European Union{'}s conflict prevention strategies","occurrence of material conflict events"],"Method":["Conflict Event Modelling","conflict risk model","Long - Short Term Memory Cell Recurrent Neural Network","early warning alarm system","conflict trigger model","event - based systems","event classification method"]},{"ID":"miller-2018-cross","title":"Cross-Document Narrative Alignment of Environmental News: A Position Paper on the Challenge of Using Event Chains to Proxy Narrative Features","abstract":"Cross-document event chain co-referencing in corpora of news articles would achieve increased precision and generalizability from a method that consistently recognizes narrative, discursive, and phenomenological features such as tense, mood, tone, canonicity and breach, person, hermeneutic composability, speed, and time. Current models that capture primarily linguistic data such as entities, times, and relations or causal relationships may only incidentally capture narrative framing features of events. That limits efforts at narrative and event chain segmentation, among other predicate tasks for narrative search and narrative-based reasoning. It further limits research on audience engagement with journalism about complex subjects. This position paper explores the above proposition with respect to narrative theory and ongoing research on segmenting event chains into narrative units. Our own work in progress approaches this task using event segmentation, word embeddings, and variable length pattern matching in a corpus of 2,000 articles describing environmental events. Our position is that narrative features may or may not be implicitly captured by current methods explicitly focused on events as linguistic phenomena, that they are not explicitly captured, and that further research is required.","year":2018,"title_abstract":"Cross-Document Narrative Alignment of Environmental News: A Position Paper on the Challenge of Using Event Chains to Proxy Narrative Features Cross-document event chain co-referencing in corpora of news articles would achieve increased precision and generalizability from a method that consistently recognizes narrative, discursive, and phenomenological features such as tense, mood, tone, canonicity and breach, person, hermeneutic composability, speed, and time. Current models that capture primarily linguistic data such as entities, times, and relations or causal relationships may only incidentally capture narrative framing features of events. That limits efforts at narrative and event chain segmentation, among other predicate tasks for narrative search and narrative-based reasoning. It further limits research on audience engagement with journalism about complex subjects. This position paper explores the above proposition with respect to narrative theory and ongoing research on segmenting event chains into narrative units. Our own work in progress approaches this task using event segmentation, word embeddings, and variable length pattern matching in a corpus of 2,000 articles describing environmental events. Our position is that narrative features may or may not be implicitly captured by current methods explicitly focused on events as linguistic phenomena, that they are not explicitly captured, and that further research is required.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1647111475,"Goal":"Sustainable Cities and Communities","Task":["Cross - Document Narrative Alignment of Environmental News","Cross - document event chain co - referencing","narrative and event chain segmentation","predicate tasks","narrative search","narrative - based reasoning","audience engagement","journalism","narrative theory","segmenting event chains","event segmentation"],"Method":["word embeddings","variable length pattern matching"]},{"ID":"mino-etal-2019-neural","title":"Neural Machine Translation System using a Content-equivalently Translated Parallel Corpus for the Newswire Translation Tasks at {WAT} 2019","abstract":"This paper describes NHK and NHK Engineering System (NHK-ES){'}s submission to the newswire translation tasks of WAT 2019 in both directions of Japanese\u2192English and English\u2192Japanese. In addition to the JIJI Corpus that was officially provided by the task organizer, we developed a corpus of 0.22M sentence pairs by manually, translating Japanese news sentences into English content- equivalently. The content-equivalent corpus was effective for improving translation quality, and our systems achieved the best human evaluation scores in the newswire translation tasks at WAT 2019.","year":2019,"title_abstract":"Neural Machine Translation System using a Content-equivalently Translated Parallel Corpus for the Newswire Translation Tasks at {WAT} 2019 This paper describes NHK and NHK Engineering System (NHK-ES){'}s submission to the newswire translation tasks of WAT 2019 in both directions of Japanese\u2192English and English\u2192Japanese. In addition to the JIJI Corpus that was officially provided by the task organizer, we developed a corpus of 0.22M sentence pairs by manually, translating Japanese news sentences into English content- equivalently. The content-equivalent corpus was effective for improving translation quality, and our systems achieved the best human evaluation scores in the newswire translation tasks at WAT 2019.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1646803319,"Goal":"Gender Equality","Task":["Newswire Translation Tasks","newswire translation tasks","translation","newswire translation tasks"],"Method":["Neural Machine Translation System","NHK","NHK Engineering System"]},{"ID":"schulder-etal-2020-enhancing","title":"Enhancing a Lexicon of Polarity Shifters through the Supervised Classification of Shifting Directions","abstract":"The sentiment polarity of an expression (whether it is perceived as positive, negative or neutral) can be influenced by a number of phenomena, foremost among them negation. Apart from closed-class negation words like {``}no{''}, {``}not{''} or {``}without{''}, negation can also be caused by so-called polarity shifters. These are content words, such as verbs, nouns or adjectives, that shift polarities in their opposite direction, e.g. {``}abandoned{''} in {``}abandoned hope{''} or {``}alleviate{''} in {``}alleviate pain{''}. Many polarity shifters can affect both positive and negative polar expressions, shifting them towards the opposing polarity. However, other shifters are restricted to a single shifting direction. {``}Recoup{''} shifts negative to positive in {``}recoup your losses{''}, but does not affect the positive polarity of {``}fortune{''} in {``}recoup a fortune{''}. Existing polarity shifter lexica only specify whether a word can, in general, cause shifting, but they do not specify when this is limited to one shifting direction. To address this issue we introduce a supervised classifier that determines the shifting direction of shifters. This classifier uses both resource-driven features, such as WordNet relations, and data-driven features like in-context polarity conflicts. Using this classifier we enhance the largest available polarity shifter lexicon.","year":2020,"title_abstract":"Enhancing a Lexicon of Polarity Shifters through the Supervised Classification of Shifting Directions The sentiment polarity of an expression (whether it is perceived as positive, negative or neutral) can be influenced by a number of phenomena, foremost among them negation. Apart from closed-class negation words like {``}no{''}, {``}not{''} or {``}without{''}, negation can also be caused by so-called polarity shifters. These are content words, such as verbs, nouns or adjectives, that shift polarities in their opposite direction, e.g. {``}abandoned{''} in {``}abandoned hope{''} or {``}alleviate{''} in {``}alleviate pain{''}. Many polarity shifters can affect both positive and negative polar expressions, shifting them towards the opposing polarity. However, other shifters are restricted to a single shifting direction. {``}Recoup{''} shifts negative to positive in {``}recoup your losses{''}, but does not affect the positive polarity of {``}fortune{''} in {``}recoup a fortune{''}. Existing polarity shifter lexica only specify whether a word can, in general, cause shifting, but they do not specify when this is limited to one shifting direction. To address this issue we introduce a supervised classifier that determines the shifting direction of shifters. This classifier uses both resource-driven features, such as WordNet relations, and data-driven features like in-context polarity conflicts. Using this classifier we enhance the largest available polarity shifter lexicon.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1646386832,"Goal":"Reduced Inequalities","Task":["Supervised Classification of Shifting Directions"],"Method":["Polarity Shifters","polarity shifters","polarity shifter lexica","supervised classifier"]},{"ID":"deleris-etal-2018-decision","title":"Decision Conversations Decoded","abstract":"We describe the vision and current version of a Natural Language Processing system aimed at group decision making facilitation. Borrowing from the scientific field of Decision Analysis, its essential role is to identify alternatives and criteria associated with a given decision, to keep track of who proposed them and of the expressed sentiment towards them. Based on this information, the system can help identify agreement and dissent or recommend an alternative. Overall, it seeks to help a group reach a decision in a natural yet auditable fashion.","year":2018,"title_abstract":"Decision Conversations Decoded We describe the vision and current version of a Natural Language Processing system aimed at group decision making facilitation. Borrowing from the scientific field of Decision Analysis, its essential role is to identify alternatives and criteria associated with a given decision, to keep track of who proposed them and of the expressed sentiment towards them. Based on this information, the system can help identify agreement and dissent or recommend an alternative. Overall, it seeks to help a group reach a decision in a natural yet auditable fashion.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1645919681,"Goal":"Climate Action","Task":["Decision Conversations Decoded","group decision making facilitation","Decision Analysis"],"Method":["Natural Language Processing system"]},{"ID":"aroyehun-gelbukh-2020-automatically","title":"Automatically Predicting Judgement Dimensions of Human Behaviour","abstract":"This paper describes our submission to the ALTA-2020 shared task on assessing behaviour from short text, We evaluate the effectiveness of traditional machine learning and recent transformers pre-trained models. Our submission with the Roberta-large model and prediction threshold achieved first place on the private leaderboard.","year":2020,"title_abstract":"Automatically Predicting Judgement Dimensions of Human Behaviour This paper describes our submission to the ALTA-2020 shared task on assessing behaviour from short text, We evaluate the effectiveness of traditional machine learning and recent transformers pre-trained models. Our submission with the Roberta-large model and prediction threshold achieved first place on the private leaderboard.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1645414382,"Goal":"Climate Action","Task":["Automatically Predicting Judgement Dimensions of Human Behaviour","ALTA - 2020 shared task","assessing behaviour"],"Method":["machine learning","transformers pre - trained models","Roberta - large model"]},{"ID":"rabinovich-etal-2020-pick","title":"Pick a Fight or Bite your Tongue: Investigation of Gender Differences in Idiomatic Language Usage","abstract":"A large body of research on gender-linked language has established foundations regarding cross-gender differences in lexical, emotional, and topical preferences, along with their sociological underpinnings. We compile a novel, large and diverse corpus of spontaneous linguistic productions annotated with speakers{'} gender, and perform a first large-scale empirical study of distinctions in the usage of figurative language between male and female authors. Our analyses suggest that (1) idiomatic choices reflect gender-specific lexical and semantic preferences in general language, (2) men{'}s and women{'}s idiomatic usages express higher emotion than their literal language, with detectable, albeit more subtle, differences between male and female authors along the dimension of dominance compared to similar distinctions in their literal utterances, and (3) contextual analysis of idiomatic expressions reveals considerable differences, reflecting subtle divergences in usage environments, shaped by cross-gender communication styles and semantic biases.","year":2020,"title_abstract":"Pick a Fight or Bite your Tongue: Investigation of Gender Differences in Idiomatic Language Usage A large body of research on gender-linked language has established foundations regarding cross-gender differences in lexical, emotional, and topical preferences, along with their sociological underpinnings. We compile a novel, large and diverse corpus of spontaneous linguistic productions annotated with speakers{'} gender, and perform a first large-scale empirical study of distinctions in the usage of figurative language between male and female authors. Our analyses suggest that (1) idiomatic choices reflect gender-specific lexical and semantic preferences in general language, (2) men{'}s and women{'}s idiomatic usages express higher emotion than their literal language, with detectable, albeit more subtle, differences between male and female authors along the dimension of dominance compared to similar distinctions in their literal utterances, and (3) contextual analysis of idiomatic expressions reveals considerable differences, reflecting subtle divergences in usage environments, shaped by cross-gender communication styles and semantic biases.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1644930989,"Goal":"Gender Equality","Task":["Gender Differences","Idiomatic Language Usage"],"Method":["contextual analysis"]},{"ID":"lyu-etal-2022-extending","title":"Extending the Scope of Out-of-Domain: Examining {QA} models in multiple subdomains","abstract":"Past work that investigates out-of-domain performance of QA systems has mainly focused on general domains (e.g. news domain, wikipedia domain), underestimating the importance of subdomains defined by the internal characteristics of QA datasets.In this paper, we extend the scope of {``}out-of-domain{''} by splitting QA examples into different subdomains according to their internal characteristics including question type, text length, answer position. We then examine the performance of QA systems trained on the data from different subdomains. Experimental results show that the performance of QA systems can be significantly reduced when the train data and test data come from different subdomains. These results question the generalizability of current QA systems in multiple subdomains, suggesting the need to combat the bias introduced by the internal characteristics of QA datasets.","year":2022,"title_abstract":"Extending the Scope of Out-of-Domain: Examining {QA} models in multiple subdomains Past work that investigates out-of-domain performance of QA systems has mainly focused on general domains (e.g. news domain, wikipedia domain), underestimating the importance of subdomains defined by the internal characteristics of QA datasets.In this paper, we extend the scope of {``}out-of-domain{''} by splitting QA examples into different subdomains according to their internal characteristics including question type, text length, answer position. We then examine the performance of QA systems trained on the data from different subdomains. Experimental results show that the performance of QA systems can be significantly reduced when the train data and test data come from different subdomains. These results question the generalizability of current QA systems in multiple subdomains, suggesting the need to combat the bias introduced by the internal characteristics of QA datasets.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1644571871,"Goal":"Quality Education","Task":["out - of - domain","QA","QA","QA","QA"],"Method":["{QA} models"]},{"ID":"chen-etal-2016-linggle","title":"Linggle Knows: A Search Engine Tells How People Write","abstract":"This paper shows the great potential of incorporating different approaches to help writing. Not only did they solve different kinds of writing problems, but also they complement and reinforce each other to be a complete and effective solution. Despite the extensive and multifaceted feedback and suggestion, writing is not all about syntactically or lexically well-written. It involves contents, structure, the certain understanding of the background, and many other factors to compose a rich, organized and sophisticated text. (e.g., conventional structure and idioms in academic writing). There is still a long way to go to accomplish the ultimate goal. We envision the future of writing to be a joyful experience with the help of instantaneous suggestion and constructive feedback.","year":2016,"title_abstract":"Linggle Knows: A Search Engine Tells How People Write This paper shows the great potential of incorporating different approaches to help writing. Not only did they solve different kinds of writing problems, but also they complement and reinforce each other to be a complete and effective solution. Despite the extensive and multifaceted feedback and suggestion, writing is not all about syntactically or lexically well-written. It involves contents, structure, the certain understanding of the background, and many other factors to compose a rich, organized and sophisticated text. (e.g., conventional structure and idioms in academic writing). There is still a long way to go to accomplish the ultimate goal. We envision the future of writing to be a joyful experience with the help of instantaneous suggestion and constructive feedback.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1643399596,"Goal":"Sustainable Cities and Communities","Task":["writing","writing problems","writing","writing"],"Method":["Search Engine"]},{"ID":"bilal-etal-2021-evaluation","title":"Evaluation of Thematic Coherence in Microblogs","abstract":"Collecting together microblogs representing opinions about the same topics within the same timeframe is useful to a number of different tasks and practitioners. A major question is how to evaluate the quality of such thematic clusters. Here we create a corpus of microblog clusters from three different domains and time windows and define the task of evaluating thematic coherence. We provide annotation guidelines and human annotations of thematic coherence by journalist experts. We subsequently investigate the efficacy of different automated evaluation metrics for the task. We consider a range of metrics including surface level metrics, ones for topic model coherence and text generation metrics (TGMs). While surface level metrics perform well, outperforming topic coherence metrics, they are not as consistent as TGMs. TGMs are more reliable than all other metrics considered for capturing thematic coherence in microblog clusters due to being less sensitive to the effect of time windows.","year":2021,"title_abstract":"Evaluation of Thematic Coherence in Microblogs Collecting together microblogs representing opinions about the same topics within the same timeframe is useful to a number of different tasks and practitioners. A major question is how to evaluate the quality of such thematic clusters. Here we create a corpus of microblog clusters from three different domains and time windows and define the task of evaluating thematic coherence. We provide annotation guidelines and human annotations of thematic coherence by journalist experts. We subsequently investigate the efficacy of different automated evaluation metrics for the task. We consider a range of metrics including surface level metrics, ones for topic model coherence and text generation metrics (TGMs). While surface level metrics perform well, outperforming topic coherence metrics, they are not as consistent as TGMs. TGMs are more reliable than all other metrics considered for capturing thematic coherence in microblog clusters due to being less sensitive to the effect of time windows.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1642889827,"Goal":"Sustainable Cities and Communities","Task":["Evaluation of Thematic Coherence","evaluating thematic coherence","capturing thematic coherence in microblog clusters"],"Method":["TGMs","TGMs"]},{"ID":"ceausu-etal-2006-acquis","title":"{A}cquis {C}ommunautaire Sentence Alignment using Support Vector Machines","abstract":"Sentence alignment is a task that requires not only accuracy, as possible errors can affect further processing, but also requires small computation resources and to be language pair independent. Although many implementations do not use translation equivalents because they are dependent on the language pair, this feature is a requirement for the accuracy increase. The paper presents a hybrid sentence aligner that has two alignment iterations. The first iteration is based mostly on sentences length, and the second is based on a translation equivalents table estimated from the results of the first iteration. The aligner uses a Support Vector Machine classifier to discriminate between positive and negative examples of sentence pairs.","year":2006,"title_abstract":"{A}cquis {C}ommunautaire Sentence Alignment using Support Vector Machines Sentence alignment is a task that requires not only accuracy, as possible errors can affect further processing, but also requires small computation resources and to be language pair independent. Although many implementations do not use translation equivalents because they are dependent on the language pair, this feature is a requirement for the accuracy increase. The paper presents a hybrid sentence aligner that has two alignment iterations. The first iteration is based mostly on sentences length, and the second is based on a translation equivalents table estimated from the results of the first iteration. The aligner uses a Support Vector Machine classifier to discriminate between positive and negative examples of sentence pairs.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1642857939,"Goal":"Gender Equality","Task":["Sentence Alignment","Sentence alignment","hybrid sentence aligner"],"Method":["Support Vector Machines","aligner","Support Vector Machine classifier"]},{"ID":"ruiter-etal-2010-human","title":"Human Language Technology and Communicative Disabilities: Requirements and Possibilities for the Future","abstract":"For some years now, the Nederlandse Taalunie (Dutch Language Union) has been active in promoting the development of human language technology (HLT) applications for users of Dutch with communication disabilities. The reason is that HLT products and services may enable these users to improve their verbal autonomy and communication skills. We sought to identify a minimum common set of HLT resources that is required to develop tools for a wide range of communication disabilities. In order to reach this goal, we investigated the specific HLT needs of communicatively disabled people and related these needs to the underlying HLT software components. By analysing the availability and quality of these essential HLT resources, we were able to identify which of the crucial elements need further research and development to become usable for developing applications for communicatively disabled users of Dutch. The results obtained in the current survey can be used to inform policy institutions on how they can stimulate the development of HLT resources for this target group. In the current study results were obtained for Dutch, but a similar approach can also be used for other languages.","year":2010,"title_abstract":"Human Language Technology and Communicative Disabilities: Requirements and Possibilities for the Future For some years now, the Nederlandse Taalunie (Dutch Language Union) has been active in promoting the development of human language technology (HLT) applications for users of Dutch with communication disabilities. The reason is that HLT products and services may enable these users to improve their verbal autonomy and communication skills. We sought to identify a minimum common set of HLT resources that is required to develop tools for a wide range of communication disabilities. In order to reach this goal, we investigated the specific HLT needs of communicatively disabled people and related these needs to the underlying HLT software components. By analysing the availability and quality of these essential HLT resources, we were able to identify which of the crucial elements need further research and development to become usable for developing applications for communicatively disabled users of Dutch. The results obtained in the current survey can be used to inform policy institutions on how they can stimulate the development of HLT resources for this target group. In the current study results were obtained for Dutch, but a similar approach can also be used for other languages.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1642440557,"Goal":"Sustainable Cities and Communities","Task":["Human Language Technology","Communicative Disabilities","human language technology","communication disabilities","HLT"],"Method":["HLT","HLT","HLT software components","HLT","HLT"]},{"ID":"basta-costa-jussa-2021-impact","title":"Impact of {COVID}-19 in Natural Language Processing Publications: a Disaggregated Study in Gender, Contribution and Experience","abstract":"This study sheds light on the effects of COVID-19 in the particular field of Computational Linguistics and Natural Language Processing within Artificial Intelligence. We provide an inter-sectional study on gender, contribution, and experience that considers one school year (from August 2019 to August 2020) as a pandemic year. August is included twice for the purpose of an inter-annual comparison. While the trend in publications increased with the crisis, the results show that the ratio between female and male publications decreased. This only helps to reduce the importance of the female role in the scientific contributions of computational linguistics (it is now far below its peak of 0.24). The pandemic has a particularly negative effect on the production of female senior researchers in the first position of authors (maximum work), followed by the female junior researchers in the last position of authors (supervision or collaborative work).","year":2021,"title_abstract":"Impact of {COVID}-19 in Natural Language Processing Publications: a Disaggregated Study in Gender, Contribution and Experience This study sheds light on the effects of COVID-19 in the particular field of Computational Linguistics and Natural Language Processing within Artificial Intelligence. We provide an inter-sectional study on gender, contribution, and experience that considers one school year (from August 2019 to August 2020) as a pandemic year. August is included twice for the purpose of an inter-annual comparison. While the trend in publications increased with the crisis, the results show that the ratio between female and male publications decreased. This only helps to reduce the importance of the female role in the scientific contributions of computational linguistics (it is now far below its peak of 0.24). The pandemic has a particularly negative effect on the production of female senior researchers in the first position of authors (maximum work), followed by the female junior researchers in the last position of authors (supervision or collaborative work).","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.164214626,"Goal":"Gender Equality","Task":["Natural Language Processing Publications","Computational Linguistics","Natural Language Processing","Artificial Intelligence","computational linguistics"],"Method":["COVID - 19"]},{"ID":"gowda-etal-2022-mucic","title":"{MUCIC}@{LT}-{EDI}-{ACL}2022: Hope Speech Detection using Data Re-Sampling and 1{D} Conv-{LSTM}","abstract":"Spreading positive vibes or hope content on social media may help many people to get motivated in their life. To address Hope Speech detection in YouTube comments, this paper presents the description of the models submitted by our team - MUCIC, to the Hope Speech Detection for Equality, Diversity, and Inclusion (HopeEDI) shared task at Association for Computational Linguistics (ACL) 2022. This shared task consists of texts in five languages, namely: English, Spanish (in Latin scripts), and Tamil, Malayalam, and Kannada (in code-mixed native and Roman scripts) with the aim of classifying the YouTube comment into {``}Hope{''}, {``}Not-Hope{''} or {``}Not-Intended{''} categories. The proposed methodology uses the re-sampling technique to deal with imbalanced data in the corpus and obtained 1st rank for English language with a macro-averaged F1-score of 0.550 and weighted-averaged F1-score of 0.860. The code to reproduce this work is available in GitHub.","year":2022,"title_abstract":"{MUCIC}@{LT}-{EDI}-{ACL}2022: Hope Speech Detection using Data Re-Sampling and 1{D} Conv-{LSTM} Spreading positive vibes or hope content on social media may help many people to get motivated in their life. To address Hope Speech detection in YouTube comments, this paper presents the description of the models submitted by our team - MUCIC, to the Hope Speech Detection for Equality, Diversity, and Inclusion (HopeEDI) shared task at Association for Computational Linguistics (ACL) 2022. This shared task consists of texts in five languages, namely: English, Spanish (in Latin scripts), and Tamil, Malayalam, and Kannada (in code-mixed native and Roman scripts) with the aim of classifying the YouTube comment into {``}Hope{''}, {``}Not-Hope{''} or {``}Not-Intended{''} categories. The proposed methodology uses the re-sampling technique to deal with imbalanced data in the corpus and obtained 1st rank for English language with a macro-averaged F1-score of 0.550 and weighted-averaged F1-score of 0.860. The code to reproduce this work is available in GitHub.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1641700268,"Goal":"Gender Equality","Task":["Hope Speech Detection","Hope Speech detection","Hope Speech Detection","Equality , Diversity","Inclusion","Computational Linguistics"],"Method":["Data Re - Sampling","1{D} Conv","MUCIC","re - sampling technique"]},{"ID":"lalor-yu-2020-dynamic","title":"Dynamic Data Selection for Curriculum Learning via Ability Estimation","abstract":"Curriculum learning methods typically rely on heuristics to estimate the difficulty of training examples or the ability of the model. In this work, we propose replacing difficulty heuristics with learned difficulty parameters. We also propose Dynamic Data selection for Curriculum Learning via Ability Estimation (DDaCLAE), a strategy that probes model ability at each training epoch to select the best training examples at that point. We show that models using learned difficulty and\/or ability outperform heuristic-based curriculum learning models on the GLUE classification tasks.","year":2020,"title_abstract":"Dynamic Data Selection for Curriculum Learning via Ability Estimation Curriculum learning methods typically rely on heuristics to estimate the difficulty of training examples or the ability of the model. In this work, we propose replacing difficulty heuristics with learned difficulty parameters. We also propose Dynamic Data selection for Curriculum Learning via Ability Estimation (DDaCLAE), a strategy that probes model ability at each training epoch to select the best training examples at that point. We show that models using learned difficulty and\/or ability outperform heuristic-based curriculum learning models on the GLUE classification tasks.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1641486883,"Goal":"Quality Education","Task":["Curriculum Learning","Ability Estimation","Curriculum Learning via Ability Estimation","classification tasks"],"Method":["Dynamic Data Selection","Curriculum learning methods","heuristics","difficulty heuristics","Dynamic Data selection","heuristic - based curriculum learning models"]},{"ID":"fleming-etal-2021-fine","title":"Fine-tuning Transformers for Identifying Self-Reporting Potential Cases and Symptoms of {COVID}-19 in Tweets","abstract":"We describe our straight-forward approach for Tasks 5 and 6 of 2021 Social Media Min- ing for Health Applications (SMM4H) shared tasks. Our system is based on fine-tuning Dis- tillBERT on each task, as well as first fine- tuning the model on the other task. In this paper, we additionally explore how much fine- tuning is necessary for accurately classifying tweets as containing self-reported COVID-19 symptoms (Task 5) or whether a tweet related to COVID-19 is self-reporting, non-personal reporting, or a literature\/news mention of the virus (Task 6).","year":2021,"title_abstract":"Fine-tuning Transformers for Identifying Self-Reporting Potential Cases and Symptoms of {COVID}-19 in Tweets We describe our straight-forward approach for Tasks 5 and 6 of 2021 Social Media Min- ing for Health Applications (SMM4H) shared tasks. Our system is based on fine-tuning Dis- tillBERT on each task, as well as first fine- tuning the model on the other task. In this paper, we additionally explore how much fine- tuning is necessary for accurately classifying tweets as containing self-reported COVID-19 symptoms (Task 5) or whether a tweet related to COVID-19 is self-reporting, non-personal reporting, or a literature\/news mention of the virus (Task 6).","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1641436964,"Goal":"Climate Action","Task":["Fine - tuning Transformers","Identifying Self - Reporting Potential Cases and Symptoms of {COVID} - 19","2021 Social Media Min - ing","fine - tuning"],"Method":["Dis - tillBERT"]},{"ID":"acharya-2020-wnut","title":"{WNUT} 2020 Shared Task-1: Conditional Random Field({CRF}) based Named Entity Recognition({NER}) for Wet Lab Protocols","abstract":"The paper describes how classifier model built using Conditional Random Field detects named entities in wet lab protocols.","year":2020,"title_abstract":"{WNUT} 2020 Shared Task-1: Conditional Random Field({CRF}) based Named Entity Recognition({NER}) for Wet Lab Protocols The paper describes how classifier model built using Conditional Random Field detects named entities in wet lab protocols.","social_need":"Clean Water and Sanitation Ensure availability and sustainable management of water and sanitation for all","cosine_similarity":0.1640453935,"Goal":"Clean Water and Sanitation","Task":["Named Entity Recognition({NER})","Wet Lab Protocols"],"Method":["Conditional Random Field({CRF})","classifier model","Conditional Random Field","wet lab protocols"]},{"ID":"redkar-etal-2017-hindi","title":"{H}indi Shabdamitra: A {W}ordnet based {E}-Learning Tool for Language Learning and Teaching","abstract":"In today{'}s technology driven digital era, education domain is undergoing a transformation from traditional approaches to more learner controlled and flexible methods of learning. This transformation has opened the new avenues for interdisciplinary research in the field of educational technology and natural language processing in developing quality digital aids for learning and teaching. The tool presented here - Hindi Shabhadamitra, developed using Hindi Wordnet for Hindi language learning, is one such e-learning tool. It has been developed as a teaching and learning aid suitable for formal school based curriculum and informal setup for self learning users. Besides vocabulary, it also provides word based grammar along with images and pronunciation for better learning and retention. This aid demonstrates that how a rich lexical resource like wordnet can be systematically remodeled for practical usage in the educational domain.","year":2017,"title_abstract":"{H}indi Shabdamitra: A {W}ordnet based {E}-Learning Tool for Language Learning and Teaching In today{'}s technology driven digital era, education domain is undergoing a transformation from traditional approaches to more learner controlled and flexible methods of learning. This transformation has opened the new avenues for interdisciplinary research in the field of educational technology and natural language processing in developing quality digital aids for learning and teaching. The tool presented here - Hindi Shabhadamitra, developed using Hindi Wordnet for Hindi language learning, is one such e-learning tool. It has been developed as a teaching and learning aid suitable for formal school based curriculum and informal setup for self learning users. Besides vocabulary, it also provides word based grammar along with images and pronunciation for better learning and retention. This aid demonstrates that how a rich lexical resource like wordnet can be systematically remodeled for practical usage in the educational domain.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1640048772,"Goal":"Quality Education","Task":["Language Learning and Teaching","education domain","learning","educational technology","natural language processing","learning and teaching","Hindi language learning","teaching and learning aid","formal school based curriculum","informal setup","self learning users","learning","retention","educational domain"],"Method":["{W}ordnet based {E} - Learning Tool","Hindi Shabhadamitra","Hindi Wordnet","e - learning tool","word based grammar","wordnet"]},{"ID":"pan-etal-2017-prerequisite","title":"Prerequisite Relation Learning for Concepts in {MOOC}s","abstract":"What prerequisite knowledge should students achieve a level of mastery before moving forward to learn subsequent coursewares? We study the extent to which the prerequisite relation between knowledge concepts in Massive Open Online Courses (MOOCs) can be inferred automatically. In particular, what kinds of information can be leverage to uncover the potential prerequisite relation between knowledge concepts. We first propose a representation learning-based method for learning latent representations of course concepts, and then investigate how different features capture the prerequisite relations between concepts. Our experiments on three datasets form Coursera show that the proposed method achieves significant improvements (+5.9-48.0{\\%} by F1-score) comparing with existing methods.","year":2017,"title_abstract":"Prerequisite Relation Learning for Concepts in {MOOC}s What prerequisite knowledge should students achieve a level of mastery before moving forward to learn subsequent coursewares? We study the extent to which the prerequisite relation between knowledge concepts in Massive Open Online Courses (MOOCs) can be inferred automatically. In particular, what kinds of information can be leverage to uncover the potential prerequisite relation between knowledge concepts. We first propose a representation learning-based method for learning latent representations of course concepts, and then investigate how different features capture the prerequisite relations between concepts. Our experiments on three datasets form Coursera show that the proposed method achieves significant improvements (+5.9-48.0{\\%} by F1-score) comparing with existing methods.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1639609188,"Goal":"Quality Education","Task":["latent representations of course concepts"],"Method":["Prerequisite Relation Learning","representation learning - based method"]},{"ID":"xu-etal-2021-detoxifying","title":"Detoxifying Language Models Risks Marginalizing Minority Voices","abstract":"Language models (LMs) must be both safe and equitable to be responsibly deployed in practice. With safety in mind, numerous detoxification techniques (e.g., Dathathri et al. 2020; Krause et al. 2020) have been proposed to mitigate toxic LM generations. In this work, we show that these detoxification techniques hurt equity: they decrease the utility of LMs on language used by marginalized groups (e.g., African-American English and minority identity mentions). In particular, we perform automatic and human evaluations of text generation quality when LMs are conditioned on inputs with different dialects and group identifiers. We find that detoxification makes LMs more brittle to distribution shift, especially on language used by marginalized groups. We identify that these failures stem from detoxification methods exploiting spurious correlations in toxicity datasets. Overall, our results highlight the tension between the controllability and distributional robustness of LMs.","year":2021,"title_abstract":"Detoxifying Language Models Risks Marginalizing Minority Voices Language models (LMs) must be both safe and equitable to be responsibly deployed in practice. With safety in mind, numerous detoxification techniques (e.g., Dathathri et al. 2020; Krause et al. 2020) have been proposed to mitigate toxic LM generations. In this work, we show that these detoxification techniques hurt equity: they decrease the utility of LMs on language used by marginalized groups (e.g., African-American English and minority identity mentions). In particular, we perform automatic and human evaluations of text generation quality when LMs are conditioned on inputs with different dialects and group identifiers. We find that detoxification makes LMs more brittle to distribution shift, especially on language used by marginalized groups. We identify that these failures stem from detoxification methods exploiting spurious correlations in toxicity datasets. Overall, our results highlight the tension between the controllability and distributional robustness of LMs.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.163950026,"Goal":"Reduced Inequalities","Task":["LM generations","detoxification"],"Method":["Detoxifying Language Models","Marginalizing Minority Voices Language models","detoxification techniques","detoxification techniques","detoxification methods"]},{"ID":"klavans-2010-task","title":"Task-based evaluation methods for machine translation, in practice and theory","abstract":"A panel of industry and government experts will discuss ways in which they have applied task-based evaluation for Machine Translation and other language technologies in their organizations and share ideas for new methods that could be tried in the future. As part of the discussion, the panelists will address some of the following points: What task-based evaluation means within their organization, i.e., how task-based evaluation is defined; How task-based evaluation impacts the use of MT technologies in their work environment; Whether task-based evaluation correlates with MT developers' automated metrics and if not, how do we arrive at automated metrics that do correlate with the more expensive task-based evaluation; What ``lessons-learned'' resulted from the course of performing task-based evaluation; How task-based evaluations can be generalized to multiple workflow environments.","year":2010,"title_abstract":"Task-based evaluation methods for machine translation, in practice and theory A panel of industry and government experts will discuss ways in which they have applied task-based evaluation for Machine Translation and other language technologies in their organizations and share ideas for new methods that could be tried in the future. As part of the discussion, the panelists will address some of the following points: What task-based evaluation means within their organization, i.e., how task-based evaluation is defined; How task-based evaluation impacts the use of MT technologies in their work environment; Whether task-based evaluation correlates with MT developers' automated metrics and if not, how do we arrive at automated metrics that do correlate with the more expensive task-based evaluation; What ``lessons-learned'' resulted from the course of performing task-based evaluation; How task-based evaluations can be generalized to multiple workflow environments.","social_need":"Decent Work and Economic Growth Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all","cosine_similarity":0.1639494896,"Goal":"Decent Work and Economic Growth","Task":["machine translation","task - based evaluation","Machine Translation","language technologies","task - based evaluation","task - based evaluation","task - based evaluation","MT","task - based evaluation","MT","task - based evaluation;"],"Method":["Task - based evaluation methods","task - based evaluation;","task - based evaluations"]},{"ID":"rambow-etal-2006-parallel","title":"Parallel Syntactic Annotation of Multiple Languages","abstract":"This paper describes an effort to investigate the incrementally deepening development of an interlingua notation, validated by human annotation of texts in English plus six languages. We begin with deep syntactic annotation, and in this paper present a series of annotation manuals for six different languages at the deep-syntactic level of representation. Many syntactic differences between languages are removed in the proposed syntactic annotation, making them useful resources for multilingual NLP projects with semantic components.","year":2006,"title_abstract":"Parallel Syntactic Annotation of Multiple Languages This paper describes an effort to investigate the incrementally deepening development of an interlingua notation, validated by human annotation of texts in English plus six languages. We begin with deep syntactic annotation, and in this paper present a series of annotation manuals for six different languages at the deep-syntactic level of representation. Many syntactic differences between languages are removed in the proposed syntactic annotation, making them useful resources for multilingual NLP projects with semantic components.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1639473885,"Goal":"Partnership for the Goals","Task":["Parallel Syntactic Annotation","incrementally deepening development","annotation","deep syntactic annotation","multilingual NLP projects"],"Method":["interlingua notation","deep - syntactic level of representation","syntactic annotation","semantic components"]},{"ID":"da-san-martino-etal-2020-prta","title":"{P}rta: A System to Support the Analysis of Propaganda Techniques in the News","abstract":"Recent events, such as the 2016 US Presidential Campaign, Brexit and the COVID-19 {``}infodemic{''}, have brought into the spotlight the dangers of online disinformation. There has been a lot of research focusing on fact-checking and disinformation detection. However, little attention has been paid to the specific rhetorical and psychological techniques used to convey propaganda messages. Revealing the use of such techniques can help promote media literacy and critical thinking, and eventually contribute to limiting the impact of {``}fake news{''} and disinformation campaigns. Prta (Propaganda Persuasion Techniques Analyzer) allows users to explore the articles crawled on a regular basis by highlighting the spans in which propaganda techniques occur and to compare them on the basis of their use of propaganda techniques. The system further reports statistics about the use of such techniques, overall and over time, or according to filtering criteria specified by the user based on time interval, keywords, and\/or political orientation of the media. Moreover, it allows users to analyze any text or URL through a dedicated interface or via an API. The system is available online: https:\/\/www.tanbih.org\/prta.","year":2020,"title_abstract":"{P}rta: A System to Support the Analysis of Propaganda Techniques in the News Recent events, such as the 2016 US Presidential Campaign, Brexit and the COVID-19 {``}infodemic{''}, have brought into the spotlight the dangers of online disinformation. There has been a lot of research focusing on fact-checking and disinformation detection. However, little attention has been paid to the specific rhetorical and psychological techniques used to convey propaganda messages. Revealing the use of such techniques can help promote media literacy and critical thinking, and eventually contribute to limiting the impact of {``}fake news{''} and disinformation campaigns. Prta (Propaganda Persuasion Techniques Analyzer) allows users to explore the articles crawled on a regular basis by highlighting the spans in which propaganda techniques occur and to compare them on the basis of their use of propaganda techniques. The system further reports statistics about the use of such techniques, overall and over time, or according to filtering criteria specified by the user based on time interval, keywords, and\/or political orientation of the media. Moreover, it allows users to analyze any text or URL through a dedicated interface or via an API. The system is available online: https:\/\/www.tanbih.org\/prta.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1639204025,"Goal":"Climate Action","Task":["Analysis of Propaganda Techniques","fact - checking","disinformation detection","media literacy","critical thinking"],"Method":["rhetorical and psychological techniques","Prta","Persuasion Techniques Analyzer)","propaganda techniques"]},{"ID":"ramezani-etal-2021-unsupervised-framework","title":"An unsupervised framework for tracing textual sources of moral change","abstract":"Morality plays an important role in social well-being, but people{'}s moral perception is not stable and changes over time. Recent advances in natural language processing have shown that text is an effective medium for informing moral change, but no attempt has been made to quantify the origins of these changes. We present a novel unsupervised framework for tracing textual sources of moral change toward entities through time. We characterize moral change with probabilistic topical distributions and infer the source text that exerts prominent influence on the moral time course. We evaluate our framework on a diverse set of data ranging from social media to news articles. We show that our framework not only captures fine-grained human moral judgments, but also identifies coherent source topics of moral change triggered by historical events. We apply our methodology to analyze the news in the COVID-19 pandemic and demonstrate its utility in identifying sources of moral change in high-impact and real-time social events.","year":2021,"title_abstract":"An unsupervised framework for tracing textual sources of moral change Morality plays an important role in social well-being, but people{'}s moral perception is not stable and changes over time. Recent advances in natural language processing have shown that text is an effective medium for informing moral change, but no attempt has been made to quantify the origins of these changes. We present a novel unsupervised framework for tracing textual sources of moral change toward entities through time. We characterize moral change with probabilistic topical distributions and infer the source text that exerts prominent influence on the moral time course. We evaluate our framework on a diverse set of data ranging from social media to news articles. We show that our framework not only captures fine-grained human moral judgments, but also identifies coherent source topics of moral change triggered by historical events. We apply our methodology to analyze the news in the COVID-19 pandemic and demonstrate its utility in identifying sources of moral change in high-impact and real-time social events.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1639191508,"Goal":"Climate Action","Task":["tracing textual sources of moral change","Morality","social well - being","natural language processing","informing moral change","tracing textual sources of moral change","sources of moral change","high - impact and real - time social events"],"Method":["unsupervised framework","unsupervised framework"]},{"ID":"yang-etal-2018-commonsense","title":"Commonsense Justification for Action Explanation","abstract":"To enable collaboration and communication between humans and agents, this paper investigates learning to acquire commonsense evidence for action justification. In particular, we have developed an approach based on the generative Conditional Variational Autoencoder(CVAE) that models object relations\/attributes of the world as latent variables and jointly learns a performer that predicts actions and an explainer that gathers commonsense evidence to justify the action. Our empirical results have shown that, compared to a typical attention-based model, CVAE achieves significantly higher performance in both action prediction and justification. A human subject study further shows that the commonsense evidence gathered by CVAE can be communicated to humans to achieve a significantly higher common ground between humans and agents.","year":2018,"title_abstract":"Commonsense Justification for Action Explanation To enable collaboration and communication between humans and agents, this paper investigates learning to acquire commonsense evidence for action justification. In particular, we have developed an approach based on the generative Conditional Variational Autoencoder(CVAE) that models object relations\/attributes of the world as latent variables and jointly learns a performer that predicts actions and an explainer that gathers commonsense evidence to justify the action. Our empirical results have shown that, compared to a typical attention-based model, CVAE achieves significantly higher performance in both action prediction and justification. A human subject study further shows that the commonsense evidence gathered by CVAE can be communicated to humans to achieve a significantly higher common ground between humans and agents.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1639107764,"Goal":"Climate Action","Task":["Commonsense Justification","Action Explanation","collaboration and communication","learning","action justification","action prediction","justification"],"Method":["generative Conditional Variational Autoencoder(CVAE)","explainer","attention - based model","CVAE","CVAE"]},{"ID":"jalili-sabet-etal-2020-simalign","title":"{S}im{A}lign: High Quality Word Alignments Without Parallel Training Data Using Static and Contextualized Embeddings","abstract":"Word alignments are useful for tasks like statistical and neural machine translation (NMT) and cross-lingual annotation projection. Statistical word aligners perform well, as do methods that extract alignments jointly with translations in NMT. However, most approaches require parallel training data and quality decreases as less training data is available. We propose word alignment methods that require no parallel data. The key idea is to leverage multilingual word embeddings {--} both static and contextualized {--} for word alignment. Our multilingual embeddings are created from monolingual data only without relying on any parallel data or dictionaries. We find that alignments created from embeddings are superior for four and comparable for two language pairs compared to those produced by traditional statistical aligners {--} even with abundant parallel data; e.g., contextualized embeddings achieve a word alignment F1 for English-German that is 5 percentage points higher than eflomal, a high-quality statistical aligner, trained on 100k parallel sentences.","year":2020,"title_abstract":"{S}im{A}lign: High Quality Word Alignments Without Parallel Training Data Using Static and Contextualized Embeddings Word alignments are useful for tasks like statistical and neural machine translation (NMT) and cross-lingual annotation projection. Statistical word aligners perform well, as do methods that extract alignments jointly with translations in NMT. However, most approaches require parallel training data and quality decreases as less training data is available. We propose word alignment methods that require no parallel data. The key idea is to leverage multilingual word embeddings {--} both static and contextualized {--} for word alignment. Our multilingual embeddings are created from monolingual data only without relying on any parallel data or dictionaries. We find that alignments created from embeddings are superior for four and comparable for two language pairs compared to those produced by traditional statistical aligners {--} even with abundant parallel data; e.g., contextualized embeddings achieve a word alignment F1 for English-German that is 5 percentage points higher than eflomal, a high-quality statistical aligner, trained on 100k parallel sentences.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1637960076,"Goal":"Gender Equality","Task":["Word Alignments","Word alignments","statistical and neural machine translation","cross - lingual annotation projection","NMT","word alignment"],"Method":["Static and Contextualized Embeddings","Statistical word aligners","word alignment methods","embeddings","statistical aligners","contextualized embeddings","eflomal","statistical aligner"]},{"ID":"oostdijk-van-den-heuvel-2014-evolving","title":"The evolving infrastructure for language resources and the role for data scientists","abstract":"In the context of ongoing developments as regards the creation of a sustainable, interoperable language resource infrastructure and spreading ideas of the need for open access, not only of research publications but also of the underlying data, various issues present themselves which require that different stakeholders reconsider their positions. In the present paper we relate the experiences from the CLARIN-NL data curation service (DCS) over the two years that it has been operational, and the future role we envisage for expertise centres like the DCS in the evolving infrastructure.","year":2014,"title_abstract":"The evolving infrastructure for language resources and the role for data scientists In the context of ongoing developments as regards the creation of a sustainable, interoperable language resource infrastructure and spreading ideas of the need for open access, not only of research publications but also of the underlying data, various issues present themselves which require that different stakeholders reconsider their positions. In the present paper we relate the experiences from the CLARIN-NL data curation service (DCS) over the two years that it has been operational, and the future role we envisage for expertise centres like the DCS in the evolving infrastructure.","social_need":"Industry, Innovation and Infrastrucure Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation","cosine_similarity":0.1637555212,"Goal":"Industry, Innovation and Infrastrucure","Task":["language resources","data scientists"],"Method":["CLARIN - NL data curation service"]},{"ID":"norregaard-derczynski-2021-danfever","title":"{D}an{FEVER}: claim verification dataset for {D}anish","abstract":"We present a dataset, DanFEVER, intended for multilingual misinformation research. The dataset is in Danish and has the same format as the well-known English FEVER dataset. It can be used for testing methods in multilingual settings, as well as for creating models in production for the Danish language.","year":2021,"title_abstract":"{D}an{FEVER}: claim verification dataset for {D}anish We present a dataset, DanFEVER, intended for multilingual misinformation research. The dataset is in Danish and has the same format as the well-known English FEVER dataset. It can be used for testing methods in multilingual settings, as well as for creating models in production for the Danish language.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1637370586,"Goal":"Climate Action","Task":["multilingual misinformation research","multilingual settings","production"],"Method":["DanFEVER"]},{"ID":"platonov-etal-2021-generating","title":"Generating Justifications in a Spatial Question-Answering Dialogue System for a Blocks World","abstract":"As AI reaches wider adoption, designing systems that are explainable and interpretable becomes a critical necessity. In particular, when it comes to dialogue systems, their reasoning must be transparent and must comply with human intuitions in order for them to be integrated seamlessly into day-to-day collaborative human-machine activities. Here, we describe our ongoing work on a (general purpose) dialogue system equipped with a spatial specialist with explanatory capabilities. We applied this system to a particular task of characterizing spatial configurations of blocks in a simple physical Blocks World (BW) domain using natural locative expressions, as well as generating justifications for the proposed spatial descriptions by indicating the factors that the system used to arrive at a particular conclusion.","year":2021,"title_abstract":"Generating Justifications in a Spatial Question-Answering Dialogue System for a Blocks World As AI reaches wider adoption, designing systems that are explainable and interpretable becomes a critical necessity. In particular, when it comes to dialogue systems, their reasoning must be transparent and must comply with human intuitions in order for them to be integrated seamlessly into day-to-day collaborative human-machine activities. Here, we describe our ongoing work on a (general purpose) dialogue system equipped with a spatial specialist with explanatory capabilities. We applied this system to a particular task of characterizing spatial configurations of blocks in a simple physical Blocks World (BW) domain using natural locative expressions, as well as generating justifications for the proposed spatial descriptions by indicating the factors that the system used to arrive at a particular conclusion.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1636339128,"Goal":"Sustainable Cities and Communities","Task":["Generating Justifications","Blocks World","AI","dialogue systems","collaborative human - machine activities","characterizing spatial configurations of blocks"],"Method":["Spatial Question - Answering Dialogue System","dialogue system","spatial specialist","explanatory capabilities"]},{"ID":"labropoulou-etal-2020-making","title":"Making Metadata Fit for Next Generation Language Technology Platforms: The Metadata Schema of the {E}uropean Language Grid","abstract":"The current scientific and technological landscape is characterised by the increasing availability of data resources and processing tools and services. In this setting, metadata have emerged as a key factor facilitating management, sharing and usage of such digital assets. In this paper we present ELG-SHARE, a rich metadata schema catering for the description of Language Resources and Technologies (processing and generation services and tools, models, corpora, term lists, etc.), as well as related entities (e.g., organizations, projects, supporting documents, etc.). The schema powers the European Language Grid platform that aims to be the primary hub and marketplace for industry-relevant Language Technology in Europe. ELG-SHARE has been based on various metadata schemas, vocabularies, and ontologies, as well as related recommendations and guidelines.","year":2020,"title_abstract":"Making Metadata Fit for Next Generation Language Technology Platforms: The Metadata Schema of the {E}uropean Language Grid The current scientific and technological landscape is characterised by the increasing availability of data resources and processing tools and services. In this setting, metadata have emerged as a key factor facilitating management, sharing and usage of such digital assets. In this paper we present ELG-SHARE, a rich metadata schema catering for the description of Language Resources and Technologies (processing and generation services and tools, models, corpora, term lists, etc.), as well as related entities (e.g., organizations, projects, supporting documents, etc.). The schema powers the European Language Grid platform that aims to be the primary hub and marketplace for industry-relevant Language Technology in Europe. ELG-SHARE has been based on various metadata schemas, vocabularies, and ontologies, as well as related recommendations and guidelines.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1635976136,"Goal":"Partnership for the Goals","Task":["Next Generation Language Technology Platforms","management","industry - relevant Language Technology"],"Method":["Metadata Schema","{E}uropean Language Grid","ELG - SHARE","metadata schema","European Language Grid platform","ELG - SHARE"]},{"ID":"perez-estruch-etal-2017-learning","title":"Learning Multimodal Gender Profile using Neural Networks","abstract":"Gender identification in social networks is one of the most popular aspects of user profile learning. Traditionally it has been linked to author profiling, a difficult problem to solve because of the little difference in the use of language between genders. This situation has led to the need of taking into account other information apart from textual data, favoring the emergence of multimodal data. The aim of this paper is to apply neural networks to perform data fusion, using an existing multimodal corpus, the NUS-MSS data set, that (not only) contains text data, but also image and location information. We improved previous results in terms of macro accuracy (87.8{\\%}) obtaining the state-of-the-art performance of 91.3{\\%}.","year":2017,"title_abstract":"Learning Multimodal Gender Profile using Neural Networks Gender identification in social networks is one of the most popular aspects of user profile learning. Traditionally it has been linked to author profiling, a difficult problem to solve because of the little difference in the use of language between genders. This situation has led to the need of taking into account other information apart from textual data, favoring the emergence of multimodal data. The aim of this paper is to apply neural networks to perform data fusion, using an existing multimodal corpus, the NUS-MSS data set, that (not only) contains text data, but also image and location information. We improved previous results in terms of macro accuracy (87.8{\\%}) obtaining the state-of-the-art performance of 91.3{\\%}.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1635787785,"Goal":"Gender Equality","Task":["Learning Multimodal Gender Profile","Gender identification","social networks","user profile learning","author profiling","data fusion"],"Method":["Neural Networks","neural networks"]},{"ID":"rueter-etal-2021-apurina","title":"{A}purin{\\~a} {U}niversal {D}ependencies Treebank","abstract":"This paper presents and discusses the first Universal Dependencies treebank for the Apurin{\\~a} language. The treebank contains 76 fully annotated sentences, applies 14 parts-of-speech, as well as seven augmented or new features {---} some of which are unique to Apurin{\\~a}. The construction of the treebank has also served as an opportunity to develop finite-state description of the language and facilitate the transfer of open-source infrastructure possibilities to an endangered language of the Amazon. The source materials used in the initial treebank represent fieldwork practices where not all tokens of all sentences are equally annotated. For this reason, establishing regular annotation practices for the entire Apurin{\\~a} treebank is an ongoing project.","year":2021,"title_abstract":"{A}purin{\\~a} {U}niversal {D}ependencies Treebank This paper presents and discusses the first Universal Dependencies treebank for the Apurin{\\~a} language. The treebank contains 76 fully annotated sentences, applies 14 parts-of-speech, as well as seven augmented or new features {---} some of which are unique to Apurin{\\~a}. The construction of the treebank has also served as an opportunity to develop finite-state description of the language and facilitate the transfer of open-source infrastructure possibilities to an endangered language of the Amazon. The source materials used in the initial treebank represent fieldwork practices where not all tokens of all sentences are equally annotated. For this reason, establishing regular annotation practices for the entire Apurin{\\~a} treebank is an ongoing project.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1635771692,"Goal":"Life on Land","Task":["open - source infrastructure possibilities","annotation"],"Method":["finite - state description"]},{"ID":"larasati-2012-improving","title":"Improving Word Alignment by Exploiting Adapted Word Similarity","abstract":"This paper presents a method to improve a word alignment model in a phrase-based Statistical Machine Translation system for a low-resourced language using a string similarity approach. Our method captures similar words that can be seen as semi-monolingual across languages, such as numbers, named entities, and adapted\/loan words. We use several string similarity metrics to measure the monolinguality of the words, such as Longest Common Subsequence Ratio (LCSR), Minimum Edit Distance Ratio (MEDR), and we also use a modified BLEU Score (modBLEU). Our approach is to add intersecting alignment points for word pairs that are orthographically similar, before applying a word alignment heuristic, to generate a better word alignment. We demonstrate this approach on Indonesian-to-English translation task, where the languages share many similar words that are poorly aligned given a limited training data. This approach gives a statistically significant improvement by up to 0.66 in terms of BLEU score.","year":2012,"title_abstract":"Improving Word Alignment by Exploiting Adapted Word Similarity This paper presents a method to improve a word alignment model in a phrase-based Statistical Machine Translation system for a low-resourced language using a string similarity approach. Our method captures similar words that can be seen as semi-monolingual across languages, such as numbers, named entities, and adapted\/loan words. We use several string similarity metrics to measure the monolinguality of the words, such as Longest Common Subsequence Ratio (LCSR), Minimum Edit Distance Ratio (MEDR), and we also use a modified BLEU Score (modBLEU). Our approach is to add intersecting alignment points for word pairs that are orthographically similar, before applying a word alignment heuristic, to generate a better word alignment. We demonstrate this approach on Indonesian-to-English translation task, where the languages share many similar words that are poorly aligned given a limited training data. This approach gives a statistically significant improvement by up to 0.66 in terms of BLEU score.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1635632664,"Goal":"Gender Equality","Task":["Word Alignment","word alignment","Indonesian - to - English translation task"],"Method":["Adapted Word Similarity","word alignment model","phrase - based Statistical Machine Translation system","string similarity approach","word alignment heuristic"]},{"ID":"jha-etal-2022-curaj","title":"{CURAJ}{\\_}{IIITDWD}@{LT}-{EDI}-{ACL} 2022: Hope Speech Detection in {E}nglish {Y}ou{T}ube Comments using Deep Learning Techniques","abstract":"Hope Speech are positive terms that help to promote or criticise a point of view without hurting the user{'}s or community{'}s feelings. Non-Hope Speech, on the other side, includes expressions that are harsh, ridiculing, or demotivating. The goal of this article is to find the hope speech comments in a YouTube dataset. The datasets were created as part of the {``}LT-EDI-ACL 2022: Hope Speech Detection for Equality, Diversity, and Inclusion{''} shared task. The shared task dataset was proposed in Malayalam, Tamil, English, Spanish, and Kannada languages. In this paper, we worked at English-language YouTube comments. We employed several deep learning based models such as DNN (dense or fully connected neural network), CNN (Convolutional Neural Network), Bi-LSTM (Bidirectional Long Short Term Memory Network), and GRU(Gated Recurrent Unit) to identify the hopeful comments. We also used Stacked LSTM-CNN and Stacked LSTM-LSTM network to train the model. The best macro average F1-score 0.67 for development dataset was obtained using the DNN model. The macro average F1-score of 0.67 was achieved for the classification done on the test data as well.","year":2022,"title_abstract":"{CURAJ}{\\_}{IIITDWD}@{LT}-{EDI}-{ACL} 2022: Hope Speech Detection in {E}nglish {Y}ou{T}ube Comments using Deep Learning Techniques Hope Speech are positive terms that help to promote or criticise a point of view without hurting the user{'}s or community{'}s feelings. Non-Hope Speech, on the other side, includes expressions that are harsh, ridiculing, or demotivating. The goal of this article is to find the hope speech comments in a YouTube dataset. The datasets were created as part of the {``}LT-EDI-ACL 2022: Hope Speech Detection for Equality, Diversity, and Inclusion{''} shared task. The shared task dataset was proposed in Malayalam, Tamil, English, Spanish, and Kannada languages. In this paper, we worked at English-language YouTube comments. We employed several deep learning based models such as DNN (dense or fully connected neural network), CNN (Convolutional Neural Network), Bi-LSTM (Bidirectional Long Short Term Memory Network), and GRU(Gated Recurrent Unit) to identify the hopeful comments. We also used Stacked LSTM-CNN and Stacked LSTM-LSTM network to train the model. The best macro average F1-score 0.67 for development dataset was obtained using the DNN model. The macro average F1-score of 0.67 was achieved for the classification done on the test data as well.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1635356545,"Goal":"Gender Equality","Task":["Hope Speech Detection","Hope Speech Detection","Equality","classification"],"Method":["Deep Learning Techniques","deep learning based models","DNN (dense or fully connected neural network)","CNN (Convolutional Neural Network)","Bi - LSTM (Bidirectional Long Short Term Memory Network)","GRU(Gated Recurrent Unit)","LSTM","CNN","Stacked LSTM - LSTM","DNN model"]},{"ID":"bordoni-mazzoli-2006-towards","title":"Towards an Ontology for Art and Colours","abstract":"To meet a variety of needs in information modeling, software development and integration as well as knowledge management and reuse, various groups within industry, academia, and government have been developing and deploying sharable and reusable models known as ontologies. Ontologies play an important role in knowledge representation. In this paper, we address the problem of capturing knowledge needed for indexing and retrieving art resources. We describe a case study in which we attempt to construct an ontology for a subset of art. The aim of the present ontology is to build an extensible repository of knowledge and information about artists, their works and materials used in artistic creations. Influenced by the recent interest in colours and colouring materials, mainly shared by French researchers and linguists, an ontology prototype has been developed using Prot{\\'e}g{\\'e}. It allows to organize and catalog information about artists, art works, colouring materials and related colours.","year":2006,"title_abstract":"Towards an Ontology for Art and Colours To meet a variety of needs in information modeling, software development and integration as well as knowledge management and reuse, various groups within industry, academia, and government have been developing and deploying sharable and reusable models known as ontologies. Ontologies play an important role in knowledge representation. In this paper, we address the problem of capturing knowledge needed for indexing and retrieving art resources. We describe a case study in which we attempt to construct an ontology for a subset of art. The aim of the present ontology is to build an extensible repository of knowledge and information about artists, their works and materials used in artistic creations. Influenced by the recent interest in colours and colouring materials, mainly shared by French researchers and linguists, an ontology prototype has been developed using Prot{\\'e}g{\\'e}. It allows to organize and catalog information about artists, art works, colouring materials and related colours.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.1635021269,"Goal":"Life Below Water","Task":["Art and Colours","information modeling","software development and integration","knowledge management and reuse","knowledge representation","capturing knowledge","indexing and retrieving art resources","art","artistic creations"],"Method":["sharable and reusable models","Ontologies","ontology prototype","Prot{\\'e}g{\\'e}"]},{"ID":"sadoun-etal-2016-multital","title":"The {M}ulti{T}al {NLP} tool infrastructure","abstract":"This paper gives an overview of the MultiTal project, which aims to create a research infrastructure that ensures long-term distribution of NLP tools descriptions. The goal is to make NLP tools more accessible and usable to end-users of different disciplines. The infrastructure is built on a meta-data scheme modelling and standardising multilingual NLP tools documentation. The model is conceptualised using an OWL ontology. The formal representation of the ontology allows us to automatically generate organised and structured documentation in different languages for each represented tool.","year":2016,"title_abstract":"The {M}ulti{T}al {NLP} tool infrastructure This paper gives an overview of the MultiTal project, which aims to create a research infrastructure that ensures long-term distribution of NLP tools descriptions. The goal is to make NLP tools more accessible and usable to end-users of different disciplines. The infrastructure is built on a meta-data scheme modelling and standardising multilingual NLP tools documentation. The model is conceptualised using an OWL ontology. The formal representation of the ontology allows us to automatically generate organised and structured documentation in different languages for each represented tool.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1634785384,"Goal":"Partnership for the Goals","Task":["MultiTal project","NLP","NLP tools","multilingual NLP tools documentation"],"Method":["meta - data scheme","OWL ontology"]},{"ID":"khashabi-etal-2017-learning","title":"Learning What is Essential in Questions","abstract":"Question answering (QA) systems are easily distracted by irrelevant or redundant words in questions, especially when faced with long or multi-sentence questions in difficult domains. This paper introduces and studies the notion of essential question terms with the goal of improving such QA solvers. We illustrate the importance of essential question terms by showing that humans{'} ability to answer questions drops significantly when essential terms are eliminated from questions.We then develop a classifier that reliably (90{\\%} mean average precision) identifies and ranks essential terms in questions. Finally, we use the classifier to demonstrate that the notion of question term essentiality allows state-of-the-art QA solver for elementary-level science questions to make better and more informed decisions,improving performance by up to 5{\\%}.We also introduce a new dataset of over 2,200 crowd-sourced essential terms annotated science questions.","year":2017,"title_abstract":"Learning What is Essential in Questions Question answering (QA) systems are easily distracted by irrelevant or redundant words in questions, especially when faced with long or multi-sentence questions in difficult domains. This paper introduces and studies the notion of essential question terms with the goal of improving such QA solvers. We illustrate the importance of essential question terms by showing that humans{'} ability to answer questions drops significantly when essential terms are eliminated from questions.We then develop a classifier that reliably (90{\\%} mean average precision) identifies and ranks essential terms in questions. Finally, we use the classifier to demonstrate that the notion of question term essentiality allows state-of-the-art QA solver for elementary-level science questions to make better and more informed decisions,improving performance by up to 5{\\%}.We also introduce a new dataset of over 2,200 crowd-sourced essential terms annotated science questions.","social_need":"Clean Water and Sanitation Ensure availability and sustainable management of water and sanitation for all","cosine_similarity":0.1634726524,"Goal":"Clean Water and Sanitation","Task":["Learning What","Questions Question answering","QA","QA"],"Method":["classifier","classifier"]},{"ID":"giovanni-moller-etal-2020-nlp","title":"{NLP} North at {WNUT}-2020 Task 2: Pre-training versus Ensembling for Detection of Informative {COVID}-19 {E}nglish Tweets","abstract":"With the COVID-19 pandemic raging world-wide since the beginning of the 2020 decade, the need for monitoring systems to track relevant information on social media is vitally important. This paper describes our submission to the WNUT-2020 Task 2: Identification of informative COVID-19 English Tweets. We investigate the effectiveness for a variety of classification models, and found that domain-specific pre-trained BERT models lead to the best performance. On top of this, we attempt a variety of ensembling strategies, but these attempts did not lead to further improvements. Our final best model, the standalone CT-BERT model, proved to be highly competitive, leading to a shared first place in the shared task. Our results emphasize the importance of domain and task-related pre-training.","year":2020,"title_abstract":"{NLP} North at {WNUT}-2020 Task 2: Pre-training versus Ensembling for Detection of Informative {COVID}-19 {E}nglish Tweets With the COVID-19 pandemic raging world-wide since the beginning of the 2020 decade, the need for monitoring systems to track relevant information on social media is vitally important. This paper describes our submission to the WNUT-2020 Task 2: Identification of informative COVID-19 English Tweets. We investigate the effectiveness for a variety of classification models, and found that domain-specific pre-trained BERT models lead to the best performance. On top of this, we attempt a variety of ensembling strategies, but these attempts did not lead to further improvements. Our final best model, the standalone CT-BERT model, proved to be highly competitive, leading to a shared first place in the shared task. Our results emphasize the importance of domain and task-related pre-training.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1634628326,"Goal":"Climate Action","Task":["Detection of Informative","WNUT - 2020 Task","Identification of informative COVID - 19","pre - training"],"Method":["Pre - training","Ensembling","monitoring systems","classification models","BERT","ensembling strategies","standalone CT - BERT"]},{"ID":"tsekouras-etal-2019-social","title":"Social Web Observatory: An entity-driven, holistic information summarization platform across sources","abstract":"The Social Web Observatory is an entity-driven, sentiment-aware, event summarization web platform, combining various methods and tools to overview trends across social media and news sources in Greek. SWO crawls, clusters and summarizes information following an entity-centric view of text streams, allowing to monitor the public sentiment towards a specific person, organization or other entity. In this paper, we overview the platform, outline the analysis pipeline and describe a user study aimed to quantify the usefulness of the system and especially the meaningfulness and coherence of discovered events.","year":2019,"title_abstract":"Social Web Observatory: An entity-driven, holistic information summarization platform across sources The Social Web Observatory is an entity-driven, sentiment-aware, event summarization web platform, combining various methods and tools to overview trends across social media and news sources in Greek. SWO crawls, clusters and summarizes information following an entity-centric view of text streams, allowing to monitor the public sentiment towards a specific person, organization or other entity. In this paper, we overview the platform, outline the analysis pipeline and describe a user study aimed to quantify the usefulness of the system and especially the meaningfulness and coherence of discovered events.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1634325981,"Goal":"Sustainable Cities and Communities","Task":["Social Web Observatory","Social Web Observatory"],"Method":["entity - driven , holistic information summarization platform","sentiment - aware , event summarization web platform","SWO","analysis pipeline"]},{"ID":"qarqaz-etal-2021-r00","title":"R00 at {NLP}4{IF}-2021 Fighting {COVID}-19 Infodemic with Transformers and More Transformers","abstract":"This paper describes the winning model in the Arabic NLP4IF shared task for fighting the COVID-19 infodemic. The goal of the shared task is to check disinformation about COVID-19 in Arabic tweets. Our proposed model has been ranked 1st with an F1-Score of 0.780 and an Accuracy score of 0.762. A variety of transformer-based pre-trained language models have been experimented with through this study. The best-scored model is an ensemble of AraBERT-Base, Asafya-BERT, and ARBERT models. One of the study{'}s key findings is showing the effect the pre-processing can have on every model{'}s score. In addition to describing the winning model, the current study shows the error analysis.","year":2021,"title_abstract":"R00 at {NLP}4{IF}-2021 Fighting {COVID}-19 Infodemic with Transformers and More Transformers This paper describes the winning model in the Arabic NLP4IF shared task for fighting the COVID-19 infodemic. The goal of the shared task is to check disinformation about COVID-19 in Arabic tweets. Our proposed model has been ranked 1st with an F1-Score of 0.780 and an Accuracy score of 0.762. A variety of transformer-based pre-trained language models have been experimented with through this study. The best-scored model is an ensemble of AraBERT-Base, Asafya-BERT, and ARBERT models. One of the study{'}s key findings is showing the effect the pre-processing can have on every model{'}s score. In addition to describing the winning model, the current study shows the error analysis.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1634292603,"Goal":"Climate Action","Task":["shared task","error analysis"],"Method":["transformer - based pre - trained language models","AraBERT - Base","Asafya - BERT","ARBERT models","pre - processing"]},{"ID":"dragoni-etal-2014-modeling","title":"Modeling, Managing, Exposing, and Linking Ontologies with a {W}iki-based Tool","abstract":"In the last decade, the need of having effective and useful tools for the creation and the management of linguistic resources significantly increased. One of the main reasons is the necessity of building linguistic resources (LRs) that, besides the goal of expressing effectively the domain that users want to model, may be exploited in several ways. In this paper we present a wiki-based collaborative tool for modeling ontologies, and more in general any kind of linguistic resources, called MoKi. This tool has been customized in the context of an EU-funded project for addressing three important aspects of LRs modeling: (i) the exposure of the created LRs, (ii) for providing features for linking the created resources to external ones, and (iii) for producing multilingual LRs in a safe manner.","year":2014,"title_abstract":"Modeling, Managing, Exposing, and Linking Ontologies with a {W}iki-based Tool In the last decade, the need of having effective and useful tools for the creation and the management of linguistic resources significantly increased. One of the main reasons is the necessity of building linguistic resources (LRs) that, besides the goal of expressing effectively the domain that users want to model, may be exploited in several ways. In this paper we present a wiki-based collaborative tool for modeling ontologies, and more in general any kind of linguistic resources, called MoKi. This tool has been customized in the context of an EU-funded project for addressing three important aspects of LRs modeling: (i) the exposure of the created LRs, (ii) for providing features for linking the created resources to external ones, and (iii) for producing multilingual LRs in a safe manner.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.1632559299,"Goal":"Life Below Water","Task":["Modeling","Managing","Exposing","Linking Ontologies","management of linguistic resources","modeling ontologies","LRs modeling","multilingual LRs"],"Method":["{W}iki - based Tool","wiki - based collaborative tool","MoKi"]},{"ID":"jones-etal-2006-toward","title":"Toward an Interagency Language Roundtable Based Assessment of Speech-to-Speech Translation Capabilities","abstract":"We present observations from three exercises designed to map the effective listening and speaking skills of an operator of a speech-to-speech translation system (S2S) to the Interagency Language Roundtable (ILR) scale. Such a mapping is non-trivial, but will be useful for government and military decision makers in managing expectations of S2S technology. We observed domain-dependent S2S capabilities in the ILR range of Level 0+ to Level 1, and interactive text-based machine translation in the Level 3 range.","year":2006,"title_abstract":"Toward an Interagency Language Roundtable Based Assessment of Speech-to-Speech Translation Capabilities We present observations from three exercises designed to map the effective listening and speaking skills of an operator of a speech-to-speech translation system (S2S) to the Interagency Language Roundtable (ILR) scale. Such a mapping is non-trivial, but will be useful for government and military decision makers in managing expectations of S2S technology. We observed domain-dependent S2S capabilities in the ILR range of Level 0+ to Level 1, and interactive text-based machine translation in the Level 3 range.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.1632310599,"Goal":"Peace, Justice and Strong Institutions","Task":["Interagency Language Roundtable Based Assessment of Speech - to - Speech Translation Capabilities","listening and speaking skills","government and military decision makers","S2S","machine translation"],"Method":["speech - to - speech translation system","S2S"]},{"ID":"srivastava-goodman-2021-question","title":"Question Generation for Adaptive Education","abstract":"Intelligent and adaptive online education systems aim to make high-quality education available for a diverse range of students. However, existing systems usually depend on a pool of hand-made questions, limiting how fine-grained and open-ended they can be in adapting to individual students. We explore targeted question generation as a controllable sequence generation task. We first show how to fine-tune pre-trained language models for deep knowledge tracing (LM-KT). This model accurately predicts the probability of a student answering a question correctly, and generalizes to questions not seen in training. We then use LM-KT to specify the objective and data for training a model to generate questions conditioned on the student and target difficulty. Our results show we succeed at generating novel, well-calibrated language translation questions for second language learners from a real online education platform.","year":2021,"title_abstract":"Question Generation for Adaptive Education Intelligent and adaptive online education systems aim to make high-quality education available for a diverse range of students. However, existing systems usually depend on a pool of hand-made questions, limiting how fine-grained and open-ended they can be in adapting to individual students. We explore targeted question generation as a controllable sequence generation task. We first show how to fine-tune pre-trained language models for deep knowledge tracing (LM-KT). This model accurately predicts the probability of a student answering a question correctly, and generalizes to questions not seen in training. We then use LM-KT to specify the objective and data for training a model to generate questions conditioned on the student and target difficulty. Our results show we succeed at generating novel, well-calibrated language translation questions for second language learners from a real online education platform.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1631716788,"Goal":"Quality Education","Task":["Question Generation","Adaptive Education","Intelligent and adaptive online education systems","targeted question generation","controllable sequence generation task","deep knowledge tracing (LM - KT)","language translation questions","second language learners","online education platform"],"Method":["language models","LM - KT"]},{"ID":"takeshita-etal-2020-existing","title":"Can Existing Methods Debias Languages Other than {E}nglish? First Attempt to Analyze and Mitigate {J}apanese Word Embeddings","abstract":"It is known that word embeddings exhibit biases inherited from the corpus, and those biases reflect social stereotypes. Recently, many studies have been conducted to analyze and mitigate biases in word embeddings. Unsupervised Bias Enumeration (UBE) (Swinger et al., 2019) is one of approach to analyze biases for English, and Hard Debias (Bolukbasi et al., 2016) is the common technique to mitigate gender bias. These methods focused on English, or, in smaller extent, on Indo-European languages. However, it is not clear whether these methods can be generalized to other languages. In this paper, we apply these analyzing and mitigating methods, UBE and Hard Debias, to Japanese word embeddings. Additionally, we examine whether these methods can be used for Japanese. We experimentally show that UBE and Hard Debias cannot be sufficiently adapted to Japanese embeddings.","year":2020,"title_abstract":"Can Existing Methods Debias Languages Other than {E}nglish? First Attempt to Analyze and Mitigate {J}apanese Word Embeddings It is known that word embeddings exhibit biases inherited from the corpus, and those biases reflect social stereotypes. Recently, many studies have been conducted to analyze and mitigate biases in word embeddings. Unsupervised Bias Enumeration (UBE) (Swinger et al., 2019) is one of approach to analyze biases for English, and Hard Debias (Bolukbasi et al., 2016) is the common technique to mitigate gender bias. These methods focused on English, or, in smaller extent, on Indo-European languages. However, it is not clear whether these methods can be generalized to other languages. In this paper, we apply these analyzing and mitigating methods, UBE and Hard Debias, to Japanese word embeddings. Additionally, we examine whether these methods can be used for Japanese. We experimentally show that UBE and Hard Debias cannot be sufficiently adapted to Japanese embeddings.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1631642282,"Goal":"Gender Equality","Task":["word embeddings"],"Method":["Unsupervised Bias Enumeration","analyzing and mitigating methods","UBE","Hard Debias","UBE","Hard Debias"]},{"ID":"m-s-p-j-a-2020-nit","title":"{NIT}{\\_}{COVID}-19 at {WNUT}-2020 Task 2: Deep Learning Model {R}o{BERT}a for Identify Informative {COVID}-19 {E}nglish Tweets","abstract":"This paper presents the model submitted by NIT COVID-19 team for identified informative COVID-19 English tweets at WNUT-2020 Task2. This shared task addresses the problem of automatically identifying whether an English tweet related to informative (novel coronavirus) or not. These informative tweets provide information about recovered, confirmed, suspected, and death cases as well as location or travel history of the cases. The proposed approach includes pre-processing techniques and pre-trained RoBERTa with suitable hyperparameters for English coronavirus tweet classification. The performance achieved by the proposed model for shared task WNUT 2020 Task2 is 89.14{\\%} in the F1-score metric.","year":2020,"title_abstract":"{NIT}{\\_}{COVID}-19 at {WNUT}-2020 Task 2: Deep Learning Model {R}o{BERT}a for Identify Informative {COVID}-19 {E}nglish Tweets This paper presents the model submitted by NIT COVID-19 team for identified informative COVID-19 English tweets at WNUT-2020 Task2. This shared task addresses the problem of automatically identifying whether an English tweet related to informative (novel coronavirus) or not. These informative tweets provide information about recovered, confirmed, suspected, and death cases as well as location or travel history of the cases. The proposed approach includes pre-processing techniques and pre-trained RoBERTa with suitable hyperparameters for English coronavirus tweet classification. The performance achieved by the proposed model for shared task WNUT 2020 Task2 is 89.14{\\%} in the F1-score metric.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1631572247,"Goal":"Climate Action","Task":["WNUT - 2020 Task2","English coronavirus tweet classification","WNUT 2020 Task2"],"Method":["Deep Learning Model","pre - processing techniques","RoBERTa"]},{"ID":"wang-etal-2022-assessing","title":"Assessing Multilingual Fairness in Pre-trained Multimodal Representations","abstract":"Recently pre-trained multimodal models, such as CLIP, have shown exceptional capabilities towards connecting images and natural language. The textual representations in English can be desirably transferred to multilingualism and support downstream multimodal tasks for different languages. Nevertheless, the principle of multilingual fairness is rarely scrutinized: do multilingual multimodal models treat languages equally? Are their performances biased towards particular languages? To answer these questions, we view language as the fairness recipient and introduce two new fairness notions, multilingual individual fairness and multilingual group fairness, for pre-trained multimodal models. Multilingual individual fairness requires that text snippets expressing similar semantics in different languages connect similarly to images, while multilingual group fairness requires equalized predictive performance across languages. We characterize the extent to which pre-trained multilingual vision-and-language representations are individually fair across languages. However, extensive experiments demonstrate that multilingual representations do not satisfy group fairness: (1) there is a severe multilingual accuracy disparity issue; (2) the errors exhibit biases across languages conditioning the group of people in the images, including race, gender and age.","year":2022,"title_abstract":"Assessing Multilingual Fairness in Pre-trained Multimodal Representations Recently pre-trained multimodal models, such as CLIP, have shown exceptional capabilities towards connecting images and natural language. The textual representations in English can be desirably transferred to multilingualism and support downstream multimodal tasks for different languages. Nevertheless, the principle of multilingual fairness is rarely scrutinized: do multilingual multimodal models treat languages equally? Are their performances biased towards particular languages? To answer these questions, we view language as the fairness recipient and introduce two new fairness notions, multilingual individual fairness and multilingual group fairness, for pre-trained multimodal models. Multilingual individual fairness requires that text snippets expressing similar semantics in different languages connect similarly to images, while multilingual group fairness requires equalized predictive performance across languages. We characterize the extent to which pre-trained multilingual vision-and-language representations are individually fair across languages. However, extensive experiments demonstrate that multilingual representations do not satisfy group fairness: (1) there is a severe multilingual accuracy disparity issue; (2) the errors exhibit biases across languages conditioning the group of people in the images, including race, gender and age.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.163136512,"Goal":"Reduced Inequalities","Task":["Assessing Multilingual Fairness","downstream multimodal tasks","Multilingual individual fairness"],"Method":["Multimodal Representations","multimodal models","CLIP","textual representations","multilingual multimodal models","multimodal models","multilingual vision - and - language representations","multilingual representations"]},{"ID":"pham-etal-2021-multilingual","title":"Multilingual Speech Translation {KIT} @ {IWSLT}2021","abstract":"This paper contains the description for the submission of Karlsruhe Institute of Technology (KIT) for the multilingual TEDx translation task in the IWSLT 2021 evaluation campaign. Our main approach is to develop both cascade and end-to-end systems and eventually combine them together to achieve the best possible results for this extremely low-resource setting. The report also confirms certain consistent architectural improvement added to the Transformer architecture, for all tasks: translation, transcription and speech translation.","year":2021,"title_abstract":"Multilingual Speech Translation {KIT} @ {IWSLT}2021 This paper contains the description for the submission of Karlsruhe Institute of Technology (KIT) for the multilingual TEDx translation task in the IWSLT 2021 evaluation campaign. Our main approach is to develop both cascade and end-to-end systems and eventually combine them together to achieve the best possible results for this extremely low-resource setting. The report also confirms certain consistent architectural improvement added to the Transformer architecture, for all tasks: translation, transcription and speech translation.","social_need":"Industry, Innovation and Infrastrucure Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation","cosine_similarity":0.1631338596,"Goal":"Industry, Innovation and Infrastrucure","Task":["Multilingual Speech Translation","multilingual TEDx translation task","IWSLT 2021 evaluation campaign","translation","transcription","speech translation"],"Method":["cascade and end - to - end systems","Transformer architecture"]},{"ID":"londhe-srihari-2017-summarizing","title":"Summarizing World Speak : A Preliminary Graph Based Approach","abstract":"Social media platforms play a crucial role in piecing together global news stories via their corresponding online discussions. Thus, in this work, we introduce the problem of automatically summarizing massively multilingual microblog text streams. We discuss the challenges involved in both generating summaries as well as evaluating them. We introduce a simple word graph based approach that utilizes node neighborhoods to identify keyphrases and thus in turn, pick summary candidates. We also demonstrate the effectiveness of our method in generating precise summaries as compared to other popular techniques.","year":2017,"title_abstract":"Summarizing World Speak : A Preliminary Graph Based Approach Social media platforms play a crucial role in piecing together global news stories via their corresponding online discussions. Thus, in this work, we introduce the problem of automatically summarizing massively multilingual microblog text streams. We discuss the challenges involved in both generating summaries as well as evaluating them. We introduce a simple word graph based approach that utilizes node neighborhoods to identify keyphrases and thus in turn, pick summary candidates. We also demonstrate the effectiveness of our method in generating precise summaries as compared to other popular techniques.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.163127616,"Goal":"Partnership for the Goals","Task":["Summarizing World Speak","automatically summarizing massively multilingual microblog text streams","generating summaries","generating precise summaries"],"Method":["Graph Based Approach","word graph based approach"]},{"ID":"soares-etal-2020-qe","title":"{QE} Viewer: an Open-Source Tool for Visualization of Machine Translation Quality Estimation Results","abstract":"QE Viewer is a web-based tool for visualizing results of a Machine Translation Quality Estimation (QE) system. It allows users to see information on the predicted post-editing distance (PED) for a given file or sentence, and highlighted words that were predicted to contain MT errors. The tool can be used in a variety of academic, educational and commercial scenarios.","year":2020,"title_abstract":"{QE} Viewer: an Open-Source Tool for Visualization of Machine Translation Quality Estimation Results QE Viewer is a web-based tool for visualizing results of a Machine Translation Quality Estimation (QE) system. It allows users to see information on the predicted post-editing distance (PED) for a given file or sentence, and highlighted words that were predicted to contain MT errors. The tool can be used in a variety of academic, educational and commercial scenarios.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1631131917,"Goal":"Quality Education","Task":["Visualization of Machine Translation Quality Estimation","QE Viewer","Machine Translation Quality Estimation"],"Method":["Viewer","Open - Source Tool","web - based tool"]},{"ID":"hu-stoehr-2021-team","title":"Team {``}{N}o{C}onflict{''} at {CASE} 2021 Task 1: Pretraining for Sentence-Level Protest Event Detection","abstract":"An ever-increasing amount of text, in the form of social media posts and news articles, gives rise to new challenges and opportunities for the automatic extraction of socio-political events. In this paper, we present our submission to the Shared Tasks on Socio-Political and Crisis Events Detection, Task 1, Multilingual Protest News Detection, Subtask 2, Event Sentence Classification, of CASE @ ACL-IJCNLP 2021. In our submission, we utilize the RoBERTa model with additional pretraining, and achieve the best F1 score of 0.8532 in event sentence classification in English and the second-best F1 score of 0.8700 in Portuguese via simple translation. We analyze the failure cases of our model. We also conduct an ablation study to show the effect of choosing the right pretrained language model, adding additional training data and data augmentation.","year":2021,"title_abstract":"Team {``}{N}o{C}onflict{''} at {CASE} 2021 Task 1: Pretraining for Sentence-Level Protest Event Detection An ever-increasing amount of text, in the form of social media posts and news articles, gives rise to new challenges and opportunities for the automatic extraction of socio-political events. In this paper, we present our submission to the Shared Tasks on Socio-Political and Crisis Events Detection, Task 1, Multilingual Protest News Detection, Subtask 2, Event Sentence Classification, of CASE @ ACL-IJCNLP 2021. In our submission, we utilize the RoBERTa model with additional pretraining, and achieve the best F1 score of 0.8532 in event sentence classification in English and the second-best F1 score of 0.8700 in Portuguese via simple translation. We analyze the failure cases of our model. We also conduct an ablation study to show the effect of choosing the right pretrained language model, adding additional training data and data augmentation.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1630983353,"Goal":"Climate Action","Task":["Sentence - Level Protest Event Detection","automatic extraction of socio - political events","Socio - Political and Crisis Events Detection","Multilingual Protest News Detection","Event Sentence Classification","event sentence classification","translation","data augmentation"],"Method":["Pretraining","RoBERTa model","pretraining","pretrained language model"]},{"ID":"mundotiya-etal-2020-nlprl","title":"{NLPRL} at {WNUT}-2020 Task 2: {ELM}o-based System for Identification of {COVID}-19 Tweets","abstract":"The Coronavirus pandemic has been a dominating news on social media for the last many months. Efforts are being made to reduce its spread and reduce the casualties as well as new infections. For this purpose, the information about the infected people and their related symptoms, as available on social media, such as Twitter, can help in prevention and taking precautions. This is an example of using noisy text processing for disaster management. This paper discusses the NLPRL results in Shared Task-2 of WNUT-2020 workshop. We have considered this problem as a binary classification problem and have used a pre-trained ELMo embedding with GRU units. This approach helps classify the tweets with accuracy as 80.85{\\%} and 78.54{\\%} as F1-score on the provided test dataset. The experimental code is available online.","year":2020,"title_abstract":"{NLPRL} at {WNUT}-2020 Task 2: {ELM}o-based System for Identification of {COVID}-19 Tweets The Coronavirus pandemic has been a dominating news on social media for the last many months. Efforts are being made to reduce its spread and reduce the casualties as well as new infections. For this purpose, the information about the infected people and their related symptoms, as available on social media, such as Twitter, can help in prevention and taking precautions. This is an example of using noisy text processing for disaster management. This paper discusses the NLPRL results in Shared Task-2 of WNUT-2020 workshop. We have considered this problem as a binary classification problem and have used a pre-trained ELMo embedding with GRU units. This approach helps classify the tweets with accuracy as 80.85{\\%} and 78.54{\\%} as F1-score on the provided test dataset. The experimental code is available online.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1630649269,"Goal":"Climate Action","Task":["Identification of {COVID} - 19 Tweets","Coronavirus pandemic","prevention","precautions","noisy text processing","disaster management","NLPRL","WNUT - 2020 workshop","binary classification problem"],"Method":["ELMo embedding","GRU units"]},{"ID":"rodriguez-boyd-graber-2021-evaluation","title":"Evaluation Paradigms in Question Answering","abstract":"Question answering (QA) primarily descends from two branches of research: (1) Alan Turing{'}s investigation of machine intelligence at Manchester University and (2) Cyril Cleverdon{'}s comparison of library card catalog indices at Cranfield University. This position paper names and distinguishes these paradigms. Despite substantial overlap, subtle but significant distinctions exert an outsize influence on research. While one evaluation paradigm values creating more intelligent QA systems, the other paradigm values building QA systems that appeal to users. By better understanding the epistemic heritage of QA, researchers, academia, and industry can more effectively accelerate QA research.","year":2021,"title_abstract":"Evaluation Paradigms in Question Answering Question answering (QA) primarily descends from two branches of research: (1) Alan Turing{'}s investigation of machine intelligence at Manchester University and (2) Cyril Cleverdon{'}s comparison of library card catalog indices at Cranfield University. This position paper names and distinguishes these paradigms. Despite substantial overlap, subtle but significant distinctions exert an outsize influence on research. While one evaluation paradigm values creating more intelligent QA systems, the other paradigm values building QA systems that appeal to users. By better understanding the epistemic heritage of QA, researchers, academia, and industry can more effectively accelerate QA research.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1630638391,"Goal":"Quality Education","Task":["Question Answering","Question answering","machine intelligence","QA","QA","QA","QA research"],"Method":["Evaluation Paradigms","evaluation paradigm"]},{"ID":"jean-etal-2017-neural","title":"Neural Machine Translation for Cross-Lingual Pronoun Prediction","abstract":"In this paper we present our systems for the DiscoMT 2017 cross-lingual pronoun prediction shared task. For all four language pairs, we trained a standard attention-based neural machine translation system as well as three variants that incorporate information from the preceding source sentence. We show that our systems, which are not specifically designed for pronoun prediction and may be used to generate complete sentence translations, generally achieve competitive results on this task.","year":2017,"title_abstract":"Neural Machine Translation for Cross-Lingual Pronoun Prediction In this paper we present our systems for the DiscoMT 2017 cross-lingual pronoun prediction shared task. For all four language pairs, we trained a standard attention-based neural machine translation system as well as three variants that incorporate information from the preceding source sentence. We show that our systems, which are not specifically designed for pronoun prediction and may be used to generate complete sentence translations, generally achieve competitive results on this task.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1630499363,"Goal":"Gender Equality","Task":["Neural Machine Translation","Cross - Lingual Pronoun Prediction","DiscoMT","cross - lingual pronoun prediction shared task","pronoun prediction"],"Method":["attention - based neural machine translation system"]},{"ID":"estevanell-valladares-etal-2021-knowledge","title":"Knowledge Discovery in {COVID}-19 Research Literature","abstract":"This paper presents the preliminary results of an ongoing project that analyzes the growing body of scientific research published around the COVID-19 pandemic. In this research, a general-purpose semantic model is used to double annotate a batch of 500 sentences that were manually selected from the CORD-19 corpus. Afterwards, a baseline text-mining pipeline is designed and evaluated via a large batch of 100,959 sentences. We present a qualitative analysis of the most interesting facts automatically extracted and highlight possible future lines of development. The preliminary results show that general-purpose semantic models are a useful tool for discovering fine-grained knowledge in large corpora of scientific documents.","year":2021,"title_abstract":"Knowledge Discovery in {COVID}-19 Research Literature This paper presents the preliminary results of an ongoing project that analyzes the growing body of scientific research published around the COVID-19 pandemic. In this research, a general-purpose semantic model is used to double annotate a batch of 500 sentences that were manually selected from the CORD-19 corpus. Afterwards, a baseline text-mining pipeline is designed and evaluated via a large batch of 100,959 sentences. We present a qualitative analysis of the most interesting facts automatically extracted and highlight possible future lines of development. The preliminary results show that general-purpose semantic models are a useful tool for discovering fine-grained knowledge in large corpora of scientific documents.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1630051136,"Goal":"Climate Action","Task":["Knowledge Discovery","discovering fine - grained knowledge"],"Method":["general - purpose semantic model","text - mining pipeline","general - purpose semantic models"]},{"ID":"bouillon-liyanapathirana-2021-using","title":"Using speech technology in the translation process workflow in international organizations: A quantitative and qualitative study","abstract":"In international organizations, the growing demand for translations has increased the need for post-editing. Different studies show that automatic speech recognition systems have the potential to increase the productivity of the translation process as well as the quality. In this talk, we will explore the possibilities of using speech in the translation process by conducting a post-editing experiment with three professional translators in an international organization. Our experiment consisted of comparing three translation methods: speaking the translation with MT as an inspiration (RESpeaking), post-editing the MT suggestions by typing (PE), and editing the MT suggestion using speech (SPE). BLEU and HTER scores were used to compare the three methods. Our study shows that translators did more edits under condition RES, whereas in SPE, the resulting translations were closer to the reference according to the BLEU score and required less edits. Time taken to translate was the least in SPE followed by PE, RES methods and the translators preferred using speech to typing.These results show the potential of speech when it is coupled with post-editing.To the best of our knowledge, this is the first quantitative study conducted on using post-editing and speech together in large scale international organizations.","year":2021,"title_abstract":"Using speech technology in the translation process workflow in international organizations: A quantitative and qualitative study In international organizations, the growing demand for translations has increased the need for post-editing. Different studies show that automatic speech recognition systems have the potential to increase the productivity of the translation process as well as the quality. In this talk, we will explore the possibilities of using speech in the translation process by conducting a post-editing experiment with three professional translators in an international organization. Our experiment consisted of comparing three translation methods: speaking the translation with MT as an inspiration (RESpeaking), post-editing the MT suggestions by typing (PE), and editing the MT suggestion using speech (SPE). BLEU and HTER scores were used to compare the three methods. Our study shows that translators did more edits under condition RES, whereas in SPE, the resulting translations were closer to the reference according to the BLEU score and required less edits. Time taken to translate was the least in SPE followed by PE, RES methods and the translators preferred using speech to typing.These results show the potential of speech when it is coupled with post-editing.To the best of our knowledge, this is the first quantitative study conducted on using post-editing and speech together in large scale international organizations.","social_need":"Decent Work and Economic Growth Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all","cosine_similarity":0.1629818976,"Goal":"Decent Work and Economic Growth","Task":["translation process workflow","translations","post - editing","translation process","translation process","post - editing","SPE","post - editing","post - editing","speech","large scale international organizations"],"Method":["speech technology","automatic speech recognition systems","speech","translation methods","MT","SPE","PE","RES methods"]},{"ID":"mueller-etal-2008-knowledge","title":"Knowledge Sources for Bridging Resolution in Multi-Party Dialog","abstract":"In this paper we investigate the coverage of the two knowledge sources WordNet and Wikipedia for the task of bridging resolution. We report on an annotation experiment which yielded pairs of bridging anaphors and their antecedents in spoken multi-party dialog. Manual inspection of the two knowledge sources showed that, with some interesting exceptions, Wikipedia is superior to WordNet when it comes to the coverage of information necessary to resolve the bridging anaphors in our data set. We further describe a simple procedure for the automatic extraction of the required knowledge from Wikipedia by means of an API, and discuss some of the implications of the procedure\u0092s performance.","year":2008,"title_abstract":"Knowledge Sources for Bridging Resolution in Multi-Party Dialog In this paper we investigate the coverage of the two knowledge sources WordNet and Wikipedia for the task of bridging resolution. We report on an annotation experiment which yielded pairs of bridging anaphors and their antecedents in spoken multi-party dialog. Manual inspection of the two knowledge sources showed that, with some interesting exceptions, Wikipedia is superior to WordNet when it comes to the coverage of information necessary to resolve the bridging anaphors in our data set. We further describe a simple procedure for the automatic extraction of the required knowledge from Wikipedia by means of an API, and discuss some of the implications of the procedure\u0092s performance.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1629480273,"Goal":"Partnership for the Goals","Task":["Bridging Resolution","Multi - Party Dialog","bridging resolution","annotation"],"Method":["Knowledge Sources","WordNet"]},{"ID":"neves-etal-2017-assessing","title":"Assessing the performance of {O}lelo, a real-time biomedical question answering application","abstract":"Question answering (QA) can support physicians and biomedical researchers to find answers to their questions in the scientific literature. Such systems process large collections of documents in real time and include many natural language processing (NLP) procedures. We recently developed Olelo, a QA system for biomedicine which includes various NLP components, such as question processing, document and passage retrieval, answer processing and multi-document summarization. In this work, we present an evaluation of our system on the the fifth BioASQ challenge. We participated with the current state of the application and with an extension based on semantic role labeling that we are currently investigating. In addition to the BioASQ evaluation, we compared our system to other on-line biomedical QA systems in terms of the response time and the quality of the answers.","year":2017,"title_abstract":"Assessing the performance of {O}lelo, a real-time biomedical question answering application Question answering (QA) can support physicians and biomedical researchers to find answers to their questions in the scientific literature. Such systems process large collections of documents in real time and include many natural language processing (NLP) procedures. We recently developed Olelo, a QA system for biomedicine which includes various NLP components, such as question processing, document and passage retrieval, answer processing and multi-document summarization. In this work, we present an evaluation of our system on the the fifth BioASQ challenge. We participated with the current state of the application and with an extension based on semantic role labeling that we are currently investigating. In addition to the BioASQ evaluation, we compared our system to other on-line biomedical QA systems in terms of the response time and the quality of the answers.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1628937125,"Goal":"Life on Land","Task":["real - time biomedical question answering application Question answering","QA","biomedicine","question processing","document and passage retrieval","answer processing","multi - document summarization","semantic role labeling","BioASQ evaluation","QA"],"Method":["{O}lelo","natural language processing","Olelo","NLP components"]},{"ID":"hu-etal-2020-weibo","title":"{W}eibo-{COV}: A Large-Scale {COVID}-19 Social Media Dataset from {W}eibo","abstract":"With the rapid development of COVID-19 around the world, people are requested to maintain {``}social distance{''} and {``}stay at home{''}. In this scenario, extensive social interactions transfer to cyberspace, especially on social media platforms like Twitter and Sina Weibo. People generate posts to share information, express opinions and seek help during the pandemic outbreak, and these kinds of data on social media are valuable for studies to prevent COVID-19 transmissions, such as early warning and outbreaks detection. Therefore, in this paper, we release a novel and fine-grained large-scale COVID-19 social media dataset collected from Sina Weibo, named Weibo-COV, contains more than 40 million posts ranging from December 1, 2019 to April 30, 2020. Moreover, this dataset includes comprehensive information nuggets like post-level information, interactive information, location information, and repost network. We hope this dataset can promote studies of COVID-19 from multiple perspectives and enable better and rapid researches to suppress the spread of this pandemic.","year":2020,"title_abstract":"{W}eibo-{COV}: A Large-Scale {COVID}-19 Social Media Dataset from {W}eibo With the rapid development of COVID-19 around the world, people are requested to maintain {``}social distance{''} and {``}stay at home{''}. In this scenario, extensive social interactions transfer to cyberspace, especially on social media platforms like Twitter and Sina Weibo. People generate posts to share information, express opinions and seek help during the pandemic outbreak, and these kinds of data on social media are valuable for studies to prevent COVID-19 transmissions, such as early warning and outbreaks detection. Therefore, in this paper, we release a novel and fine-grained large-scale COVID-19 social media dataset collected from Sina Weibo, named Weibo-COV, contains more than 40 million posts ranging from December 1, 2019 to April 30, 2020. Moreover, this dataset includes comprehensive information nuggets like post-level information, interactive information, location information, and repost network. We hope this dataset can promote studies of COVID-19 from multiple perspectives and enable better and rapid researches to suppress the spread of this pandemic.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1628927886,"Goal":"Climate Action","Task":["COVID - 19","early warning and outbreaks detection","COVID - 19"],"Method":["COVID - 19"]},{"ID":"bar-haim-etal-2017-stance","title":"Stance Classification of Context-Dependent Claims","abstract":"Recent work has addressed the problem of detecting relevant claims for a given controversial topic. We introduce the complementary task of Claim Stance Classification, along with the first benchmark dataset for this task. We decompose this problem into: (a) open-domain target identification for topic and claim (b) sentiment classification for each target, and (c) open-domain contrast detection between the topic and the claim targets. Manual annotation of the dataset confirms the applicability and validity of our model. We describe an implementation of our model, focusing on a novel algorithm for contrast detection. Our approach achieves promising results, and is shown to outperform several baselines, which represent the common practice of applying a single, monolithic classifier for stance classification.","year":2017,"title_abstract":"Stance Classification of Context-Dependent Claims Recent work has addressed the problem of detecting relevant claims for a given controversial topic. We introduce the complementary task of Claim Stance Classification, along with the first benchmark dataset for this task. We decompose this problem into: (a) open-domain target identification for topic and claim (b) sentiment classification for each target, and (c) open-domain contrast detection between the topic and the claim targets. Manual annotation of the dataset confirms the applicability and validity of our model. We describe an implementation of our model, focusing on a novel algorithm for contrast detection. Our approach achieves promising results, and is shown to outperform several baselines, which represent the common practice of applying a single, monolithic classifier for stance classification.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1628457308,"Goal":"Climate Action","Task":["Stance Classification of Context - Dependent Claims","detecting relevant claims","complementary task","Claim Stance Classification","open - domain target identification","topic and claim","sentiment classification","open - domain contrast detection","contrast detection","stance classification"],"Method":["monolithic classifier"]},{"ID":"skachkova-kruijff-korbayova-2021-automatic","title":"Automatic Assignment of Semantic Frames in Disaster Response Team Communication Dialogues","abstract":"We investigate frame semantics as a meaning representation framework for team communication in a disaster response scenario. We focus on the automatic frame assignment and retrain PAFIBERT, which is one of the state-of-the-art frame classifiers, on English and German disaster response team communication data, obtaining accuracy around 90{\\%}. We examine the performance of both models and discuss their adjustments, such as sampling of additional training instances from an unrelated domain and adding extra lexical and discourse features to input token representations. We show that sampling has some positive effect on the German frame classifier, discuss an unexpected impact of extra features on the models{'} behaviour and perform a careful error analysis.","year":2021,"title_abstract":"Automatic Assignment of Semantic Frames in Disaster Response Team Communication Dialogues We investigate frame semantics as a meaning representation framework for team communication in a disaster response scenario. We focus on the automatic frame assignment and retrain PAFIBERT, which is one of the state-of-the-art frame classifiers, on English and German disaster response team communication data, obtaining accuracy around 90{\\%}. We examine the performance of both models and discuss their adjustments, such as sampling of additional training instances from an unrelated domain and adding extra lexical and discourse features to input token representations. We show that sampling has some positive effect on the German frame classifier, discuss an unexpected impact of extra features on the models{'} behaviour and perform a careful error analysis.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1627270281,"Goal":"Climate Action","Task":["Automatic Assignment of Semantic Frames","Disaster Response Team Communication Dialogues","team communication","disaster response scenario","error analysis"],"Method":["frame semantics","meaning representation framework","automatic frame assignment","retrain PAFIBERT","frame classifiers","token representations","sampling","German frame classifier"]},{"ID":"mino-etal-2021-nhks","title":"{NHK}{'}s Lexically-Constrained Neural Machine Translation at {WAT} 2021","abstract":"This paper describes the system of our team (NHK) for the WAT 2021 Japanese-English restricted machine translation task. In this task, the aim is to improve quality while maintaining consistent terminology for scientific paper translation. This task has a unique feature, where some words in a target sentence are given in addition to a source sentence. In this paper, we use a lexically-constrained neural machine translation (NMT), which concatenates the source sentence and constrained words with a special token to input them into the encoder of NMT. The key to the successful lexically-constrained NMT is the way to extract constraints from a target sentence of training data. We propose two extraction methods: proper-noun constraint and mistranslated-word constraint. These two methods consider the importance of words and fallibility of NMT, respectively. The evaluation results demonstrate the effectiveness of our lexical-constraint method.","year":2021,"title_abstract":"{NHK}{'}s Lexically-Constrained Neural Machine Translation at {WAT} 2021 This paper describes the system of our team (NHK) for the WAT 2021 Japanese-English restricted machine translation task. In this task, the aim is to improve quality while maintaining consistent terminology for scientific paper translation. This task has a unique feature, where some words in a target sentence are given in addition to a source sentence. In this paper, we use a lexically-constrained neural machine translation (NMT), which concatenates the source sentence and constrained words with a special token to input them into the encoder of NMT. The key to the successful lexically-constrained NMT is the way to extract constraints from a target sentence of training data. We propose two extraction methods: proper-noun constraint and mistranslated-word constraint. These two methods consider the importance of words and fallibility of NMT, respectively. The evaluation results demonstrate the effectiveness of our lexical-constraint method.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1627141684,"Goal":"Gender Equality","Task":["Lexically - Constrained Neural Machine Translation","WAT 2021 Japanese - English restricted machine translation task","translation"],"Method":["lexically - constrained neural machine translation","NMT","lexically - constrained NMT","extraction methods","NMT","lexical - constraint method"]},{"ID":"ultes-etal-2014-first","title":"First Insight into Quality-Adaptive Dialogue","abstract":"While Spoken Dialogue Systems have gained in importance in recent years, most systems applied in the real world are still static and error-prone. To overcome this, the user is put into the focus of dialogue management. Hence, an approach for adapting the course of the dialogue to Interaction Quality, an objective variant of user satisfaction, is presented in this work. In general, rendering the dialogue adaptive to user satisfaction enables the dialogue system to improve the course of the dialogue and to handle problematic situations better. In this contribution, we present a pilot study of quality-adaptive dialogue. By selecting the confirmation strategy based on the current IQ value, the course of the dialogue is adapted in order to improve the overall user experience. In a user experiment comparing three different confirmation strategies in a train booking domain, the adaptive strategy performs successful and is among the two best rated strategies based on the overall user experience.","year":2014,"title_abstract":"First Insight into Quality-Adaptive Dialogue While Spoken Dialogue Systems have gained in importance in recent years, most systems applied in the real world are still static and error-prone. To overcome this, the user is put into the focus of dialogue management. Hence, an approach for adapting the course of the dialogue to Interaction Quality, an objective variant of user satisfaction, is presented in this work. In general, rendering the dialogue adaptive to user satisfaction enables the dialogue system to improve the course of the dialogue and to handle problematic situations better. In this contribution, we present a pilot study of quality-adaptive dialogue. By selecting the confirmation strategy based on the current IQ value, the course of the dialogue is adapted in order to improve the overall user experience. In a user experiment comparing three different confirmation strategies in a train booking domain, the adaptive strategy performs successful and is among the two best rated strategies based on the overall user experience.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1625932157,"Goal":"Quality Education","Task":["Quality - Adaptive Dialogue","Spoken Dialogue Systems","dialogue management","quality - adaptive dialogue","train booking domain"],"Method":["dialogue system","confirmation strategy","confirmation strategies","adaptive strategy"]},{"ID":"emmery-etal-2017-simple","title":"Simple Queries as Distant Labels for Predicting Gender on {T}witter","abstract":"The majority of research on extracting missing user attributes from social media profiles use costly hand-annotated labels for supervised learning. Distantly supervised methods exist, although these generally rely on knowledge gathered using external sources. This paper demonstrates the effectiveness of gathering distant labels for self-reported gender on Twitter using simple queries. We confirm the reliability of this query heuristic by comparing with manual annotation. Moreover, using these labels for distant supervision, we demonstrate competitive model performance on the same data as models trained on manual annotations. As such, we offer a cheap, extensible, and fast alternative that can be employed beyond the task of gender classification.","year":2017,"title_abstract":"Simple Queries as Distant Labels for Predicting Gender on {T}witter The majority of research on extracting missing user attributes from social media profiles use costly hand-annotated labels for supervised learning. Distantly supervised methods exist, although these generally rely on knowledge gathered using external sources. This paper demonstrates the effectiveness of gathering distant labels for self-reported gender on Twitter using simple queries. We confirm the reliability of this query heuristic by comparing with manual annotation. Moreover, using these labels for distant supervision, we demonstrate competitive model performance on the same data as models trained on manual annotations. As such, we offer a cheap, extensible, and fast alternative that can be employed beyond the task of gender classification.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1625652313,"Goal":"Gender Equality","Task":["Predicting Gender","extracting missing user attributes","supervised learning","manual annotation","distant supervision","gender classification"],"Method":["Distantly supervised methods","query heuristic"]},{"ID":"baruah-mundotiya-2020-nlprl","title":"{NLPRL} {O}dia-{E}nglish: Indic Language Neural Machine Translation System","abstract":"In this manuscript, we (team name is NLPRL) describe systems description that was submitted to the translation shared tasks at WAT 2020. We describe our model as transformer based NMT by using byte-level based BPE (BBPE). We used the OdiEnCorp 2.0 parallel corpus provided by the shared task organizer where the training, validation, and test data contain 69370, 13544, and 14344 lines of parallel sentences, respectively. The evaluation results show the BLEU score of English-to-Oria below the Organizer (1.34) and Oria-to-English direction shows above the Organizer (11.33).","year":2020,"title_abstract":"{NLPRL} {O}dia-{E}nglish: Indic Language Neural Machine Translation System In this manuscript, we (team name is NLPRL) describe systems description that was submitted to the translation shared tasks at WAT 2020. We describe our model as transformer based NMT by using byte-level based BPE (BBPE). We used the OdiEnCorp 2.0 parallel corpus provided by the shared task organizer where the training, validation, and test data contain 69370, 13544, and 14344 lines of parallel sentences, respectively. The evaluation results show the BLEU score of English-to-Oria below the Organizer (1.34) and Oria-to-English direction shows above the Organizer (11.33).","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1625512242,"Goal":"Gender Equality","Task":["translation shared tasks"],"Method":["Indic Language Neural Machine Translation System","NLPRL)","transformer based NMT","byte - level based BPE"]},{"ID":"aker-etal-2012-assessing","title":"Assessing Crowdsourcing Quality through Objective Tasks","abstract":"The emergence of crowdsourcing as a commonly used approach to collect vast quantities of human assessments on a variety of tasks represents nothing less than a paradigm shift. This is particularly true in academic research where it has suddenly become possible to collect (high-quality) annotations rapidly without the need of an expert. In this paper we investigate factors which can influence the quality of the results obtained through Amazon's Mechanical Turk crowdsourcing platform. We investigated the impact of different presentation methods (free text versus radio buttons), workers' base (USA versus India as the main bases of MTurk workers) and payment scale (about {\\$}4, {\\$}8 and {\\$}10 per hour) on the quality of the results. For each run we assessed the results provided by 25 workers on a set of 10 tasks. We run two different experiments using objective tasks: maths and general text questions. In both tasks the answers are unique, which eliminates the uncertainty usually present in subjective tasks, where it is not clear whether the unexpected answer is caused by a lack of worker's motivation, the worker's interpretation of the task or genuine ambiguity. In this work we present our results comparing the influence of the different factors used. One of the interesting findings is that our results do not confirm previous studies which concluded that an increase in payment attracts more noise. We also find that the country of origin only has an impact in some of the categories and only in general text questions but there is no significant difference at the top pay.","year":2012,"title_abstract":"Assessing Crowdsourcing Quality through Objective Tasks The emergence of crowdsourcing as a commonly used approach to collect vast quantities of human assessments on a variety of tasks represents nothing less than a paradigm shift. This is particularly true in academic research where it has suddenly become possible to collect (high-quality) annotations rapidly without the need of an expert. In this paper we investigate factors which can influence the quality of the results obtained through Amazon's Mechanical Turk crowdsourcing platform. We investigated the impact of different presentation methods (free text versus radio buttons), workers' base (USA versus India as the main bases of MTurk workers) and payment scale (about {\\$}4, {\\$}8 and {\\$}10 per hour) on the quality of the results. For each run we assessed the results provided by 25 workers on a set of 10 tasks. We run two different experiments using objective tasks: maths and general text questions. In both tasks the answers are unique, which eliminates the uncertainty usually present in subjective tasks, where it is not clear whether the unexpected answer is caused by a lack of worker's motivation, the worker's interpretation of the task or genuine ambiguity. In this work we present our results comparing the influence of the different factors used. One of the interesting findings is that our results do not confirm previous studies which concluded that an increase in payment attracts more noise. We also find that the country of origin only has an impact in some of the categories and only in general text questions but there is no significant difference at the top pay.","social_need":"Decent Work and Economic Growth Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all","cosine_similarity":0.1623612046,"Goal":"Decent Work and Economic Growth","Task":["Assessing Crowdsourcing Quality","Objective Tasks","objective tasks","maths","subjective tasks","payment"],"Method":["crowdsourcing","Amazon's Mechanical Turk crowdsourcing platform","presentation methods"]},{"ID":"subramanian-etal-2021-fairness","title":"Fairness-aware Class Imbalanced Learning","abstract":"Class imbalance is a common challenge in many NLP tasks, and has clear connections to bias, in that bias in training data often leads to higher accuracy for majority groups at the expense of minority groups. However there has traditionally been a disconnect between research on class-imbalanced learning and mitigating bias, and only recently have the two been looked at through a common lens. In this work we evaluate long-tail learning methods for tweet sentiment and occupation classification, and extend a margin-loss based approach with methods to enforce fairness. We empirically show through controlled experiments that the proposed approaches help mitigate both class imbalance and demographic biases.","year":2021,"title_abstract":"Fairness-aware Class Imbalanced Learning Class imbalance is a common challenge in many NLP tasks, and has clear connections to bias, in that bias in training data often leads to higher accuracy for majority groups at the expense of minority groups. However there has traditionally been a disconnect between research on class-imbalanced learning and mitigating bias, and only recently have the two been looked at through a common lens. In this work we evaluate long-tail learning methods for tweet sentiment and occupation classification, and extend a margin-loss based approach with methods to enforce fairness. We empirically show through controlled experiments that the proposed approaches help mitigate both class imbalance and demographic biases.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1623518616,"Goal":"Gender Equality","Task":["Fairness - aware Class Imbalanced Learning","Class imbalance","NLP tasks","class - imbalanced learning","mitigating bias","tweet sentiment and occupation classification"],"Method":["long - tail learning methods","margin - loss based approach"]},{"ID":"yang-etal-2019-nonsense","title":"Nonsense!: Quality Control via Two-Step Reason Selection for Annotating Local Acceptability and Related Attributes in News Editorials","abstract":"Annotation quality control is a critical aspect for building reliable corpora through linguistic annotation. In this study, we present a simple but powerful quality control method using two-step reason selection. We gathered sentential annotations of local acceptance and three related attributes through a crowdsourcing platform. For each attribute, the reason for the choice of the attribute value is selected in a two-step manner. The options given for reason selection were designed to facilitate the detection of a nonsensical reason selection. We assume that a sentential annotation that contains a nonsensical reason is less reliable than the one without such reason. Our method, based solely on this assumption, is found to retain the annotations with satisfactory quality out of the entire annotations mixed with those of low quality.","year":2019,"title_abstract":"Nonsense!: Quality Control via Two-Step Reason Selection for Annotating Local Acceptability and Related Attributes in News Editorials Annotation quality control is a critical aspect for building reliable corpora through linguistic annotation. In this study, we present a simple but powerful quality control method using two-step reason selection. We gathered sentential annotations of local acceptance and three related attributes through a crowdsourcing platform. For each attribute, the reason for the choice of the attribute value is selected in a two-step manner. The options given for reason selection were designed to facilitate the detection of a nonsensical reason selection. We assume that a sentential annotation that contains a nonsensical reason is less reliable than the one without such reason. Our method, based solely on this assumption, is found to retain the annotations with satisfactory quality out of the entire annotations mixed with those of low quality.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.162345916,"Goal":"Sustainable Cities and Communities","Task":["Annotating Local Acceptability","Annotation quality control","linguistic annotation","reason selection","nonsensical reason selection"],"Method":["Quality Control","Reason Selection","quality control method","reason selection","crowdsourcing platform"]},{"ID":"montero-etal-2008-conceptual","title":"Conceptual Modeling of Ontology-based Linguistic Resources with a Focus on Semantic Relations","abstract":"Although ontologies and linguistic resources play a key role in applied AI and NLP, they have not been developed in a common and systematic way. The lack of a systematic methodology for their development has lead to the production of resources that exhibit common flaws between them, and that, at least when it come to ontologies, negatively impact their results and reusability. In this paper, we introduce a software-engineering methodology for the construction of ontology-based linguistic resources, and present a sound conceptual schema that takes into account several considerations for the construction of software tools that allow the systematic and controlled construction of ontology-based linguistic resources.","year":2008,"title_abstract":"Conceptual Modeling of Ontology-based Linguistic Resources with a Focus on Semantic Relations Although ontologies and linguistic resources play a key role in applied AI and NLP, they have not been developed in a common and systematic way. The lack of a systematic methodology for their development has lead to the production of resources that exhibit common flaws between them, and that, at least when it come to ontologies, negatively impact their results and reusability. In this paper, we introduce a software-engineering methodology for the construction of ontology-based linguistic resources, and present a sound conceptual schema that takes into account several considerations for the construction of software tools that allow the systematic and controlled construction of ontology-based linguistic resources.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.1622988731,"Goal":"Life Below Water","Task":["AI","NLP","ontology - based linguistic resources","systematic and controlled construction of ontology - based linguistic resources"],"Method":["Conceptual Modeling","Ontology - based Linguistic Resources","software - engineering methodology","conceptual schema","software tools"]},{"ID":"lu-2010-mt","title":"Where can {MT} be most successful and what are the best {MT} engines for various languages?","abstract":"CA{'}s globalization team has a long term goal of reaching fully loaded costs of 10 cents per word. Fully loaded costs include the costs incurred for translation, localization QA, engineering, project management, and overall management. While translation budgets are gradually decreasing and volumes increasing, machine translation becomes an alternative source to produce more with less. This paper describes how CA Technologies tries to accomplish this long term goal with the deployment of MT systems to increase productivity with less cost, in a relatively short time.","year":2010,"title_abstract":"Where can {MT} be most successful and what are the best {MT} engines for various languages? CA{'}s globalization team has a long term goal of reaching fully loaded costs of 10 cents per word. Fully loaded costs include the costs incurred for translation, localization QA, engineering, project management, and overall management. While translation budgets are gradually decreasing and volumes increasing, machine translation becomes an alternative source to produce more with less. This paper describes how CA Technologies tries to accomplish this long term goal with the deployment of MT systems to increase productivity with less cost, in a relatively short time.","social_need":"Decent Work and Economic Growth Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all","cosine_similarity":0.1622554362,"Goal":"Decent Work and Economic Growth","Task":["translation","localization QA","engineering","project management","overall management","translation","machine translation","MT"],"Method":["CA Technologies"]},{"ID":"garibo-i-orts-2019-multilingual","title":"Multilingual Detection of Hate Speech Against Immigrants and Women in {T}witter at {S}em{E}val-2019 Task 5: Frequency Analysis Interpolation for Hate in Speech Detection","abstract":"This document describes a text change of representation approach to the task of Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter, as part of SemEval-2019 1 . The task is divided in two sub-tasks. Sub-task A consists in classifying tweets as being hateful or not hateful, whereas sub-task B requires fine tuning the classification by classifying the hateful tweets as being directed to single individuals or generic, if the tweet is aggressive or not. Our approach consists of a change of the space of representation of text into statistical descriptors which characterize the text. In addition, dimensional reduction is performed to 6 characteristics per class in order to make the method suitable for a Big Data environment. Frequency Analysis Interpolation (FAI) is the approach we use to achieve rank 5th in Spanish language and 9th in English language in sub-task B in both cases.","year":2019,"title_abstract":"Multilingual Detection of Hate Speech Against Immigrants and Women in {T}witter at {S}em{E}val-2019 Task 5: Frequency Analysis Interpolation for Hate in Speech Detection This document describes a text change of representation approach to the task of Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter, as part of SemEval-2019 1 . The task is divided in two sub-tasks. Sub-task A consists in classifying tweets as being hateful or not hateful, whereas sub-task B requires fine tuning the classification by classifying the hateful tweets as being directed to single individuals or generic, if the tweet is aggressive or not. Our approach consists of a change of the space of representation of text into statistical descriptors which characterize the text. In addition, dimensional reduction is performed to 6 characteristics per class in order to make the method suitable for a Big Data environment. Frequency Analysis Interpolation (FAI) is the approach we use to achieve rank 5th in Spanish language and 9th in English language in sub-task B in both cases.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1622456461,"Goal":"Reduced Inequalities","Task":["Multilingual Detection of Hate Speech","Hate in Speech Detection","Multilingual Detection of Hate Speech","Sub - task","classification","dimensional reduction"],"Method":["Frequency Analysis Interpolation","text change of representation approach","space of representation of text","statistical descriptors","Frequency Analysis Interpolation"]},{"ID":"barreaux-besagni-2020-experiment","title":"An Experiment in Annotating Animal Species Names from {ISTEX} Resources","abstract":"To exploit scientific publications from global research for TDM purposes, the ISTEX platform enriched its data with value-added information to ease access to its full-text documents. We built an experiment to explore new enrichment possibilities in documents focussing on scientific named entities recognition which could be integrated into ISTEX resources. This led to testing two detection tools for animal species names in a corpus of 100 documents in zoology. This makes it possible to provide the French scientific community with an annotated reference corpus available for use to measure these tools{'} performance.","year":2020,"title_abstract":"An Experiment in Annotating Animal Species Names from {ISTEX} Resources To exploit scientific publications from global research for TDM purposes, the ISTEX platform enriched its data with value-added information to ease access to its full-text documents. We built an experiment to explore new enrichment possibilities in documents focussing on scientific named entities recognition which could be integrated into ISTEX resources. This led to testing two detection tools for animal species names in a corpus of 100 documents in zoology. This makes it possible to provide the French scientific community with an annotated reference corpus available for use to measure these tools{'} performance.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1622432321,"Goal":"Life on Land","Task":["TDM purposes","scientific named entities recognition","animal species names"],"Method":["ISTEX platform","detection tools"]},{"ID":"bosnjak-karan-2019-data","title":"Data Set for Stance and Sentiment Analysis from User Comments on {C}roatian News","abstract":"Nowadays it is becoming more important than ever to find new ways of extracting useful information from the evergrowing amount of user-generated data available online. In this paper, we describe the creation of a data set that contains news articles and corresponding comments from Croatian news outlet 24 sata. Our annotation scheme is specifically tailored for the task of detecting stances and sentiment from user comments as well as assessing if commentator claims are verifiable. Through this data, we hope to get a better understanding of the publics viewpoint on various events. In addition, we also explore the potential of applying supervised machine learning models toautomate annotation of more data.","year":2019,"title_abstract":"Data Set for Stance and Sentiment Analysis from User Comments on {C}roatian News Nowadays it is becoming more important than ever to find new ways of extracting useful information from the evergrowing amount of user-generated data available online. In this paper, we describe the creation of a data set that contains news articles and corresponding comments from Croatian news outlet 24 sata. Our annotation scheme is specifically tailored for the task of detecting stances and sentiment from user comments as well as assessing if commentator claims are verifiable. Through this data, we hope to get a better understanding of the publics viewpoint on various events. In addition, we also explore the potential of applying supervised machine learning models toautomate annotation of more data.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1622256637,"Goal":"Climate Action","Task":["Stance and Sentiment Analysis","detecting stances and sentiment","annotation"],"Method":["annotation scheme","supervised machine learning models"]},{"ID":"li-etal-2021-improving-stance","title":"Improving Stance Detection with Multi-Dataset Learning and Knowledge Distillation","abstract":"Stance detection determines whether the author of a text is in favor of, against or neutral to a specific target and provides valuable insights into important events such as legalization of abortion. Despite significant progress on this task, one of the remaining challenges is the scarcity of annotations. Besides, most previous works focused on a hard-label training in which meaningful similarities among categories are discarded during training. To address these challenges, first, we evaluate a multi-target and a multi-dataset training settings by training one model on each dataset and datasets of different domains, respectively. We show that models can learn more universal representations with respect to targets in these settings. Second, we investigate the knowledge distillation in stance detection and observe that transferring knowledge from a teacher model to a student model can be beneficial in our proposed training settings. Moreover, we propose an Adaptive Knowledge Distillation (AKD) method that applies instance-specific temperature scaling to the teacher and student predictions. Results show that the multi-dataset model performs best on all datasets and it can be further improved by the proposed AKD, outperforming the state-of-the-art by a large margin. We publicly release our code.","year":2021,"title_abstract":"Improving Stance Detection with Multi-Dataset Learning and Knowledge Distillation Stance detection determines whether the author of a text is in favor of, against or neutral to a specific target and provides valuable insights into important events such as legalization of abortion. Despite significant progress on this task, one of the remaining challenges is the scarcity of annotations. Besides, most previous works focused on a hard-label training in which meaningful similarities among categories are discarded during training. To address these challenges, first, we evaluate a multi-target and a multi-dataset training settings by training one model on each dataset and datasets of different domains, respectively. We show that models can learn more universal representations with respect to targets in these settings. Second, we investigate the knowledge distillation in stance detection and observe that transferring knowledge from a teacher model to a student model can be beneficial in our proposed training settings. Moreover, we propose an Adaptive Knowledge Distillation (AKD) method that applies instance-specific temperature scaling to the teacher and student predictions. Results show that the multi-dataset model performs best on all datasets and it can be further improved by the proposed AKD, outperforming the state-of-the-art by a large margin. We publicly release our code.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1622250676,"Goal":"Climate Action","Task":["Stance Detection","Multi - Dataset Learning","Knowledge Distillation","Stance detection","legalization of abortion","hard - label training","knowledge distillation","stance detection","teacher and student predictions"],"Method":["universal representations","teacher model","student model","Adaptive Knowledge Distillation","instance - specific temperature scaling","multi - dataset model","AKD"]},{"ID":"niehues-etal-2018-iwslt","title":"The {IWSLT} 2018 Evaluation Campaign","abstract":"The International Workshop of Spoken Language Translation (IWSLT) 2018 Evaluation Campaign featured two tasks: low-resource machine translation and speech translation. In the first task, manually transcribed speech had to be translated from Basque to English. Since this translation direction is a under-resourced language pair, participants were encouraged to use additional parallel data from related languages. In the second task, participants had to translate English audio into German text with a full speech-translation system. In the baseline condition, participants were free to use composite architectures, while in the end-to-end condition they were restricted to use a single model for the task. This year, eight research groups took part in the low-resource machine translation task and nine in the speech translation task.","year":2018,"title_abstract":"The {IWSLT} 2018 Evaluation Campaign The International Workshop of Spoken Language Translation (IWSLT) 2018 Evaluation Campaign featured two tasks: low-resource machine translation and speech translation. In the first task, manually transcribed speech had to be translated from Basque to English. Since this translation direction is a under-resourced language pair, participants were encouraged to use additional parallel data from related languages. In the second task, participants had to translate English audio into German text with a full speech-translation system. In the baseline condition, participants were free to use composite architectures, while in the end-to-end condition they were restricted to use a single model for the task. This year, eight research groups took part in the low-resource machine translation task and nine in the speech translation task.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1622155607,"Goal":"Gender Equality","Task":["International Workshop of Spoken Language Translation","2018 Evaluation Campaign","low - resource machine translation","speech translation","translation direction","translation","low - resource machine translation task","speech translation task"],"Method":["composite architectures"]},{"ID":"cromieres-etal-2016-kyoto","title":"{K}yoto {U}niversity Participation to {WAT} 2016","abstract":"We describe here our approaches and results on the WAT 2016 shared translation tasks. We tried to use both an example-based machine translation (MT) system and a neural MT system. We report very good translation results, especially when using neural MT for Chinese-to-Japanese translation.","year":2016,"title_abstract":"{K}yoto {U}niversity Participation to {WAT} 2016 We describe here our approaches and results on the WAT 2016 shared translation tasks. We tried to use both an example-based machine translation (MT) system and a neural MT system. We report very good translation results, especially when using neural MT for Chinese-to-Japanese translation.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1620504558,"Goal":"Gender Equality","Task":["WAT 2016 shared translation tasks","translation","Chinese - to - Japanese translation"],"Method":["example - based machine translation (MT) system","neural MT system","neural MT"]},{"ID":"lee-etal-2020-empowering","title":"{E}mpowering {A}ctive {L}earning to {J}ointly {O}ptimize {S}ystem and {U}ser {D}emands","abstract":"Existing approaches to active learning maximize the system performance by sampling unlabeled instances for annotation that yield the most efficient training. However, when active learning is integrated with an end-user application, this can lead to frustration for participating users, as they spend time labeling instances that they would not otherwise be interested in reading. In this paper, we propose a new active learning approach that jointly optimizes the seemingly counteracting objectives of the active learning system (training efficiently) and the user (receiving useful instances). We study our approach in an educational application, which particularly benefits from this technique as the system needs to rapidly learn to predict the appropriateness of an exercise to a particular user, while the users should receive only exercises that match their skills. We evaluate multiple learning strategies and user types with data from real users and find that our joint approach better satisfies both objectives when alternative methods lead to many unsuitable exercises for end users.","year":2020,"title_abstract":"{E}mpowering {A}ctive {L}earning to {J}ointly {O}ptimize {S}ystem and {U}ser {D}emands Existing approaches to active learning maximize the system performance by sampling unlabeled instances for annotation that yield the most efficient training. However, when active learning is integrated with an end-user application, this can lead to frustration for participating users, as they spend time labeling instances that they would not otherwise be interested in reading. In this paper, we propose a new active learning approach that jointly optimizes the seemingly counteracting objectives of the active learning system (training efficiently) and the user (receiving useful instances). We study our approach in an educational application, which particularly benefits from this technique as the system needs to rapidly learn to predict the appropriateness of an exercise to a particular user, while the users should receive only exercises that match their skills. We evaluate multiple learning strategies and user types with data from real users and find that our joint approach better satisfies both objectives when alternative methods lead to many unsuitable exercises for end users.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1620468646,"Goal":"Quality Education","Task":["active learning","annotation","end - user application","educational application"],"Method":["active learning","active learning approach","active learning system","learning strategies"]},{"ID":"morarescu-2006-principles","title":"Principles for annotating and reasoning with spatial information","abstract":"In this paper we present the first phase of the ongoing SpaceBank project that attempts to create a linguistic resource for annotating and reasoning with spatial information from text. SpaceBank is the spatial counterpart of TimeBank, an electronic resource for temporal semantics and reasoning. The paper focuses on building an ontology of lexicalized spatial concepts. The textual occurrences of the concepts in this ontology will be annotated using the SpaceML language, briefly described here. SpaceBank is designed to be integrated with TimeBank, for a spatio-temporal model of the textual information.","year":2006,"title_abstract":"Principles for annotating and reasoning with spatial information In this paper we present the first phase of the ongoing SpaceBank project that attempts to create a linguistic resource for annotating and reasoning with spatial information from text. SpaceBank is the spatial counterpart of TimeBank, an electronic resource for temporal semantics and reasoning. The paper focuses on building an ontology of lexicalized spatial concepts. The textual occurrences of the concepts in this ontology will be annotated using the SpaceML language, briefly described here. SpaceBank is designed to be integrated with TimeBank, for a spatio-temporal model of the textual information.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1619992554,"Goal":"Sustainable Cities and Communities","Task":["annotating and reasoning with spatial information","SpaceBank project","annotating and reasoning with spatial information","temporal semantics","reasoning","ontology of lexicalized spatial concepts","textual information"],"Method":["linguistic resource","SpaceBank","TimeBank","SpaceML language","SpaceBank","TimeBank","spatio - temporal model"]},{"ID":"kiritchenko-mohammad-2016-happy","title":"Happy Accident: A Sentiment Composition Lexicon for Opposing Polarity Phrases","abstract":"Sentiment composition is the determining of sentiment of a multi-word linguistic unit, such as a phrase or a sentence, based on its constituents. We focus on sentiment composition in phrases formed by at least one positive and at least one negative word \u2015 phrases like {`}happy accident{'} and {`}best winter break{'}. We refer to such phrases as opposing polarity phrases. We manually annotate a collection of opposing polarity phrases and their constituent single words with real-valued sentiment intensity scores using a method known as Best\u2015Worst Scaling. We show that the obtained annotations are consistent. We explore the entries in the lexicon for linguistic regularities that govern sentiment composition in opposing polarity phrases. Finally, we list the current and possible future applications of the lexicon.","year":2016,"title_abstract":"Happy Accident: A Sentiment Composition Lexicon for Opposing Polarity Phrases Sentiment composition is the determining of sentiment of a multi-word linguistic unit, such as a phrase or a sentence, based on its constituents. We focus on sentiment composition in phrases formed by at least one positive and at least one negative word \u2015 phrases like {`}happy accident{'} and {`}best winter break{'}. We refer to such phrases as opposing polarity phrases. We manually annotate a collection of opposing polarity phrases and their constituent single words with real-valued sentiment intensity scores using a method known as Best\u2015Worst Scaling. We show that the obtained annotations are consistent. We explore the entries in the lexicon for linguistic regularities that govern sentiment composition in opposing polarity phrases. Finally, we list the current and possible future applications of the lexicon.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1619833708,"Goal":"Reduced Inequalities","Task":["Opposing Polarity Phrases","Sentiment composition","determining of sentiment","sentiment composition","sentiment composition"],"Method":["Sentiment Composition Lexicon","Best\u2015Worst Scaling"]},{"ID":"ashby-etal-2021-results","title":"Results of the Second {SIGMORPHON} Shared Task on Multilingual Grapheme-to-Phoneme Conversion","abstract":"Grapheme-to-phoneme conversion is an important component in many speech technologies, but until recently there were no multilingual benchmarks for this task. The second iteration of the SIGMORPHON shared task on multilingual grapheme-to-phoneme conversion features many improvements from the previous year{'}s task (Gorman et al. 2020), including additional languages, a stronger baseline, three subtasks varying the amount of available resources, extensive quality assurance procedures, and automated error analyses. Four teams submitted a total of thirteen systems, at best achieving relative reductions of word error rate of 11{\\%} in the high-resource subtask and 4{\\%} in the low-resource subtask.","year":2021,"title_abstract":"Results of the Second {SIGMORPHON} Shared Task on Multilingual Grapheme-to-Phoneme Conversion Grapheme-to-phoneme conversion is an important component in many speech technologies, but until recently there were no multilingual benchmarks for this task. The second iteration of the SIGMORPHON shared task on multilingual grapheme-to-phoneme conversion features many improvements from the previous year{'}s task (Gorman et al. 2020), including additional languages, a stronger baseline, three subtasks varying the amount of available resources, extensive quality assurance procedures, and automated error analyses. Four teams submitted a total of thirteen systems, at best achieving relative reductions of word error rate of 11{\\%} in the high-resource subtask and 4{\\%} in the low-resource subtask.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.161930114,"Goal":"Partnership for the Goals","Task":["Multilingual Grapheme - to - Phoneme Conversion","Grapheme - to - phoneme conversion","speech technologies","SIGMORPHON shared task","multilingual grapheme - to - phoneme conversion"],"Method":["automated error analyses"]},{"ID":"stoehr-etal-2021-classifying","title":"Classifying Dyads for Militarized Conflict Analysis","abstract":"Understanding the origins of militarized conflict is a complex, yet important undertaking. Existing research seeks to build this understanding by considering bi-lateral relationships between entity pairs (dyadic causes) and multi-lateral relationships among multiple entities (systemic causes). The aim of this work is to compare these two causes in terms of how they correlate with conflict between two entities. We do this by devising a set of textual and graph-based features which represent each of the causes. The features are extracted from Wikipedia and modeled as a large graph. Nodes in this graph represent entities connected by labeled edges representing ally or enemy-relationships. This allows casting the problem as an edge classification task, which we term dyad classification. We propose and evaluate classifiers to determine if a particular pair of entities are allies or enemies. Our results suggest that our systemic features might be slightly better correlates of conflict. Further, we find that Wikipedia articles of allies are semantically more similar than enemies.","year":2021,"title_abstract":"Classifying Dyads for Militarized Conflict Analysis Understanding the origins of militarized conflict is a complex, yet important undertaking. Existing research seeks to build this understanding by considering bi-lateral relationships between entity pairs (dyadic causes) and multi-lateral relationships among multiple entities (systemic causes). The aim of this work is to compare these two causes in terms of how they correlate with conflict between two entities. We do this by devising a set of textual and graph-based features which represent each of the causes. The features are extracted from Wikipedia and modeled as a large graph. Nodes in this graph represent entities connected by labeled edges representing ally or enemy-relationships. This allows casting the problem as an edge classification task, which we term dyad classification. We propose and evaluate classifiers to determine if a particular pair of entities are allies or enemies. Our results suggest that our systemic features might be slightly better correlates of conflict. Further, we find that Wikipedia articles of allies are semantically more similar than enemies.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.1619043648,"Goal":"Peace, Justice and Strong Institutions","Task":["Classifying Dyads","Militarized Conflict Analysis","militarized conflict","edge classification task","dyad classification"],"Method":["textual and graph - based features","classifiers"]},{"ID":"kocmi-etal-2017-cuni","title":"{CUNI} {NMT} System for {WAT} 2017 Translation Tasks","abstract":"The paper presents this year{'}s CUNI submissions to the WAT 2017 Translation Task focusing on the Japanese-English translation, namely Scientific papers subtask, Patents subtask and Newswire subtask. We compare two neural network architectures, the standard sequence-to-sequence with attention (Seq2Seq) and an architecture using convolutional sentence encoder (FBConv2Seq), both implemented in the NMT framework Neural Monkey that we currently participate in developing. We also compare various types of preprocessing of the source Japanese sentences and their impact on the overall results. Furthermore, we include the results of our experiments with out-of-domain data obtained by combining the corpora provided for each subtask.","year":2017,"title_abstract":"{CUNI} {NMT} System for {WAT} 2017 Translation Tasks The paper presents this year{'}s CUNI submissions to the WAT 2017 Translation Task focusing on the Japanese-English translation, namely Scientific papers subtask, Patents subtask and Newswire subtask. We compare two neural network architectures, the standard sequence-to-sequence with attention (Seq2Seq) and an architecture using convolutional sentence encoder (FBConv2Seq), both implemented in the NMT framework Neural Monkey that we currently participate in developing. We also compare various types of preprocessing of the source Japanese sentences and their impact on the overall results. Furthermore, we include the results of our experiments with out-of-domain data obtained by combining the corpora provided for each subtask.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.161900565,"Goal":"Gender Equality","Task":["Translation Tasks","Translation Task","Japanese - English translation"],"Method":["neural network architectures","sequence - to - sequence with attention","convolutional sentence encoder","NMT"]},{"ID":"lata-kumar-2010-development","title":"Development of Linguistic Resources and Tools for Providing Multilingual Solutions in {I}ndian Languages {---} A Report on National Initiative","abstract":"The multilingual diversity of India is one of the most unique in world. Currently there are 22 constitutionally recognized languages with 12 scripts. Apart from these, there are at least 35 different languages and 2000 dialects in 4 major language families. It is thus evident that, development and proliferation of software solutions in the Indic multilingual environment requires continuous and sustained effort to edge out challenges in all core areas namely storage and encoding, input mechanism, browser support and data exchange. Linguistic Resources and Tools are the key building blocks to develop multilingual solutions. In this paper, we shall present an overview of the major national initiative in India for the development and standardization of Linguistic Resources and Tools for developing and deployment of multilingual ICT solutions in India.","year":2010,"title_abstract":"Development of Linguistic Resources and Tools for Providing Multilingual Solutions in {I}ndian Languages {---} A Report on National Initiative The multilingual diversity of India is one of the most unique in world. Currently there are 22 constitutionally recognized languages with 12 scripts. Apart from these, there are at least 35 different languages and 2000 dialects in 4 major language families. It is thus evident that, development and proliferation of software solutions in the Indic multilingual environment requires continuous and sustained effort to edge out challenges in all core areas namely storage and encoding, input mechanism, browser support and data exchange. Linguistic Resources and Tools are the key building blocks to develop multilingual solutions. In this paper, we shall present an overview of the major national initiative in India for the development and standardization of Linguistic Resources and Tools for developing and deployment of multilingual ICT solutions in India.","social_need":"Industry, Innovation and Infrastrucure Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation","cosine_similarity":0.1618376225,"Goal":"Industry, Innovation and Infrastrucure","Task":["Multilingual Solutions","Indic multilingual environment","storage and encoding","input mechanism","browser support","data exchange","Linguistic Resources","multilingual ICT solutions"],"Method":["Linguistic Resources","software solutions","multilingual solutions"]},{"ID":"ray-etal-2019-sunny","title":"Sunny and Dark Outside?! Improving Answer Consistency in {VQA} through Entailed Question Generation","abstract":"While models for Visual Question Answering (VQA) have steadily improved over the years, interacting with one quickly reveals that these models lack consistency. For instance, if a model answers {``}red{''} to {``}What color is the balloon?{''}, it might answer {``}no{''} if asked, {``}Is the balloon red?{''}. These responses violate simple notions of entailment and raise questions about how effectively VQA models ground language. In this work, we introduce a dataset, ConVQA, and metrics that enable quantitative evaluation of consistency in VQA. For a given observable fact in an image (e.g. the balloon{'}s color), we generate a set of logically consistent question-answer (QA) pairs (e.g. Is the balloon red?) and also collect a human-annotated set of common-sense based consistent QA pairs (e.g. Is the balloon the same color as tomato sauce?). Further, we propose a consistency-improving data augmentation module, a Consistency Teacher Module (CTM). CTM automatically generates entailed (or similar-intent) questions for a source QA pair and fine-tunes the VQA model if the VQA{'}s answer to the entailed question is consistent with the source QA pair. We demonstrate that our CTM-based training improves the consistency of VQA models on the Con-VQA datasets and is a strong baseline for further research.","year":2019,"title_abstract":"Sunny and Dark Outside?! Improving Answer Consistency in {VQA} through Entailed Question Generation While models for Visual Question Answering (VQA) have steadily improved over the years, interacting with one quickly reveals that these models lack consistency. For instance, if a model answers {``}red{''} to {``}What color is the balloon?{''}, it might answer {``}no{''} if asked, {``}Is the balloon red?{''}. These responses violate simple notions of entailment and raise questions about how effectively VQA models ground language. In this work, we introduce a dataset, ConVQA, and metrics that enable quantitative evaluation of consistency in VQA. For a given observable fact in an image (e.g. the balloon{'}s color), we generate a set of logically consistent question-answer (QA) pairs (e.g. Is the balloon red?) and also collect a human-annotated set of common-sense based consistent QA pairs (e.g. Is the balloon the same color as tomato sauce?). Further, we propose a consistency-improving data augmentation module, a Consistency Teacher Module (CTM). CTM automatically generates entailed (or similar-intent) questions for a source QA pair and fine-tunes the VQA model if the VQA{'}s answer to the entailed question is consistent with the source QA pair. We demonstrate that our CTM-based training improves the consistency of VQA models on the Con-VQA datasets and is a strong baseline for further research.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1618365347,"Goal":"Quality Education","Task":["Visual Question Answering","VQA","ConVQA","quantitative evaluation","VQA","entailed (or similar - intent) questions","VQA","VQA"],"Method":["{VQA}","Entailed Question Generation","consistency - improving data augmentation module","Consistency Teacher Module","CTM","CTM"]},{"ID":"zhang-etal-2022-focus","title":"Focus on the Action: Learning to Highlight and Summarize Jointly for Email To-Do Items Summarization","abstract":"Automatic email to-do item generation is the task of generating to-do items from a given email to help people overview emails and schedule daily work. Different from prior research on email summarization, to-do item generation focuses on generating action mentions to provide more structured summaries of email text.Prior work either requires large amount of annotation for key sentences with potential actions or fails to pay attention to nuanced actions from these unstructured emails, and thus often lead to unfaithful summaries. To fill these gaps, we propose a simple and effective learning to highlight and summarize framework (LHS) to learn to identify the most salient text and actions, and incorporate these structured representations to generate more faithful to-do items. Experiments show that our LHS model outperforms the baselines and achieves the state-of-the-art performance in terms of both quantitative evaluation and human judgement. We also discussed specific challenges that current models faced with email to-do summarization.","year":2022,"title_abstract":"Focus on the Action: Learning to Highlight and Summarize Jointly for Email To-Do Items Summarization Automatic email to-do item generation is the task of generating to-do items from a given email to help people overview emails and schedule daily work. Different from prior research on email summarization, to-do item generation focuses on generating action mentions to provide more structured summaries of email text.Prior work either requires large amount of annotation for key sentences with potential actions or fails to pay attention to nuanced actions from these unstructured emails, and thus often lead to unfaithful summaries. To fill these gaps, we propose a simple and effective learning to highlight and summarize framework (LHS) to learn to identify the most salient text and actions, and incorporate these structured representations to generate more faithful to-do items. Experiments show that our LHS model outperforms the baselines and achieves the state-of-the-art performance in terms of both quantitative evaluation and human judgement. We also discussed specific challenges that current models faced with email to-do summarization.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1618339121,"Goal":"Climate Action","Task":["Highlight","Email To - Do","Summarization","email to - do item generation","generating to - do items","email summarization","to - do item generation","action mentions","email to - do summarization"],"Method":["highlight and summarize framework","structured representations","LHS model"]},{"ID":"may-etal-2019-measuring","title":"On Measuring Social Biases in Sentence Encoders","abstract":"The Word Embedding Association Test shows that GloVe and word2vec word embeddings exhibit human-like implicit biases based on gender, race, and other social constructs (Caliskan et al., 2017). Meanwhile, research on learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption. Accordingly, we extend the Word Embedding Association Test to measure bias in sentence encoders. We then test several sentence encoders, including state-of-the-art methods such as ELMo and BERT, for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level. We observe mixed results including suspicious patterns of sensitivity that suggest the test{'}s assumptions may not hold in general. We conclude by proposing directions for future work on measuring bias in sentence encoders.","year":2019,"title_abstract":"On Measuring Social Biases in Sentence Encoders The Word Embedding Association Test shows that GloVe and word2vec word embeddings exhibit human-like implicit biases based on gender, race, and other social constructs (Caliskan et al., 2017). Meanwhile, research on learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption. Accordingly, we extend the Word Embedding Association Test to measure bias in sentence encoders. We then test several sentence encoders, including state-of-the-art methods such as ELMo and BERT, for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level. We observe mixed results including suspicious patterns of sensitivity that suggest the test{'}s assumptions may not hold in general. We conclude by proposing directions for future work on measuring bias in sentence encoders.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1617245674,"Goal":"Gender Equality","Task":["Measuring Social Biases","Sentence Encoders","Word Embedding Association Test","reusable text representations","bias in sentence encoders","measuring bias in sentence encoders"],"Method":["GloVe","word2vec word embeddings","sentence encoders","Word Embedding Association Test","encoders","ELMo","BERT"]},{"ID":"lehmann-derczynski-2019-political","title":"Political Stance in {D}anish","abstract":"The task of stance detection consists of classifying the opinion within a text towards some target. This paper seeks to generate a dataset of quotes from Danish politicians, label this dataset to allow the task of stance detection to be performed, and present annotation guidelines to allow further expansion of the generated dataset. Furthermore, three models based on an LSTM architecture are designed, implemented and optimized to perform the task of stance detection for the generated dataset. Experiments are performed using conditionality and bi-directionality for these models, and using either singular word embeddings or averaged word embeddings for an entire quote, to determine the optimal model design. The simplest model design, applying neither conditionality or bi-directionality, and averaged word embeddings across quotes, yields the strongest results. Furthermore, it was found that inclusion of the quotes politician, and the party affiliation of the quoted politician, greatly improved performance of the strongest model.","year":2019,"title_abstract":"Political Stance in {D}anish The task of stance detection consists of classifying the opinion within a text towards some target. This paper seeks to generate a dataset of quotes from Danish politicians, label this dataset to allow the task of stance detection to be performed, and present annotation guidelines to allow further expansion of the generated dataset. Furthermore, three models based on an LSTM architecture are designed, implemented and optimized to perform the task of stance detection for the generated dataset. Experiments are performed using conditionality and bi-directionality for these models, and using either singular word embeddings or averaged word embeddings for an entire quote, to determine the optimal model design. The simplest model design, applying neither conditionality or bi-directionality, and averaged word embeddings across quotes, yields the strongest results. Furthermore, it was found that inclusion of the quotes politician, and the party affiliation of the quoted politician, greatly improved performance of the strongest model.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1616985202,"Goal":"Climate Action","Task":["Political Stance","stance detection","stance detection","stance detection"],"Method":["LSTM architecture","averaged word embeddings","model design","model design"]},{"ID":"pascucci-etal-2020-role","title":"The Role of Computational Stylometry in Identifying (Misogynistic) Aggression in {E}nglish Social Media Texts","abstract":"In this paper, we describe UniOr{\\_}ExpSys team participation in TRAC-2 (Trolling, Aggression and Cyberbullying) shared task, a workshop organized as part of LREC 2020. TRAC-2 shared task is organized in two sub-tasks: Aggression Identification (a 3-way classification between {``}Overtly Aggressive{''}, {``}Covertly Aggressive{''} and {``}Non-aggressive{''} text data) and Misogynistic Aggression Identification (a binary classifier for classifying the texts as {``}gendered{''} or {``}non-gendered{''}). Our approach is based on linguistic rules, stylistic features extraction through stylometric analysis and Sequential Minimal Optimization algorithm in building the two classifiers.","year":2020,"title_abstract":"The Role of Computational Stylometry in Identifying (Misogynistic) Aggression in {E}nglish Social Media Texts In this paper, we describe UniOr{\\_}ExpSys team participation in TRAC-2 (Trolling, Aggression and Cyberbullying) shared task, a workshop organized as part of LREC 2020. TRAC-2 shared task is organized in two sub-tasks: Aggression Identification (a 3-way classification between {``}Overtly Aggressive{''}, {``}Covertly Aggressive{''} and {``}Non-aggressive{''} text data) and Misogynistic Aggression Identification (a binary classifier for classifying the texts as {``}gendered{''} or {``}non-gendered{''}). Our approach is based on linguistic rules, stylistic features extraction through stylometric analysis and Sequential Minimal Optimization algorithm in building the two classifiers.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1616709232,"Goal":"Gender Equality","Task":["Computational Stylometry","Identifying (Misogynistic) Aggression","TRAC - 2","Aggression","Cyberbullying) shared task","LREC","TRAC - 2","Aggression Identification","3 - way classification"],"Method":["Misogynistic Aggression Identification","binary classifier","stylistic features extraction","stylometric analysis","Sequential Minimal Optimization algorithm"]},{"ID":"antypas-etal-2021-covid","title":"{COVID}-19 and Misinformation: A Large-Scale Lexical Analysis on {T}witter","abstract":"Social media is often used by individuals and organisations as a platform to spread misinformation. With the recent coronavirus pandemic we have seen a surge of misinformation on Twitter, posing a danger to public health. In this paper, we compile a large COVID-19 Twitter misinformation corpus and perform an analysis to discover patterns with respect to vocabulary usage. Among others, our analysis reveals that the variety of topics and vocabulary usage are considerably more limited and negative in tweets related to misinformation than in randomly extracted tweets. In addition to our qualitative analysis, our experimental results show that a simple linear model based only on lexical features is effective in identifying misinformation-related tweets (with accuracy over 80{\\%}), providing evidence to the fact that the vocabulary used in misinformation largely differs from generic tweets.","year":2021,"title_abstract":"{COVID}-19 and Misinformation: A Large-Scale Lexical Analysis on {T}witter Social media is often used by individuals and organisations as a platform to spread misinformation. With the recent coronavirus pandemic we have seen a surge of misinformation on Twitter, posing a danger to public health. In this paper, we compile a large COVID-19 Twitter misinformation corpus and perform an analysis to discover patterns with respect to vocabulary usage. Among others, our analysis reveals that the variety of topics and vocabulary usage are considerably more limited and negative in tweets related to misinformation than in randomly extracted tweets. In addition to our qualitative analysis, our experimental results show that a simple linear model based only on lexical features is effective in identifying misinformation-related tweets (with accuracy over 80{\\%}), providing evidence to the fact that the vocabulary used in misinformation largely differs from generic tweets.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1616695523,"Goal":"Climate Action","Task":["Large - Scale Lexical Analysis","coronavirus pandemic"],"Method":["linear model"]},{"ID":"el-ghali-vigile-hoareau-2010-une","title":"Une approche cognitive de la fouille de grandes collections de documents","abstract":"La r{\\'e}cente {\\'e}closion du Web2.0 engendre un accroissement consid{\\'e}rable de volumes textuels et intensifie ainsi l{'}importance d{'}une r{\\'e}flexion sur l{'}exploitation des connaissances {\\`a} partir de grandes collections de documents. Dans cet article, nous pr{\\'e}sentons une approche de rechercher d{'}information qui s{'}inspire des certaines recherches issues de la psychologie cognitive pour la fouille de larges collections de documents. Nous utilisons un document comme requ{\\^e}te permettant de r{\\'e}cup{\\'e}rer des informations {\\`a} partir d{'}une collection repr{\\'e}sent{\\'e}e dans un espace s{\\'e}mantique. Nous d{\\'e}finissons les notions d{'}identit{\\'e} s{\\'e}mantique et de pollution s{\\'e}mantique dans un espace de documents. Nous illustrons notre approche par la description d{'}un syst{\\`e}me appel{\\'e} BRAT (Blogosphere Random Analysis using Texts) bas{\\'e} sur les notions pr{\\'e}alablement introduites d{'}identit{\\'e} et de pollution s{\\'e}matique appliqu{\\'e}es {\\`a} une t{\\^a}che d{'}identification des actualit{\\'e}s dans la blogosph{\\`e}re mondiale lors du concours TREC{'}09. Les premiers r{\\'e}sultats produits sont tout {\\`a} fait encourageant et indiquent les pistes des recherches {\\`a} mettre en oeuvre afin d{'}am{\\'e}liorer les performances de BRAT.","year":2010,"title_abstract":"Une approche cognitive de la fouille de grandes collections de documents La r{\\'e}cente {\\'e}closion du Web2.0 engendre un accroissement consid{\\'e}rable de volumes textuels et intensifie ainsi l{'}importance d{'}une r{\\'e}flexion sur l{'}exploitation des connaissances {\\`a} partir de grandes collections de documents. Dans cet article, nous pr{\\'e}sentons une approche de rechercher d{'}information qui s{'}inspire des certaines recherches issues de la psychologie cognitive pour la fouille de larges collections de documents. Nous utilisons un document comme requ{\\^e}te permettant de r{\\'e}cup{\\'e}rer des informations {\\`a} partir d{'}une collection repr{\\'e}sent{\\'e}e dans un espace s{\\'e}mantique. Nous d{\\'e}finissons les notions d{'}identit{\\'e} s{\\'e}mantique et de pollution s{\\'e}mantique dans un espace de documents. Nous illustrons notre approche par la description d{'}un syst{\\`e}me appel{\\'e} BRAT (Blogosphere Random Analysis using Texts) bas{\\'e} sur les notions pr{\\'e}alablement introduites d{'}identit{\\'e} et de pollution s{\\'e}matique appliqu{\\'e}es {\\`a} une t{\\^a}che d{'}identification des actualit{\\'e}s dans la blogosph{\\`e}re mondiale lors du concours TREC{'}09. Les premiers r{\\'e}sultats produits sont tout {\\`a} fait encourageant et indiquent les pistes des recherches {\\`a} mettre en oeuvre afin d{'}am{\\'e}liorer les performances de BRAT.","social_need":"Responsible Consumption and Production Ensure sustainable consumption and production patterns","cosine_similarity":0.1616271138,"Goal":"Responsible Consumption and Production","Task":["psychologie cognitive"],"Method":["Random Analysis"]},{"ID":"webb-etal-2010-evaluating","title":"Evaluating Human-Machine Conversation for Appropriateness","abstract":"Evaluation of complex, collaborative dialogue systems is a difficult task. Traditionally, developers have relied upon subjective feedback from the user, and parametrisation over observable metrics. However, both models place some reliance on the notion of a task; that is, the system is helping to user achieve some clearly defined goal, such as book a flight or complete a banking transaction. It is not clear that such metrics are as useful when dealing with a system that has a more complex task, or even no definable task at all, beyond maintain and performing a collaborative dialogue. Working within the EU funded COMPANIONS program, we investigate the use of appropriateness as a measure of conversation quality, the hypothesis being that good companions need to be good conversational partners . We report initial work in the direction of annotating dialogue for indicators of good conversation, including the annotation and comparison of the output of two generations of the same dialogue system.","year":2010,"title_abstract":"Evaluating Human-Machine Conversation for Appropriateness Evaluation of complex, collaborative dialogue systems is a difficult task. Traditionally, developers have relied upon subjective feedback from the user, and parametrisation over observable metrics. However, both models place some reliance on the notion of a task; that is, the system is helping to user achieve some clearly defined goal, such as book a flight or complete a banking transaction. It is not clear that such metrics are as useful when dealing with a system that has a more complex task, or even no definable task at all, beyond maintain and performing a collaborative dialogue. Working within the EU funded COMPANIONS program, we investigate the use of appropriateness as a measure of conversation quality, the hypothesis being that good companions need to be good conversational partners . We report initial work in the direction of annotating dialogue for indicators of good conversation, including the annotation and comparison of the output of two generations of the same dialogue system.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1616015732,"Goal":"Partnership for the Goals","Task":["Evaluating Human - Machine Conversation","Appropriateness Evaluation of complex , collaborative dialogue systems","collaborative dialogue","COMPANIONS program","annotating dialogue","conversation","annotation"],"Method":["dialogue system"]},{"ID":"chen-etal-2018-sequence","title":"Sequence-to-Action: End-to-End Semantic Graph Generation for Semantic Parsing","abstract":"This paper proposes a neural semantic parsing approach {--} Sequence-to-Action, which models semantic parsing as an end-to-end semantic graph generation process. Our method simultaneously leverages the advantages from two recent promising directions of semantic parsing. Firstly, our model uses a semantic graph to represent the meaning of a sentence, which has a tight-coupling with knowledge bases. Secondly, by leveraging the powerful representation learning and prediction ability of neural network models, we propose a RNN model which can effectively map sentences to action sequences for semantic graph generation. Experiments show that our method achieves state-of-the-art performance on Overnight dataset and gets competitive performance on Geo and Atis datasets.","year":2018,"title_abstract":"Sequence-to-Action: End-to-End Semantic Graph Generation for Semantic Parsing This paper proposes a neural semantic parsing approach {--} Sequence-to-Action, which models semantic parsing as an end-to-end semantic graph generation process. Our method simultaneously leverages the advantages from two recent promising directions of semantic parsing. Firstly, our model uses a semantic graph to represent the meaning of a sentence, which has a tight-coupling with knowledge bases. Secondly, by leveraging the powerful representation learning and prediction ability of neural network models, we propose a RNN model which can effectively map sentences to action sequences for semantic graph generation. Experiments show that our method achieves state-of-the-art performance on Overnight dataset and gets competitive performance on Geo and Atis datasets.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1615970135,"Goal":"Climate Action","Task":["Sequence - to - Action","End - to - End Semantic Graph Generation","Semantic Parsing","semantic parsing","end - to - end semantic graph generation process","semantic parsing","semantic graph generation"],"Method":["neural semantic parsing approach","semantic graph","representation learning","prediction ability","neural network models","RNN model"]},{"ID":"gorman-etal-2020-sigmorphon","title":"The {SIGMORPHON} 2020 Shared Task on Multilingual Grapheme-to-Phoneme Conversion","abstract":"We describe the design and findings of the SIGMORPHON 2020 shared task on multilingual grapheme-to-phoneme conversion. Participants were asked to submit systems which take in a sequence of graphemes in a given language as input, then output a sequence of phonemes representing the pronunciation of that grapheme sequence. Nine teams submitted a total of 23 systems, at best achieving a 18{\\%} relative reduction in word error rate (macro-averaged over languages), versus strong neural sequence-to-sequence baselines. To facilitate error analysis, we publicly release the complete outputs for all systems{---}a first for the SIGMORPHON workshop.","year":2020,"title_abstract":"The {SIGMORPHON} 2020 Shared Task on Multilingual Grapheme-to-Phoneme Conversion We describe the design and findings of the SIGMORPHON 2020 shared task on multilingual grapheme-to-phoneme conversion. Participants were asked to submit systems which take in a sequence of graphemes in a given language as input, then output a sequence of phonemes representing the pronunciation of that grapheme sequence. Nine teams submitted a total of 23 systems, at best achieving a 18{\\%} relative reduction in word error rate (macro-averaged over languages), versus strong neural sequence-to-sequence baselines. To facilitate error analysis, we publicly release the complete outputs for all systems{---}a first for the SIGMORPHON workshop.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1615840197,"Goal":"Gender Equality","Task":["Multilingual Grapheme - to - Phoneme Conversion","SIGMORPHON 2020 shared task","multilingual grapheme - to - phoneme conversion","error analysis"],"Method":["neural sequence - to - sequence baselines"]},{"ID":"schluter-2017-limits","title":"The limits of automatic summarisation according to {ROUGE}","abstract":"This paper discusses some central caveats of summarisation, incurred in the use of the ROUGE metric for evaluation, with respect to optimal solutions. The task is NP-hard, of which we give the first proof. Still, as we show empirically for three central benchmark datasets for the task, greedy algorithms empirically seem to perform optimally according to the metric. Additionally, overall quality assurance is problematic: there is no natural upper bound on the quality of summarisation systems, and even humans are excluded from performing optimal summarisation.","year":2017,"title_abstract":"The limits of automatic summarisation according to {ROUGE} This paper discusses some central caveats of summarisation, incurred in the use of the ROUGE metric for evaluation, with respect to optimal solutions. The task is NP-hard, of which we give the first proof. Still, as we show empirically for three central benchmark datasets for the task, greedy algorithms empirically seem to perform optimally according to the metric. Additionally, overall quality assurance is problematic: there is no natural upper bound on the quality of summarisation systems, and even humans are excluded from performing optimal summarisation.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1615650654,"Goal":"Reduced Inequalities","Task":["automatic summarisation","summarisation","evaluation","summarisation","summarisation"],"Method":["greedy algorithms"]},{"ID":"strock-2010-practical","title":"Practical uses of {MT} at Global Language Translations and Consulting: A case study of {MT} use for profit","abstract":"This document describes the use of MT at GLTaC and provides an approach to determining if offering MT services is right for you. There is no single answer or approach to providing MT services so this is just one way an LSP has chosen to provide MT services.","year":2010,"title_abstract":"Practical uses of {MT} at Global Language Translations and Consulting: A case study of {MT} use for profit This document describes the use of MT at GLTaC and provides an approach to determining if offering MT services is right for you. There is no single answer or approach to providing MT services so this is just one way an LSP has chosen to provide MT services.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1614023298,"Goal":"Partnership for the Goals","Task":["Global Language Translations","Consulting","profit","MT","MT services","MT services"],"Method":["{MT}","{MT}","GLTaC","LSP","MT services"]},{"ID":"mahendran-etal-2020-nlp","title":"{NLP}@{VCU}: Identifying Adverse Effects in {E}nglish Tweets for Unbalanced Data","abstract":"This paper describes our participation in the Social Media Mining for Health Application (SMM4H 2020) Challenge Track 2 for identifying tweets containing Adverse Effects (AEs). Our system uses Convolutional Neural Networks. We explore downsampling, oversampling, and adjusting the class weights to account for the imbalanced nature of the dataset. Our results showed downsampling outperformed oversampling and adjusting the class weights on the test set however all three obtained similar results on the development set.","year":2020,"title_abstract":"{NLP}@{VCU}: Identifying Adverse Effects in {E}nglish Tweets for Unbalanced Data This paper describes our participation in the Social Media Mining for Health Application (SMM4H 2020) Challenge Track 2 for identifying tweets containing Adverse Effects (AEs). Our system uses Convolutional Neural Networks. We explore downsampling, oversampling, and adjusting the class weights to account for the imbalanced nature of the dataset. Our results showed downsampling outperformed oversampling and adjusting the class weights on the test set however all three obtained similar results on the development set.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1613701135,"Goal":"Climate Action","Task":["Identifying Adverse Effects","Social Media Mining for Health Application","identifying tweets containing Adverse Effects"],"Method":["Convolutional Neural Networks","oversampling","downsampling","oversampling"]},{"ID":"xu-etal-2020-review","title":"A Review of Dataset and Labeling Methods for Causality Extraction","abstract":"Causality represents the most important kind of correlation between events. Extracting causali-ty from text has become a promising hot topic in NLP. However, there is no mature research systems and datasets for public evaluation. Moreover, there is a lack of unified causal sequence label methods, which constitute the key factors that hinder the progress of causality extraction research. We survey the limitations and shortcomings of existing causality research field com-prehensively from the aspects of basic concepts, extraction methods, experimental data, and la-bel methods, so as to provide reference for future research on causality extraction. We summa-rize the existing causality datasets, explore their practicability and extensibility from multiple perspectives and create a new causal dataset ESC. Aiming at the problem of causal sequence labeling, we analyse the existing methods with a summarization of its regulation and propose a new causal label method of core word. Multiple candidate causal label sequences are put for-ward according to label controversy to explore the optimal label method through experiments, and suggestions are provided for selecting label method.","year":2020,"title_abstract":"A Review of Dataset and Labeling Methods for Causality Extraction Causality represents the most important kind of correlation between events. Extracting causali-ty from text has become a promising hot topic in NLP. However, there is no mature research systems and datasets for public evaluation. Moreover, there is a lack of unified causal sequence label methods, which constitute the key factors that hinder the progress of causality extraction research. We survey the limitations and shortcomings of existing causality research field com-prehensively from the aspects of basic concepts, extraction methods, experimental data, and la-bel methods, so as to provide reference for future research on causality extraction. We summa-rize the existing causality datasets, explore their practicability and extensibility from multiple perspectives and create a new causal dataset ESC. Aiming at the problem of causal sequence labeling, we analyse the existing methods with a summarization of its regulation and propose a new causal label method of core word. Multiple candidate causal label sequences are put for-ward according to label controversy to explore the optimal label method through experiments, and suggestions are provided for selecting label method.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1613534689,"Goal":"Climate Action","Task":["Causality Extraction","Causality","Extracting causali - ty","NLP","public evaluation","causality extraction research","causality extraction","causal sequence labeling"],"Method":["Labeling Methods","causal sequence label methods","extraction methods","la - bel methods","causal label method","label method","label method"]},{"ID":"johnson-goldwasser-2018-classification","title":"Classification of Moral Foundations in Microblog Political Discourse","abstract":"Previous works in computer science, as well as political and social science, have shown correlation in text between political ideologies and the moral foundations expressed within that text. Additional work has shown that policy frames, which are used by politicians to bias the public towards their stance on an issue, are also correlated with political ideology. Based on these associations, this work takes a first step towards modeling both the language and how politicians frame issues on Twitter, in order to predict the moral foundations that are used by politicians to express their stances on issues. The contributions of this work includes a dataset annotated for the moral foundations, annotation guidelines, and probabilistic graphical models which show the usefulness of jointly modeling abstract political slogans, as opposed to the unigrams of previous works, with policy frames for the prediction of the morality underlying political tweets.","year":2018,"title_abstract":"Classification of Moral Foundations in Microblog Political Discourse Previous works in computer science, as well as political and social science, have shown correlation in text between political ideologies and the moral foundations expressed within that text. Additional work has shown that policy frames, which are used by politicians to bias the public towards their stance on an issue, are also correlated with political ideology. Based on these associations, this work takes a first step towards modeling both the language and how politicians frame issues on Twitter, in order to predict the moral foundations that are used by politicians to express their stances on issues. The contributions of this work includes a dataset annotated for the moral foundations, annotation guidelines, and probabilistic graphical models which show the usefulness of jointly modeling abstract political slogans, as opposed to the unigrams of previous works, with policy frames for the prediction of the morality underlying political tweets.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.161347568,"Goal":"Peace, Justice and Strong Institutions","Task":["Classification of Moral Foundations","Microblog Political Discourse","computer science","political and social science","prediction of the morality underlying political tweets"],"Method":["probabilistic graphical models","policy frames"]},{"ID":"eckart-de-castilho-etal-2019-multi","title":"A Multi-Platform Annotation Ecosystem for Domain Adaptation","abstract":"This paper describes an ecosystem consisting of three independent text annotation platforms. To demonstrate their ability to work in concert, we illustrate how to use them to address an interactive domain adaptation task in biomedical entity recognition. The platforms and the approach are in general domain-independent and can be readily applied to other areas of science.","year":2019,"title_abstract":"A Multi-Platform Annotation Ecosystem for Domain Adaptation This paper describes an ecosystem consisting of three independent text annotation platforms. To demonstrate their ability to work in concert, we illustrate how to use them to address an interactive domain adaptation task in biomedical entity recognition. The platforms and the approach are in general domain-independent and can be readily applied to other areas of science.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.161344558,"Goal":"Life on Land","Task":["Domain Adaptation","interactive domain adaptation task","biomedical entity recognition","science"],"Method":["Multi - Platform Annotation Ecosystem","text annotation platforms"]},{"ID":"pei-jurgens-2021-measuring","title":"Measuring Sentence-Level and Aspect-Level (Un)certainty in Science Communications","abstract":"Certainty and uncertainty are fundamental to science communication. Hedges have widely been used as proxies for uncertainty. However, certainty is a complex construct, with authors expressing not only the degree but the type and aspects of uncertainty in order to give the reader a certain impression of what is known. Here, we introduce a new study of certainty that models both the level and the aspects of certainty in scientific findings. Using a new dataset of 2167 annotated scientific findings, we demonstrate that hedges alone account for only a partial explanation of certainty. We show that both the overall certainty and individual aspects can be predicted with pre-trained language models, providing a more complete picture of the author{'}s intended communication. Downstream analyses on 431K scientific findings from news and scientific abstracts demonstrate that modeling sentence-level and aspect-level certainty is meaningful for areas like science communication. Both the model and datasets used in this paper are released at https:\/\/blablablab.si.umich.edu\/projects\/certainty\/.","year":2021,"title_abstract":"Measuring Sentence-Level and Aspect-Level (Un)certainty in Science Communications Certainty and uncertainty are fundamental to science communication. Hedges have widely been used as proxies for uncertainty. However, certainty is a complex construct, with authors expressing not only the degree but the type and aspects of uncertainty in order to give the reader a certain impression of what is known. Here, we introduce a new study of certainty that models both the level and the aspects of certainty in scientific findings. Using a new dataset of 2167 annotated scientific findings, we demonstrate that hedges alone account for only a partial explanation of certainty. We show that both the overall certainty and individual aspects can be predicted with pre-trained language models, providing a more complete picture of the author{'}s intended communication. Downstream analyses on 431K scientific findings from news and scientific abstracts demonstrate that modeling sentence-level and aspect-level certainty is meaningful for areas like science communication. Both the model and datasets used in this paper are released at https:\/\/blablablab.si.umich.edu\/projects\/certainty\/.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1613368988,"Goal":"Climate Action","Task":["Measuring Sentence - Level","Aspect - Level (Un)certainty in Science Communications","science communication","author{'}s intended communication","science communication"],"Method":["Hedges","hedges","language models"]},{"ID":"dou-neubig-2021-word","title":"Word Alignment by Fine-tuning Embeddings on Parallel Corpora","abstract":"Word alignment over parallel corpora has a wide variety of applications, including learning translation lexicons, cross-lingual transfer of language processing tools, and automatic evaluation or analysis of translation outputs. The great majority of past work on word alignment has worked by performing unsupervised learning on parallel text. Recently, however, other work has demonstrated that pre-trained contextualized word embeddings derived from multilingually trained language models (LMs) prove an attractive alternative, achieving competitive results on the word alignment task even in the absence of explicit training on parallel data. In this paper, we examine methods to marry the two approaches: leveraging pre-trained LMs but fine-tuning them on parallel text with objectives designed to improve alignment quality, and proposing methods to effectively extract alignments from these fine-tuned models. We perform experiments on five language pairs and demonstrate that our model can consistently outperform previous state-of-the-art models of all varieties. In addition, we demonstrate that we are able to train multilingual word aligners that can obtain robust performance on different language pairs.","year":2021,"title_abstract":"Word Alignment by Fine-tuning Embeddings on Parallel Corpora Word alignment over parallel corpora has a wide variety of applications, including learning translation lexicons, cross-lingual transfer of language processing tools, and automatic evaluation or analysis of translation outputs. The great majority of past work on word alignment has worked by performing unsupervised learning on parallel text. Recently, however, other work has demonstrated that pre-trained contextualized word embeddings derived from multilingually trained language models (LMs) prove an attractive alternative, achieving competitive results on the word alignment task even in the absence of explicit training on parallel data. In this paper, we examine methods to marry the two approaches: leveraging pre-trained LMs but fine-tuning them on parallel text with objectives designed to improve alignment quality, and proposing methods to effectively extract alignments from these fine-tuned models. We perform experiments on five language pairs and demonstrate that our model can consistently outperform previous state-of-the-art models of all varieties. In addition, we demonstrate that we are able to train multilingual word aligners that can obtain robust performance on different language pairs.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1613088846,"Goal":"Gender Equality","Task":["Word Alignment","Fine - tuning Embeddings","Word alignment","learning translation lexicons","cross - lingual transfer of language processing tools","automatic evaluation or analysis of translation outputs","word alignment","word alignment task"],"Method":["unsupervised learning","contextualized word embeddings","multilingually trained language models","explicit training","LMs","fine - tuned models","multilingual word aligners"]},{"ID":"cardoso-2012-rembrandt","title":"Rembrandt - a named-entity recognition framework","abstract":"Rembrandt is a named entity recognition system specially crafted to annotate documents by classifying named entities and ground them into unique identifiers. Rembrandt played an important role within our research over geographic IR, thus evolving into a more capable framework where documents can be annotated, manually curated and indexed. The goal of this paper is to present Rembrandt's simple but powerful annotation framework to the NLP community.","year":2012,"title_abstract":"Rembrandt - a named-entity recognition framework Rembrandt is a named entity recognition system specially crafted to annotate documents by classifying named entities and ground them into unique identifiers. Rembrandt played an important role within our research over geographic IR, thus evolving into a more capable framework where documents can be annotated, manually curated and indexed. The goal of this paper is to present Rembrandt's simple but powerful annotation framework to the NLP community.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1613060534,"Goal":"Sustainable Cities and Communities","Task":["geographic IR","NLP community"],"Method":["named - entity recognition framework","Rembrandt","named entity recognition system","Rembrandt","Rembrandt's","annotation framework"]},{"ID":"jansen-etal-2016-whats","title":"What{'}s in an Explanation? Characterizing Knowledge and Inference Requirements for Elementary Science Exams","abstract":"QA systems have been making steady advances in the challenging elementary science exam domain. In this work, we develop an explanation-based analysis of knowledge and inference requirements, which supports a fine-grained characterization of the challenges. In particular, we model the requirements based on appropriate sources of evidence to be used for the QA task. We create requirements by first identifying suitable sentences in a knowledge base that support the correct answer, then use these to build explanations, filling in any necessary missing information. These explanations are used to create a fine-grained categorization of the requirements. Using these requirements, we compare a retrieval and an inference solver on 212 questions. The analysis validates the gains of the inference solver, demonstrating that it answers more questions requiring complex inference, while also providing insights into the relative strengths of the solvers and knowledge sources. We release the annotated questions and explanations as a resource with broad utility for science exam QA, including determining knowledge base construction targets, as well as supporting information aggregation in automated inference.","year":2016,"title_abstract":"What{'}s in an Explanation? Characterizing Knowledge and Inference Requirements for Elementary Science Exams QA systems have been making steady advances in the challenging elementary science exam domain. In this work, we develop an explanation-based analysis of knowledge and inference requirements, which supports a fine-grained characterization of the challenges. In particular, we model the requirements based on appropriate sources of evidence to be used for the QA task. We create requirements by first identifying suitable sentences in a knowledge base that support the correct answer, then use these to build explanations, filling in any necessary missing information. These explanations are used to create a fine-grained categorization of the requirements. Using these requirements, we compare a retrieval and an inference solver on 212 questions. The analysis validates the gains of the inference solver, demonstrating that it answers more questions requiring complex inference, while also providing insights into the relative strengths of the solvers and knowledge sources. We release the annotated questions and explanations as a resource with broad utility for science exam QA, including determining knowledge base construction targets, as well as supporting information aggregation in automated inference.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1612523496,"Goal":"Quality Education","Task":["Explanation? Characterizing Knowledge and Inference Requirements","QA","elementary science exam domain","QA task","inference","science exam QA","knowledge base construction targets","information aggregation","automated inference"],"Method":["explanation - based analysis of knowledge and inference requirements","retrieval","inference solver","inference solver"]},{"ID":"sundriyal-etal-2022-document","title":"Document Retrieval and Claim Verification to Mitigate {COVID}-19 Misinformation","abstract":"During the COVID-19 pandemic, the spread of misinformation on online social media has grown exponentially. Unverified bogus claims on these platforms regularly mislead people, leading them to believe in half-baked truths. The current vogue is to employ manual fact-checkers to verify claims to combat this avalanche of misinformation. However, establishing such claims{'} veracity is becoming increasingly challenging, partly due to the plethora of information available, which is difficult to process manually. Thus, it becomes imperative to verify claims automatically without human interventions. To cope up with this issue, we propose an automated claim verification solution encompassing two steps {--} document retrieval and veracity prediction. For the retrieval module, we employ a hybrid search-based system with BM25 as a base retriever and experiment with recent state-of-the-art transformer-based models for re-ranking. Furthermore, we use a BART-based textual entailment architecture to authenticate the retrieved documents in the later step. We report experimental findings, demonstrating that our retrieval module outperforms the best baseline system by 10.32 NDCG@100 points. We escort a demonstration to assess the efficacy and impact of our suggested solution. As a byproduct of this study, we present an open-source, easily deployable, and user-friendly Python API that the community can adopt.","year":2022,"title_abstract":"Document Retrieval and Claim Verification to Mitigate {COVID}-19 Misinformation During the COVID-19 pandemic, the spread of misinformation on online social media has grown exponentially. Unverified bogus claims on these platforms regularly mislead people, leading them to believe in half-baked truths. The current vogue is to employ manual fact-checkers to verify claims to combat this avalanche of misinformation. However, establishing such claims{'} veracity is becoming increasingly challenging, partly due to the plethora of information available, which is difficult to process manually. Thus, it becomes imperative to verify claims automatically without human interventions. To cope up with this issue, we propose an automated claim verification solution encompassing two steps {--} document retrieval and veracity prediction. For the retrieval module, we employ a hybrid search-based system with BM25 as a base retriever and experiment with recent state-of-the-art transformer-based models for re-ranking. Furthermore, we use a BART-based textual entailment architecture to authenticate the retrieved documents in the later step. We report experimental findings, demonstrating that our retrieval module outperforms the best baseline system by 10.32 NDCG@100 points. We escort a demonstration to assess the efficacy and impact of our suggested solution. As a byproduct of this study, we present an open-source, easily deployable, and user-friendly Python API that the community can adopt.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.16119802,"Goal":"Climate Action","Task":["Document Retrieval","Claim Verification","COVID - 19 pandemic","document retrieval","veracity prediction","re - ranking"],"Method":["manual fact - checkers","automated claim verification solution","retrieval module","hybrid search - based system","BM25","retriever","transformer - based models","BART - based textual entailment architecture","retrieval module","Python API"]},{"ID":"luu-malamud-2020-annotating","title":"Annotating Coherence Relations for Studying Topic Transitions in Social Talk","abstract":"This study develops the strand of research on topic transitions in social talk which aims to gain a better understanding of interlocutors{'} conversational goals. L\u01b0u and Malamud (2020) proposed that one way to identify such transitions is to annotate coherence relations, and then to identify utterances potentially expressing new topics as those that fail to participate in these relations. This work validates and refines their suggested annotation methodology, focusing on annotating most prominent coherence relations in face-to-face social dialogue. The result is a publicly accessible gold standard corpus with efficient and reliable annotation, whose broad coverage provides a foundation for future steps of identifying and classifying new topic utterances.","year":2020,"title_abstract":"Annotating Coherence Relations for Studying Topic Transitions in Social Talk This study develops the strand of research on topic transitions in social talk which aims to gain a better understanding of interlocutors{'} conversational goals. L\u01b0u and Malamud (2020) proposed that one way to identify such transitions is to annotate coherence relations, and then to identify utterances potentially expressing new topics as those that fail to participate in these relations. This work validates and refines their suggested annotation methodology, focusing on annotating most prominent coherence relations in face-to-face social dialogue. The result is a publicly accessible gold standard corpus with efficient and reliable annotation, whose broad coverage provides a foundation for future steps of identifying and classifying new topic utterances.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1611815244,"Goal":"Partnership for the Goals","Task":["Annotating Coherence Relations","Topic Transitions in Social Talk","topic transitions","social talk","face - to - face social dialogue","annotation","classifying new topic utterances"],"Method":["annotation methodology"]},{"ID":"kovatchev-etal-2020-mind","title":"{``}What is on your mind?{''} Automated Scoring of Mindreading in Childhood and Early Adolescence","abstract":"In this paper we present the first work on the automated scoring of mindreading ability in middle childhood and early adolescence. We create MIND-CA, a new corpus of 11,311 question-answer pairs in English from 1,066 children aged from 7 to 14. We perform machine learning experiments and carry out extensive quantitative and qualitative evaluation. We obtain promising results, demonstrating the applicability of state-of-the-art NLP solutions to a new domain and task.","year":2020,"title_abstract":"{``}What is on your mind?{''} Automated Scoring of Mindreading in Childhood and Early Adolescence In this paper we present the first work on the automated scoring of mindreading ability in middle childhood and early adolescence. We create MIND-CA, a new corpus of 11,311 question-answer pairs in English from 1,066 children aged from 7 to 14. We perform machine learning experiments and carry out extensive quantitative and qualitative evaluation. We obtain promising results, demonstrating the applicability of state-of-the-art NLP solutions to a new domain and task.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.1611257195,"Goal":"Good Health and Well-Being","Task":["Automated Scoring of Mindreading","automated scoring of mindreading ability"],"Method":["machine learning","NLP solutions"]},{"ID":"lauscher-etal-2021-sustainable-modular","title":"Sustainable Modular Debiasing of Language Models","abstract":"Unfair stereotypical biases (e.g., gender, racial, or religious biases) encoded in modern pretrained language models (PLMs) have negative ethical implications for widespread adoption of state-of-the-art language technology. To remedy for this, a wide range of debiasing techniques have recently been introduced to remove such stereotypical biases from PLMs. Existing debiasing methods, however, directly modify all of the PLMs parameters, which {--} besides being computationally expensive {--} comes with the inherent risk of (catastrophic) forgetting of useful language knowledge acquired in pretraining. In this work, we propose a more sustainable modular debiasing approach based on dedicated debiasing adapters, dubbed ADELE. Concretely, we (1) inject adapter modules into the original PLM layers and (2) update only the adapters (i.e., we keep the original PLM parameters frozen) via language modeling training on a counterfactually augmented corpus. We showcase ADELE, in gender debiasing of BERT: our extensive evaluation, encompassing three intrinsic and two extrinsic bias measures, renders ADELE, very effective in bias mitigation. We further show that {--} due to its modular nature {--} ADELE, coupled with task adapters, retains fairness even after large-scale downstream training. Finally, by means of multilingual BERT, we successfully transfer ADELE, to six target languages.","year":2021,"title_abstract":"Sustainable Modular Debiasing of Language Models Unfair stereotypical biases (e.g., gender, racial, or religious biases) encoded in modern pretrained language models (PLMs) have negative ethical implications for widespread adoption of state-of-the-art language technology. To remedy for this, a wide range of debiasing techniques have recently been introduced to remove such stereotypical biases from PLMs. Existing debiasing methods, however, directly modify all of the PLMs parameters, which {--} besides being computationally expensive {--} comes with the inherent risk of (catastrophic) forgetting of useful language knowledge acquired in pretraining. In this work, we propose a more sustainable modular debiasing approach based on dedicated debiasing adapters, dubbed ADELE. Concretely, we (1) inject adapter modules into the original PLM layers and (2) update only the adapters (i.e., we keep the original PLM parameters frozen) via language modeling training on a counterfactually augmented corpus. We showcase ADELE, in gender debiasing of BERT: our extensive evaluation, encompassing three intrinsic and two extrinsic bias measures, renders ADELE, very effective in bias mitigation. We further show that {--} due to its modular nature {--} ADELE, coupled with task adapters, retains fairness even after large-scale downstream training. Finally, by means of multilingual BERT, we successfully transfer ADELE, to six target languages.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1611218601,"Goal":"Gender Equality","Task":["gender debiasing of BERT","bias mitigation","downstream training"],"Method":["Sustainable Modular Debiasing of Language Models","pretrained language models","language technology","debiasing techniques","PLMs","debiasing methods","modular debiasing approach","debiasing adapters","ADELE","adapter modules","PLM layers","language modeling","ADELE","ADELE","ADELE","task adapters","BERT","ADELE"]},{"ID":"putri-etal-2020-iswara","title":"{ISWARA} at {WNUT}-2020 Task 2: Identification of Informative {COVID}-19 {E}nglish Tweets using {BERT} and {F}ast{T}ext Embeddings","abstract":"This paper presents Iswara{'}s participation in the WNUT-2020 Task 2 {``}Identification of Informative COVID-19 English Tweets using BERT and FastText Embeddings{''},which tries to classify whether a certain tweet is considered informative or not. We proposed a method that utilizes word embeddings and using word occurrence related to the topic for this task. We compare several models to get the best performance. Results show that pairing BERT with word occurrences outperforms fastText with F1-Score, precision, recall, and accuracy on test data of 76{\\%}, 81{\\%}, 72{\\%}, and 79{\\%}, respectively","year":2020,"title_abstract":"{ISWARA} at {WNUT}-2020 Task 2: Identification of Informative {COVID}-19 {E}nglish Tweets using {BERT} and {F}ast{T}ext Embeddings This paper presents Iswara{'}s participation in the WNUT-2020 Task 2 {``}Identification of Informative COVID-19 English Tweets using BERT and FastText Embeddings{''},which tries to classify whether a certain tweet is considered informative or not. We proposed a method that utilizes word embeddings and using word occurrence related to the topic for this task. We compare several models to get the best performance. Results show that pairing BERT with word occurrences outperforms fastText with F1-Score, precision, recall, and accuracy on test data of 76{\\%}, 81{\\%}, 72{\\%}, and 79{\\%}, respectively","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1610975116,"Goal":"Climate Action","Task":["Identification of Informative {COVID} - 19","WNUT - 2020 Task"],"Method":["BERT","FastText Embeddings{''}","BERT","fastText"]},{"ID":"pershan-2020-moderating","title":"Moderating Our (Dis)Content: Renewing the Regulatory Approach","abstract":"As online platforms become central to our democracies, the problem of toxic content threatens the free flow of information and the enjoyment of fundamental rights. But effective policy response to toxic content must grasp the idiosyncrasies and interconnectedness of content moderation across a fragmented online landscape. This report urges regulators and legislators to consider a range of platforms and moderation approaches in the regulation. In particular, it calls for a holistic, process-oriented regulatory approach that accounts for actors beyond the handful of dominant platforms that currently shape public debate.","year":2020,"title_abstract":"Moderating Our (Dis)Content: Renewing the Regulatory Approach As online platforms become central to our democracies, the problem of toxic content threatens the free flow of information and the enjoyment of fundamental rights. But effective policy response to toxic content must grasp the idiosyncrasies and interconnectedness of content moderation across a fragmented online landscape. This report urges regulators and legislators to consider a range of platforms and moderation approaches in the regulation. In particular, it calls for a holistic, process-oriented regulatory approach that accounts for actors beyond the handful of dominant platforms that currently shape public debate.","social_need":"Responsible Consumption and Production Ensure sustainable consumption and production patterns","cosine_similarity":0.1610741317,"Goal":"Responsible Consumption and Production","Task":["policy response","regulation"],"Method":["Regulatory Approach","moderation approaches","process - oriented regulatory approach"]},{"ID":"singh-etal-2008-estimating","title":"Estimating the Resource Adaption Cost from a Resource Rich Language to a Similar Resource Poor Language","abstract":"Developing resources which can be used for Natural Language Processing is an extremely difficult task for any language, but is even more so for less privileged (or less computerized) languages. One way to overcome this difficulty is to adapt the resources of a linguistically close resource rich language. In this paper we discuss how the cost of such adaption can be estimated using subjective and objective measures of linguistic similarity for allocating financial resources, time, manpower etc. Since this is the first work of its kind, the method described in this paper should be seen as only a preliminary method, indicative of how better methods can be developed. Corpora of several less computerized languages had to be collected for the work described in the paper, which was difficult because for many of these varieties there is not much electronic data available. Even if it is, it is in non-standard encodings, which means that we had to build encoding converters for these varieties. The varieties we have focused on are some of the varieties spoken in the South Asian region.","year":2008,"title_abstract":"Estimating the Resource Adaption Cost from a Resource Rich Language to a Similar Resource Poor Language Developing resources which can be used for Natural Language Processing is an extremely difficult task for any language, but is even more so for less privileged (or less computerized) languages. One way to overcome this difficulty is to adapt the resources of a linguistically close resource rich language. In this paper we discuss how the cost of such adaption can be estimated using subjective and objective measures of linguistic similarity for allocating financial resources, time, manpower etc. Since this is the first work of its kind, the method described in this paper should be seen as only a preliminary method, indicative of how better methods can be developed. Corpora of several less computerized languages had to be collected for the work described in the paper, which was difficult because for many of these varieties there is not much electronic data available. Even if it is, it is in non-standard encodings, which means that we had to build encoding converters for these varieties. The varieties we have focused on are some of the varieties spoken in the South Asian region.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1610568762,"Goal":"Reduced Inequalities","Task":["Resource Adaption Cost","Natural Language Processing"],"Method":["encoding converters"]},{"ID":"abd-yusof-etal-2017-analysing","title":"Analysing the Causes of Depressed Mood from Depression Vulnerable Individuals","abstract":"We develop a computational model to discover the potential causes of depression by analysing the topics in a usergenerated text. We show the most prominent causes, and how these causes evolve over time. Also, we highlight the differences in causes between students with low and high neuroticism. Our studies demonstrate that the topics reveal valuable clues about the causes contributing to depressed mood. Identifying causes can have a significant impact on improving the quality of depression care; thereby providing greater insights into a patient{'}s state for pertinent treatment recommendations. Hence, this study significantly expands the ability to discover the potential factors that trigger depression, making it possible to increase the efficiency of depression treatment.","year":2017,"title_abstract":"Analysing the Causes of Depressed Mood from Depression Vulnerable Individuals We develop a computational model to discover the potential causes of depression by analysing the topics in a usergenerated text. We show the most prominent causes, and how these causes evolve over time. Also, we highlight the differences in causes between students with low and high neuroticism. Our studies demonstrate that the topics reveal valuable clues about the causes contributing to depressed mood. Identifying causes can have a significant impact on improving the quality of depression care; thereby providing greater insights into a patient{'}s state for pertinent treatment recommendations. Hence, this study significantly expands the ability to discover the potential factors that trigger depression, making it possible to increase the efficiency of depression treatment.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1610365659,"Goal":"Reduced Inequalities","Task":["Identifying causes","depression care;","treatment recommendations","depression treatment"],"Method":["computational model"]},{"ID":"leinonen-etal-2021-grapheme","title":"Grapheme-Based Cross-Language Forced Alignment: Results with Uralic Languages","abstract":"Forced alignment is an effective process to speed up linguistic research. However, most forced aligners are language-dependent, and under-resourced languages rarely have enough resources to train an acoustic model for an aligner. We present a new Finnish grapheme-based forced aligner and demonstrate its performance by aligning multiple Uralic languages and English as an unrelated language. We show that even a simple non-expert created grapheme-to-phoneme mapping can result in useful word alignments.","year":2021,"title_abstract":"Grapheme-Based Cross-Language Forced Alignment: Results with Uralic Languages Forced alignment is an effective process to speed up linguistic research. However, most forced aligners are language-dependent, and under-resourced languages rarely have enough resources to train an acoustic model for an aligner. We present a new Finnish grapheme-based forced aligner and demonstrate its performance by aligning multiple Uralic languages and English as an unrelated language. We show that even a simple non-expert created grapheme-to-phoneme mapping can result in useful word alignments.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1610074341,"Goal":"Gender Equality","Task":["Grapheme - Based Cross - Language Forced Alignment","Uralic Languages","Forced alignment","linguistic research","aligner"],"Method":["aligners","acoustic model","Finnish grapheme - based forced aligner","grapheme - to - phoneme mapping"]},{"ID":"kokkinakis-2008-mesh","title":"{M}e{SH}{\\copyright}: from a Controlled Vocabulary to a Processable Resource","abstract":"Large repositories of life science data in the form of domain-specific literature and large specialised textual collections increase on a daily basis to a level beyond the human mind can grasp and interpret. As the volume of data continues to increase, substantial support from new information technologies and computational techniques grounded in the mining paradigm is becoming apparent. These emerging technologies play a critical role in aiding research productivity, and they provide the means for reducing the workload for information access and decision support and for speeding up and enhancing the knowledge discovery process. In order to accomplish these higher level goals a fundamental and unavoidable starting point is the identification and mapping of terminology from unstructured data to biomedical knowledge sources and concept hierarchies. This paper provides a description of the work regarding terminology recognition using the Swedish MeSH{\\copyright} thesaurus and its corresponding English source. The various transformation and refinement steps applied to the original database tables into a fully-fledged processing-oriented annotating resource are explained. Particular attention has been given to a number of these steps in order to automatically map the extensive variability of lexical terms to structured MeSH{\\copyright} nodes. Issues on annotation and coverage are also discussed.","year":2008,"title_abstract":"{M}e{SH}{\\copyright}: from a Controlled Vocabulary to a Processable Resource Large repositories of life science data in the form of domain-specific literature and large specialised textual collections increase on a daily basis to a level beyond the human mind can grasp and interpret. As the volume of data continues to increase, substantial support from new information technologies and computational techniques grounded in the mining paradigm is becoming apparent. These emerging technologies play a critical role in aiding research productivity, and they provide the means for reducing the workload for information access and decision support and for speeding up and enhancing the knowledge discovery process. In order to accomplish these higher level goals a fundamental and unavoidable starting point is the identification and mapping of terminology from unstructured data to biomedical knowledge sources and concept hierarchies. This paper provides a description of the work regarding terminology recognition using the Swedish MeSH{\\copyright} thesaurus and its corresponding English source. The various transformation and refinement steps applied to the original database tables into a fully-fledged processing-oriented annotating resource are explained. Particular attention has been given to a number of these steps in order to automatically map the extensive variability of lexical terms to structured MeSH{\\copyright} nodes. Issues on annotation and coverage are also discussed.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1609677374,"Goal":"Life on Land","Task":["research productivity","information access","decision support","knowledge discovery process","identification","mapping of terminology","terminology recognition","processing - oriented annotating resource","annotation","coverage"],"Method":["information technologies","computational techniques","mining paradigm"]},{"ID":"ma-etal-2019-results","title":"Results of the {WMT}19 Metrics Shared Task: Segment-Level and Strong {MT} Systems Pose Big Challenges","abstract":"This paper presents the results of the WMT19 Metrics Shared Task. Participants were asked to score the outputs of the translations systems competing in the WMT19 News Translation Task with automatic metrics. 13 research groups submitted 24 metrics, 10 of which are reference-less {``}metrics{''} and constitute submissions to the joint task with WMT19 Quality Estimation Task, {``}QE as a Metric{''}. In addition, we computed 11 baseline metrics, with 8 commonly applied baselines (BLEU, SentBLEU, NIST, WER, PER, TER, CDER, and chrF) and 3 reimplementations (chrF+, sacreBLEU-BLEU, and sacreBLEU-chrF). Metrics were evaluated on the system level, how well a given metric correlates with the WMT19 official manual ranking, and segment level, how well the metric correlates with human judgements of segment quality. This year, we use direct assessment (DA) as our only form of manual evaluation.","year":2019,"title_abstract":"Results of the {WMT}19 Metrics Shared Task: Segment-Level and Strong {MT} Systems Pose Big Challenges This paper presents the results of the WMT19 Metrics Shared Task. Participants were asked to score the outputs of the translations systems competing in the WMT19 News Translation Task with automatic metrics. 13 research groups submitted 24 metrics, 10 of which are reference-less {``}metrics{''} and constitute submissions to the joint task with WMT19 Quality Estimation Task, {``}QE as a Metric{''}. In addition, we computed 11 baseline metrics, with 8 commonly applied baselines (BLEU, SentBLEU, NIST, WER, PER, TER, CDER, and chrF) and 3 reimplementations (chrF+, sacreBLEU-BLEU, and sacreBLEU-chrF). Metrics were evaluated on the system level, how well a given metric correlates with the WMT19 official manual ranking, and segment level, how well the metric correlates with human judgements of segment quality. This year, we use direct assessment (DA) as our only form of manual evaluation.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1609505415,"Goal":"Partnership for the Goals","Task":["WMT19 News Translation Task","joint task","WMT19 Quality Estimation Task"],"Method":["WMT19","translations systems","SentBLEU","PER","TER","CDER","chrF)","sacreBLEU - BLEU","sacreBLEU - chrF)","direct assessment"]},{"ID":"liberman-2016-human","title":"From Human Language Technology to Human Language Science","abstract":"Thirty years ago, in order to get past roadblocks in Machine Translation and Automatic Speech Recognition, DARPA invented a new way to organize and manage technological R{\\&}D : a {``}common task{''} is defined by a formal quantitative evaluation metric and a body of shared training data, and researchers join an open competition to compare approaches. Over the past three decades, this method has produced steadily improving technologies, with many practical applications now possible. And Moore{'}s law has created a sort of digital shadow universe, which increasingly mirrors the real world in flows and stores of bits, while the same improvements in digital hardware and software make it increasingly easy to pull content out of the these rivers and oceans of information. It{'}s natural to be excited about these technologies, where we can see an open road to rapid improvements beyond the current state of the art, and an explosion of near-term commercial applications. But there are some important opportunities in a less obvious direction. Several areas of scientific and humanistic research are being revolutionized by the application of Human Language Technology. At a minimum, orders of magnitude more data can be addressed with orders of magnitude less effort - but this change also transforms old theoretical questions, and poses new ones. And eventually, new modes of research organization and funding are likely to emerge..","year":2016,"title_abstract":"From Human Language Technology to Human Language Science Thirty years ago, in order to get past roadblocks in Machine Translation and Automatic Speech Recognition, DARPA invented a new way to organize and manage technological R{\\&}D : a {``}common task{''} is defined by a formal quantitative evaluation metric and a body of shared training data, and researchers join an open competition to compare approaches. Over the past three decades, this method has produced steadily improving technologies, with many practical applications now possible. And Moore{'}s law has created a sort of digital shadow universe, which increasingly mirrors the real world in flows and stores of bits, while the same improvements in digital hardware and software make it increasingly easy to pull content out of the these rivers and oceans of information. It{'}s natural to be excited about these technologies, where we can see an open road to rapid improvements beyond the current state of the art, and an explosion of near-term commercial applications. But there are some important opportunities in a less obvious direction. Several areas of scientific and humanistic research are being revolutionized by the application of Human Language Technology. At a minimum, orders of magnitude more data can be addressed with orders of magnitude less effort - but this change also transforms old theoretical questions, and poses new ones. And eventually, new modes of research organization and funding are likely to emerge..","social_need":"Industry, Innovation and Infrastrucure Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation","cosine_similarity":0.160861522,"Goal":"Industry, Innovation and Infrastrucure","Task":["Human Language Technology","Human Language Science","Machine Translation","Automatic Speech Recognition","scientific and humanistic research","research organization","funding"],"Method":["Human Language Technology"]},{"ID":"rezapour-etal-2019-enhancing","title":"Enhancing the Measurement of Social Effects by Capturing Morality","abstract":"We investigate the relationship between basic principles of human morality and the expression of opinions in user-generated text data. We assume that people{'}s backgrounds, culture, and values are associated with their perceptions and expressions of everyday topics, and that people{'}s language use reflects these perceptions. While personal values and social effects are abstract and complex concepts, they have practical implications and are relevant for a wide range of NLP applications. To extract human values (in this paper, morality) and measure social effects (morality and stance), we empirically evaluate the usage of a morality lexicon that we expanded via a quality controlled, human in the loop process. As a result, we enhanced the Moral Foundations Dictionary in size (from 324 to 4,636 syntactically disambiguated entries) and scope. We used both lexica for feature-based and deep learning classification (SVM, RF, and LSTM) to test their usefulness for measuring social effects. We find that the enhancement of the original lexicon led to measurable improvements in prediction accuracy for the selected NLP tasks.","year":2019,"title_abstract":"Enhancing the Measurement of Social Effects by Capturing Morality We investigate the relationship between basic principles of human morality and the expression of opinions in user-generated text data. We assume that people{'}s backgrounds, culture, and values are associated with their perceptions and expressions of everyday topics, and that people{'}s language use reflects these perceptions. While personal values and social effects are abstract and complex concepts, they have practical implications and are relevant for a wide range of NLP applications. To extract human values (in this paper, morality) and measure social effects (morality and stance), we empirically evaluate the usage of a morality lexicon that we expanded via a quality controlled, human in the loop process. As a result, we enhanced the Moral Foundations Dictionary in size (from 324 to 4,636 syntactically disambiguated entries) and scope. We used both lexica for feature-based and deep learning classification (SVM, RF, and LSTM) to test their usefulness for measuring social effects. We find that the enhancement of the original lexicon led to measurable improvements in prediction accuracy for the selected NLP tasks.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.16084674,"Goal":"Peace, Justice and Strong Institutions","Task":["Measurement of Social Effects","Capturing Morality","NLP applications","human in the loop process","measuring social effects","NLP tasks"],"Method":["lexica","feature - based and deep learning classification","RF","LSTM)"]},{"ID":"naplava-straka-2019-cuni","title":"{CUNI} System for the Building Educational Applications 2019 Shared Task: Grammatical Error Correction","abstract":"Our submitted models are NMT systems based on the Transformer model, which we improve by incorporating several enhancements: applying dropout to whole source and target words, weighting target subwords, averaging model checkpoints, and using the trained model iteratively for correcting the intermediate translations. The system in the Restricted Track is trained on the provided corpora with oversampled {``}cleaner{''} sentences and reaches 59.39 F0.5 score on the test set. The system in the Low-Resource Track is trained from Wikipedia revision histories and reaches 44.13 F0.5 score. Finally, we finetune the system from the Low-Resource Track on restricted data and achieve 64.55 F0.5 score.","year":2019,"title_abstract":"{CUNI} System for the Building Educational Applications 2019 Shared Task: Grammatical Error Correction Our submitted models are NMT systems based on the Transformer model, which we improve by incorporating several enhancements: applying dropout to whole source and target words, weighting target subwords, averaging model checkpoints, and using the trained model iteratively for correcting the intermediate translations. The system in the Restricted Track is trained on the provided corpora with oversampled {``}cleaner{''} sentences and reaches 59.39 F0.5 score on the test set. The system in the Low-Resource Track is trained from Wikipedia revision histories and reaches 44.13 F0.5 score. Finally, we finetune the system from the Low-Resource Track on restricted data and achieve 64.55 F0.5 score.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1608397663,"Goal":"Quality Education","Task":["Building Educational Applications","Grammatical Error Correction","NMT"],"Method":["Transformer model","dropout","averaging model checkpoints"]},{"ID":"hurriyetoglu-etal-2021-challenges","title":"Challenges and Applications of Automated Extraction of Socio-political Events from Text ({CASE} 2021): Workshop and Shared Task Report","abstract":"This workshop is the fourth issue of a series of workshops on automatic extraction of socio-political events from news, organized by the Emerging Market Welfare Project, with the support of the Joint Research Centre of the European Commission and with contributions from many other prominent scholars in this field. The purpose of this series of workshops is to foster research and development of reliable, valid, robust, and practical solutions for automatically detecting descriptions of socio-political events, such as protests, riots, wars and armed conflicts, in text streams. This year workshop contributors make use of the state-of-the-art NLP technologies, such as Deep Learning, Word Embeddings and Transformers and cover a wide range of topics from text classification to news bias detection. Around 40 teams have registered and 15 teams contributed to three tasks that are i) multilingual protest news detection detection, ii) fine-grained classification of socio-political events, and iii) discovering Black Lives Matter protest events. The workshop also highlights two keynote and four invited talks about various aspects of creating event data sets and multi- and cross-lingual machine learning in few- and zero-shot settings.","year":2021,"title_abstract":"Challenges and Applications of Automated Extraction of Socio-political Events from Text ({CASE} 2021): Workshop and Shared Task Report This workshop is the fourth issue of a series of workshops on automatic extraction of socio-political events from news, organized by the Emerging Market Welfare Project, with the support of the Joint Research Centre of the European Commission and with contributions from many other prominent scholars in this field. The purpose of this series of workshops is to foster research and development of reliable, valid, robust, and practical solutions for automatically detecting descriptions of socio-political events, such as protests, riots, wars and armed conflicts, in text streams. This year workshop contributors make use of the state-of-the-art NLP technologies, such as Deep Learning, Word Embeddings and Transformers and cover a wide range of topics from text classification to news bias detection. Around 40 teams have registered and 15 teams contributed to three tasks that are i) multilingual protest news detection detection, ii) fine-grained classification of socio-political events, and iii) discovering Black Lives Matter protest events. The workshop also highlights two keynote and four invited talks about various aspects of creating event data sets and multi- and cross-lingual machine learning in few- and zero-shot settings.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1608265936,"Goal":"Climate Action","Task":["Automated Extraction of Socio - political Events","automatic extraction of socio - political events","automatically detecting descriptions of socio - political events","text classification","news bias detection","multilingual protest news detection detection","fine - grained classification of socio - political events","discovering Black Lives Matter protest events","multi - and cross - lingual machine learning","few - and zero - shot settings"],"Method":["NLP technologies","Deep Learning","Word Embeddings","Transformers"]},{"ID":"fu-etal-2020-facilitating","title":"Facilitating the Communication of Politeness through Fine-Grained Paraphrasing","abstract":"Aided by technology, people are increasingly able to communicate across geographical, cultural, and language barriers. This ability also results in new challenges, as interlocutors need to adapt their communication approaches to increasingly diverse circumstances. In this work, we take the first steps towards automatically assisting people in adjusting their language to a specific communication circumstance. As a case study, we focus on facilitating the accurate transmission of pragmatic intentions and introduce a methodology for suggesting paraphrases that achieve the intended level of politeness under a given communication circumstance. We demonstrate the feasibility of this approach by evaluating our method in two realistic communication scenarios and show that it can reduce the potential for misalignment between the speaker{'}s intentions and the listener{'}s perceptions in both cases.","year":2020,"title_abstract":"Facilitating the Communication of Politeness through Fine-Grained Paraphrasing Aided by technology, people are increasingly able to communicate across geographical, cultural, and language barriers. This ability also results in new challenges, as interlocutors need to adapt their communication approaches to increasingly diverse circumstances. In this work, we take the first steps towards automatically assisting people in adjusting their language to a specific communication circumstance. As a case study, we focus on facilitating the accurate transmission of pragmatic intentions and introduce a methodology for suggesting paraphrases that achieve the intended level of politeness under a given communication circumstance. We demonstrate the feasibility of this approach by evaluating our method in two realistic communication scenarios and show that it can reduce the potential for misalignment between the speaker{'}s intentions and the listener{'}s perceptions in both cases.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1607901901,"Goal":"Climate Action","Task":["Communication of Politeness","accurate transmission of pragmatic intentions","paraphrases","communication scenarios"],"Method":["Fine - Grained Paraphrasing","communication approaches"]},{"ID":"subramanian-etal-2021-evaluating","title":"Evaluating Debiasing Techniques for Intersectional Biases","abstract":"Bias is pervasive for NLP models, motivating the development of automatic debiasing techniques. Evaluation of NLP debiasing methods has largely been limited to binary attributes in isolation, e.g., debiasing with respect to binary gender or race, however many corpora involve multiple such attributes, possibly with higher cardinality. In this paper we argue that a truly fair model must consider {`}gerrymandering{'} groups which comprise not only single attributes, but also intersectional groups. We evaluate a form of bias-constrained model which is new to NLP, as well an extension of the iterative nullspace projection technique which can handle multiple identities.","year":2021,"title_abstract":"Evaluating Debiasing Techniques for Intersectional Biases Bias is pervasive for NLP models, motivating the development of automatic debiasing techniques. Evaluation of NLP debiasing methods has largely been limited to binary attributes in isolation, e.g., debiasing with respect to binary gender or race, however many corpora involve multiple such attributes, possibly with higher cardinality. In this paper we argue that a truly fair model must consider {`}gerrymandering{'} groups which comprise not only single attributes, but also intersectional groups. We evaluate a form of bias-constrained model which is new to NLP, as well an extension of the iterative nullspace projection technique which can handle multiple identities.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.160780251,"Goal":"Gender Equality","Task":["NLP","debiasing","NLP"],"Method":["Debiasing Techniques","automatic debiasing techniques","NLP debiasing methods","bias - constrained model","iterative nullspace projection technique"]},{"ID":"dias-paraboni-2020-cross","title":"Cross-domain Author Gender Classification in {B}razilian {P}ortuguese","abstract":"Author profiling models predict demographic characteristics of a target author based on the text that they have written. Systems of this kind will often follow a single-domain approach, in which the model is trained from a corpus of labelled texts in a given domain, and it is subsequently validated against a test corpus built from precisely the same domain. Although single-domain settings are arguably ideal, this strategy gives rise to the question of how to proceed when no suitable training corpus (i.e., a corpus that matches the test domain) is available. To shed light on this issue, this paper discusses a cross-domain gender classification task based on four domains (Facebook, crowd sourced opinions, Blogs and E-gov requests) in the Brazilian Portuguese language. A number of simple gender classification models using word- and psycholinguistics-based features alike are introduced, and their results are compared in two kinds of cross-domain setting: first, by making use of a single text source as training data for each task, and subsequently by combining multiple sources. Results confirm previous findings related to the effects of corpus size and domain similarity in English, and pave the way for further studies in the field.","year":2020,"title_abstract":"Cross-domain Author Gender Classification in {B}razilian {P}ortuguese Author profiling models predict demographic characteristics of a target author based on the text that they have written. Systems of this kind will often follow a single-domain approach, in which the model is trained from a corpus of labelled texts in a given domain, and it is subsequently validated against a test corpus built from precisely the same domain. Although single-domain settings are arguably ideal, this strategy gives rise to the question of how to proceed when no suitable training corpus (i.e., a corpus that matches the test domain) is available. To shed light on this issue, this paper discusses a cross-domain gender classification task based on four domains (Facebook, crowd sourced opinions, Blogs and E-gov requests) in the Brazilian Portuguese language. A number of simple gender classification models using word- and psycholinguistics-based features alike are introduced, and their results are compared in two kinds of cross-domain setting: first, by making use of a single text source as training data for each task, and subsequently by combining multiple sources. Results confirm previous findings related to the effects of corpus size and domain similarity in English, and pave the way for further studies in the field.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1607774794,"Goal":"Gender Equality","Task":["Cross - domain Author Gender Classification","cross - domain gender classification task","cross - domain setting"],"Method":["Author profiling models","single - domain approach","gender classification models"]},{"ID":"ranasinghe-etal-2020-rgcl","title":"{RGCL} at {S}em{E}val-2020 Task 6: Neural Approaches to {D}efinition{E}xtraction","abstract":"This paper presents the RGCL team submission to SemEval 2020 Task 6: DeftEval, subtasks 1 and 2. The system classifies definitions at the sentence and token levels. It utilises state-of-the-art neural network architectures, which have some task-specific adaptations, including an automatically extended training set. Overall, the approach achieves acceptable evaluation scores, while maintaining flexibility in architecture selection.","year":2020,"title_abstract":"{RGCL} at {S}em{E}val-2020 Task 6: Neural Approaches to {D}efinition{E}xtraction This paper presents the RGCL team submission to SemEval 2020 Task 6: DeftEval, subtasks 1 and 2. The system classifies definitions at the sentence and token levels. It utilises state-of-the-art neural network architectures, which have some task-specific adaptations, including an automatically extended training set. Overall, the approach achieves acceptable evaluation scores, while maintaining flexibility in architecture selection.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1607599556,"Goal":"Gender Equality","Task":["SemEval 2020 Task","DeftEval","architecture selection"],"Method":["Neural Approaches","RGCL","neural network architectures"]},{"ID":"grimes-etal-2012-automatic","title":"Automatic word alignment tools to scale production of manually aligned parallel texts","abstract":"We have been creating large-scale manual word alignment corpora for Arabic-English and Chinese-English language pairs in genres such as newsire, broadcast news and conversation, and web blogs. We are now meeting the challenge of word aligning further varieties of web data for Chinese and Arabic ''''''``dialects''''''''. Human word alignment annotation can be costly and arduous. Alignment guidelines may be imprecise or underspecified in cases where parallel sentences are hard to compare -- due to non-literal translations or differences between language structures. In order to speed annotation, we examine the effect that seeding manual alignments with automatic aligner output has on annotation speed and accuracy. We use automatic alignment methods that produce alignment results which are high precision and low recall to minimize annotator corrections. Results suggest that annotation time can be reduced by up to 20{\\%}, but we also found that reviewing and correcting automatic alignments requires more time than anticipated. We discuss throughout the paper crucial decisions on data structures for word alignment that likely have a significant impact on our results.","year":2012,"title_abstract":"Automatic word alignment tools to scale production of manually aligned parallel texts We have been creating large-scale manual word alignment corpora for Arabic-English and Chinese-English language pairs in genres such as newsire, broadcast news and conversation, and web blogs. We are now meeting the challenge of word aligning further varieties of web data for Chinese and Arabic ''''''``dialects''''''''. Human word alignment annotation can be costly and arduous. Alignment guidelines may be imprecise or underspecified in cases where parallel sentences are hard to compare -- due to non-literal translations or differences between language structures. In order to speed annotation, we examine the effect that seeding manual alignments with automatic aligner output has on annotation speed and accuracy. We use automatic alignment methods that produce alignment results which are high precision and low recall to minimize annotator corrections. Results suggest that annotation time can be reduced by up to 20{\\%}, but we also found that reviewing and correcting automatic alignments requires more time than anticipated. We discuss throughout the paper crucial decisions on data structures for word alignment that likely have a significant impact on our results.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1607530117,"Goal":"Gender Equality","Task":["Automatic word alignment tools","word aligning","Human word alignment annotation","Alignment guidelines","annotation","alignment","automatic alignments","word alignment"],"Method":["automatic alignment methods"]},{"ID":"moura-etal-2020-ist","title":"{IST}-Unbabel Participation in the {WMT}20 Quality Estimation Shared Task","abstract":"We present the joint contribution of IST and Unbabel to the WMT 2020 Shared Task on Quality Estimation. Our team participated on all tracks (Direct Assessment, Post-Editing Effort, Document-Level), encompassing a total of 14 submissions. Our submitted systems were developed by extending the OpenKiwi framework to a transformer-based predictor-estimator architecture, and to cope with glass-box, uncertainty-based features coming from neural machine translation systems.","year":2020,"title_abstract":"{IST}-Unbabel Participation in the {WMT}20 Quality Estimation Shared Task We present the joint contribution of IST and Unbabel to the WMT 2020 Shared Task on Quality Estimation. Our team participated on all tracks (Direct Assessment, Post-Editing Effort, Document-Level), encompassing a total of 14 submissions. Our submitted systems were developed by extending the OpenKiwi framework to a transformer-based predictor-estimator architecture, and to cope with glass-box, uncertainty-based features coming from neural machine translation systems.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1607297659,"Goal":"Quality Education","Task":["Quality Estimation Shared Task","WMT 2020 Shared Task","Quality Estimation","Post - Editing Effort"],"Method":["OpenKiwi framework","transformer - based predictor - estimator architecture","neural machine translation systems"]},{"ID":"conkie-etal-2012-building","title":"Building Text-To-Speech Voices in the Cloud","abstract":"The AT{\\&}T VoiceBuilder provides a new tool to researchers and practitioners who want to have their voices synthesized by a high-quality commercial-grade text-to-speech system without the need to install, configure, or manage speech processing software and equipment.It is implemented as a web service on the AT{\\&}T Speech Mashup Portal.The system records and validates users' utterances, processes them to build a synthetic voice and provides a web service API to make the voice available to real-time applications through a scalable cloud-based processing platform. All the procedures are automated to avoid human intervention. We present experimental comparisons of voices built using the system.","year":2012,"title_abstract":"Building Text-To-Speech Voices in the Cloud The AT{\\&}T VoiceBuilder provides a new tool to researchers and practitioners who want to have their voices synthesized by a high-quality commercial-grade text-to-speech system without the need to install, configure, or manage speech processing software and equipment.It is implemented as a web service on the AT{\\&}T Speech Mashup Portal.The system records and validates users' utterances, processes them to build a synthetic voice and provides a web service API to make the voice available to real-time applications through a scalable cloud-based processing platform. All the procedures are automated to avoid human intervention. We present experimental comparisons of voices built using the system.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1606812179,"Goal":"Sustainable Cities and Communities","Task":["Text - To - Speech Voices"],"Method":["AT{\\&}T VoiceBuilder","speech system","speech processing software","web service","AT{\\&}T Speech Mashup Portal","web service API","cloud - based processing platform"]},{"ID":"huang-etal-2020-reducing","title":"Reducing Sentiment Bias in Language Models via Counterfactual Evaluation","abstract":"Advances in language modeling architectures and the availability of large text corpora have driven progress in automatic text generation. While this results in models capable of generating coherent texts, it also prompts models to internalize social biases present in the training corpus. This paper aims to quantify and reduce a particular type of bias exhibited by language models: bias in the sentiment of generated text. Given a conditioning context (e.g., a writing prompt) and a language model, we analyze if (and how) the sentiment of the generated text is affected by changes in values of sensitive attributes (e.g., country names, occupations, genders) in the conditioning context using a form of counterfactual evaluation. We quantify sentiment bias by adopting individual and group fairness metrics from the fair machine learning literature, and demonstrate that large-scale models trained on two different corpora (news articles, and Wikipedia) exhibit considerable levels of bias. We then propose embedding and sentiment prediction-derived regularization on the language model{'}s latent representations. The regularizations improve fairness metrics while retaining comparable levels of perplexity and semantic similarity.","year":2020,"title_abstract":"Reducing Sentiment Bias in Language Models via Counterfactual Evaluation Advances in language modeling architectures and the availability of large text corpora have driven progress in automatic text generation. While this results in models capable of generating coherent texts, it also prompts models to internalize social biases present in the training corpus. This paper aims to quantify and reduce a particular type of bias exhibited by language models: bias in the sentiment of generated text. Given a conditioning context (e.g., a writing prompt) and a language model, we analyze if (and how) the sentiment of the generated text is affected by changes in values of sensitive attributes (e.g., country names, occupations, genders) in the conditioning context using a form of counterfactual evaluation. We quantify sentiment bias by adopting individual and group fairness metrics from the fair machine learning literature, and demonstrate that large-scale models trained on two different corpora (news articles, and Wikipedia) exhibit considerable levels of bias. We then propose embedding and sentiment prediction-derived regularization on the language model{'}s latent representations. The regularizations improve fairness metrics while retaining comparable levels of perplexity and semantic similarity.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1606169343,"Goal":"Reduced Inequalities","Task":["Sentiment Bias","automatic text generation","fair machine learning literature"],"Method":["Language Models","Counterfactual Evaluation","language modeling architectures","language models","language model","counterfactual evaluation","large - scale models","embedding and sentiment prediction - derived regularization","language model{'}s latent representations","regularizations"]},{"ID":"chen-kong-2021-cs-english","title":"cs{\\_}english@{LT}-{EDI}-{EACL}2021: Hope Speech Detection Based On Fine-tuning {ALBERT} Model","abstract":"This paper mainly introduces the relevant content of the task {``}Hope Speech Detection for Equality, Diversity, and Inclusion at LT-EDI 2021-EACL 2021{''}. A total of three language datasets were provided, and we chose the English dataset to complete this task. The specific task objective is to classify the given speech into {`}Hope speech{'}, {`}Not Hope speech{'}, and {`}Not in intended language{'}. In terms of method, we use fine-tuned ALBERT and K fold cross-validation to accomplish this task. In the end, we achieved a good result in the rank list of the task result, and the final F1 score was 0.93, tying for first place. However, we will continue to try to improve methods to get better results in future work.","year":2021,"title_abstract":"cs{\\_}english@{LT}-{EDI}-{EACL}2021: Hope Speech Detection Based On Fine-tuning {ALBERT} Model This paper mainly introduces the relevant content of the task {``}Hope Speech Detection for Equality, Diversity, and Inclusion at LT-EDI 2021-EACL 2021{''}. A total of three language datasets were provided, and we chose the English dataset to complete this task. The specific task objective is to classify the given speech into {`}Hope speech{'}, {`}Not Hope speech{'}, and {`}Not in intended language{'}. In terms of method, we use fine-tuned ALBERT and K fold cross-validation to accomplish this task. In the end, we achieved a good result in the rank list of the task result, and the final F1 score was 0.93, tying for first place. However, we will continue to try to improve methods to get better results in future work.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1605809331,"Goal":"Gender Equality","Task":["Hope Speech Detection","Speech Detection","Equality"],"Method":["Fine - tuning","ALBERT","K fold cross - validation"]},{"ID":"klie-etal-2018-inception","title":"The {INCE}p{TION} Platform: Machine-Assisted and Knowledge-Oriented Interactive Annotation","abstract":"We introduce INCEpTION, a new annotation platform for tasks including interactive and semantic annotation (e.g., concept linking, fact linking, knowledge base population, semantic frame annotation). These tasks are very time consuming and demanding for annotators, especially when knowledge bases are used. We address these issues by developing an annotation platform that incorporates machine learning capabilities which actively assist and guide annotators. The platform is both generic and modular. It targets a range of research domains in need of semantic annotation, such as digital humanities, bioinformatics, or linguistics. INCEpTION is publicly available as open-source software.","year":2018,"title_abstract":"The {INCE}p{TION} Platform: Machine-Assisted and Knowledge-Oriented Interactive Annotation We introduce INCEpTION, a new annotation platform for tasks including interactive and semantic annotation (e.g., concept linking, fact linking, knowledge base population, semantic frame annotation). These tasks are very time consuming and demanding for annotators, especially when knowledge bases are used. We address these issues by developing an annotation platform that incorporates machine learning capabilities which actively assist and guide annotators. The platform is both generic and modular. It targets a range of research domains in need of semantic annotation, such as digital humanities, bioinformatics, or linguistics. INCEpTION is publicly available as open-source software.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1605288684,"Goal":"Life on Land","Task":["Machine - Assisted and Knowledge - Oriented Interactive Annotation","interactive and semantic annotation","concept linking","fact linking","knowledge base population","semantic frame annotation)","annotators","annotators","semantic annotation","digital humanities","bioinformatics","linguistics"],"Method":["{INCE}p{TION} Platform","INCEpTION","annotation platform","annotation platform","machine learning capabilities","INCEpTION"]},{"ID":"muniz-etal-2018-extending","title":"Extending {W}ordnet to Geological Times","abstract":"This paper describes work extending Princeton WordNet to the domain of geological texts, associated with the time periods of the geological eras of the Earth History. We intend this extension to be considered as an example for any other domain extension that we might want to pursue. To provide this extension, we first produce a textual version of Princeton WordNet. Then we map a fragment of the International Commission on Stratigraphy (ICS) ontologies to WordNet and create the appropriate new synsets. We check the extended ontology on a small corpus of sentences from Gas and Oil technical reports and realize that more work needs to be done, as we need new words, new senses and new compounds in our extended WordNet.","year":2018,"title_abstract":"Extending {W}ordnet to Geological Times This paper describes work extending Princeton WordNet to the domain of geological texts, associated with the time periods of the geological eras of the Earth History. We intend this extension to be considered as an example for any other domain extension that we might want to pursue. To provide this extension, we first produce a textual version of Princeton WordNet. Then we map a fragment of the International Commission on Stratigraphy (ICS) ontologies to WordNet and create the appropriate new synsets. We check the extended ontology on a small corpus of sentences from Gas and Oil technical reports and realize that more work needs to be done, as we need new words, new senses and new compounds in our extended WordNet.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.1605282575,"Goal":"Life Below Water","Task":["domain extension"],"Method":["textual version","WordNet"]},{"ID":"coleman-etal-2020-architecture","title":"Architecture of a Scalable, Secure and Resilient Translation Platform for Multilingual News Media","abstract":"This paper presents an example architecture for a scalable, secure and resilient Machine Translation (MT) platform, using components available via Amazon Web Services (AWS). It is increasingly common for a single news organisation to publish and monitor news sources in multiple languages. A growth in news sources makes this increasingly challenging and time-consuming but MT can help automate some aspects of this process. Building a translation service provides a single integration point for news room tools that use translation technology allowing MT models to be integrated into a system once, rather than each time the translation technology is needed. By using a range of services provided by AWS, it is possible to architect a platform where multiple pre-existing technologies are combined to build a solution, as opposed to developing software from scratch for deployment on a single virtual machine. This increases the speed at which a platform can be developed and allows the use of well-maintained services. However, a single service also provides challenges. It is key to consider how the platform will scale when handling many users and how to ensure the platform is resilient.","year":2020,"title_abstract":"Architecture of a Scalable, Secure and Resilient Translation Platform for Multilingual News Media This paper presents an example architecture for a scalable, secure and resilient Machine Translation (MT) platform, using components available via Amazon Web Services (AWS). It is increasingly common for a single news organisation to publish and monitor news sources in multiple languages. A growth in news sources makes this increasingly challenging and time-consuming but MT can help automate some aspects of this process. Building a translation service provides a single integration point for news room tools that use translation technology allowing MT models to be integrated into a system once, rather than each time the translation technology is needed. By using a range of services provided by AWS, it is possible to architect a platform where multiple pre-existing technologies are combined to build a solution, as opposed to developing software from scratch for deployment on a single virtual machine. This increases the speed at which a platform can be developed and allows the use of well-maintained services. However, a single service also provides challenges. It is key to consider how the platform will scale when handling many users and how to ensure the platform is resilient.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.1604888737,"Goal":"Peace, Justice and Strong Institutions","Task":["Multilingual News Media","scalable , secure and resilient Machine Translation (MT) platform","MT","news room tools"],"Method":["Scalable , Secure and Resilient Translation Platform","Amazon Web Services","translation service","translation technology","MT models","translation technology","AWS"]},{"ID":"mboning-etal-2020-ntealan","title":"{NT}e{AL}an Dictionaries Platforms: An Example Of Collaboration-Based Model","abstract":"Nowadays the scarcity and dispersion of open-source NLP resources and tools in and for African languages make it difficult for researchers to truly fit these languages into current algorithms of artificial intelligence, resulting in the stagnation of these numerous languages, as far as technological progress is concerned. Created in 2017, with the aim of building communities of voluntary contributors around African native and\/or national languages, cultures, NLP technologies and artificial intelligence, the NTeALan association has set up a series of web collaborative platforms intended to allow the aforementioned communities to create and manage their own lexicographic and linguistic resources. This paper aims at presenting the first versions of three lexicographic platforms that we developed in and for African languages: the REST\/GraphQL API for saving lexicographic resources, the dictionary management platform and the collaborative dictionary platform. We also describe the data representation format used for these resources. After experimenting with a few dictionaries and looking at users feedback, we are convinced that only collaboration-based approaches and platforms can effectively respond to challenges of producing quality resources in and for African native and\/or national languages.","year":2020,"title_abstract":"{NT}e{AL}an Dictionaries Platforms: An Example Of Collaboration-Based Model Nowadays the scarcity and dispersion of open-source NLP resources and tools in and for African languages make it difficult for researchers to truly fit these languages into current algorithms of artificial intelligence, resulting in the stagnation of these numerous languages, as far as technological progress is concerned. Created in 2017, with the aim of building communities of voluntary contributors around African native and\/or national languages, cultures, NLP technologies and artificial intelligence, the NTeALan association has set up a series of web collaborative platforms intended to allow the aforementioned communities to create and manage their own lexicographic and linguistic resources. This paper aims at presenting the first versions of three lexicographic platforms that we developed in and for African languages: the REST\/GraphQL API for saving lexicographic resources, the dictionary management platform and the collaborative dictionary platform. We also describe the data representation format used for these resources. After experimenting with a few dictionaries and looking at users feedback, we are convinced that only collaboration-based approaches and platforms can effectively respond to challenges of producing quality resources in and for African native and\/or national languages.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1604582071,"Goal":"Partnership for the Goals","Task":["artificial intelligence","NLP","artificial intelligence","saving lexicographic resources"],"Method":["Dictionaries Platforms","Collaboration - Based Model","NTeALan association","web collaborative platforms","lexicographic platforms","REST\/GraphQL API","dictionary management platform","collaborative dictionary platform","data representation format","collaboration - based approaches"]},{"ID":"li-scarton-2020-revisiting","title":"Revisiting Rumour Stance Classification: Dealing with Imbalanced Data","abstract":"Correctly classifying stances of replies can be significantly helpful for the automatic detection and classification of online rumours. One major challenge is that there are considerably more non-relevant replies (comments) than informative ones (supports and denies), making the task highly imbalanced. In this paper we revisit the task of rumour stance classification, aiming to improve the performance over the informative minority classes. We experiment with traditional methods for imbalanced data treatment with feature- and BERT-based classifiers. Our models outperform all systems in RumourEval 2017 shared task and rank second in RumourEval 2019.","year":2020,"title_abstract":"Revisiting Rumour Stance Classification: Dealing with Imbalanced Data Correctly classifying stances of replies can be significantly helpful for the automatic detection and classification of online rumours. One major challenge is that there are considerably more non-relevant replies (comments) than informative ones (supports and denies), making the task highly imbalanced. In this paper we revisit the task of rumour stance classification, aiming to improve the performance over the informative minority classes. We experiment with traditional methods for imbalanced data treatment with feature- and BERT-based classifiers. Our models outperform all systems in RumourEval 2017 shared task and rank second in RumourEval 2019.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1604447365,"Goal":"Climate Action","Task":["Rumour Stance Classification","classifying stances of replies","automatic detection and classification of online rumours","rumour stance classification","imbalanced data treatment"],"Method":["feature - and BERT - based classifiers"]},{"ID":"hou-etal-2019-modeling","title":"Modeling language learning using specialized Elo rating","abstract":"Automatic assessment of the proficiency levels of the learner is a critical part of Intelligent Tutoring Systems. We present methods for assessment in the context of language learning. We use a specialized Elo formula used in conjunction with educational data mining. We simultaneously obtain ratings for the proficiency of the learners and for the difficulty of the linguistic concepts that the learners are trying to master. From the same data we also learn a graph structure representing a domain model capturing the relations among the concepts. This application of Elo provides ratings for learners and concepts which correlate well with subjective proficiency levels of the learners and difficulty levels of the concepts.","year":2019,"title_abstract":"Modeling language learning using specialized Elo rating Automatic assessment of the proficiency levels of the learner is a critical part of Intelligent Tutoring Systems. We present methods for assessment in the context of language learning. We use a specialized Elo formula used in conjunction with educational data mining. We simultaneously obtain ratings for the proficiency of the learners and for the difficulty of the linguistic concepts that the learners are trying to master. From the same data we also learn a graph structure representing a domain model capturing the relations among the concepts. This application of Elo provides ratings for learners and concepts which correlate well with subjective proficiency levels of the learners and difficulty levels of the concepts.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1604312509,"Goal":"Quality Education","Task":["Modeling language learning","Automatic assessment","Intelligent Tutoring Systems","assessment","language learning","educational data mining"],"Method":["Elo rating","Elo formula","graph structure","domain model","Elo"]},{"ID":"sorato-zavala-rojas-2021-multilingual","title":"The Multilingual Corpus of Survey Questionnaires Query Interface","abstract":"The dawn of the digital age led to increasing demands for digital research resources, which shall be quickly processed and handled by computers. Due to the amount of data created by this digitization process, the design of tools that enable the analysis and management of data and metadata has become a relevant topic. In this context, the Multilingual Corpus of Survey Questionnaires (MCSQ) contributes to the creation and distribution of data for the Social Sciences and Humanities (SSH) following FAIR (Findable, Accessible, Interoperable and Reusable) principles, and provides functionalities for end-users that are not acquainted with programming through an easy-to-use interface. By simply applying the desired filters in the graphic interface, users can build linguistic resources for the survey research and translation areas, such as translation memories, thus facilitating data access and usage.","year":2021,"title_abstract":"The Multilingual Corpus of Survey Questionnaires Query Interface The dawn of the digital age led to increasing demands for digital research resources, which shall be quickly processed and handled by computers. Due to the amount of data created by this digitization process, the design of tools that enable the analysis and management of data and metadata has become a relevant topic. In this context, the Multilingual Corpus of Survey Questionnaires (MCSQ) contributes to the creation and distribution of data for the Social Sciences and Humanities (SSH) following FAIR (Findable, Accessible, Interoperable and Reusable) principles, and provides functionalities for end-users that are not acquainted with programming through an easy-to-use interface. By simply applying the desired filters in the graphic interface, users can build linguistic resources for the survey research and translation areas, such as translation memories, thus facilitating data access and usage.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1603832692,"Goal":"Sustainable Cities and Communities","Task":["analysis and management of data and metadata","distribution of data","Social Sciences","survey research and translation areas","data access","usage"],"Method":["digitization process"]},{"ID":"costa-panissod-2003-systran","title":"{SYSTRAN} Review Manager","abstract":"The SYSTRAN Review Manager (SRM) is one of the components that comprise the SYSTRAN Linguistics Platform (SLP), a comprehensive enterprise solution for managing MT customization and localization projects. The SRM is a productivity tool used for the review, quality assessment and maintenance of linguistic resources combined with a SYSTRAN solution. The SRM is used in-house by SYSTRAN{'}s development team and is also licensed to corporate customers as it addresses leading linguistic challenges, such as terminology and homographs, which makes it a key component of the QA process. Extremely flexible, the SRM adapts to localization and MT customization projects from small to large-scale. Its Web-based interface and multi-user architecture enable a centralized and efficient work environment for local and geographically disbursed individual users and teams. Users segment a given corpus to fluidly review and evaluate translations, as well as identify the typology of errors. Corpus metrics, terminology extraction and detailed reporting capabilities facilitate prioritizing tasks, resulting in immediate focus on those issues that significantly impact MT quality. Data and statistics are tracked throughout the customization process and are always available for regression tests and overall project management. This environment is highly conducive to increased productivity and efficient QA in the MT customization effort.","year":2003,"title_abstract":"{SYSTRAN} Review Manager The SYSTRAN Review Manager (SRM) is one of the components that comprise the SYSTRAN Linguistics Platform (SLP), a comprehensive enterprise solution for managing MT customization and localization projects. The SRM is a productivity tool used for the review, quality assessment and maintenance of linguistic resources combined with a SYSTRAN solution. The SRM is used in-house by SYSTRAN{'}s development team and is also licensed to corporate customers as it addresses leading linguistic challenges, such as terminology and homographs, which makes it a key component of the QA process. Extremely flexible, the SRM adapts to localization and MT customization projects from small to large-scale. Its Web-based interface and multi-user architecture enable a centralized and efficient work environment for local and geographically disbursed individual users and teams. Users segment a given corpus to fluidly review and evaluate translations, as well as identify the typology of errors. Corpus metrics, terminology extraction and detailed reporting capabilities facilitate prioritizing tasks, resulting in immediate focus on those issues that significantly impact MT quality. Data and statistics are tracked throughout the customization process and are always available for regression tests and overall project management. This environment is highly conducive to increased productivity and efficient QA in the MT customization effort.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.160377264,"Goal":"Partnership for the Goals","Task":["MT customization and localization projects","review","quality assessment","maintenance of linguistic resources","linguistic challenges","QA process","localization","MT customization projects","terminology extraction","prioritizing tasks","customization process","regression tests","project management","QA","MT customization effort"],"Method":["Review Manager","SYSTRAN Review Manager","SYSTRAN Linguistics Platform","enterprise solution","SRM","productivity tool","SYSTRAN solution","SRM","SYSTRAN{'}s","SRM","Web - based interface","multi - user architecture","detailed reporting capabilities"]},{"ID":"huang-etal-2020-coda","title":"{CODA-19}: Using a Non-Expert Crowd to Annotate Research Aspects on 10,000+ Abstracts in the {COVID-19} Open Research Dataset","abstract":"This paper introduces CODA-19, a human-annotated dataset that codes the Background, Purpose, Method, Finding\/Contribution, and Other sections of 10,966 English abstracts in the COVID-19 Open Research Dataset. CODA-19 was created by 248 crowd workers from Amazon Mechanical Turk within 10 days, and achieved labeling quality comparable to that of experts. Each abstract was annotated by nine different workers, and the final labels were acquired by majority vote. The inter-annotator agreement (Cohen{'}s kappa) between the crowd and the biomedical expert (0.741) is comparable to inter-expert agreement (0.788). CODA-19{'}s labels have an accuracy of 82.2{\\%} when compared to the biomedical expert{'}s labels, while the accuracy between experts was 85.0{\\%}. Reliable human annotations help scientists access and integrate the rapidly accelerating coronavirus literature, and also serve as the battery of AI\/NLP research, but obtaining expert annotations can be slow. We demonstrated that a non-expert crowd can be rapidly employed at scale to join the fight against COVID-19.","year":2020,"title_abstract":"{CODA-19}: Using a Non-Expert Crowd to Annotate Research Aspects on 10,000+ Abstracts in the {COVID-19} Open Research Dataset This paper introduces CODA-19, a human-annotated dataset that codes the Background, Purpose, Method, Finding\/Contribution, and Other sections of 10,966 English abstracts in the COVID-19 Open Research Dataset. CODA-19 was created by 248 crowd workers from Amazon Mechanical Turk within 10 days, and achieved labeling quality comparable to that of experts. Each abstract was annotated by nine different workers, and the final labels were acquired by majority vote. The inter-annotator agreement (Cohen{'}s kappa) between the crowd and the biomedical expert (0.741) is comparable to inter-expert agreement (0.788). CODA-19{'}s labels have an accuracy of 82.2{\\%} when compared to the biomedical expert{'}s labels, while the accuracy between experts was 85.0{\\%}. Reliable human annotations help scientists access and integrate the rapidly accelerating coronavirus literature, and also serve as the battery of AI\/NLP research, but obtaining expert annotations can be slow. We demonstrated that a non-expert crowd can be rapidly employed at scale to join the fight against COVID-19.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1603732556,"Goal":"Climate Action","Task":["AI\/NLP research"],"Method":["CODA - 19","majority vote","CODA - 19{'}s"]},{"ID":"emami-etal-2019-knowref","title":"The {K}now{R}ef Coreference Corpus: Removing Gender and Number Cues for Difficult Pronominal Anaphora Resolution","abstract":"We introduce a new benchmark for coreference resolution and NLI, KnowRef, that targets common-sense understanding and world knowledge. Previous coreference resolution tasks can largely be solved by exploiting the number and gender of the antecedents, or have been handcrafted and do not reflect the diversity of naturally occurring text. We present a corpus of over 8,000 annotated text passages with ambiguous pronominal anaphora. These instances are both challenging and realistic. We show that various coreference systems, whether rule-based, feature-rich, or neural, perform significantly worse on the task than humans, who display high inter-annotator agreement. To explain this performance gap, we show empirically that state-of-the art models often fail to capture context, instead relying on the gender or number of candidate antecedents to make a decision. We then use problem-specific insights to propose a data-augmentation trick called antecedent switching to alleviate this tendency in models. Finally, we show that antecedent switching yields promising results on other tasks as well: we use it to achieve state-of-the-art results on the GAP coreference task.","year":2019,"title_abstract":"The {K}now{R}ef Coreference Corpus: Removing Gender and Number Cues for Difficult Pronominal Anaphora Resolution We introduce a new benchmark for coreference resolution and NLI, KnowRef, that targets common-sense understanding and world knowledge. Previous coreference resolution tasks can largely be solved by exploiting the number and gender of the antecedents, or have been handcrafted and do not reflect the diversity of naturally occurring text. We present a corpus of over 8,000 annotated text passages with ambiguous pronominal anaphora. These instances are both challenging and realistic. We show that various coreference systems, whether rule-based, feature-rich, or neural, perform significantly worse on the task than humans, who display high inter-annotator agreement. To explain this performance gap, we show empirically that state-of-the art models often fail to capture context, instead relying on the gender or number of candidate antecedents to make a decision. We then use problem-specific insights to propose a data-augmentation trick called antecedent switching to alleviate this tendency in models. Finally, we show that antecedent switching yields promising results on other tasks as well: we use it to achieve state-of-the-art results on the GAP coreference task.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1603267491,"Goal":"Gender Equality","Task":["Difficult Pronominal Anaphora Resolution","coreference resolution","NLI","KnowRef","common - sense understanding","coreference resolution tasks","GAP coreference task"],"Method":["coreference systems","rule - based","feature - rich","neural","data - augmentation trick","antecedent switching","antecedent switching"]},{"ID":"byamugisha-etal-2017-toward","title":"Toward an {NLG} System for {B}antu languages: first steps with {R}unyankore (demo)","abstract":"There are many domain-specific and language-specific NLG systems, of which it may be possible to adapt to related domains and languages. The languages in the Bantu language family have their own set of features distinct from other major groups, which therefore severely limits the options to bootstrap an NLG system from existing ones. We present here our first proof-of-concept application for knowledge-to-text NLG as a plugin to the Protege 5.x ontology development system, tailored to Runyankore, a Bantu language indigenous to Uganda. It comprises a basic annotation model for linguistic information such as noun class, an implementation of existing verbalisation rules and a CFG for verbs, and a basic interface for data entry.","year":2017,"title_abstract":"Toward an {NLG} System for {B}antu languages: first steps with {R}unyankore (demo) There are many domain-specific and language-specific NLG systems, of which it may be possible to adapt to related domains and languages. The languages in the Bantu language family have their own set of features distinct from other major groups, which therefore severely limits the options to bootstrap an NLG system from existing ones. We present here our first proof-of-concept application for knowledge-to-text NLG as a plugin to the Protege 5.x ontology development system, tailored to Runyankore, a Bantu language indigenous to Uganda. It comprises a basic annotation model for linguistic information such as noun class, an implementation of existing verbalisation rules and a CFG for verbs, and a basic interface for data entry.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.160311237,"Goal":"Life Below Water","Task":["NLG","NLG","knowledge - to - text NLG","data entry"],"Method":["{NLG}","Protege 5","ontology development system","annotation model","verbalisation rules","CFG"]},{"ID":"xu-etal-2019-postac","title":"{P}ost{A}c : A Visual Interactive Search, Exploration, and Analysis Platform for {P}h{D} Intensive Job Postings","abstract":"Over 60{\\%} of Australian PhD graduates land their first job after graduation outside academia, but this job market remains largely hidden to these job seekers. Employers{'} low awareness and interest in attracting PhD graduates means that the term {``}PhD{''} is rarely used as a keyword in job advertisements; 80{\\%} of companies looking to employ similar researchers do not specifically ask for a PhD qualification. As a result, typing in {``}PhD{''} to a job search engine tends to return mostly academic jobs. We set out to make the market for advanced research skills more visible to job seekers. In this paper, we present PostAc, an online platform of authentic job postings that helps PhD graduates sharpen their career thinking. The platform is underpinned by research on the key factors that identify what an employer is looking for when they want to hire a highly skilled researcher. Its ranking model leverages the free-form text embedded in the job description to quantify the most sought-after PhD skills and educate information seekers about the Australian job-market appetite for PhD skills. The platform makes visible the geographic location, industry sector, job title, working hours, continuity, and wage of the research intensive jobs. This is the first data-driven exploration in this field. Both empirical results and online platform will be presented in this paper.","year":2019,"title_abstract":"{P}ost{A}c : A Visual Interactive Search, Exploration, and Analysis Platform for {P}h{D} Intensive Job Postings Over 60{\\%} of Australian PhD graduates land their first job after graduation outside academia, but this job market remains largely hidden to these job seekers. Employers{'} low awareness and interest in attracting PhD graduates means that the term {``}PhD{''} is rarely used as a keyword in job advertisements; 80{\\%} of companies looking to employ similar researchers do not specifically ask for a PhD qualification. As a result, typing in {``}PhD{''} to a job search engine tends to return mostly academic jobs. We set out to make the market for advanced research skills more visible to job seekers. In this paper, we present PostAc, an online platform of authentic job postings that helps PhD graduates sharpen their career thinking. The platform is underpinned by research on the key factors that identify what an employer is looking for when they want to hire a highly skilled researcher. Its ranking model leverages the free-form text embedded in the job description to quantify the most sought-after PhD skills and educate information seekers about the Australian job-market appetite for PhD skills. The platform makes visible the geographic location, industry sector, job title, working hours, continuity, and wage of the research intensive jobs. This is the first data-driven exploration in this field. Both empirical results and online platform will be presented in this paper.","social_need":"Decent Work and Economic Growth Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all","cosine_similarity":0.160307616,"Goal":"Decent Work and Economic Growth","Task":["Job Postings","authentic job postings"],"Method":["Visual Interactive Search , Exploration , and Analysis Platform","job search engine","PostAc","online platform","ranking model","online platform"]},{"ID":"oladipo-rufai-2019-implementing","title":"Implementing a Multi-lingual Chatbot for Positive Reinforcement in Young Learners","abstract":"This is a humanitarian work {--}a counter-terrorism effort. The presentation describes the experiences of developing a multi-lingua, interactive chatbot trained on the corpus of two Nigerian Languages (Hausa and Fulfude), with simultaneous translation to a third (Kanuri), to stimulate conversations, deliver tailored contents to the users thereby aiding in the detection of the probability and degree of radicalization in young learners through data analysis of the games moves and vocabularies. As chatbots have the ability to simulate a human conversation based on rhetorical behavior, the system is able to learn the need of individual user through constant interaction and deliver tailored contents that promote good behavior in Hausa, Fulfulde and Kanuri languages.","year":2019,"title_abstract":"Implementing a Multi-lingual Chatbot for Positive Reinforcement in Young Learners This is a humanitarian work {--}a counter-terrorism effort. The presentation describes the experiences of developing a multi-lingua, interactive chatbot trained on the corpus of two Nigerian Languages (Hausa and Fulfude), with simultaneous translation to a third (Kanuri), to stimulate conversations, deliver tailored contents to the users thereby aiding in the detection of the probability and degree of radicalization in young learners through data analysis of the games moves and vocabularies. As chatbots have the ability to simulate a human conversation based on rhetorical behavior, the system is able to learn the need of individual user through constant interaction and deliver tailored contents that promote good behavior in Hausa, Fulfulde and Kanuri languages.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.1602743566,"Goal":"Peace, Justice and Strong Institutions","Task":["Positive Reinforcement","Young Learners","humanitarian work { - - }a counter - terrorism effort","data analysis"],"Method":["Multi - lingual Chatbot","multi - lingua , interactive chatbot"]},{"ID":"dayanik-pado-2021-disentangling","title":"Disentangling Document Topic and Author Gender in Multiple Languages: Lessons for Adversarial Debiasing","abstract":"Text classification is a central tool in NLP. However, when the target classes are strongly correlated with other textual attributes, text classification models can pick up {``}wrong{''} features, leading to bad generalization and biases. In social media analysis, this problem surfaces for demographic user classes such as language, topic, or gender, which influence the generate text to a substantial extent. Adversarial training has been claimed to mitigate this problem, but thorough evaluation is missing. In this paper, we experiment with text classification of the correlated attributes of document topic and author gender, using a novel multilingual parallel corpus of TED talk transcripts. Our findings are: (a) individual classifiers for topic and author gender are indeed biased; (b) debiasing with adversarial training works for topic, but breaks down for author gender; (c) gender debiasing results differ across languages. We interpret the result in terms of feature space overlap, highlighting the role of linguistic surface realization of the target classes.","year":2021,"title_abstract":"Disentangling Document Topic and Author Gender in Multiple Languages: Lessons for Adversarial Debiasing Text classification is a central tool in NLP. However, when the target classes are strongly correlated with other textual attributes, text classification models can pick up {``}wrong{''} features, leading to bad generalization and biases. In social media analysis, this problem surfaces for demographic user classes such as language, topic, or gender, which influence the generate text to a substantial extent. Adversarial training has been claimed to mitigate this problem, but thorough evaluation is missing. In this paper, we experiment with text classification of the correlated attributes of document topic and author gender, using a novel multilingual parallel corpus of TED talk transcripts. Our findings are: (a) individual classifiers for topic and author gender are indeed biased; (b) debiasing with adversarial training works for topic, but breaks down for author gender; (c) gender debiasing results differ across languages. We interpret the result in terms of feature space overlap, highlighting the role of linguistic surface realization of the target classes.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1602538228,"Goal":"Gender Equality","Task":["Adversarial Debiasing","Text classification","NLP","generalization","social media analysis","text classification","author gender","author gender;","gender debiasing"],"Method":["text classification models","Adversarial training","classifiers","debiasing","adversarial training"]},{"ID":"bie-etal-2020-neural","title":"Neural Translation for the {E}uropean {U}nion ({NTEU}) Project","abstract":"The Neural Translation for the European Union (NTEU) project aims to build a neural engine farm with all European official language combinations for eTranslation, without the necessity to use a high-resourced language as a pivot. NTEU started in September 2019 and will run until August 2021.","year":2020,"title_abstract":"Neural Translation for the {E}uropean {U}nion ({NTEU}) Project The Neural Translation for the European Union (NTEU) project aims to build a neural engine farm with all European official language combinations for eTranslation, without the necessity to use a high-resourced language as a pivot. NTEU started in September 2019 and will run until August 2021.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1602456272,"Goal":"Gender Equality","Task":["Neural Translation","Neural Translation","eTranslation"],"Method":["neural engine"]},{"ID":"morgado-da-costa-2020-pinchah","title":"Pinchah Kristang: A Dictionary of Kristang","abstract":"This paper describes the development and current state of Pinchah Kristang {--} an online dictionary for Kristang. Kristang is a critically endangered language of the Portuguese-Eurasian communities residing mainly in Malacca and Singapore. Pinchah Kristang has been a central tool to the revitalization efforts of Kristang in Singapore, and collates information from multiple sources, including existing dictionaries and wordlists, ongoing language documentation work, and new words that emerge regularly from relexification efforts by the community. This online dictionary is powered by the Princeton Wordnet and the Open Kristang Wordnet {--} a choice that brings both advantages and disadvantages. This paper will introduce the current version of this dictionary, motivate some of its design choices, and discuss possible future directions.","year":2020,"title_abstract":"Pinchah Kristang: A Dictionary of Kristang This paper describes the development and current state of Pinchah Kristang {--} an online dictionary for Kristang. Kristang is a critically endangered language of the Portuguese-Eurasian communities residing mainly in Malacca and Singapore. Pinchah Kristang has been a central tool to the revitalization efforts of Kristang in Singapore, and collates information from multiple sources, including existing dictionaries and wordlists, ongoing language documentation work, and new words that emerge regularly from relexification efforts by the community. This online dictionary is powered by the Princeton Wordnet and the Open Kristang Wordnet {--} a choice that brings both advantages and disadvantages. This paper will introduce the current version of this dictionary, motivate some of its design choices, and discuss possible future directions.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1602330804,"Goal":"Sustainable Cities and Communities","Task":["revitalization efforts","language documentation work"],"Method":["Pinchah Kristang","Kristang","Pinchah Kristang","Kristang","Princeton Wordnet","Open Kristang Wordnet"]},{"ID":"uszkoreit-etal-2017-common","title":"Common Round: Application of Language Technologies to Large-Scale Web Debates","abstract":"Web debates play an important role in enabling broad participation of constituencies in social, political and economic decision-taking. However, it is challenging to organize, structure, and navigate a vast number of diverse argumentations and comments collected from many participants over a long time period. In this paper we demonstrate Common Round, a next generation platform for large-scale web debates, which provides functions for eliciting the semantic content and structures from the contributions of participants. In particular, Common Round applies language technologies for the extraction of semantic essence from textual input, aggregation of the formulated opinions and arguments. The platform also provides a cross-lingual access to debates using machine translation.","year":2017,"title_abstract":"Common Round: Application of Language Technologies to Large-Scale Web Debates Web debates play an important role in enabling broad participation of constituencies in social, political and economic decision-taking. However, it is challenging to organize, structure, and navigate a vast number of diverse argumentations and comments collected from many participants over a long time period. In this paper we demonstrate Common Round, a next generation platform for large-scale web debates, which provides functions for eliciting the semantic content and structures from the contributions of participants. In particular, Common Round applies language technologies for the extraction of semantic essence from textual input, aggregation of the formulated opinions and arguments. The platform also provides a cross-lingual access to debates using machine translation.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1602128744,"Goal":"Reduced Inequalities","Task":["Large - Scale Web Debates","Web debates","social , political and economic decision - taking","large - scale web debates","extraction of semantic essence"],"Method":["Language Technologies","Common Round","language technologies","machine translation"]},{"ID":"daudert-2017-analysing","title":"Analysing Market Sentiments: Utilising Deep Learning to Exploit Relationships within the Economy","abstract":"In today{'}s world, globalisation is not only affecting inter-culturalism but also linking markets across the globe. Given that all markets are affecting each other and are not only driven by fundamental data but also by sentiments, sentiment analysis regarding the markets becomes a tool to predict, anticipate, and milden future economic crises such as the one we faced in 2008. In this paper, an approach to improve sentiment analysis by exploiting relationships among different kinds of sentiment, together with supplementary information, from and across various data sources is proposed.","year":2017,"title_abstract":"Analysing Market Sentiments: Utilising Deep Learning to Exploit Relationships within the Economy In today{'}s world, globalisation is not only affecting inter-culturalism but also linking markets across the globe. Given that all markets are affecting each other and are not only driven by fundamental data but also by sentiments, sentiment analysis regarding the markets becomes a tool to predict, anticipate, and milden future economic crises such as the one we faced in 2008. In this paper, an approach to improve sentiment analysis by exploiting relationships among different kinds of sentiment, together with supplementary information, from and across various data sources is proposed.","social_need":"Industry, Innovation and Infrastrucure Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation","cosine_similarity":0.1602086574,"Goal":"Industry, Innovation and Infrastrucure","Task":["Analysing Market Sentiments","sentiment analysis","sentiment analysis"],"Method":["Deep Learning"]},{"ID":"declerck-2008-framework","title":"A Framework for Standardized Syntactic Annotation","abstract":"This poster presents an ISO framework for the standardization of syntactic annotation (SynAF). The normative part SynAF is concerned with a metamodel for syntactic annotation that covers both dimensions of constituency and dependency, and propose thus a multi-layered annotation framework that allows the combined and interrelated annotation of language data along both lines of consideration. This standard is designed to be used in close conjuncion with the metamodel presented in the Linguistic Annotation Framework (LAF) and with ISO 12620, Terminology and other language resources - Data categories.","year":2008,"title_abstract":"A Framework for Standardized Syntactic Annotation This poster presents an ISO framework for the standardization of syntactic annotation (SynAF). The normative part SynAF is concerned with a metamodel for syntactic annotation that covers both dimensions of constituency and dependency, and propose thus a multi-layered annotation framework that allows the combined and interrelated annotation of language data along both lines of consideration. This standard is designed to be used in close conjuncion with the metamodel presented in the Linguistic Annotation Framework (LAF) and with ISO 12620, Terminology and other language resources - Data categories.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1601874232,"Goal":"Partnership for the Goals","Task":["Syntactic Annotation","standardization of syntactic annotation","syntactic annotation","annotation"],"Method":["ISO framework","SynAF","metamodel","multi - layered annotation framework","Linguistic Annotation Framework"]},{"ID":"schmaltz-2018-utility","title":"On the Utility of Lay Summaries and {AI} Safety Disclosures: Toward Robust, Open Research Oversight","abstract":"In this position paper, we propose that the community consider encouraging researchers to include two riders, a {``}Lay Summary{''} and an {``}AI Safety Disclosure{''}, as part of future NLP papers published in ACL forums that present user-facing systems. The goal is to encourage researchers{--}via a relatively non-intrusive mechanism{--}to consider the societal implications of technologies carrying (un)known and\/or (un)knowable long-term risks, to highlight failure cases, and to provide a mechanism by which the general public (and scientists in other disciplines) can more readily engage in the discussion in an informed manner. This simple proposal requires minimal additional up-front costs for researchers; the lay summary, at least, has significant precedence in the medical literature and other areas of science; and the proposal is aimed to supplement, rather than replace, existing approaches for encouraging researchers to consider the ethical implications of their work, such as those of the Collaborative Institutional Training Initiative (CITI) Program and institutional review boards (IRBs).","year":2018,"title_abstract":"On the Utility of Lay Summaries and {AI} Safety Disclosures: Toward Robust, Open Research Oversight In this position paper, we propose that the community consider encouraging researchers to include two riders, a {``}Lay Summary{''} and an {``}AI Safety Disclosure{''}, as part of future NLP papers published in ACL forums that present user-facing systems. The goal is to encourage researchers{--}via a relatively non-intrusive mechanism{--}to consider the societal implications of technologies carrying (un)known and\/or (un)knowable long-term risks, to highlight failure cases, and to provide a mechanism by which the general public (and scientists in other disciplines) can more readily engage in the discussion in an informed manner. This simple proposal requires minimal additional up-front costs for researchers; the lay summary, at least, has significant precedence in the medical literature and other areas of science; and the proposal is aimed to supplement, rather than replace, existing approaches for encouraging researchers to consider the ethical implications of their work, such as those of the Collaborative Institutional Training Initiative (CITI) Program and institutional review boards (IRBs).","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1601480246,"Goal":"Sustainable Cities and Communities","Task":["NLP","user - facing systems"],"Method":["Collaborative Institutional Training Initiative"]},{"ID":"glavas-etal-2019-computational","title":"Computational Analysis of Political Texts: Bridging Research Efforts Across Communities","abstract":"In the last twenty years, political scientists started adopting and developing natural language processing (NLP) methods more actively in order to exploit text as an additional source of data in their analyses. Over the last decade the usage of computational methods for analysis of political texts has drastically expanded in scope, allowing for a sustained growth of the text-as-data community in political science. In political science, NLP methods have been extensively used for a number of analyses types and tasks, including inferring policy position of actors from textual evidence, detecting topics in political texts, and analyzing stylistic aspects of political texts (e.g., assessing the role of language ambiguity in framing the political agenda). Just like in numerous other domains, much of the work on computational analysis of political texts has been enabled and facilitated by the development of resources such as, the topically coded electoral programmes (e.g., the Manifesto Corpus) or topically coded legislative texts (e.g., the Comparative Agenda Project). Political scientists created resources and used available NLP methods to process textual data largely in isolation from the NLP community. At the same time, NLP researchers addressed closely related tasks such as election prediction, ideology classification, and stance detection. In other words, these two communities have been largely agnostic of one another, with NLP researchers mostly unaware of interesting applications in political science and political scientists not applying cutting-edge NLP methodology to their problems. The main goal of this tutorial is to systematize and analyze the body of research work on political texts from both communities. We aim to provide a gentle, all-round introduction to methods and tasks related to computational analysis of political texts. Our vision is to bring the two research communities closer to each other and contribute to faster and more significant developments in this interdisciplinary research area.","year":2019,"title_abstract":"Computational Analysis of Political Texts: Bridging Research Efforts Across Communities In the last twenty years, political scientists started adopting and developing natural language processing (NLP) methods more actively in order to exploit text as an additional source of data in their analyses. Over the last decade the usage of computational methods for analysis of political texts has drastically expanded in scope, allowing for a sustained growth of the text-as-data community in political science. In political science, NLP methods have been extensively used for a number of analyses types and tasks, including inferring policy position of actors from textual evidence, detecting topics in political texts, and analyzing stylistic aspects of political texts (e.g., assessing the role of language ambiguity in framing the political agenda). Just like in numerous other domains, much of the work on computational analysis of political texts has been enabled and facilitated by the development of resources such as, the topically coded electoral programmes (e.g., the Manifesto Corpus) or topically coded legislative texts (e.g., the Comparative Agenda Project). Political scientists created resources and used available NLP methods to process textual data largely in isolation from the NLP community. At the same time, NLP researchers addressed closely related tasks such as election prediction, ideology classification, and stance detection. In other words, these two communities have been largely agnostic of one another, with NLP researchers mostly unaware of interesting applications in political science and political scientists not applying cutting-edge NLP methodology to their problems. The main goal of this tutorial is to systematize and analyze the body of research work on political texts from both communities. We aim to provide a gentle, all-round introduction to methods and tasks related to computational analysis of political texts. Our vision is to bring the two research communities closer to each other and contribute to faster and more significant developments in this interdisciplinary research area.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1601224244,"Goal":"Climate Action","Task":["Computational Analysis of Political Texts","analysis of political texts","text - as - data community","political science","political science","analyses types","inferring policy position of actors","computational analysis of political texts","NLP","election prediction","ideology classification","stance detection","political science","computational analysis of political texts"],"Method":["natural language processing (NLP) methods","computational methods","NLP methods","NLP methods","NLP","NLP researchers","cutting - edge NLP methodology"]},{"ID":"fu-etal-2022-theres","title":"There{'}s a Time and Place for Reasoning Beyond the Image","abstract":"Images are often more significant than only the pixels to human eyes, as we can infer, associate, and reason with contextual information from other sources to establish a more complete picture. For example, in Figure 1, we can find a way to identify the news articles related to the picture through segment-wise understandings of the signs, the buildings, the crowds, and more. This reasoning could provide the time and place the image was taken, which will help us in subsequent tasks, such as automatic storyline construction, correction of image source in intended effect photographs, and upper-stream processing such as image clustering for certain location or time.In this work, we formulate this problem and introduce TARA: a dataset with 16k images with their associated news, time, and location, automatically extracted from New York Times, and an additional 61k examples as distant supervision from WIT. On top of the extractions, we present a crowdsourced subset in which we believe it is possible to find the images{'} spatio-temporal information for evaluation purpose. We show that there exists a 70{\\%} gap between a state-of-the-art joint model and human performance, which is slightly filled by our proposed model that uses segment-wise reasoning, motivating higher-level vision-language joint models that can conduct open-ended reasoning with world knowledge.The data and code are publicly available at https:\/\/github.com\/zeyofu\/TARA.","year":2022,"title_abstract":"There{'}s a Time and Place for Reasoning Beyond the Image Images are often more significant than only the pixels to human eyes, as we can infer, associate, and reason with contextual information from other sources to establish a more complete picture. For example, in Figure 1, we can find a way to identify the news articles related to the picture through segment-wise understandings of the signs, the buildings, the crowds, and more. This reasoning could provide the time and place the image was taken, which will help us in subsequent tasks, such as automatic storyline construction, correction of image source in intended effect photographs, and upper-stream processing such as image clustering for certain location or time.In this work, we formulate this problem and introduce TARA: a dataset with 16k images with their associated news, time, and location, automatically extracted from New York Times, and an additional 61k examples as distant supervision from WIT. On top of the extractions, we present a crowdsourced subset in which we believe it is possible to find the images{'} spatio-temporal information for evaluation purpose. We show that there exists a 70{\\%} gap between a state-of-the-art joint model and human performance, which is slightly filled by our proposed model that uses segment-wise reasoning, motivating higher-level vision-language joint models that can conduct open-ended reasoning with world knowledge.The data and code are publicly available at https:\/\/github.com\/zeyofu\/TARA.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1601044685,"Goal":"Sustainable Cities and Communities","Task":["Reasoning","automatic storyline construction","correction of image source","upper - stream processing","image clustering","open - ended reasoning"],"Method":["joint model","segment - wise reasoning","higher - level vision - language joint models"]},{"ID":"chakrabarty-etal-2021-entrust","title":"{ENTRUST}: Argument Reframing with Language Models and Entailment","abstract":"Framing involves the positive or negative presentation of an argument or issue depending on the audience and goal of the speaker. Differences in lexical framing, the focus of our work, can have large effects on peoples{'} opinions and beliefs. To make progress towards reframing arguments for positive effects, we create a dataset and method for this task. We use a lexical resource for {``}connotations{''} to create a parallel corpus and propose a method for argument reframing that combines controllable text generation (positive connotation) with a post-decoding entailment component (same denotation). Our results show that our method is effective compared to strong baselines along the dimensions of fluency, meaning, and trustworthiness\/reduction of fear.","year":2021,"title_abstract":"{ENTRUST}: Argument Reframing with Language Models and Entailment Framing involves the positive or negative presentation of an argument or issue depending on the audience and goal of the speaker. Differences in lexical framing, the focus of our work, can have large effects on peoples{'} opinions and beliefs. To make progress towards reframing arguments for positive effects, we create a dataset and method for this task. We use a lexical resource for {``}connotations{''} to create a parallel corpus and propose a method for argument reframing that combines controllable text generation (positive connotation) with a post-decoding entailment component (same denotation). Our results show that our method is effective compared to strong baselines along the dimensions of fluency, meaning, and trustworthiness\/reduction of fear.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1601016521,"Goal":"Reduced Inequalities","Task":["Argument Reframing","lexical framing","reframing arguments","argument reframing","controllable text generation"],"Method":["Language Models","Entailment Framing","connotation)","post - decoding entailment component"]},{"ID":"gupta-srinivasaraghavan-2020-explanation","title":"Explanation Regeneration via Multi-Hop {ILP} Inference over Knowledge Base","abstract":"Textgraphs 2020 Workshop organized a shared task on {`}Explanation Regeneration{'} that required reconstructing gold explanations for elementary science questions. This work describes our submission to the task which is based on multiple components: a BERT baseline ranking, an Integer Linear Program (ILP) based re-scoring and a regression model for re-ranking the explanation facts. Our system achieved a Mean Average Precision score of 0.3659.","year":2020,"title_abstract":"Explanation Regeneration via Multi-Hop {ILP} Inference over Knowledge Base Textgraphs 2020 Workshop organized a shared task on {`}Explanation Regeneration{'} that required reconstructing gold explanations for elementary science questions. This work describes our submission to the task which is based on multiple components: a BERT baseline ranking, an Integer Linear Program (ILP) based re-scoring and a regression model for re-ranking the explanation facts. Our system achieved a Mean Average Precision score of 0.3659.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1600106359,"Goal":"Quality Education","Task":["Explanation Regeneration","Regeneration{'}","re - ranking","explanation facts"],"Method":["Multi - Hop {ILP} Inference","BERT baseline ranking","Integer Linear Program (ILP) based re - scoring","regression model"]},{"ID":"filighera-etal-2022-answer","title":"Your Answer is Incorrect... Would you like to know why? Introducing a Bilingual Short Answer Feedback Dataset","abstract":"Handing in a paper or exercise and merely receiving {``}bad{''} or {``}incorrect{''} as feedback is not very helpful when the goal is to improve. Unfortunately, this is currently the kind of feedback given by Automatic Short Answer Grading (ASAG) systems. One of the reasons for this is a lack of content-focused elaborated feedback datasets. To encourage research on explainable and understandable feedback systems, we present the Short Answer Feedback dataset (SAF). Similar to other ASAG datasets, SAF contains learner responses and reference answers to German and English questions. However, instead of only assigning a label or score to the learners{'} answers, SAF also contains elaborated feedback explaining the given score. Thus, SAF enables supervised training of models that grade answers and explain where and why mistakes were made. This paper discusses the need for enhanced feedback models in real-world pedagogical scenarios, describes the dataset annotation process, gives a comprehensive analysis of SAF, and provides T5-based baselines for future comparison.","year":2022,"title_abstract":"Your Answer is Incorrect... Would you like to know why? Introducing a Bilingual Short Answer Feedback Dataset Handing in a paper or exercise and merely receiving {``}bad{''} or {``}incorrect{''} as feedback is not very helpful when the goal is to improve. Unfortunately, this is currently the kind of feedback given by Automatic Short Answer Grading (ASAG) systems. One of the reasons for this is a lack of content-focused elaborated feedback datasets. To encourage research on explainable and understandable feedback systems, we present the Short Answer Feedback dataset (SAF). Similar to other ASAG datasets, SAF contains learner responses and reference answers to German and English questions. However, instead of only assigning a label or score to the learners{'} answers, SAF also contains elaborated feedback explaining the given score. Thus, SAF enables supervised training of models that grade answers and explain where and why mistakes were made. This paper discusses the need for enhanced feedback models in real-world pedagogical scenarios, describes the dataset annotation process, gives a comprehensive analysis of SAF, and provides T5-based baselines for future comparison.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1599800289,"Goal":"Quality Education","Task":["explainable and understandable feedback systems","supervised training","real - world pedagogical scenarios","dataset annotation process","SAF"],"Method":["Automatic Short Answer Grading","SAF","SAF","SAF","feedback models","T5 - based baselines"]},{"ID":"chen-etal-2021-mask","title":"Mask-Align: Self-Supervised Neural Word Alignment","abstract":"Word alignment, which aims to align translationally equivalent words between source and target sentences, plays an important role in many natural language processing tasks. Current unsupervised neural alignment methods focus on inducing alignments from neural machine translation models, which does not leverage the full context in the target sequence. In this paper, we propose Mask-Align, a self-supervised word alignment model that takes advantage of the full context on the target side. Our model masks out each target token and predicts it conditioned on both source and the remaining target tokens. This two-step process is based on the assumption that the source token contributing most to recovering the masked target token should be aligned. We also introduce an attention variant called leaky attention, which alleviates the problem of unexpected high cross-attention weights on special tokens such as periods. Experiments on four language pairs show that our model outperforms previous unsupervised neural aligners and obtains new state-of-the-art results.","year":2021,"title_abstract":"Mask-Align: Self-Supervised Neural Word Alignment Word alignment, which aims to align translationally equivalent words between source and target sentences, plays an important role in many natural language processing tasks. Current unsupervised neural alignment methods focus on inducing alignments from neural machine translation models, which does not leverage the full context in the target sequence. In this paper, we propose Mask-Align, a self-supervised word alignment model that takes advantage of the full context on the target side. Our model masks out each target token and predicts it conditioned on both source and the remaining target tokens. This two-step process is based on the assumption that the source token contributing most to recovering the masked target token should be aligned. We also introduce an attention variant called leaky attention, which alleviates the problem of unexpected high cross-attention weights on special tokens such as periods. Experiments on four language pairs show that our model outperforms previous unsupervised neural aligners and obtains new state-of-the-art results.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1599788964,"Goal":"Gender Equality","Task":["Word alignment","natural language processing tasks","inducing alignments"],"Method":["Mask - Align","Self - Supervised Neural Word Alignment","unsupervised neural alignment methods","neural machine translation models","Mask - Align","self - supervised word alignment model","attention variant","leaky attention","unsupervised neural aligners"]},{"ID":"rahimi-litman-2018-weighting","title":"Weighting Model Based on Group Dynamics to Measure Convergence in Multi-party Dialogue","abstract":"This paper proposes a new weighting method for extending a dyad-level measure of convergence to multi-party dialogues by considering group dynamics instead of simply averaging. Experiments indicate the usefulness of the proposed weighted measure and also show that in general a proper weighting of the dyad-level measures performs better than non-weighted averaging in multiple tasks.","year":2018,"title_abstract":"Weighting Model Based on Group Dynamics to Measure Convergence in Multi-party Dialogue This paper proposes a new weighting method for extending a dyad-level measure of convergence to multi-party dialogues by considering group dynamics instead of simply averaging. Experiments indicate the usefulness of the proposed weighted measure and also show that in general a proper weighting of the dyad-level measures performs better than non-weighted averaging in multiple tasks.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1599324495,"Goal":"Partnership for the Goals","Task":["Convergence","Multi - party Dialogue","dyad - level measure of convergence","multi - party dialogues","multiple tasks"],"Method":["Weighting Model","Group Dynamics","weighting method","averaging","weighted measure","dyad - level measures","non - weighted averaging"]},{"ID":"cadotte-etal-2022-challenges","title":"Challenges and Perspectives for Innu-Aimun within Indigenous Language Technologies","abstract":"Innu-Aimun is an Algonquian language spoken in Eastern Canada. It is the language of the Innu, an Indigenous people that now lives for the most part in a dozen communities across Quebec and Labrador. Although it is alive, Innu-Aimun sees important preservation and revitalization challenges and issues. The state of its technology is still nascent, with very few existing applications. This paper proposes a first survey of the available linguistic resources and existing technology for Innu-Aimun. Considering the existing linguistic and textual resources, we argue that developing language technology is feasible and propose first steps towards NLP applications like machine translation. The goal of developing such technologies is first and foremost to help efforts in improving language transmission and cultural safety and preservation for Innu-Aimun speakers, as those are considered urgent and vital issues. Finally, we discuss the importance of close collaboration and consultation with the Innu community in order to ensure that language technologies are developed respectfully and in accordance with that goal.","year":2022,"title_abstract":"Challenges and Perspectives for Innu-Aimun within Indigenous Language Technologies Innu-Aimun is an Algonquian language spoken in Eastern Canada. It is the language of the Innu, an Indigenous people that now lives for the most part in a dozen communities across Quebec and Labrador. Although it is alive, Innu-Aimun sees important preservation and revitalization challenges and issues. The state of its technology is still nascent, with very few existing applications. This paper proposes a first survey of the available linguistic resources and existing technology for Innu-Aimun. Considering the existing linguistic and textual resources, we argue that developing language technology is feasible and propose first steps towards NLP applications like machine translation. The goal of developing such technologies is first and foremost to help efforts in improving language transmission and cultural safety and preservation for Innu-Aimun speakers, as those are considered urgent and vital issues. Finally, we discuss the importance of close collaboration and consultation with the Innu community in order to ensure that language technologies are developed respectfully and in accordance with that goal.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.1598637253,"Goal":"Peace, Justice and Strong Institutions","Task":["NLP applications","machine translation","language transmission","cultural safety","preservation","Innu - Aimun speakers"],"Method":["Indigenous Language Technologies","language technology","language technologies"]},{"ID":"chollampatt-ng-2017-connecting","title":"Connecting the Dots: Towards Human-Level Grammatical Error Correction","abstract":"We build a grammatical error correction (GEC) system primarily based on the state-of-the-art statistical machine translation (SMT) approach, using task-specific features and tuning, and further enhance it with the modeling power of neural network joint models. The SMT-based system is weak in generalizing beyond patterns seen during training and lacks granularity below the word level. To address this issue, we incorporate a character-level SMT component targeting the misspelled words that the original SMT-based system fails to correct. Our final system achieves 53.14{\\%} F 0.5 score on the benchmark CoNLL-2014 test set, an improvement of 3.62{\\%} F 0.5 over the best previous published score.","year":2017,"title_abstract":"Connecting the Dots: Towards Human-Level Grammatical Error Correction We build a grammatical error correction (GEC) system primarily based on the state-of-the-art statistical machine translation (SMT) approach, using task-specific features and tuning, and further enhance it with the modeling power of neural network joint models. The SMT-based system is weak in generalizing beyond patterns seen during training and lacks granularity below the word level. To address this issue, we incorporate a character-level SMT component targeting the misspelled words that the original SMT-based system fails to correct. Our final system achieves 53.14{\\%} F 0.5 score on the benchmark CoNLL-2014 test set, an improvement of 3.62{\\%} F 0.5 over the best previous published score.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1597287953,"Goal":"Gender Equality","Task":["Human - Level Grammatical Error Correction"],"Method":["grammatical error correction","statistical machine translation (SMT) approach","tuning","modeling power of neural network joint models","SMT - based system","SMT","SMT - based system"]},{"ID":"zhou-etal-2020-universal","title":"{U}niversal {D}ependency Treebank for {X}ibe","abstract":"We present our work of constructing the first treebank for the Xibe language following the Universal Dependencies (UD) annotation scheme. Xibe is a low-resourced and severely endangered Tungusic language spoken by the Xibe minority living in the Xinjiang Uygur Autonomous Region of China. We collected 810 sentences so far, including 544 sentences from a grammar book on written Xibe and 266 sentences from Cabcal News. We annotated those sentences manually from scratch. In this paper, we report the procedure of building this treebank and analyze several important annotation issues of our treebank. Finally, we propose our plans for future work.","year":2020,"title_abstract":"{U}niversal {D}ependency Treebank for {X}ibe We present our work of constructing the first treebank for the Xibe language following the Universal Dependencies (UD) annotation scheme. Xibe is a low-resourced and severely endangered Tungusic language spoken by the Xibe minority living in the Xinjiang Uygur Autonomous Region of China. We collected 810 sentences so far, including 544 sentences from a grammar book on written Xibe and 266 sentences from Cabcal News. We annotated those sentences manually from scratch. In this paper, we report the procedure of building this treebank and analyze several important annotation issues of our treebank. Finally, we propose our plans for future work.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1597136855,"Goal":"Life on Land","Task":["annotation"],"Method":["Universal Dependencies","treebank"]},{"ID":"reynaert-2008-errors","title":"All, and only, the Errors: more Complete and Consistent Spelling and {OCR}-Error Correction Evaluation","abstract":"Some time in the future, some spelling error correction system will correct all the errors, and only the errors. We need evaluation metrics that will tell us when this has been achieved and that can help guide us there. We survey the current practice in the form of the evaluation scheme of the latest major publication on spelling correction in a leading journal. We are forced to conclude that while the metric used there can tell us exactly when the ultimate goal of spelling correction research has been achieved, it offers little in the way of directions to be followed to eventually get there. We propose to consistently use the well-known metrics Recall and Precision, as combined in the F score, on 5 possible levels of measurement that should guide us more informedly along that path. We describe briefly what is then measured or measurable at these levels and propose a framework that should allow for concisely stating what it is one performs in one\u0092s evaluations. We finally contrast our preferred metrics to Accuracy, which is widely used in this field to this day and to the Area-Under-the-Curve, which is increasingly finding acceptance in other fields.","year":2008,"title_abstract":"All, and only, the Errors: more Complete and Consistent Spelling and {OCR}-Error Correction Evaluation Some time in the future, some spelling error correction system will correct all the errors, and only the errors. We need evaluation metrics that will tell us when this has been achieved and that can help guide us there. We survey the current practice in the form of the evaluation scheme of the latest major publication on spelling correction in a leading journal. We are forced to conclude that while the metric used there can tell us exactly when the ultimate goal of spelling correction research has been achieved, it offers little in the way of directions to be followed to eventually get there. We propose to consistently use the well-known metrics Recall and Precision, as combined in the F score, on 5 possible levels of measurement that should guide us more informedly along that path. We describe briefly what is then measured or measurable at these levels and propose a framework that should allow for concisely stating what it is one performs in one\u0092s evaluations. We finally contrast our preferred metrics to Accuracy, which is widely used in this field to this day and to the Area-Under-the-Curve, which is increasingly finding acceptance in other fields.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1596960723,"Goal":"Gender Equality","Task":["spelling correction","spelling correction research"],"Method":["spelling error correction system"]},{"ID":"jindal-etal-2021-breakingbert","title":"{B}reaking{BERT}@{IITK} at {S}em{E}val-2021 Task 9: Statement Verification and Evidence Finding with Tables","abstract":"Recently, there has been an interest in the research on factual verification and prediction over structured data like tables and graphs. To circumvent any false news incident, it is necessary to not only model and predict over structured data efficiently but also to explain those predictions. In this paper, as the part of the SemEval-2021 Task 9, we tackle the problem of fact verification and evidence finding over tabular data. There are two subtasks, in which given a table and a statement\/fact, the subtask A is to determine whether the statement is inferred from the tabular data and the subtask B is to determine which cells in the table provide evidence for the former subtask. We make a comparison of the baselines and state of the art approaches over the given SemTabFact dataset. We also propose a novel approach CellBERT to solve the task of evidence finding, as a form of Natural Language Inference task. We obtain a 3-way F1 score of 0.69 on subtask A and an F1 score of 0.65 on subtask B.","year":2021,"title_abstract":"{B}reaking{BERT}@{IITK} at {S}em{E}val-2021 Task 9: Statement Verification and Evidence Finding with Tables Recently, there has been an interest in the research on factual verification and prediction over structured data like tables and graphs. To circumvent any false news incident, it is necessary to not only model and predict over structured data efficiently but also to explain those predictions. In this paper, as the part of the SemEval-2021 Task 9, we tackle the problem of fact verification and evidence finding over tabular data. There are two subtasks, in which given a table and a statement\/fact, the subtask A is to determine whether the statement is inferred from the tabular data and the subtask B is to determine which cells in the table provide evidence for the former subtask. We make a comparison of the baselines and state of the art approaches over the given SemTabFact dataset. We also propose a novel approach CellBERT to solve the task of evidence finding, as a form of Natural Language Inference task. We obtain a 3-way F1 score of 0.69 on subtask A and an F1 score of 0.65 on subtask B.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1596752703,"Goal":"Climate Action","Task":["Statement Verification","Evidence Finding","factual verification","prediction","fact verification","evidence finding","evidence finding","Natural Language Inference task"],"Method":["CellBERT"]},{"ID":"gonzalez-etal-2017-elirf","title":"{EL}i{RF}-{UPV} at {S}em{E}val-2017 Task 4: Sentiment Analysis using Deep Learning","abstract":"This paper describes the participation of ELiRF-UPV team at task 4 of SemEval2017. Our approach is based on the use of convolutional and recurrent neural networks and the combination of general and specific word embeddings with polarity lexicons. We participated in all of the proposed subtasks both for English and Arabic languages using the same system with small variations.","year":2017,"title_abstract":"{EL}i{RF}-{UPV} at {S}em{E}val-2017 Task 4: Sentiment Analysis using Deep Learning This paper describes the participation of ELiRF-UPV team at task 4 of SemEval2017. Our approach is based on the use of convolutional and recurrent neural networks and the combination of general and specific word embeddings with polarity lexicons. We participated in all of the proposed subtasks both for English and Arabic languages using the same system with small variations.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.159648478,"Goal":"Gender Equality","Task":["Sentiment Analysis"],"Method":["Deep Learning","ELiRF - UPV team","convolutional and recurrent neural networks","general and specific word embeddings","polarity lexicons"]},{"ID":"shi-etal-2021-highland","title":"{H}ighland {P}uebla {N}ahuatl Speech Translation Corpus for Endangered Language Documentation","abstract":"Documentation of endangered languages (ELs) has become increasingly urgent as thousands of languages are on the verge of disappearing by the end of the 21st century. One challenging aspect of documentation is to develop machine learning tools to automate the processing of EL audio via automatic speech recognition (ASR), machine translation (MT), or speech translation (ST). This paper presents an open-access speech translation corpus of Highland Puebla Nahuatl (glottocode high1278), an EL spoken in central Mexico. It then addresses machine learning contributions to endangered language documentation and argues for the importance of speech translation as a key element in the documentation process. In our experiments, we observed that state-of-the-art end-to-end ST models could outperform a cascaded ST (ASR {\\textgreater} MT) pipeline when translating endangered language documentation materials.","year":2021,"title_abstract":"{H}ighland {P}uebla {N}ahuatl Speech Translation Corpus for Endangered Language Documentation Documentation of endangered languages (ELs) has become increasingly urgent as thousands of languages are on the verge of disappearing by the end of the 21st century. One challenging aspect of documentation is to develop machine learning tools to automate the processing of EL audio via automatic speech recognition (ASR), machine translation (MT), or speech translation (ST). This paper presents an open-access speech translation corpus of Highland Puebla Nahuatl (glottocode high1278), an EL spoken in central Mexico. It then addresses machine learning contributions to endangered language documentation and argues for the importance of speech translation as a key element in the documentation process. In our experiments, we observed that state-of-the-art end-to-end ST models could outperform a cascaded ST (ASR {\\textgreater} MT) pipeline when translating endangered language documentation materials.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1596319824,"Goal":"Life on Land","Task":["Endangered Language Documentation Documentation of endangered languages","documentation","EL audio","automatic speech recognition","machine translation","speech translation","endangered language documentation","speech translation","documentation process","endangered language documentation materials"],"Method":["machine learning tools","machine learning","end - to - end ST models","cascaded ST","MT)"]},{"ID":"san-vicente-etal-2021-gepsa","title":"{GEPSA}, a tool for monitoring social challenges in digital press","abstract":"This papers presents a platform for monitoring press narratives with respect to several social challenges, including gender equality, migrations and minority languages. As narratives are encoded in natural language, we have to use natural processing techniques to automate their analysis. Thus, crawled news are processed by means of several NLP modules, including named entity recognition, keyword extraction,document classification for social challenge detection, and sentiment analysis. A Flask powered interface provides data visualization for a user-based analysis of the data. This paper presents the architecture of the system and describes in detail its different components. Evaluation is provided for the modules related to extraction and classification of information regarding social challenges.","year":2021,"title_abstract":"{GEPSA}, a tool for monitoring social challenges in digital press This papers presents a platform for monitoring press narratives with respect to several social challenges, including gender equality, migrations and minority languages. As narratives are encoded in natural language, we have to use natural processing techniques to automate their analysis. Thus, crawled news are processed by means of several NLP modules, including named entity recognition, keyword extraction,document classification for social challenge detection, and sentiment analysis. A Flask powered interface provides data visualization for a user-based analysis of the data. This paper presents the architecture of the system and describes in detail its different components. Evaluation is provided for the modules related to extraction and classification of information regarding social challenges.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.159594059,"Goal":"Sustainable Cities and Communities","Task":["monitoring social challenges","monitoring press narratives","analysis","document classification","social challenge detection","sentiment analysis","data visualization","user - based analysis","extraction and classification of information regarding social challenges"],"Method":["{GEPSA}","natural processing techniques","NLP modules","named entity recognition","keyword extraction","Flask powered interface"]},{"ID":"el-beltagy-etal-2017-niletmrg","title":"{N}ile{TMRG} at {S}em{E}val-2017 Task 4: {A}rabic Sentiment Analysis","abstract":"This paper describes two systems that were used by the NileTMRG for addressing Arabic Sentiment Analysis as part of SemEval-2017, task 4. NileTMRG participated in three Arabic related subtasks which are: Subtask A (Message Polarity Classification), Subtask B (Topic-Based Message Polarity classification) and Subtask D (Tweet quantification). For subtask A, we made use of NU{'}s sentiment analyzer which we augmented with a scored lexicon. For subtasks B and D, we used an ensemble of three different classifiers. The first classifier was a convolutional neural network that used trained (word2vec) word embeddings. The second classifier consisted of a MultiLayer Perceptron while the third classifier was a Logistic regression model that takes the same input as the second classifier. Voting between the three classifiers was used to determine the final outcome. In all three Arabic related tasks in which NileTMRG participated, the team ranked at number one.","year":2017,"title_abstract":"{N}ile{TMRG} at {S}em{E}val-2017 Task 4: {A}rabic Sentiment Analysis This paper describes two systems that were used by the NileTMRG for addressing Arabic Sentiment Analysis as part of SemEval-2017, task 4. NileTMRG participated in three Arabic related subtasks which are: Subtask A (Message Polarity Classification), Subtask B (Topic-Based Message Polarity classification) and Subtask D (Tweet quantification). For subtask A, we made use of NU{'}s sentiment analyzer which we augmented with a scored lexicon. For subtasks B and D, we used an ensemble of three different classifiers. The first classifier was a convolutional neural network that used trained (word2vec) word embeddings. The second classifier consisted of a MultiLayer Perceptron while the third classifier was a Logistic regression model that takes the same input as the second classifier. Voting between the three classifiers was used to determine the final outcome. In all three Arabic related tasks in which NileTMRG participated, the team ranked at number one.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1595619321,"Goal":"Gender Equality","Task":["Sentiment Analysis","Arabic Sentiment Analysis","SemEval - 2017","Polarity Classification)","Message Polarity classification)","quantification)","Arabic related tasks","NileTMRG"],"Method":["NileTMRG","NileTMRG","NU{'}s sentiment analyzer","classifiers","classifier","convolutional neural network","word embeddings","classifier","MultiLayer Perceptron","classifier","Logistic regression model","classifier","classifiers"]},{"ID":"mayfield-black-2019-stance","title":"Stance Classification, Outcome Prediction, and Impact Assessment: {NLP} Tasks for Studying Group Decision-Making","abstract":"In group decision-making, the nuanced process of conflict and resolution that leads to consensus formation is closely tied to the quality of decisions made. Behavioral scientists rarely have rich access to process variables, though, as unstructured discussion transcripts are difficult to analyze. Here, we define ways for NLP researchers to contribute to the study of groups and teams. We introduce three tasks alongside a large new corpus of over 400,000 group debates on Wikipedia. We describe the tasks and their importance, then provide baselines showing that BERT contextualized word embeddings consistently outperform other language representations.","year":2019,"title_abstract":"Stance Classification, Outcome Prediction, and Impact Assessment: {NLP} Tasks for Studying Group Decision-Making In group decision-making, the nuanced process of conflict and resolution that leads to consensus formation is closely tied to the quality of decisions made. Behavioral scientists rarely have rich access to process variables, though, as unstructured discussion transcripts are difficult to analyze. Here, we define ways for NLP researchers to contribute to the study of groups and teams. We introduce three tasks alongside a large new corpus of over 400,000 group debates on Wikipedia. We describe the tasks and their importance, then provide baselines showing that BERT contextualized word embeddings consistently outperform other language representations.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.1595518142,"Goal":"Peace, Justice and Strong Institutions","Task":["Stance Classification","Outcome Prediction","Impact Assessment","{NLP} Tasks","Group Decision - Making","group decision - making","nuanced process of conflict and resolution","consensus formation","Behavioral scientists","NLP researchers"],"Method":["BERT contextualized word embeddings","language representations"]},{"ID":"zhang-etal-2008-comparative","title":"A Comparative Evaluation of Term Recognition Algorithms","abstract":"Automatic Term recognition (ATR) is a fundamental processing step preceding more complex tasks such as semantic search and ontology learning. From a large number of methodologies available in the literature only a few are able to handle both single and multi-word terms. In this paper we present a comparison of five such algorithms and propose a combined approach us{\\neg}ing a voting mechanism. We evaluated the six approaches using two different corpora and show how the voting algo{\\neg}rithm performs best on one corpus (a collection of texts from Wikipedia) and less well using the Genia corpus (a standard life science corpus). This indicates that choice and design of corpus has a major impact on the evaluation of term recog{\\neg}nition algorithms. Our experiments also showed that single-word terms can be equally important and occupy a fairly large proportion in certain domains. As a result, algorithms that ignore single-word terms may cause problems to tasks built on top of ATR. Effective ATR systems also need to take into account both the unstructured text and the structured aspects and this means information extraction techniques need to be integrated into the term recognition process.","year":2008,"title_abstract":"A Comparative Evaluation of Term Recognition Algorithms Automatic Term recognition (ATR) is a fundamental processing step preceding more complex tasks such as semantic search and ontology learning. From a large number of methodologies available in the literature only a few are able to handle both single and multi-word terms. In this paper we present a comparison of five such algorithms and propose a combined approach us{\\neg}ing a voting mechanism. We evaluated the six approaches using two different corpora and show how the voting algo{\\neg}rithm performs best on one corpus (a collection of texts from Wikipedia) and less well using the Genia corpus (a standard life science corpus). This indicates that choice and design of corpus has a major impact on the evaluation of term recog{\\neg}nition algorithms. Our experiments also showed that single-word terms can be equally important and occupy a fairly large proportion in certain domains. As a result, algorithms that ignore single-word terms may cause problems to tasks built on top of ATR. Effective ATR systems also need to take into account both the unstructured text and the structured aspects and this means information extraction techniques need to be integrated into the term recognition process.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1595372856,"Goal":"Life on Land","Task":["Automatic Term recognition","semantic search","ontology learning","ATR"],"Method":["Term Recognition Algorithms","voting mechanism","voting algo{\\neg}rithm","term recog{\\neg}nition algorithms","ATR","information extraction techniques","term recognition process"]},{"ID":"b-etal-2022-findings-shared","title":"Findings of the Shared Task on Speech Recognition for Vulnerable Individuals in {T}amil","abstract":"This paper illustrates the overview of the sharedtask on automatic speech recognition in the Tamillanguage. In the shared task, spontaneousTamil speech data gathered from elderly andtransgender people was given for recognitionand evaluation. These utterances were collected from people when they communicatedin the public locations such as hospitals, markets, vegetable shop, etc. The speech corpusincludes utterances of male, female, and transgender and was split into training and testingdata. The given task was evaluated using WER(Word Error Rate). The participants used thetransformer-based model for automatic speechrecognition. Different results using differentpre-trained transformer models are discussedin this overview paper.","year":2022,"title_abstract":"Findings of the Shared Task on Speech Recognition for Vulnerable Individuals in {T}amil This paper illustrates the overview of the sharedtask on automatic speech recognition in the Tamillanguage. In the shared task, spontaneousTamil speech data gathered from elderly andtransgender people was given for recognitionand evaluation. These utterances were collected from people when they communicatedin the public locations such as hospitals, markets, vegetable shop, etc. The speech corpusincludes utterances of male, female, and transgender and was split into training and testingdata. The given task was evaluated using WER(Word Error Rate). The participants used thetransformer-based model for automatic speechrecognition. Different results using differentpre-trained transformer models are discussedin this overview paper.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1595218927,"Goal":"Gender Equality","Task":["Speech Recognition","automatic speech recognition","recognitionand evaluation","automatic speechrecognition"],"Method":["transformer models"]},{"ID":"pratapa-etal-2020-constrained","title":"{C}onstrained {F}act {V}erification for {FEVER}","abstract":"Fact-verification systems are well explored in the NLP literature with growing attention owing to shared tasks like FEVER. Though the task requires reasoning on extracted evidence to verify a claim{'}s factuality, there is little work on understanding the reasoning process. In this work, we propose a new methodology for fact-verification, specifically FEVER, that enforces a closed-world reliance on extracted evidence. We present an extensive evaluation of state-of-the-art verification models under these constraints.","year":2020,"title_abstract":"{C}onstrained {F}act {V}erification for {FEVER} Fact-verification systems are well explored in the NLP literature with growing attention owing to shared tasks like FEVER. Though the task requires reasoning on extracted evidence to verify a claim{'}s factuality, there is little work on understanding the reasoning process. In this work, we propose a new methodology for fact-verification, specifically FEVER, that enforces a closed-world reliance on extracted evidence. We present an extensive evaluation of state-of-the-art verification models under these constraints.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1594892293,"Goal":"Climate Action","Task":["NLP","reasoning process","fact - verification"],"Method":["Fact - verification systems","FEVER","verification models"]},{"ID":"wang-etal-2022-feeding","title":"Feeding What You Need by Understanding What You Learned","abstract":"Machine Reading Comprehension (MRC) reveals the ability to understand a given text passage and answer questions based on it. Existing research works in MRC rely heavily on large-size models and corpus to improve the performance evaluated by metrics such as Exact Match ($EM$) and $F_1$. However, such a paradigm lacks sufficient interpretation to model capability and can not efficiently train a model with a large corpus. In this paper, we argue that a deep understanding of model capabilities and data properties can help us feed a model with appropriate training data based on its learning status. Specifically, we design an MRC capability assessment framework that assesses model capabilities in an explainable and multi-dimensional manner. Based on it, we further uncover and disentangle the connections between various data properties and model performance. Finally, to verify the effectiveness of the proposed MRC capability assessment framework, we incorporate it into a curriculum learning pipeline and devise a Capability Boundary Breakthrough Curriculum (CBBC) strategy, which performs a model capability-based training to maximize the data value and improve training efficiency. Extensive experiments demonstrate that our approach significantly improves performance, achieving up to an 11.22{\\%} \/ 8.71{\\%} improvement of $EM$ \/ $F_1$ on MRC tasks.","year":2022,"title_abstract":"Feeding What You Need by Understanding What You Learned Machine Reading Comprehension (MRC) reveals the ability to understand a given text passage and answer questions based on it. Existing research works in MRC rely heavily on large-size models and corpus to improve the performance evaluated by metrics such as Exact Match ($EM$) and $F_1$. However, such a paradigm lacks sufficient interpretation to model capability and can not efficiently train a model with a large corpus. In this paper, we argue that a deep understanding of model capabilities and data properties can help us feed a model with appropriate training data based on its learning status. Specifically, we design an MRC capability assessment framework that assesses model capabilities in an explainable and multi-dimensional manner. Based on it, we further uncover and disentangle the connections between various data properties and model performance. Finally, to verify the effectiveness of the proposed MRC capability assessment framework, we incorporate it into a curriculum learning pipeline and devise a Capability Boundary Breakthrough Curriculum (CBBC) strategy, which performs a model capability-based training to maximize the data value and improve training efficiency. Extensive experiments demonstrate that our approach significantly improves performance, achieving up to an 11.22{\\%} \/ 8.71{\\%} improvement of $EM$ \/ $F_1$ on MRC tasks.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1594789028,"Goal":"Quality Education","Task":["Machine Reading Comprehension","MRC","MRC tasks"],"Method":["large - size models","MRC capability assessment framework","MRC capability assessment framework","curriculum learning pipeline","Capability Boundary Breakthrough Curriculum","model capability - based training"]},{"ID":"mishra-etal-2020-generating","title":"Generating Fact Checking Summaries for Web Claims","abstract":"We present SUMO, a neural attention-based approach that learns to establish correctness of textual claims based on evidence in the form of text documents (e.g., news articles or web documents). SUMO further generates an extractive summary by presenting a diversified set of sentences from the documents that explain its decision on the correctness of the textual claim. Prior approaches to address the problem of fact checking and evidence extraction have relied on simple concatenation of claim and document word embeddings as an input to claim driven attention weight computation. This is done so as to extract salient words and sentences from the documents that help establish the correctness of the claim. However this design of claim-driven attention fails to capture the contextual information in documents properly. We improve on the prior art by using improved claim and title guided hierarchical attention to model effective contextual cues. We show the efficacy of our approach on political, healthcare, and environmental datasets.","year":2020,"title_abstract":"Generating Fact Checking Summaries for Web Claims We present SUMO, a neural attention-based approach that learns to establish correctness of textual claims based on evidence in the form of text documents (e.g., news articles or web documents). SUMO further generates an extractive summary by presenting a diversified set of sentences from the documents that explain its decision on the correctness of the textual claim. Prior approaches to address the problem of fact checking and evidence extraction have relied on simple concatenation of claim and document word embeddings as an input to claim driven attention weight computation. This is done so as to extract salient words and sentences from the documents that help establish the correctness of the claim. However this design of claim-driven attention fails to capture the contextual information in documents properly. We improve on the prior art by using improved claim and title guided hierarchical attention to model effective contextual cues. We show the efficacy of our approach on political, healthcare, and environmental datasets.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1594733298,"Goal":"Climate Action","Task":["Fact Checking Summaries","Web Claims","extractive summary","fact checking and evidence extraction","claim driven attention weight computation"],"Method":["SUMO","neural attention - based approach","SUMO","concatenation of claim and document word embeddings","claim - driven attention","claim and title guided hierarchical attention"]},{"ID":"arranz-etal-2014-elras","title":"{ELRA}{'}s Consolidated Services for the {HLT} Community","abstract":"This paper emphasises on ELRA\u0092s contribution to the HLT field thanks to the consolidation of its services since LREC 2012. Among the most recent contributions is the establishment of the International Standard Language Resource Number (ISLRN), with the creation and exploitation of an associated web portal to enable the procurement of unique identifiers for Language Resources. Interoperability, consolidation and synchronization remain also a strong focus in ELRA\u0092s cataloguing work, in particular with ELRA\u0092s involvement in the META-SHARE project, whose platform is to become ELRA\u0092s next instrument of sharing LRs. Since last LREC, ELRA has continued its action to offer free LRs to the research community. Cooperation is another watchword within ELRA\u0092s activities on multiple aspects: 1) at the legal level, ELRA is supporting the EC in identifying the gaps to be fulfilled to reach harmonized copyright regulations for the HLT community in Europe; 2) at the production level, ELRA is participating in several international projects, in the field of LR production and evaluation of technologies; 3) at the communication level, ELRA has organised the NLP12 meeting with the aim of boosting co-operation and strengthening the bridges between various communities.","year":2014,"title_abstract":"{ELRA}{'}s Consolidated Services for the {HLT} Community This paper emphasises on ELRA\u0092s contribution to the HLT field thanks to the consolidation of its services since LREC 2012. Among the most recent contributions is the establishment of the International Standard Language Resource Number (ISLRN), with the creation and exploitation of an associated web portal to enable the procurement of unique identifiers for Language Resources. Interoperability, consolidation and synchronization remain also a strong focus in ELRA\u0092s cataloguing work, in particular with ELRA\u0092s involvement in the META-SHARE project, whose platform is to become ELRA\u0092s next instrument of sharing LRs. Since last LREC, ELRA has continued its action to offer free LRs to the research community. Cooperation is another watchword within ELRA\u0092s activities on multiple aspects: 1) at the legal level, ELRA is supporting the EC in identifying the gaps to be fulfilled to reach harmonized copyright regulations for the HLT community in Europe; 2) at the production level, ELRA is participating in several international projects, in the field of LR production and evaluation of technologies; 3) at the communication level, ELRA has organised the NLP12 meeting with the aim of boosting co-operation and strengthening the bridges between various communities.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1594396234,"Goal":"Partnership for the Goals","Task":["HLT","Interoperability","consolidation","synchronization","ELRA\u0092s cataloguing work","META - SHARE project","sharing LRs","Cooperation","HLT community","LR production","evaluation of technologies;","NLP12 meeting"],"Method":["web portal","LREC","ELRA","ELRA","ELRA"]},{"ID":"abdou-etal-2021-language","title":"Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color","abstract":"Pretrained language models have been shown to encode relational information, such as the relations between entities or concepts in knowledge-bases {---} (Paris, Capital, France). However, simple relations of this type can often be recovered heuristically and the extent to which models implicitly reflect topological structure that is grounded in world, such as perceptual structure, is unknown. To explore this question, we conduct a thorough case study on color. Namely, we employ a dataset of monolexemic color terms and color chips represented in CIELAB, a color space with a perceptually meaningful distance metric. Using two methods of evaluating the structural alignment of colors in this space with text-derived color term representations, we find significant correspondence. Analyzing the differences in alignment across the color spectrum, we find that warmer colors are, on average, better aligned to the perceptual color space than cooler ones, suggesting an intriguing connection to findings from recent work on efficient communication in color naming. Further analysis suggests that differences in alignment are, in part, mediated by collocationality and differences in syntactic usage, posing questions as to the relationship between color perception and usage and context.","year":2021,"title_abstract":"Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color Pretrained language models have been shown to encode relational information, such as the relations between entities or concepts in knowledge-bases {---} (Paris, Capital, France). However, simple relations of this type can often be recovered heuristically and the extent to which models implicitly reflect topological structure that is grounded in world, such as perceptual structure, is unknown. To explore this question, we conduct a thorough case study on color. Namely, we employ a dataset of monolexemic color terms and color chips represented in CIELAB, a color space with a perceptually meaningful distance metric. Using two methods of evaluating the structural alignment of colors in this space with text-derived color term representations, we find significant correspondence. Analyzing the differences in alignment across the color spectrum, we find that warmer colors are, on average, better aligned to the perceptual color space than cooler ones, suggesting an intriguing connection to findings from recent work on efficient communication in color naming. Further analysis suggests that differences in alignment are, in part, mediated by collocationality and differences in syntactic usage, posing questions as to the relationship between color perception and usage and context.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1594192386,"Goal":"Reduced Inequalities","Task":["alignment","communication","color naming","alignment","color perception"],"Method":["Language Models","Color Pretrained language models","text - derived color term representations"]},{"ID":"ananiadou-etal-2010-evaluating","title":"Evaluating a Text Mining Based Educational Search Portal","abstract":"In this paper, we present the main features of a text mining based search engine for the UK Educational Evidence Portal available at the UK National Centre for Text Mining (NaCTeM), together with a user-centred framework for the evaluation of the search engine. The framework is adapted from an existing proposal by the ISLE (EAGLES) Evaluation Working group. We introduce the metrics employed for the evaluation, and explain how these relate to the text mining based search engine. Following this, we describe how we applied the framework to the evaluation of a number of key text mining features of the search engine, namely the automatic clustering of search results, classification of search results according to a taxonomy, and identification of topics and other documents that are related to a chosen document. Finally, we present the results of the evaluation in terms of the strengths, weaknesses and improvements identified for each of these features.","year":2010,"title_abstract":"Evaluating a Text Mining Based Educational Search Portal In this paper, we present the main features of a text mining based search engine for the UK Educational Evidence Portal available at the UK National Centre for Text Mining (NaCTeM), together with a user-centred framework for the evaluation of the search engine. The framework is adapted from an existing proposal by the ISLE (EAGLES) Evaluation Working group. We introduce the metrics employed for the evaluation, and explain how these relate to the text mining based search engine. Following this, we describe how we applied the framework to the evaluation of a number of key text mining features of the search engine, namely the automatic clustering of search results, classification of search results according to a taxonomy, and identification of topics and other documents that are related to a chosen document. Finally, we present the results of the evaluation in terms of the strengths, weaknesses and improvements identified for each of these features.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1594098061,"Goal":"Quality Education","Task":["Text Mining","evaluation","automatic clustering of search results","classification of search results"],"Method":["Text Mining Based Educational Search Portal","text mining based search engine","user - centred framework","search engine","text mining based search engine","text mining features","search engine"]},{"ID":"haouari-etal-2021-arcov19","title":"{A}r{COV}19-Rumors: {A}rabic {COVID}-19 {T}witter Dataset for Misinformation Detection","abstract":"In this paper we introduce ArCOV19-Rumors, an Arabic COVID-19 Twitter dataset for misinformation detection composed of tweets containing claims from 27th January till the end of April 2020. We collected 138 verified claims, mostly from popular fact-checking websites, and identified 9.4K relevant tweets to those claims. Tweets were manually-annotated by veracity to support research on misinformation detection, which is one of the major problems faced during a pandemic. ArCOV19-Rumors supports two levels of misinformation detection over Twitter: verifying free-text claims (called claim-level verification) and verifying claims expressed in tweets (called tweet-level verification). Our dataset covers, in addition to health, claims related to other topical categories that were influenced by COVID-19, namely, social, politics, sports, entertainment, and religious. Moreover, we present benchmarking results for tweet-level verification on the dataset. We experimented with SOTA models of versatile approaches that either exploit content, user profiles features, temporal features and propagation structure of the conversational threads for tweet verification.","year":2021,"title_abstract":"{A}r{COV}19-Rumors: {A}rabic {COVID}-19 {T}witter Dataset for Misinformation Detection In this paper we introduce ArCOV19-Rumors, an Arabic COVID-19 Twitter dataset for misinformation detection composed of tweets containing claims from 27th January till the end of April 2020. We collected 138 verified claims, mostly from popular fact-checking websites, and identified 9.4K relevant tweets to those claims. Tweets were manually-annotated by veracity to support research on misinformation detection, which is one of the major problems faced during a pandemic. ArCOV19-Rumors supports two levels of misinformation detection over Twitter: verifying free-text claims (called claim-level verification) and verifying claims expressed in tweets (called tweet-level verification). Our dataset covers, in addition to health, claims related to other topical categories that were influenced by COVID-19, namely, social, politics, sports, entertainment, and religious. Moreover, we present benchmarking results for tweet-level verification on the dataset. We experimented with SOTA models of versatile approaches that either exploit content, user profiles features, temporal features and propagation structure of the conversational threads for tweet verification.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1593831778,"Goal":"Climate Action","Task":["Misinformation Detection","misinformation detection","misinformation detection","misinformation detection","verifying free - text claims","claim - level verification)","verifying claims","tweet - level verification)","tweet - level verification","tweet verification"],"Method":["SOTA models","versatile approaches"]},{"ID":"kaplan-etal-2010-annotation","title":"Annotation Process Management Revisited","abstract":"Proper annotation process management is crucial to the construction of corpora, which are in turn indispensable to the data-driven techniques that have come to the forefront in NLP during the last two decades. It is still common to see ad-hoc tools created for a specific annotation project, but it is time this changed; creation of such tools is labor and time expensive, and is secondary to corpus creation. In addition, such tools likely lack proper annotation process management, increasingly more important as corpora sizes grow in size and complexity. This paper first raises a list of ten needs that any general purpose annotation system should address moving forward, such as user {\\&} role management, delegation {\\&} monitoring of work, diffing {\\&} merging annotators\u0092 work, versioning of corpora, multilingual support, import\/export format flexibility, and so on. A framework to address these needs is then proposed, and how having proper annotation process management can be beneficial to the creation and maintenance of corpora explained. The paper then introduces SLATE (Segment and Link-based Annotation Tool Enhanced), the second iteration of a web-based annotation tool, which is being rewritten to implement the proposed framework.","year":2010,"title_abstract":"Annotation Process Management Revisited Proper annotation process management is crucial to the construction of corpora, which are in turn indispensable to the data-driven techniques that have come to the forefront in NLP during the last two decades. It is still common to see ad-hoc tools created for a specific annotation project, but it is time this changed; creation of such tools is labor and time expensive, and is secondary to corpus creation. In addition, such tools likely lack proper annotation process management, increasingly more important as corpora sizes grow in size and complexity. This paper first raises a list of ten needs that any general purpose annotation system should address moving forward, such as user {\\&} role management, delegation {\\&} monitoring of work, diffing {\\&} merging annotators\u0092 work, versioning of corpora, multilingual support, import\/export format flexibility, and so on. A framework to address these needs is then proposed, and how having proper annotation process management can be beneficial to the creation and maintenance of corpora explained. The paper then introduces SLATE (Segment and Link-based Annotation Tool Enhanced), the second iteration of a web-based annotation tool, which is being rewritten to implement the proposed framework.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1593758315,"Goal":"Partnership for the Goals","Task":["Annotation Process Management","annotation process management","construction of corpora","NLP","annotation project","corpus creation","annotation","role management","monitoring of work","creation and maintenance of corpora"],"Method":["data - driven techniques","annotation process management","annotation process management","SLATE (Segment and Link - based Annotation Tool","web - based annotation tool"]},{"ID":"cho-etal-2021-agenda","title":"Agenda Pushing in Email to Thwart Phishing","abstract":"In this work, we draw parallels between automatically responding to emails for combating social-engineering attacks and document-grounded response generation and lay out the blueprint of our approach. Phishing emails are longer than dialogue utterances and often contain multiple intents. Hence, we need to make decisions similar to those for document-grounded responses in deciding what parts of long text to use and how to address each intent to generate a knowledgeable multi-component response that pushes scammers towards agendas that aid in attribution and linking attacks. We propose , a hybrid system that uses customizable probabilistic finite state transducers to orchestrate pushing agendas coupled with neural dialogue systems that generate responses to unexpected prompts, as a promising solution to this end. We emphasize the need for this system by highlighting each component{'}s strengths and weaknesses and show how they complement each other.","year":2021,"title_abstract":"Agenda Pushing in Email to Thwart Phishing In this work, we draw parallels between automatically responding to emails for combating social-engineering attacks and document-grounded response generation and lay out the blueprint of our approach. Phishing emails are longer than dialogue utterances and often contain multiple intents. Hence, we need to make decisions similar to those for document-grounded responses in deciding what parts of long text to use and how to address each intent to generate a knowledgeable multi-component response that pushes scammers towards agendas that aid in attribution and linking attacks. We propose , a hybrid system that uses customizable probabilistic finite state transducers to orchestrate pushing agendas coupled with neural dialogue systems that generate responses to unexpected prompts, as a promising solution to this end. We emphasize the need for this system by highlighting each component{'}s strengths and weaknesses and show how they complement each other.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1593625247,"Goal":"Climate Action","Task":["Agenda Pushing","Thwart Phishing","social - engineering attacks","document - grounded response generation","Phishing emails","document - grounded responses","attribution","linking attacks","pushing agendas"],"Method":["knowledgeable multi - component response","hybrid system","probabilistic finite state transducers","neural dialogue systems"]},{"ID":"stodden-kallmeyer-2020-multi","title":"A multi-lingual and cross-domain analysis of features for text simplification","abstract":"In text simplification and readability research, several features have been proposed to estimate or simplify a complex text, e.g., readability scores, sentence length, or proportion of POS tags. These features are however mainly developed for English. In this paper, we investigate their relevance for Czech, German, English, Spanish, and Italian text simplification corpora. Our multi-lingual and multi-domain corpus analysis shows that the relevance of different features for text simplification is different per corpora, language, and domain. For example, the relevance of the lexical complexity is different across all languages, the BLEU score across all domains, and 14 features within the web domain corpora. Overall, the negative statistical tests regarding the other features across and within domains and languages lead to the assumption that text simplification models may be transferable between different domains or different languages.","year":2020,"title_abstract":"A multi-lingual and cross-domain analysis of features for text simplification In text simplification and readability research, several features have been proposed to estimate or simplify a complex text, e.g., readability scores, sentence length, or proportion of POS tags. These features are however mainly developed for English. In this paper, we investigate their relevance for Czech, German, English, Spanish, and Italian text simplification corpora. Our multi-lingual and multi-domain corpus analysis shows that the relevance of different features for text simplification is different per corpora, language, and domain. For example, the relevance of the lexical complexity is different across all languages, the BLEU score across all domains, and 14 features within the web domain corpora. Overall, the negative statistical tests regarding the other features across and within domains and languages lead to the assumption that text simplification models may be transferable between different domains or different languages.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1593415737,"Goal":"Reduced Inequalities","Task":["multi - lingual and cross - domain analysis of features","text simplification","text simplification","readability research","multi - lingual and multi - domain corpus analysis","text simplification"],"Method":["text simplification models"]},{"ID":"schuff-etal-2020-f1","title":"{F}1 is {N}ot {E}nough! {M}odels and {E}valuation {T}owards {U}ser-{C}entered {E}xplainable {Q}uestion {A}nswering","abstract":"Explainable question answering systems predict an answer together with an explanation showing why the answer has been selected. The goal is to enable users to assess the correctness of the system and understand its reasoning process. However, we show that current models and evaluation settings have shortcomings regarding the coupling of answer and explanation which might cause serious issues in user experience. As a remedy, we propose a hierarchical model and a new regularization term to strengthen the answer-explanation coupling as well as two evaluation scores to quantify the coupling. We conduct experiments on the HOTPOTQA benchmark data set and perform a user study. The user study shows that our models increase the ability of the users to judge the correctness of the system and that scores like F1 are not enough to estimate the usefulness of a model in a practical setting with human users. Our scores are better aligned with user experience, making them promising candidates for model selection.","year":2020,"title_abstract":"{F}1 is {N}ot {E}nough! {M}odels and {E}valuation {T}owards {U}ser-{C}entered {E}xplainable {Q}uestion {A}nswering Explainable question answering systems predict an answer together with an explanation showing why the answer has been selected. The goal is to enable users to assess the correctness of the system and understand its reasoning process. However, we show that current models and evaluation settings have shortcomings regarding the coupling of answer and explanation which might cause serious issues in user experience. As a remedy, we propose a hierarchical model and a new regularization term to strengthen the answer-explanation coupling as well as two evaluation scores to quantify the coupling. We conduct experiments on the HOTPOTQA benchmark data set and perform a user study. The user study shows that our models increase the ability of the users to judge the correctness of the system and that scores like F1 are not enough to estimate the usefulness of a model in a practical setting with human users. Our scores are better aligned with user experience, making them promising candidates for model selection.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1593364328,"Goal":"Quality Education","Task":["answer","explanation"],"Method":["Explainable question answering systems","reasoning process","hierarchical model","regularization term","answer - explanation coupling","model selection"]},{"ID":"ying-thomas-2022-label","title":"Label Errors in {BANKING}77","abstract":"We investigate potential label errors present in the popular BANKING77 dataset and the associated negative impacts on intent classification methods. Motivated by our own negative results when constructing an intent classifier, we applied two automated approaches to identify potential label errors in the dataset. We found that over 1,400 (14{\\%}) of the 10,003 training utterances may have been incorrectly labelled. In a simple experiment, we found that by removing the utterances with potential errors, our intent classifier saw an increase of 4.5{\\%} and 8{\\%} for the F1-Score and Adjusted Rand Index, respectively, in supervised and unsupervised classification. This paper serves as a warning of the potential of noisy labels in popular NLP datasets. Further study is needed to fully identify the breadth and depth of label errors in BANKING77 and other datasets.","year":2022,"title_abstract":"Label Errors in {BANKING}77 We investigate potential label errors present in the popular BANKING77 dataset and the associated negative impacts on intent classification methods. Motivated by our own negative results when constructing an intent classifier, we applied two automated approaches to identify potential label errors in the dataset. We found that over 1,400 (14{\\%}) of the 10,003 training utterances may have been incorrectly labelled. In a simple experiment, we found that by removing the utterances with potential errors, our intent classifier saw an increase of 4.5{\\%} and 8{\\%} for the F1-Score and Adjusted Rand Index, respectively, in supervised and unsupervised classification. This paper serves as a warning of the potential of noisy labels in popular NLP datasets. Further study is needed to fully identify the breadth and depth of label errors in BANKING77 and other datasets.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1593104899,"Goal":"Climate Action","Task":["Label Errors","supervised and unsupervised classification"],"Method":["intent classification methods","intent classifier","automated approaches","intent classifier"]},{"ID":"van-son-etal-2016-grasp","title":"{GR}a{SP}: A Multilayered Annotation Scheme for Perspectives","abstract":"This paper presents a framework and methodology for the annotation of perspectives in text. In the last decade, different aspects of linguistic encoding of perspectives have been targeted as separated phenomena through different annotation initiatives. We propose an annotation scheme that integrates these different phenomena. We use a multilayered annotation approach, splitting the annotation of different aspects of perspectives into small subsequent subtasks in order to reduce the complexity of the task and to better monitor interactions between layers. Currently, we have included four layers of perspective annotation: events, attribution, factuality and opinion. The annotations are integrated in a formal model called GRaSP, which provides the means to represent instances (e.g. events, entities) and propositions in the (real or assumed) world in relation to their mentions in text. Then, the relation between the source and target of a perspective is characterized by means of perspective annotations. This enables us to place alternative perspectives on the same entity, event or proposition next to each other.","year":2016,"title_abstract":"{GR}a{SP}: A Multilayered Annotation Scheme for Perspectives This paper presents a framework and methodology for the annotation of perspectives in text. In the last decade, different aspects of linguistic encoding of perspectives have been targeted as separated phenomena through different annotation initiatives. We propose an annotation scheme that integrates these different phenomena. We use a multilayered annotation approach, splitting the annotation of different aspects of perspectives into small subsequent subtasks in order to reduce the complexity of the task and to better monitor interactions between layers. Currently, we have included four layers of perspective annotation: events, attribution, factuality and opinion. The annotations are integrated in a formal model called GRaSP, which provides the means to represent instances (e.g. events, entities) and propositions in the (real or assumed) world in relation to their mentions in text. Then, the relation between the source and target of a perspective is characterized by means of perspective annotations. This enables us to place alternative perspectives on the same entity, event or proposition next to each other.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.159245044,"Goal":"Partnership for the Goals","Task":["annotation of perspectives in text","linguistic encoding of perspectives","annotation","perspective annotation"],"Method":["Multilayered Annotation Scheme","annotation scheme","multilayered annotation approach","formal model","GRaSP"]},{"ID":"liu-etal-2019-investigating","title":"Investigating Cross-Lingual Alignment Methods for Contextualized Embeddings with Token-Level Evaluation","abstract":"In this paper, we present a thorough investigation on methods that align pre-trained contextualized embeddings into shared cross-lingual context-aware embedding space, providing strong reference benchmarks for future context-aware crosslingual models. We propose a novel and challenging task, Bilingual Token-level Sense Retrieval (BTSR). It specifically evaluates the accurate alignment of words with the same meaning in cross-lingual non-parallel contexts, currently not evaluated by existing tasks such as Bilingual Contextual Word Similarity and Sentence Retrieval. We show how the proposed BTSR task highlights the merits of different alignment methods. In particular, we find that using context average type-level alignment is effective in transferring monolingual contextualized embeddings cross-lingually especially in non-parallel contexts, and at the same time improves the monolingual space. Furthermore, aligning independently trained models yields better performance than aligning multilingual embeddings with shared vocabulary.","year":2019,"title_abstract":"Investigating Cross-Lingual Alignment Methods for Contextualized Embeddings with Token-Level Evaluation In this paper, we present a thorough investigation on methods that align pre-trained contextualized embeddings into shared cross-lingual context-aware embedding space, providing strong reference benchmarks for future context-aware crosslingual models. We propose a novel and challenging task, Bilingual Token-level Sense Retrieval (BTSR). It specifically evaluates the accurate alignment of words with the same meaning in cross-lingual non-parallel contexts, currently not evaluated by existing tasks such as Bilingual Contextual Word Similarity and Sentence Retrieval. We show how the proposed BTSR task highlights the merits of different alignment methods. In particular, we find that using context average type-level alignment is effective in transferring monolingual contextualized embeddings cross-lingually especially in non-parallel contexts, and at the same time improves the monolingual space. Furthermore, aligning independently trained models yields better performance than aligning multilingual embeddings with shared vocabulary.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1592359245,"Goal":"Gender Equality","Task":["Contextualized Embeddings","Token - Level Evaluation","Bilingual Token - level Sense Retrieval","accurate alignment of words","Bilingual Contextual Word Similarity","Sentence Retrieval","monolingual contextualized embeddings"],"Method":["Cross - Lingual Alignment Methods","contextualized embeddings","context - aware crosslingual models","BTSR","alignment methods","context average type - level alignment"]},{"ID":"dufter-schutze-2019-analytical","title":"Analytical Methods for Interpretable Ultradense Word Embeddings","abstract":"Word embeddings are useful for a wide variety of tasks, but they lack interpretability. By rotating word spaces, interpretable dimensions can be identified while preserving the information contained in the embeddings without any loss. In this work, we investigate three methods for making word spaces interpretable by rotation: Densifier (Rothe et al., 2016), linear SVMs and DensRay, a new method we propose. In contrast to Densifier, DensRay can be computed in closed form, is hyperparameter-free and thus more robust than Densifier. We evaluate the three methods on lexicon induction and set-based word analogy. In addition we provide qualitative insights as to how interpretable word spaces can be used for removing gender bias from embeddings.","year":2019,"title_abstract":"Analytical Methods for Interpretable Ultradense Word Embeddings Word embeddings are useful for a wide variety of tasks, but they lack interpretability. By rotating word spaces, interpretable dimensions can be identified while preserving the information contained in the embeddings without any loss. In this work, we investigate three methods for making word spaces interpretable by rotation: Densifier (Rothe et al., 2016), linear SVMs and DensRay, a new method we propose. In contrast to Densifier, DensRay can be computed in closed form, is hyperparameter-free and thus more robust than Densifier. We evaluate the three methods on lexicon induction and set-based word analogy. In addition we provide qualitative insights as to how interpretable word spaces can be used for removing gender bias from embeddings.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1592343748,"Goal":"Gender Equality","Task":["Interpretable Ultradense Word Embeddings","Word embeddings","word spaces","rotation","lexicon induction","set - based word analogy","removing gender bias","embeddings"],"Method":["Analytical Methods","Densifier","linear SVMs","DensRay","Densifier","DensRay","Densifier"]},{"ID":"blaette-etal-2020-europeanization","title":"The {E}uropeanization of Parliamentary Debates on Migration in {A}ustria, {F}rance, {G}ermany, and the {N}etherlands","abstract":"Corpora of plenary debates in national parliaments are available for many European states. For comparative research on political discourse, a persisting problem is that the periods covered by corpora differ and that a lack of standardization of data formats inhibits the integration of corpora into a single analytical framework. The solution we pursue is a {`}Framework for Parsing Plenary Protocols{'} (frappp), which has been used to prepare corpora of the Assembl{\\'e}e Nationale ({`}{`}ParisParl{''}), the German Bundestag ({`}{`}GermaParl{''}), the Tweede Kamer of the Netherlands ({`}{`}TweedeTwee{''}), and the Austrian Nationalrat ({`}{`}AustroParl{''}) for the first two decades of the 21st century (2000-2019). To demonstrate the usefulness of the data gained, we investigate the Europeanization of migration debates in these Western European countries of immigration, i.e. references to a European dimension of policy-making in speeches on migration and integration. Based on a segmentation of the corpora into speeches, the method we use is topic modeling, and the analysis of joint occurrences of topics indicating migration and European affairs, respectively. A major finding is that after 2015, we see an increasing Europeanization of migration debates in the small EU member states in our sample (Austria and the Netherlands), and a regression of respective Europeanization in France and {--} more notably {--} in Germany.","year":2020,"title_abstract":"The {E}uropeanization of Parliamentary Debates on Migration in {A}ustria, {F}rance, {G}ermany, and the {N}etherlands Corpora of plenary debates in national parliaments are available for many European states. For comparative research on political discourse, a persisting problem is that the periods covered by corpora differ and that a lack of standardization of data formats inhibits the integration of corpora into a single analytical framework. The solution we pursue is a {`}Framework for Parsing Plenary Protocols{'} (frappp), which has been used to prepare corpora of the Assembl{\\'e}e Nationale ({`}{`}ParisParl{''}), the German Bundestag ({`}{`}GermaParl{''}), the Tweede Kamer of the Netherlands ({`}{`}TweedeTwee{''}), and the Austrian Nationalrat ({`}{`}AustroParl{''}) for the first two decades of the 21st century (2000-2019). To demonstrate the usefulness of the data gained, we investigate the Europeanization of migration debates in these Western European countries of immigration, i.e. references to a European dimension of policy-making in speeches on migration and integration. Based on a segmentation of the corpora into speeches, the method we use is topic modeling, and the analysis of joint occurrences of topics indicating migration and European affairs, respectively. A major finding is that after 2015, we see an increasing Europeanization of migration debates in the small EU member states in our sample (Austria and the Netherlands), and a regression of respective Europeanization in France and {--} more notably {--} in Germany.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.159232989,"Goal":"Reduced Inequalities","Task":["Migration","political discourse","Parsing Plenary Protocols{'}","Europeanization of migration debates","policy - making","migration","integration","analysis of joint occurrences of topics indicating migration","European affairs","migration debates"],"Method":["analytical framework","topic modeling"]},{"ID":"choudhury-etal-2019-processing","title":"Processing and Understanding Mixed Language Data","abstract":"Multilingual communities exhibit code-mixing, that is, mixing of two or more socially stable languages in a single conversation, sometimes even in a single utterance. This phenomenon has been widely studied by linguists and interaction scientists in the spoken language of such communities. However, with the prevalence of social media and other informal interactive platforms, code-switching is now also ubiquitously observed in user-generated text. As multilingual communities are more the norm from a global perspective, it becomes essential that code-switched text and speech are adequately handled by language technologies and NUIs.Code-mixing is extremely prevalent in all multilingual societies. Current studies have shown that as much as 20{\\%} of user generated content from some geographies, like South Asia, parts of Europe, and Singapore, are code-mixed. Thus, it is very important to handle code-mixed content as a part of NLP systems and applications for these geographies.In the past 5 years, there has been an active interest in computational models for code-mixing with a substantive research outcome in terms of publications, datasets and systems. However, it is not easy to find a single point of access for a complete and coherent overview of the research. This tutorial is expecting to fill this gap and provide new researchers in the area with a foundation in both linguistic and computational aspects of code-mixing. We hope that this then becomes a starting point for those who wish to pursue research, design, development and deployment of code-mixed systems in multilingual societies.","year":2019,"title_abstract":"Processing and Understanding Mixed Language Data Multilingual communities exhibit code-mixing, that is, mixing of two or more socially stable languages in a single conversation, sometimes even in a single utterance. This phenomenon has been widely studied by linguists and interaction scientists in the spoken language of such communities. However, with the prevalence of social media and other informal interactive platforms, code-switching is now also ubiquitously observed in user-generated text. As multilingual communities are more the norm from a global perspective, it becomes essential that code-switched text and speech are adequately handled by language technologies and NUIs.Code-mixing is extremely prevalent in all multilingual societies. Current studies have shown that as much as 20{\\%} of user generated content from some geographies, like South Asia, parts of Europe, and Singapore, are code-mixed. Thus, it is very important to handle code-mixed content as a part of NLP systems and applications for these geographies.In the past 5 years, there has been an active interest in computational models for code-mixing with a substantive research outcome in terms of publications, datasets and systems. However, it is not easy to find a single point of access for a complete and coherent overview of the research. This tutorial is expecting to fill this gap and provide new researchers in the area with a foundation in both linguistic and computational aspects of code-mixing. We hope that this then becomes a starting point for those who wish to pursue research, design, development and deployment of code-mixed systems in multilingual societies.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1591691375,"Goal":"Sustainable Cities and Communities","Task":["Processing and Understanding Mixed Language Data","Multilingual communities","code - mixing","code - mixing","linguistic and computational aspects","code - mixing","code - mixed systems","multilingual societies"],"Method":["linguists and interaction scientists","code - switching","language technologies","NUIs","Code - mixing","NLP systems","computational models"]},{"ID":"ruopp-2010-moses","title":"The {``}{M}oses for Localization{''} Open Source Project","abstract":"The open source statistical machine translation toolkit Moses has recently drawn a lot of attention in the localization industry. Companies see the chance to use Moses to leverage their existing translation assets and integrate MT into their localization processes. Due to the academic origins of Moses there are some obstacles to overcome when using it in an industry setting. In this paper we discuss what these obstacles are and how they are addressed by the newly established Moses for Localization open source project. We describe the different components of the project and the benefits a company can gain from using this open source project.","year":2010,"title_abstract":"The {``}{M}oses for Localization{''} Open Source Project The open source statistical machine translation toolkit Moses has recently drawn a lot of attention in the localization industry. Companies see the chance to use Moses to leverage their existing translation assets and integrate MT into their localization processes. Due to the academic origins of Moses there are some obstacles to overcome when using it in an industry setting. In this paper we discuss what these obstacles are and how they are addressed by the newly established Moses for Localization open source project. We describe the different components of the project and the benefits a company can gain from using this open source project.","social_need":"No Poverty End poverty in all its forms everywhere","cosine_similarity":0.1590641141,"Goal":"No Poverty","Task":["Localization{''} Open Source Project","localization industry","localization processes","Localization"],"Method":["Moses","Moses","translation assets","MT","Moses"]},{"ID":"fonseca-etal-2019-findings","title":"Findings of the {WMT} 2019 Shared Tasks on Quality Estimation","abstract":"We report the results of the WMT19 shared task on Quality Estimation, i.e. the task of predicting the quality of the output of machine translation systems given just the source text and the hypothesis translations. The task includes estimation at three granularity levels: word, sentence and document. A novel addition is evaluating sentence-level QE against human judgments: in other words, designing MT metrics that do not need a reference translation. This year we include three language pairs, produced solely by neural machine translation systems. Participating teams from eleven institutions submitted a variety of systems to different task variants and language pairs.","year":2019,"title_abstract":"Findings of the {WMT} 2019 Shared Tasks on Quality Estimation We report the results of the WMT19 shared task on Quality Estimation, i.e. the task of predicting the quality of the output of machine translation systems given just the source text and the hypothesis translations. The task includes estimation at three granularity levels: word, sentence and document. A novel addition is evaluating sentence-level QE against human judgments: in other words, designing MT metrics that do not need a reference translation. This year we include three language pairs, produced solely by neural machine translation systems. Participating teams from eleven institutions submitted a variety of systems to different task variants and language pairs.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1589569747,"Goal":"Quality Education","Task":["Quality Estimation","WMT19 shared task","Quality Estimation","machine translation","estimation","MT"],"Method":["neural machine translation systems"]},{"ID":"pranesh-etal-2021-cmta","title":"{CMTA}: {COVID}-19 Misinformation Multilingual Analysis on {T}witter","abstract":"The internet has actually come to be an essential resource of health knowledge for individuals around the world in the present situation of the coronavirus condition pandemic(COVID-19). During pandemic situations, myths, sensationalism, rumours and misinformation, generated intentionally or unintentionally, spread rapidly through social networks. Twitter is one of these popular social networks people use to share COVID-19 related news, information, and thoughts that reflect their perception and opinion about the pandemic. Evaluation of tweets for recognizing misinformation can create beneficial understanding to review the top quality and also the readability of online information concerning the COVID-19. This paper presents a multilingual COVID-19 related tweet analysis method, CMTA, that uses BERT, a deep learning model for multilingual tweet misinformation detection and classification. CMTA extracts features from multilingual textual data, which is then categorized into specific information classes. Classification is done by a Dense-CNN model trained on tweets manually annotated into information classes (i.e., {`}false{'}, {`}partly false{'}, {`}misleading{'}). The paper presents an analysis of multilingual tweets from February to June, showing the distribution type of information spread across different languages. To access the performance of the CMTA multilingual model, we performed a comparative analysis of 8 monolingual model and CMTA for the misinformation detection task. The results show that our proposed CMTA model has surpassed various monolingual models which consolidated the fact that through transfer learning a multilingual framework could be developed.","year":2021,"title_abstract":"{CMTA}: {COVID}-19 Misinformation Multilingual Analysis on {T}witter The internet has actually come to be an essential resource of health knowledge for individuals around the world in the present situation of the coronavirus condition pandemic(COVID-19). During pandemic situations, myths, sensationalism, rumours and misinformation, generated intentionally or unintentionally, spread rapidly through social networks. Twitter is one of these popular social networks people use to share COVID-19 related news, information, and thoughts that reflect their perception and opinion about the pandemic. Evaluation of tweets for recognizing misinformation can create beneficial understanding to review the top quality and also the readability of online information concerning the COVID-19. This paper presents a multilingual COVID-19 related tweet analysis method, CMTA, that uses BERT, a deep learning model for multilingual tweet misinformation detection and classification. CMTA extracts features from multilingual textual data, which is then categorized into specific information classes. Classification is done by a Dense-CNN model trained on tweets manually annotated into information classes (i.e., {`}false{'}, {`}partly false{'}, {`}misleading{'}). The paper presents an analysis of multilingual tweets from February to June, showing the distribution type of information spread across different languages. To access the performance of the CMTA multilingual model, we performed a comparative analysis of 8 monolingual model and CMTA for the misinformation detection task. The results show that our proposed CMTA model has surpassed various monolingual models which consolidated the fact that through transfer learning a multilingual framework could be developed.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1589000821,"Goal":"Climate Action","Task":["Multilingual Analysis","coronavirus condition pandemic(COVID - 19)","Evaluation of tweets","recognizing misinformation","multilingual tweet misinformation detection and classification","Classification","misinformation detection task"],"Method":["COVID - 19","CMTA","BERT","deep learning model","CMTA","Dense - CNN model","CMTA multilingual model","monolingual model","CMTA","CMTA","monolingual models","transfer learning","multilingual framework"]},{"ID":"grusky-etal-2018-newsroom","title":"{N}ewsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies","abstract":"We present NEWSROOM, a summarization dataset of 1.3 million articles and summaries written by authors and editors in newsrooms of 38 major news publications. Extracted from search and social media metadata between 1998 and 2017, these high-quality summaries demonstrate high diversity of summarization styles. In particular, the summaries combine abstractive and extractive strategies, borrowing words and phrases from articles at varying rates. We analyze the extraction strategies used in NEWSROOM summaries against other datasets to quantify the diversity and difficulty of our new data, and train existing methods on the data to evaluate its utility and challenges. The dataset is available online at summari.es.","year":2018,"title_abstract":"{N}ewsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies We present NEWSROOM, a summarization dataset of 1.3 million articles and summaries written by authors and editors in newsrooms of 38 major news publications. Extracted from search and social media metadata between 1998 and 2017, these high-quality summaries demonstrate high diversity of summarization styles. In particular, the summaries combine abstractive and extractive strategies, borrowing words and phrases from articles at varying rates. We analyze the extraction strategies used in NEWSROOM summaries against other datasets to quantify the diversity and difficulty of our new data, and train existing methods on the data to evaluate its utility and challenges. The dataset is available online at summari.es.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1588506699,"Goal":"Partnership for the Goals","Task":["summarization"],"Method":["Extractive Strategies","abstractive and extractive strategies","extraction strategies"]},{"ID":"sun-wang-2022-adjusting","title":"Adjusting the Precision-Recall Trade-Off with Align-and-Predict Decoding for Grammatical Error Correction","abstract":"Modern writing assistance applications are always equipped with a Grammatical Error Correction (GEC) model to correct errors in user-entered sentences. Different scenarios have varying requirements for correction behavior, e.g., performing more precise corrections (high precision) or providing more candidates for users (high recall). However, previous works adjust such trade-off only for sequence labeling approaches. In this paper, we propose a simple yet effective counterpart {--} Align-and-Predict Decoding (APD) for the most popular sequence-to-sequence models to offer more flexibility for the precision-recall trade-off. During inference, APD aligns the already generated sequence with input and adjusts scores of the following tokens. Experiments in both English and Chinese GEC benchmarks show that our approach not only adapts a single model to precision-oriented and recall-oriented inference, but also maximizes its potential to achieve state-of-the-art results. Our code is available at https:\/\/github.com\/AutoTemp\/Align-and-Predict.","year":2022,"title_abstract":"Adjusting the Precision-Recall Trade-Off with Align-and-Predict Decoding for Grammatical Error Correction Modern writing assistance applications are always equipped with a Grammatical Error Correction (GEC) model to correct errors in user-entered sentences. Different scenarios have varying requirements for correction behavior, e.g., performing more precise corrections (high precision) or providing more candidates for users (high recall). However, previous works adjust such trade-off only for sequence labeling approaches. In this paper, we propose a simple yet effective counterpart {--} Align-and-Predict Decoding (APD) for the most popular sequence-to-sequence models to offer more flexibility for the precision-recall trade-off. During inference, APD aligns the already generated sequence with input and adjusts scores of the following tokens. Experiments in both English and Chinese GEC benchmarks show that our approach not only adapts a single model to precision-oriented and recall-oriented inference, but also maximizes its potential to achieve state-of-the-art results. Our code is available at https:\/\/github.com\/AutoTemp\/Align-and-Predict.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.158817336,"Goal":"Gender Equality","Task":["Grammatical Error Correction","writing assistance applications","inference","GEC","precision - oriented and recall - oriented inference"],"Method":["Align - and - Predict Decoding","Grammatical Error Correction (GEC) model","sequence labeling approaches","Align - and - Predict Decoding","sequence - to - sequence models","APD"]},{"ID":"skachkova-kruijff-korbayova-2020-reference","title":"Reference in Team Communication for Robot-Assisted Disaster Response: An Initial Analysis","abstract":"We analyze reference phenomena in a corpus of robot-assisted disaster response team communication. The annotation scheme we designed for this purpose distinguishes different types of entities, roles, reference units and relations. We focus particularly on mission-relevant objects, locations and actors and also annotate a rich set of reference links, including co-reference and various other kinds of relations. We explain the categories used in our annotation, present their distribution in the corpus and discuss challenging cases.","year":2020,"title_abstract":"Reference in Team Communication for Robot-Assisted Disaster Response: An Initial Analysis We analyze reference phenomena in a corpus of robot-assisted disaster response team communication. The annotation scheme we designed for this purpose distinguishes different types of entities, roles, reference units and relations. We focus particularly on mission-relevant objects, locations and actors and also annotate a rich set of reference links, including co-reference and various other kinds of relations. We explain the categories used in our annotation, present their distribution in the corpus and discuss challenging cases.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1587807238,"Goal":"Sustainable Cities and Communities","Task":["Reference in Team Communication","Robot - Assisted Disaster Response","robot - assisted disaster response team communication"],"Method":["annotation scheme"]},{"ID":"derczynski-etal-2017-semeval","title":"{S}em{E}val-2017 Task 8: {R}umour{E}val: Determining rumour veracity and support for rumours","abstract":"Media is full of false claims. Even Oxford Dictionaries named {``}post-truth{''} as the word of 2016. This makes it more important than ever to build systems that can identify the veracity of a story, and the nature of the discourse around it. RumourEval is a SemEval shared task that aims to identify and handle rumours and reactions to them, in text. We present an annotation scheme, a large dataset covering multiple topics {--} each having their own families of claims and replies {--} and use these to pose two concrete challenges as well as the results achieved by participants on these challenges.","year":2017,"title_abstract":"{S}em{E}val-2017 Task 8: {R}umour{E}val: Determining rumour veracity and support for rumours Media is full of false claims. Even Oxford Dictionaries named {``}post-truth{''} as the word of 2016. This makes it more important than ever to build systems that can identify the veracity of a story, and the nature of the discourse around it. RumourEval is a SemEval shared task that aims to identify and handle rumours and reactions to them, in text. We present an annotation scheme, a large dataset covering multiple topics {--} each having their own families of claims and replies {--} and use these to pose two concrete challenges as well as the results achieved by participants on these challenges.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1587751359,"Goal":"Climate Action","Task":["Determining rumour veracity","support","rumours Media","SemEval shared task"],"Method":["RumourEval","annotation scheme"]},{"ID":"krasselt-etal-2020-swiss","title":"{S}wiss-{AL}: A Multilingual {S}wiss Web Corpus for Applied Linguistics","abstract":"The Swiss Web Corpus for Applied Linguistics (Swiss-AL) is a multilingual (German, French, Italian) collection of texts from selected web sources. Unlike most other web corpora it is not intended for NLP purposes, but rather designed to support data-based and data-driven research on societal and political discourses in Switzerland. It currently contains 8 million texts (approx. 1.55 billion tokens), including news and specialist publications, governmental opinions, and parliamentary records, web sites of political parties, companies, and universities, statements from industry associations and NGOs, etc. A flexible processing pipeline using state-of-the-art components allows researchers in applied linguistics to create tailor-made subcorpora for studying discourse in a wide range of domains. So far, Swiss-AL has been used successfully in research on Swiss public discourses on energy and on antibiotic resistance.","year":2020,"title_abstract":"{S}wiss-{AL}: A Multilingual {S}wiss Web Corpus for Applied Linguistics The Swiss Web Corpus for Applied Linguistics (Swiss-AL) is a multilingual (German, French, Italian) collection of texts from selected web sources. Unlike most other web corpora it is not intended for NLP purposes, but rather designed to support data-based and data-driven research on societal and political discourses in Switzerland. It currently contains 8 million texts (approx. 1.55 billion tokens), including news and specialist publications, governmental opinions, and parliamentary records, web sites of political parties, companies, and universities, statements from industry associations and NGOs, etc. A flexible processing pipeline using state-of-the-art components allows researchers in applied linguistics to create tailor-made subcorpora for studying discourse in a wide range of domains. So far, Swiss-AL has been used successfully in research on Swiss public discourses on energy and on antibiotic resistance.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.158755064,"Goal":"Reduced Inequalities","Task":["NLP purposes","data - based and data - driven research on societal and political discourses","applied linguistics","discourse","antibiotic resistance"],"Method":["processing pipeline"]},{"ID":"maxwelll-smith-etal-2022-scoping","title":"Scoping natural language processing in {I}ndonesian and {M}alay for education applications","abstract":"Indonesian and Malay are underrepresented in the development of natural language processing (NLP) technologies and available resources are difficult to find. A clear picture of existing work can invigorate and inform how researchers conceptualise worthwhile projects. Using an education sector project to motivate the study, we conducted a wide-ranging overview of Indonesian and Malay human language technologies and corpus work. We charted 657 included studies according to Hirschberg and Manning{'}s 2015 description of NLP, concluding that the field was dominated by exploratory corpus work, machine reading of text gathered from the Internet, and sentiment analysis. In this paper, we identify most published authors and research hubs, and make a number of recommendations to encourage future collaboration and efficiency within NLP in Indonesian and Malay.","year":2022,"title_abstract":"Scoping natural language processing in {I}ndonesian and {M}alay for education applications Indonesian and Malay are underrepresented in the development of natural language processing (NLP) technologies and available resources are difficult to find. A clear picture of existing work can invigorate and inform how researchers conceptualise worthwhile projects. Using an education sector project to motivate the study, we conducted a wide-ranging overview of Indonesian and Malay human language technologies and corpus work. We charted 657 included studies according to Hirschberg and Manning{'}s 2015 description of NLP, concluding that the field was dominated by exploratory corpus work, machine reading of text gathered from the Internet, and sentiment analysis. In this paper, we identify most published authors and research hubs, and make a number of recommendations to encourage future collaboration and efficiency within NLP in Indonesian and Malay.","social_need":"Decent Work and Economic Growth Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all","cosine_similarity":0.1587441564,"Goal":"Decent Work and Economic Growth","Task":["natural language processing","education applications","natural language processing","education sector project","corpus work","machine reading","sentiment analysis","NLP"],"Method":["NLP"]},{"ID":"silva-etal-2021-towards","title":"Towards a Comprehensive Understanding and Accurate Evaluation of Societal Biases in Pre-Trained Transformers","abstract":"The ease of access to pre-trained transformers has enabled developers to leverage large-scale language models to build exciting applications for their users. While such pre-trained models offer convenient starting points for researchers and developers, there is little consideration for the societal biases captured within these model risking perpetuation of racial, gender, and other harmful biases when these models are deployed at scale. In this paper, we investigate gender and racial bias across ubiquitous pre-trained language models, including GPT-2, XLNet, BERT, RoBERTa, ALBERT and DistilBERT. We evaluate bias within pre-trained transformers using three metrics: WEAT, sequence likelihood, and pronoun ranking. We conclude with an experiment demonstrating the ineffectiveness of word-embedding techniques, such as WEAT, signaling the need for more robust bias testing in transformers.","year":2021,"title_abstract":"Towards a Comprehensive Understanding and Accurate Evaluation of Societal Biases in Pre-Trained Transformers The ease of access to pre-trained transformers has enabled developers to leverage large-scale language models to build exciting applications for their users. While such pre-trained models offer convenient starting points for researchers and developers, there is little consideration for the societal biases captured within these model risking perpetuation of racial, gender, and other harmful biases when these models are deployed at scale. In this paper, we investigate gender and racial bias across ubiquitous pre-trained language models, including GPT-2, XLNet, BERT, RoBERTa, ALBERT and DistilBERT. We evaluate bias within pre-trained transformers using three metrics: WEAT, sequence likelihood, and pronoun ranking. We conclude with an experiment demonstrating the ineffectiveness of word-embedding techniques, such as WEAT, signaling the need for more robust bias testing in transformers.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1587360203,"Goal":"Gender Equality","Task":["Evaluation of Societal Biases","pronoun ranking","bias testing","transformers"],"Method":["language models","pre - trained models","language models","GPT - 2","XLNet","BERT","ALBERT","DistilBERT","WEAT","sequence likelihood","word - embedding techniques","WEAT"]},{"ID":"bao-etal-2022-learning","title":"Learning to Mediate Disparities Towards Pragmatic Communication","abstract":"Human communication is a collaborative process. Speakers, on top of conveying their own intent, adjust the content and language expressions by taking the listeners into account, including their knowledge background, personalities, and physical capabilities. Towards building AI agents with similar abilities in language communication, we propose a novel rational reasoning framework, Pragmatic Rational Speaker (PRS), where the speaker attempts to learn the speaker-listener disparity and adjust the speech accordingly, by adding a light-weighted disparity adjustment layer into working memory on top of speaker{'}s long-term memory system. By fixing the long-term memory, the PRS only needs to update its working memory to learn and adapt to different types of listeners. To validate our framework, we create a dataset that simulates different types of speaker-listener disparities in the context of referential games. Our empirical results demonstrate that the PRS is able to shift its output towards the language that listeners are able to understand, significantly improve the collaborative task outcome, and learn the disparity more efficiently than joint training.","year":2022,"title_abstract":"Learning to Mediate Disparities Towards Pragmatic Communication Human communication is a collaborative process. Speakers, on top of conveying their own intent, adjust the content and language expressions by taking the listeners into account, including their knowledge background, personalities, and physical capabilities. Towards building AI agents with similar abilities in language communication, we propose a novel rational reasoning framework, Pragmatic Rational Speaker (PRS), where the speaker attempts to learn the speaker-listener disparity and adjust the speech accordingly, by adding a light-weighted disparity adjustment layer into working memory on top of speaker{'}s long-term memory system. By fixing the long-term memory, the PRS only needs to update its working memory to learn and adapt to different types of listeners. To validate our framework, we create a dataset that simulates different types of speaker-listener disparities in the context of referential games. Our empirical results demonstrate that the PRS is able to shift its output towards the language that listeners are able to understand, significantly improve the collaborative task outcome, and learn the disparity more efficiently than joint training.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1586569548,"Goal":"Reduced Inequalities","Task":["Mediate Disparities","Pragmatic Communication","Human communication","collaborative process","language communication","referential games"],"Method":["AI agents","rational reasoning framework","Pragmatic Rational Speaker","light - weighted disparity adjustment layer","speaker{'}s long - term memory system","PRS","PRS","joint training"]},{"ID":"parupalli-etal-2018-towards-enhancing","title":"Towards Enhancing Lexical Resource and Using Sense-annotations of {O}nto{S}ense{N}et for Sentiment Analysis","abstract":"This paper illustrates the interface of the tool we developed for crowd sourcing and we explain the annotation procedure in detail. Our tool is named as {`}\u0c2a\u0c3e\u0c30\u0c41\u0c2a\u0c32\u0c4d\u0c32\u0c3f \u0c2a\u0c26\u0c1c\u0c3e\u0c32\u0c02{'} (Parupalli Padajaalam) which means web of words by Parupalli. The aim of this tool is to populate the OntoSenseNet, sentiment polarity annotated Telugu resource. Recent works have shown the importance of word-level annotations on sentiment analysis. With this as basis, we aim to analyze the importance of sense-annotations obtained from OntoSenseNet in performing the task of sentiment analysis. We explain the features extracted from OntoSenseNet (Telugu). Furthermore we compute and explain the adverbial class distribution of verbs in OntoSenseNet. This task is known to aid in disambiguating word-senses which helps in enhancing the performance of word-sense disambiguation (WSD) task(s).","year":2018,"title_abstract":"Towards Enhancing Lexical Resource and Using Sense-annotations of {O}nto{S}ense{N}et for Sentiment Analysis This paper illustrates the interface of the tool we developed for crowd sourcing and we explain the annotation procedure in detail. Our tool is named as {`}\u0c2a\u0c3e\u0c30\u0c41\u0c2a\u0c32\u0c4d\u0c32\u0c3f \u0c2a\u0c26\u0c1c\u0c3e\u0c32\u0c02{'} (Parupalli Padajaalam) which means web of words by Parupalli. The aim of this tool is to populate the OntoSenseNet, sentiment polarity annotated Telugu resource. Recent works have shown the importance of word-level annotations on sentiment analysis. With this as basis, we aim to analyze the importance of sense-annotations obtained from OntoSenseNet in performing the task of sentiment analysis. We explain the features extracted from OntoSenseNet (Telugu). Furthermore we compute and explain the adverbial class distribution of verbs in OntoSenseNet. This task is known to aid in disambiguating word-senses which helps in enhancing the performance of word-sense disambiguation (WSD) task(s).","social_need":"Clean Water and Sanitation Ensure availability and sustainable management of water and sanitation for all","cosine_similarity":0.1586253196,"Goal":"Clean Water and Sanitation","Task":["Lexical Resource","Sentiment Analysis","crowd sourcing","annotation procedure","sentiment analysis","sentiment analysis","disambiguating word - senses","word - sense disambiguation"],"Method":["OntoSenseNet"]},{"ID":"wu-etal-2022-mirroralign","title":"{M}irror{A}lign: A Super Lightweight Unsupervised Word Alignment Model via Cross-Lingual Contrastive Learning","abstract":"Word alignment is essential for the downstream cross-lingual language understanding and generation tasks. Recently, the performance of the neural word alignment models has exceeded that of statistical models. However, they heavily rely on sophisticated translation models. In this study, we propose a super lightweight unsupervised word alignment model named MirrorAlign, in which bidirectional symmetric attention trained with a contrastive learning objective is introduced, and an agreement loss is employed to bind the attention maps, such that the alignments follow mirror-like symmetry hypothesis. Experimental results on several public benchmarks demonstrate that our model achieves competitive, if not better, performance compared to the state of the art in word alignment while significantly reducing the training and decoding time on average. Further ablation analysis and case studies show the superiority of our proposed MirrorAlign. Notably, we recognize our model as a pioneer attempt to unify bilingual word embedding and word alignments. Encouragingly, our approach achieves 16.4X speedup against GIZA++, and 50X parameter compression compared with the Transformer-based alignment methods. We release our code to facilitate the community: https:\/\/github.com\/moore3930\/MirrorAlign.","year":2022,"title_abstract":"{M}irror{A}lign: A Super Lightweight Unsupervised Word Alignment Model via Cross-Lingual Contrastive Learning Word alignment is essential for the downstream cross-lingual language understanding and generation tasks. Recently, the performance of the neural word alignment models has exceeded that of statistical models. However, they heavily rely on sophisticated translation models. In this study, we propose a super lightweight unsupervised word alignment model named MirrorAlign, in which bidirectional symmetric attention trained with a contrastive learning objective is introduced, and an agreement loss is employed to bind the attention maps, such that the alignments follow mirror-like symmetry hypothesis. Experimental results on several public benchmarks demonstrate that our model achieves competitive, if not better, performance compared to the state of the art in word alignment while significantly reducing the training and decoding time on average. Further ablation analysis and case studies show the superiority of our proposed MirrorAlign. Notably, we recognize our model as a pioneer attempt to unify bilingual word embedding and word alignments. Encouragingly, our approach achieves 16.4X speedup against GIZA++, and 50X parameter compression compared with the Transformer-based alignment methods. We release our code to facilitate the community: https:\/\/github.com\/moore3930\/MirrorAlign.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1586241722,"Goal":"Gender Equality","Task":["Word alignment","downstream cross - lingual language understanding and generation tasks","word alignment","ablation analysis","bilingual word embedding","word alignments"],"Method":["Super Lightweight Unsupervised Word Alignment Model","Cross - Lingual Contrastive Learning","neural word alignment models","statistical models","translation models","super lightweight unsupervised word alignment model","MirrorAlign","bidirectional symmetric attention","contrastive learning objective","MirrorAlign","GIZA++","Transformer - based alignment methods"]},{"ID":"azevedo-moustafa-2019-veritas","title":"Veritas Annotator: Discovering the Origin of a Rumour","abstract":"Defined as the intentional or unintentionalspread of false information (K et al., 2019)through context and\/or content manipulation,fake news has become one of the most seriousproblems associated with online information(Waldrop, 2017). Consequently, it comes asno surprise that Fake News Detection hasbecome one of the major foci of variousfields of machine learning and while machinelearning models have allowed individualsand companies to automate decision-basedprocesses that were once thought to be onlydoable by humans, it is no secret that thereal-life applications of such models are notviable without the existence of an adequatetraining dataset. In this paper we describethe Veritas Annotator, a web application formanually identifying the origin of a rumour.These rumours, often referred as claims,were previously checked for validity byFact-Checking Agencies.","year":2019,"title_abstract":"Veritas Annotator: Discovering the Origin of a Rumour Defined as the intentional or unintentionalspread of false information (K et al., 2019)through context and\/or content manipulation,fake news has become one of the most seriousproblems associated with online information(Waldrop, 2017). Consequently, it comes asno surprise that Fake News Detection hasbecome one of the major foci of variousfields of machine learning and while machinelearning models have allowed individualsand companies to automate decision-basedprocesses that were once thought to be onlydoable by humans, it is no secret that thereal-life applications of such models are notviable without the existence of an adequatetraining dataset. In this paper we describethe Veritas Annotator, a web application formanually identifying the origin of a rumour.These rumours, often referred as claims,were previously checked for validity byFact-Checking Agencies.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1585728228,"Goal":"Climate Action","Task":["content manipulation","Fake News Detection","decision - basedprocesses"],"Method":["Veritas Annotator","machine learning","machinelearning models","Veritas Annotator","web application"]},{"ID":"hou-2018-enhanced","title":"Enhanced Word Representations for Bridging Anaphora Resolution","abstract":"Most current models of word representations (e.g., GloVe) have successfully captured fine-grained semantics. However, semantic similarity exhibited in these word embeddings is not suitable for resolving bridging anaphora, which requires the knowledge of associative similarity (i.e., relatedness) instead of semantic similarity information between synonyms or hypernyms. We create word embeddings (embeddings{\\_}PP) to capture such relatedness by exploring the syntactic structure of noun phrases. We demonstrate that using embeddings {\\_}PP alone achieves around 30{\\%} of accuracy for bridging anaphora resolution on the ISNotes corpus. Furthermore, we achieve a substantial gain over the state-of-the-art system (Hou et al., 2013b) for bridging antecedent selection.","year":2018,"title_abstract":"Enhanced Word Representations for Bridging Anaphora Resolution Most current models of word representations (e.g., GloVe) have successfully captured fine-grained semantics. However, semantic similarity exhibited in these word embeddings is not suitable for resolving bridging anaphora, which requires the knowledge of associative similarity (i.e., relatedness) instead of semantic similarity information between synonyms or hypernyms. We create word embeddings (embeddings{\\_}PP) to capture such relatedness by exploring the syntactic structure of noun phrases. We demonstrate that using embeddings {\\_}PP alone achieves around 30{\\%} of accuracy for bridging anaphora resolution on the ISNotes corpus. Furthermore, we achieve a substantial gain over the state-of-the-art system (Hou et al., 2013b) for bridging antecedent selection.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.1585642695,"Goal":"Peace, Justice and Strong Institutions","Task":["Bridging Anaphora Resolution","bridging anaphora","bridging anaphora resolution","bridging antecedent selection"],"Method":["Enhanced Word Representations","word representations","GloVe)","word embeddings","word embeddings","embeddings"]},{"ID":"pujari-goldwasser-2021-understanding","title":"Understanding Politics via Contextualized Discourse Processing","abstract":"Politicians often have underlying agendas when reacting to events. Arguments in contexts of various events reflect a fairly consistent set of agendas for a given entity. In spite of recent advances in Pretrained Language Models, those text representations are not designed to capture such nuanced patterns. In this paper, we propose a Compositional Reader model consisting of encoder and composer modules, that captures and leverages such information to generate more effective representations for entities, issues, and events. These representations are contextualized by tweets, press releases, issues, news articles, and participating entities. Our model processes several documents at once and generates composed representations for multiple entities over several issues or events. Via qualitative and quantitative empirical analysis, we show that these representations are meaningful and effective.","year":2021,"title_abstract":"Understanding Politics via Contextualized Discourse Processing Politicians often have underlying agendas when reacting to events. Arguments in contexts of various events reflect a fairly consistent set of agendas for a given entity. In spite of recent advances in Pretrained Language Models, those text representations are not designed to capture such nuanced patterns. In this paper, we propose a Compositional Reader model consisting of encoder and composer modules, that captures and leverages such information to generate more effective representations for entities, issues, and events. These representations are contextualized by tweets, press releases, issues, news articles, and participating entities. Our model processes several documents at once and generates composed representations for multiple entities over several issues or events. Via qualitative and quantitative empirical analysis, we show that these representations are meaningful and effective.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.1585158557,"Goal":"Peace, Justice and Strong Institutions","Task":["Understanding Politics"],"Method":["Contextualized Discourse Processing","Pretrained Language Models","text representations","Compositional Reader model","encoder and composer modules","composed representations"]},{"ID":"gilmanov-etal-2014-swift","title":"{SWIFT} Aligner, A Multifunctional Tool for Parallel Corpora: Visualization, Word Alignment, and (Morpho)-Syntactic Cross-Language Transfer","abstract":"It is well known that word aligned parallel corpora are valuable linguistic resources. Since many factors affect automatic alignment quality, manual post-editing may be required in some applications. While there are several state-of-the-art word-aligners, such as GIZA++ and Berkeley, there is no simple visual tool that would enable correcting and editing aligned corpora of different formats. We have developed SWIFT Aligner, a free, portable software that allows for visual representation and editing of aligned corpora from several most commonly used formats: TALP, GIZA, and NAACL. In addition, our tool has incorporated part-of-speech and syntactic dependency transfer from an annotated source language into an unannotated target language, by means of word-alignment.","year":2014,"title_abstract":"{SWIFT} Aligner, A Multifunctional Tool for Parallel Corpora: Visualization, Word Alignment, and (Morpho)-Syntactic Cross-Language Transfer It is well known that word aligned parallel corpora are valuable linguistic resources. Since many factors affect automatic alignment quality, manual post-editing may be required in some applications. While there are several state-of-the-art word-aligners, such as GIZA++ and Berkeley, there is no simple visual tool that would enable correcting and editing aligned corpora of different formats. We have developed SWIFT Aligner, a free, portable software that allows for visual representation and editing of aligned corpora from several most commonly used formats: TALP, GIZA, and NAACL. In addition, our tool has incorporated part-of-speech and syntactic dependency transfer from an annotated source language into an unannotated target language, by means of word-alignment.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1585010439,"Goal":"Gender Equality","Task":["Visualization","Word Alignment","(Morpho) - Syntactic Cross - Language Transfer","manual post - editing","visual representation and editing of aligned corpora","part - of - speech"],"Method":["Aligner","Multifunctional Tool","word - aligners","GIZA++","Berkeley","visual tool","SWIFT Aligner","portable software","word - alignment"]},{"ID":"ma-etal-2020-powertransformer","title":"{P}ower{T}ransformer: Unsupervised Controllable Revision for Biased Language Correction","abstract":"Unconscious biases continue to be prevalent in modern text and media, calling for algorithms that can assist writers with bias correction. For example, a female character in a story is often portrayed as passive and powerless ({``}{\\_}She daydreams about being a doctor{\\_}{''}) while a man is portrayed as more proactive and powerful ({``}{\\_}He pursues his dream of being a doctor{\\_}{''}). We formulate **Controllable Debiasing**, a new revision task that aims to rewrite a given text to correct the implicit and potentially undesirable bias in character portrayals. We then introduce PowerTransformer as an approach that debiases text through the lens of connotation frames (Sap et al., 2017), which encode pragmatic knowledge of implied power dynamics with respect to verb predicates. One key challenge of our task is the lack of parallel corpora. To address this challenge, we adopt an unsupervised approach using auxiliary supervision with related tasks such as paraphrasing and self-supervision based on a reconstruction loss, building on pretrained language models. Through comprehensive experiments based on automatic and human evaluations, we demonstrate that our approach outperforms ablations and existing methods from related tasks. Furthermore, we demonstrate the use of PowerTransformer as a step toward mitigating the well-documented gender bias in character portrayal in movie scripts.","year":2020,"title_abstract":"{P}ower{T}ransformer: Unsupervised Controllable Revision for Biased Language Correction Unconscious biases continue to be prevalent in modern text and media, calling for algorithms that can assist writers with bias correction. For example, a female character in a story is often portrayed as passive and powerless ({``}{\\_}She daydreams about being a doctor{\\_}{''}) while a man is portrayed as more proactive and powerful ({``}{\\_}He pursues his dream of being a doctor{\\_}{''}). We formulate **Controllable Debiasing**, a new revision task that aims to rewrite a given text to correct the implicit and potentially undesirable bias in character portrayals. We then introduce PowerTransformer as an approach that debiases text through the lens of connotation frames (Sap et al., 2017), which encode pragmatic knowledge of implied power dynamics with respect to verb predicates. One key challenge of our task is the lack of parallel corpora. To address this challenge, we adopt an unsupervised approach using auxiliary supervision with related tasks such as paraphrasing and self-supervision based on a reconstruction loss, building on pretrained language models. Through comprehensive experiments based on automatic and human evaluations, we demonstrate that our approach outperforms ablations and existing methods from related tasks. Furthermore, we demonstrate the use of PowerTransformer as a step toward mitigating the well-documented gender bias in character portrayal in movie scripts.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1584870368,"Goal":"Gender Equality","Task":["Unsupervised Controllable Revision","Biased Language Correction","bias correction","revision task","paraphrasing","self - supervision","gender bias in character portrayal"],"Method":["**Controllable Debiasing**","PowerTransformer","unsupervised approach","auxiliary supervision","reconstruction loss","pretrained language models","ablations","PowerTransformer"]},{"ID":"salehi-etal-2017-huntsville","title":"Huntsville, hospitals, and hockey teams: Names can reveal your location","abstract":"Geolocation is the task of identifying a social media user{'}s primary location, and in natural language processing, there is a growing literature on to what extent automated analysis of social media posts can help. However, not all content features are equally revealing of a user{'}s location. In this paper, we evaluate nine name entity (NE) types. Using various metrics, we find that GEO-LOC, FACILITY and SPORT-TEAM are more informative for geolocation than other NE types. Using these types, we improve geolocation accuracy and reduce distance error over various famous text-based methods.","year":2017,"title_abstract":"Huntsville, hospitals, and hockey teams: Names can reveal your location Geolocation is the task of identifying a social media user{'}s primary location, and in natural language processing, there is a growing literature on to what extent automated analysis of social media posts can help. However, not all content features are equally revealing of a user{'}s location. In this paper, we evaluate nine name entity (NE) types. Using various metrics, we find that GEO-LOC, FACILITY and SPORT-TEAM are more informative for geolocation than other NE types. Using these types, we improve geolocation accuracy and reduce distance error over various famous text-based methods.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1584623158,"Goal":"Sustainable Cities and Communities","Task":["natural language processing","automated analysis of social media posts","geolocation"],"Method":["text - based methods"]},{"ID":"abbasi-etal-2021-constructing","title":"Constructing a Psychometric Testbed for Fair Natural Language Processing","abstract":"Psychometric measures of ability, attitudes, perceptions, and beliefs are crucial for understanding user behavior in various contexts including health, security, e-commerce, and finance. Traditionally, psychometric dimensions have been measured and collected using survey-based methods. Inferring such constructs from user-generated text could allow timely, unobtrusive collection and analysis. In this paper we describe our efforts to construct a corpus for psychometric natural language processing (NLP) related to important dimensions such as trust, anxiety, numeracy, and literacy, in the health domain. We discuss our multi-step process to align user text with their survey-based response items and provide an overview of the resulting testbed which encompasses survey-based psychometric measures and accompanying user-generated text from 8,502 respondents. Our testbed also encompasses self-reported demographic information, including race, sex, age, income, and education - thereby affording opportunities for measuring bias and benchmarking fairness of text classification methods. We report preliminary results on use of the text to predict\/categorize users{'} survey response labels - and on the fairness of these models. We also discuss the important implications of our work and resulting testbed for future NLP research on psychometrics and fairness.","year":2021,"title_abstract":"Constructing a Psychometric Testbed for Fair Natural Language Processing Psychometric measures of ability, attitudes, perceptions, and beliefs are crucial for understanding user behavior in various contexts including health, security, e-commerce, and finance. Traditionally, psychometric dimensions have been measured and collected using survey-based methods. Inferring such constructs from user-generated text could allow timely, unobtrusive collection and analysis. In this paper we describe our efforts to construct a corpus for psychometric natural language processing (NLP) related to important dimensions such as trust, anxiety, numeracy, and literacy, in the health domain. We discuss our multi-step process to align user text with their survey-based response items and provide an overview of the resulting testbed which encompasses survey-based psychometric measures and accompanying user-generated text from 8,502 respondents. Our testbed also encompasses self-reported demographic information, including race, sex, age, income, and education - thereby affording opportunities for measuring bias and benchmarking fairness of text classification methods. We report preliminary results on use of the text to predict\/categorize users{'} survey response labels - and on the fairness of these models. We also discuss the important implications of our work and resulting testbed for future NLP research on psychometrics and fairness.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.1584412158,"Goal":"Good Health and Well-Being","Task":["Fair Natural Language Processing","Psychometric measures of ability","user behavior","health","security","e - commerce","finance","psychometric natural language processing","literacy","health domain","NLP","psychometrics","fairness"],"Method":["Psychometric Testbed","survey - based methods","multi - step process","text classification methods"]},{"ID":"broeder-etal-2010-data","title":"A Data Category Registry- and Component-based Metadata Framework","abstract":"We describe our computer-supported framework to overcome the rule of metadata schism. It combines the use of controlled vocabularies, managed by a data category registry, with a component-based approach, where the categories can be combined to yield complex metadata structures. A metadata scheme devised in this way will thus be grounded in its use of categories. Schema designers will profit from existing prefabricated larger building blocks, motivating re-use at a larger scale. The common base of any two metadata schemes within this framework will solve, at least to a good extent, the semantic interoperability problem, and consequently, further promote systematic use of metadata for existing resources and tools to be shared.","year":2010,"title_abstract":"A Data Category Registry- and Component-based Metadata Framework We describe our computer-supported framework to overcome the rule of metadata schism. It combines the use of controlled vocabularies, managed by a data category registry, with a component-based approach, where the categories can be combined to yield complex metadata structures. A metadata scheme devised in this way will thus be grounded in its use of categories. Schema designers will profit from existing prefabricated larger building blocks, motivating re-use at a larger scale. The common base of any two metadata schemes within this framework will solve, at least to a good extent, the semantic interoperability problem, and consequently, further promote systematic use of metadata for existing resources and tools to be shared.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.1583783627,"Goal":"Life Below Water","Task":["semantic interoperability problem"],"Method":["Data Category Registry - and Component - based Metadata Framework","computer - supported framework","data category registry","component - based approach","metadata scheme","Schema designers","metadata schemes"]},{"ID":"ripplinger-1998-emis","title":"{EMIS}","abstract":"The objective of the emis project is the conception and realization of a web-based multilingual information system on European media law with the following functionalities: search by words, a combination of words, phrases or keywords; guided search by using a so-called thematic structure; cross language retrieval of documents in different languages with one monolingual query by using language processing and MT technology; exploitation of additional information for the retrieved documents, which is stored in a database; structured representation of the document archive, the so-called dogmatic structure; multilingual user interface.","year":1998,"title_abstract":"{EMIS} The objective of the emis project is the conception and realization of a web-based multilingual information system on European media law with the following functionalities: search by words, a combination of words, phrases or keywords; guided search by using a so-called thematic structure; cross language retrieval of documents in different languages with one monolingual query by using language processing and MT technology; exploitation of additional information for the retrieved documents, which is stored in a database; structured representation of the document archive, the so-called dogmatic structure; multilingual user interface.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1583524644,"Goal":"Gender Equality","Task":["web - based multilingual information system","guided search","cross language retrieval of documents"],"Method":["emis project","language processing","MT technology;","database; structured representation","dogmatic structure; multilingual user interface"]},{"ID":"upadhyay-etal-2021-hopeful","title":"Hopeful Men@{LT}-{EDI}-{EACL}2021: Hope Speech Detection Using Indic Transliteration and Transformers","abstract":"This paper aims to describe the approach we used to detect hope speech in the HopeEDI dataset. We experimented with two approaches. In the first approach, we used contextual embeddings to train classifiers using logistic regression, random forest, SVM, and LSTM based models. The second approach involved using a majority voting ensemble of 11 models which were obtained by fine-tuning pre-trained transformer models (BERT, ALBERT, RoBERTa, IndicBERT) after adding an output layer. We found that the second approach was superior for English, Tamil and Malayalam. Our solution got a weighted F1 score of 0.93, 0.75 and 0.49 for English, Malayalam and Tamil respectively. Our solution ranked 1st in English, 8th in Malayalam and 11th in Tamil.","year":2021,"title_abstract":"Hopeful Men@{LT}-{EDI}-{EACL}2021: Hope Speech Detection Using Indic Transliteration and Transformers This paper aims to describe the approach we used to detect hope speech in the HopeEDI dataset. We experimented with two approaches. In the first approach, we used contextual embeddings to train classifiers using logistic regression, random forest, SVM, and LSTM based models. The second approach involved using a majority voting ensemble of 11 models which were obtained by fine-tuning pre-trained transformer models (BERT, ALBERT, RoBERTa, IndicBERT) after adding an output layer. We found that the second approach was superior for English, Tamil and Malayalam. Our solution got a weighted F1 score of 0.93, 0.75 and 0.49 for English, Malayalam and Tamil respectively. Our solution ranked 1st in English, 8th in Malayalam and 11th in Tamil.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1583393812,"Goal":"Gender Equality","Task":["Hope Speech Detection"],"Method":["Indic Transliteration","Transformers","classifiers","logistic regression","random forest","SVM","LSTM based models","majority voting ensemble","transformer models","IndicBERT)","output layer"]},{"ID":"shang-etal-2020-energy","title":"Energy-based Self-attentive Learning of Abstractive Communities for Spoken Language Understanding","abstract":"Abstractive community detection is an important spoken language understanding task, whose goal is to group utterances in a conversation according to whether they can be jointly summarized by a common abstractive sentence. This paper provides a novel approach to this task. We first introduce a neural contextual utterance encoder featuring three types of self-attention mechanisms. We then train it using the siamese and triplet energy-based meta-architectures. Experiments on the AMI corpus show that our system outperforms multiple energy-based and non-energy based baselines from the state-of-the-art. Code and data are publicly available.","year":2020,"title_abstract":"Energy-based Self-attentive Learning of Abstractive Communities for Spoken Language Understanding Abstractive community detection is an important spoken language understanding task, whose goal is to group utterances in a conversation according to whether they can be jointly summarized by a common abstractive sentence. This paper provides a novel approach to this task. We first introduce a neural contextual utterance encoder featuring three types of self-attention mechanisms. We then train it using the siamese and triplet energy-based meta-architectures. Experiments on the AMI corpus show that our system outperforms multiple energy-based and non-energy based baselines from the state-of-the-art. Code and data are publicly available.","social_need":"Affordable and Clean Energy Ensure access to affordable, reliable, sustainable and modern energy for all","cosine_similarity":0.1581968814,"Goal":"Affordable and Clean Energy","Task":["Spoken Language Understanding","Abstractive community detection","spoken language understanding task"],"Method":["Energy - based Self - attentive Learning of Abstractive Communities","neural contextual utterance encoder","self - attention mechanisms","siamese and triplet energy - based meta - architectures","energy - based and non - energy based baselines"]},{"ID":"baek-etal-2020-patquest","title":"{PATQUEST}: Papago Translation Quality Estimation","abstract":"This paper describes the system submitted by Papago team for the quality estimation task at WMT 2020. It proposes two key strategies for quality estimation: (1) task-specific pretraining scheme, and (2) task-specific data augmentation. The former focuses on devising learning signals for pretraining that are closely related to the downstream task. We also present data augmentation techniques that simulate the varying levels of errors that the downstream dataset may contain. Thus, our PATQUEST models are exposed to erroneous translations in both stages of task-specific pretraining and finetuning, effectively enhancing their generalization capability. Our submitted models achieve significant improvement over the baselines for Task 1 (Sentence-Level Direct Assessment; EN-DE only), and Task 3 (Document-Level Score).","year":2020,"title_abstract":"{PATQUEST}: Papago Translation Quality Estimation This paper describes the system submitted by Papago team for the quality estimation task at WMT 2020. It proposes two key strategies for quality estimation: (1) task-specific pretraining scheme, and (2) task-specific data augmentation. The former focuses on devising learning signals for pretraining that are closely related to the downstream task. We also present data augmentation techniques that simulate the varying levels of errors that the downstream dataset may contain. Thus, our PATQUEST models are exposed to erroneous translations in both stages of task-specific pretraining and finetuning, effectively enhancing their generalization capability. Our submitted models achieve significant improvement over the baselines for Task 1 (Sentence-Level Direct Assessment; EN-DE only), and Task 3 (Document-Level Score).","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1581653953,"Goal":"Quality Education","Task":["Papago Translation Quality Estimation","quality estimation task","WMT 2020","quality estimation","task - specific data augmentation","pretraining","downstream task","task - specific pretraining","finetuning"],"Method":["task - specific pretraining scheme","data augmentation techniques","PATQUEST models"]},{"ID":"nishihara-etal-2019-controllable","title":"Controllable Text Simplification with Lexical Constraint Loss","abstract":"We propose a method to control the level of a sentence in a text simplification task. Text simplification is a monolingual translation task translating a complex sentence into a simpler and easier to understand the alternative. In this study, we use the grade level of the US education system as the level of the sentence. Our text simplification method succeeds in translating an input into a specific grade level by considering levels of both sentences and words. Sentence level is considered by adding the target grade level as input. By contrast, the word level is considered by adding weights to the training loss based on words that frequently appear in sentences of the desired grade level. Although existing models that consider only the sentence level may control the syntactic complexity, they tend to generate words beyond the target level. Our approach can control both the lexical and syntactic complexity and achieve an aggressive rewriting. Experiment results indicate that the proposed method improves the metrics of both BLEU and SARI.","year":2019,"title_abstract":"Controllable Text Simplification with Lexical Constraint Loss We propose a method to control the level of a sentence in a text simplification task. Text simplification is a monolingual translation task translating a complex sentence into a simpler and easier to understand the alternative. In this study, we use the grade level of the US education system as the level of the sentence. Our text simplification method succeeds in translating an input into a specific grade level by considering levels of both sentences and words. Sentence level is considered by adding the target grade level as input. By contrast, the word level is considered by adding weights to the training loss based on words that frequently appear in sentences of the desired grade level. Although existing models that consider only the sentence level may control the syntactic complexity, they tend to generate words beyond the target level. Our approach can control both the lexical and syntactic complexity and achieve an aggressive rewriting. Experiment results indicate that the proposed method improves the metrics of both BLEU and SARI.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1581395864,"Goal":"Reduced Inequalities","Task":["Controllable Text Simplification","text simplification task","Text simplification","monolingual translation task","aggressive rewriting"],"Method":["Lexical Constraint Loss","text simplification method"]},{"ID":"tziafas-etal-2021-fighting","title":"Fighting the {COVID}-19 Infodemic with a Holistic {BERT} Ensemble","abstract":"This paper describes the TOKOFOU system, an ensemble model for misinformation detection tasks based on six different transformer-based pre-trained encoders, implemented in the context of the COVID-19 Infodemic Shared Task for English. We fine tune each model on each of the task{'}s questions and aggregate their prediction scores using a majority voting approach. TOKOFOU obtains an overall F1 score of 89.7{\\%}, ranking first.","year":2021,"title_abstract":"Fighting the {COVID}-19 Infodemic with a Holistic {BERT} Ensemble This paper describes the TOKOFOU system, an ensemble model for misinformation detection tasks based on six different transformer-based pre-trained encoders, implemented in the context of the COVID-19 Infodemic Shared Task for English. We fine tune each model on each of the task{'}s questions and aggregate their prediction scores using a majority voting approach. TOKOFOU obtains an overall F1 score of 89.7{\\%}, ranking first.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1581148952,"Goal":"Climate Action","Task":["misinformation detection tasks"],"Method":["Holistic {BERT} Ensemble","TOKOFOU system","ensemble model","transformer - based pre - trained encoders","majority voting approach","TOKOFOU"]},{"ID":"waibel-1999-translation","title":"Translation systems under the {C}-{STAR} framework","abstract":"This talk will review our work on Speech Translation under the recent worldwide C-STAR demonstration. C-STAR is the Consortium for Speech Translation Advanced Research and now includes 6 partners and 20 partner\/affiliate laboratories around the world. The work demonstrated concludes the second phase of the consortium, which has focused on translating conversational spontaneous speech as opposed to well formed, well structured text. As such, much of the work has focused on exploiting semantic and pragmatic constraints derived from the task domain and dialog situation to produce an understandable translation. Six partners have connected their respective systems with each other and allowed travel related spoken dialogs to provide communication between each of them. A common Interlingua representation was developed and used between the partners to make this multilingual deployment possible. The systems were also complemented by the introduction of Web based shared workspaces that allow one user in one country to communicate pictures, documents, sounds, tables, etc. to the other over the Web while referring to these documents in the dialog. Some of the partners' systems were also deployed in wearable situations, such as a traveler exploring a foreign city. In this case speech and language technology was installed on a wearable computer with a small hand-held display. It was used to provide language translation as well as human-machine information access for the purpose of navigation (using GPS localization) and tour guidance. This combination of human-machine and human-machine-human dialogs could allow a user explore a foreign environment more effectively by resorting to human-machine and human-human dialogs wherever most appropriate.","year":1999,"title_abstract":"Translation systems under the {C}-{STAR} framework This talk will review our work on Speech Translation under the recent worldwide C-STAR demonstration. C-STAR is the Consortium for Speech Translation Advanced Research and now includes 6 partners and 20 partner\/affiliate laboratories around the world. The work demonstrated concludes the second phase of the consortium, which has focused on translating conversational spontaneous speech as opposed to well formed, well structured text. As such, much of the work has focused on exploiting semantic and pragmatic constraints derived from the task domain and dialog situation to produce an understandable translation. Six partners have connected their respective systems with each other and allowed travel related spoken dialogs to provide communication between each of them. A common Interlingua representation was developed and used between the partners to make this multilingual deployment possible. The systems were also complemented by the introduction of Web based shared workspaces that allow one user in one country to communicate pictures, documents, sounds, tables, etc. to the other over the Web while referring to these documents in the dialog. Some of the partners' systems were also deployed in wearable situations, such as a traveler exploring a foreign city. In this case speech and language technology was installed on a wearable computer with a small hand-held display. It was used to provide language translation as well as human-machine information access for the purpose of navigation (using GPS localization) and tour guidance. This combination of human-machine and human-machine-human dialogs could allow a user explore a foreign environment more effectively by resorting to human-machine and human-human dialogs wherever most appropriate.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1580994129,"Goal":"Sustainable Cities and Communities","Task":["Translation","Speech Translation","Speech Translation","understandable translation","multilingual deployment","wearable situations","traveler exploring a foreign city","speech and language technology","language translation","human - machine information access","navigation","GPS localization)","tour guidance"],"Method":["C - STAR","Interlingua representation","wearable computer","human - machine - human dialogs"]},{"ID":"liu-etal-2020-mitigating","title":"Mitigating Gender Bias for Neural Dialogue Generation with Adversarial Learning","abstract":"Dialogue systems play an increasingly important role in various aspects of our daily life. It is evident from recent research that dialogue systems trained on human conversation data are biased. In particular, they can produce responses that reflect people{'}s gender prejudice. Many debiasing methods have been developed for various NLP tasks, such as word embedding. However, they are not directly applicable to dialogue systems because they are likely to force dialogue models to generate similar responses for different genders. This greatly degrades the diversity of the generated responses and immensely hurts the performance of the dialogue models. In this paper, we propose a novel adversarial learning framework Debiased-Chat to train dialogue models free from gender bias while keeping their performance. Extensive experiments on two real-world conversation datasets show that our framework significantly reduces gender bias in dialogue models while maintaining the response quality.","year":2020,"title_abstract":"Mitigating Gender Bias for Neural Dialogue Generation with Adversarial Learning Dialogue systems play an increasingly important role in various aspects of our daily life. It is evident from recent research that dialogue systems trained on human conversation data are biased. In particular, they can produce responses that reflect people{'}s gender prejudice. Many debiasing methods have been developed for various NLP tasks, such as word embedding. However, they are not directly applicable to dialogue systems because they are likely to force dialogue models to generate similar responses for different genders. This greatly degrades the diversity of the generated responses and immensely hurts the performance of the dialogue models. In this paper, we propose a novel adversarial learning framework Debiased-Chat to train dialogue models free from gender bias while keeping their performance. Extensive experiments on two real-world conversation datasets show that our framework significantly reduces gender bias in dialogue models while maintaining the response quality.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1580834985,"Goal":"Gender Equality","Task":["Mitigating Gender Bias","Neural Dialogue Generation","NLP tasks","word embedding","dialogue systems"],"Method":["Adversarial Learning","Dialogue systems","dialogue systems","debiasing methods","dialogue models","dialogue models","adversarial learning framework","Debiased - Chat","dialogue models","dialogue models"]},{"ID":"ding-riloff-2018-human","title":"Human Needs Categorization of Affective Events Using Labeled and Unlabeled Data","abstract":"We often talk about events that impact us positively or negatively. For example {``}I got a job{''} is good news, but {``}I lost my job{''} is bad news. When we discuss an event, we not only understand its affective polarity but also the reason why the event is beneficial or detrimental. For example, getting or losing a job has affective polarity primarily because it impacts us financially. Our work aims to categorize affective events based upon human need categories that often explain people{'}s motivations and desires: PHYSIOLOGICAL, HEALTH, LEISURE, SOCIAL, FINANCIAL, COGNITION, and FREEDOM. We create classification models based on event expressions as well as models that use contexts surrounding event mentions. We also design a co-training model that learns from unlabeled data by simultaneously training event expression and event context classifiers in an iterative learning process. Our results show that co-training performs well, producing substantially better results than the individual classifiers.","year":2018,"title_abstract":"Human Needs Categorization of Affective Events Using Labeled and Unlabeled Data We often talk about events that impact us positively or negatively. For example {``}I got a job{''} is good news, but {``}I lost my job{''} is bad news. When we discuss an event, we not only understand its affective polarity but also the reason why the event is beneficial or detrimental. For example, getting or losing a job has affective polarity primarily because it impacts us financially. Our work aims to categorize affective events based upon human need categories that often explain people{'}s motivations and desires: PHYSIOLOGICAL, HEALTH, LEISURE, SOCIAL, FINANCIAL, COGNITION, and FREEDOM. We create classification models based on event expressions as well as models that use contexts surrounding event mentions. We also design a co-training model that learns from unlabeled data by simultaneously training event expression and event context classifiers in an iterative learning process. Our results show that co-training performs well, producing substantially better results than the individual classifiers.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1580432951,"Goal":"Climate Action","Task":["Human Needs Categorization of Affective Events"],"Method":["classification models","co - training model","event expression and event context classifiers","iterative learning process","co - training"]},{"ID":"nie-etal-2020-simple","title":"Simple Compounded-Label Training for Fact Extraction and Verification","abstract":"Automatic fact checking is an important task motivated by the need for detecting and preventing the spread of misinformation across the web. The recently released FEVER challenge provides a benchmark task that assesses systems{'} capability for both the retrieval of required evidence and the identification of authentic claims. Previous approaches share a similar pipeline training paradigm that decomposes the task into three subtasks, with each component built and trained separately. Although achieving acceptable scores, these methods induce difficulty for practical application development due to unnecessary complexity and expensive computation. In this paper, we explore the potential of simplifying the system design and reducing training computation by proposing a joint training setup in which a single sequence matching model is trained with compounded labels that give supervision for both sentence selection and claim verification subtasks, eliminating the duplicate computation that occurs when models are designed and trained separately. Empirical results on FEVER indicate that our method: (1) outperforms the typical multi-task learning approach, and (2) gets comparable results to top performing systems with a much simpler training setup and less training computation (in terms of the amount of data consumed and the number of model parameters), facilitating future works on the automatic fact checking task and its practical usage.","year":2020,"title_abstract":"Simple Compounded-Label Training for Fact Extraction and Verification Automatic fact checking is an important task motivated by the need for detecting and preventing the spread of misinformation across the web. The recently released FEVER challenge provides a benchmark task that assesses systems{'} capability for both the retrieval of required evidence and the identification of authentic claims. Previous approaches share a similar pipeline training paradigm that decomposes the task into three subtasks, with each component built and trained separately. Although achieving acceptable scores, these methods induce difficulty for practical application development due to unnecessary complexity and expensive computation. In this paper, we explore the potential of simplifying the system design and reducing training computation by proposing a joint training setup in which a single sequence matching model is trained with compounded labels that give supervision for both sentence selection and claim verification subtasks, eliminating the duplicate computation that occurs when models are designed and trained separately. Empirical results on FEVER indicate that our method: (1) outperforms the typical multi-task learning approach, and (2) gets comparable results to top performing systems with a much simpler training setup and less training computation (in terms of the amount of data consumed and the number of model parameters), facilitating future works on the automatic fact checking task and its practical usage.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1580132991,"Goal":"Climate Action","Task":["Fact Extraction and Verification","Automatic fact checking","detecting and","spread of misinformation","FEVER challenge","retrieval of required evidence","identification of authentic claims","system design","training computation","sentence selection","claim verification subtasks","duplicate computation","automatic fact checking task"],"Method":["Compounded - Label Training","pipeline training paradigm","joint training setup","sequence matching model","(1)","multi - task learning approach"]},{"ID":"smaili-etal-2006-linguistic","title":"Linguistic features modeling based on Partial New Cache","abstract":"The agreement in gender and number is a critical problem in statistical language modeling. One of the main problems in the speech recognition of French language is the presence of misrecognized words due to the bad agreement (in gender and number) between words. Statistical language models do not treat this phenomenon directly. This paper focuses on how to handle the issue of agreements. We introduce an original model called Features-Cache (FC) to estimate the gender and the number of the word to predict. It is a dynamic variable-length Features-Cache for which the size is determined in accordance to syntagm delimitors. This model does not need any syntactic parsing, it is used as any other statistical language model. Several models have been carried out and the best one achieves an improvement of more than 8 points in terms of perplexity.","year":2006,"title_abstract":"Linguistic features modeling based on Partial New Cache The agreement in gender and number is a critical problem in statistical language modeling. One of the main problems in the speech recognition of French language is the presence of misrecognized words due to the bad agreement (in gender and number) between words. Statistical language models do not treat this phenomenon directly. This paper focuses on how to handle the issue of agreements. We introduce an original model called Features-Cache (FC) to estimate the gender and the number of the word to predict. It is a dynamic variable-length Features-Cache for which the size is determined in accordance to syntagm delimitors. This model does not need any syntactic parsing, it is used as any other statistical language model. Several models have been carried out and the best one achieves an improvement of more than 8 points in terms of perplexity.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1579930186,"Goal":"Gender Equality","Task":["statistical language modeling","speech recognition of French language"],"Method":["Linguistic features modeling","Partial New Cache","Statistical language models","Features - Cache","dynamic variable - length Features - Cache","syntactic parsing","statistical language model"]},{"ID":"anderson-etal-2010-base","title":"Base Concepts in the {A}frican Languages Compared to Upper Ontologies and the {W}ord{N}et Top Ontology","abstract":"Ontologies, and in particular upper ontologies, are foundational to the establishment of the Semantic Web. Upper ontologies are used as equivalence formalisms between domain specific ontologies. Multilingualism brings one of the key challenges to the development of these ontologies. Fundamental to the challenges of defining upper ontologies is the assumption that concepts are universally shared. The approach to developing linguistic ontologies aligned to upper ontologies, particularly in the non-Indo-European language families, has highlighted these challenges. Previously two approaches to developing new linguistic ontologies and the influence of these approaches on the upper ontologies have been well documented. These approaches are examined in a unique new context: the African, and in particular, the Bantu languages. In particular, we address the following two questions: Which approach is better for the alignment of the African languages to upper ontologies? Can the concepts that are linguistically shared amongst the African languages be aligned easily with upper ontology concepts claimed to be universally shared?","year":2010,"title_abstract":"Base Concepts in the {A}frican Languages Compared to Upper Ontologies and the {W}ord{N}et Top Ontology Ontologies, and in particular upper ontologies, are foundational to the establishment of the Semantic Web. Upper ontologies are used as equivalence formalisms between domain specific ontologies. Multilingualism brings one of the key challenges to the development of these ontologies. Fundamental to the challenges of defining upper ontologies is the assumption that concepts are universally shared. The approach to developing linguistic ontologies aligned to upper ontologies, particularly in the non-Indo-European language families, has highlighted these challenges. Previously two approaches to developing new linguistic ontologies and the influence of these approaches on the upper ontologies have been well documented. These approaches are examined in a unique new context: the African, and in particular, the Bantu languages. In particular, we address the following two questions: Which approach is better for the alignment of the African languages to upper ontologies? Can the concepts that are linguistically shared amongst the African languages be aligned easily with upper ontology concepts claimed to be universally shared?","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.1579912007,"Goal":"Life Below Water","Task":["Semantic Web","Multilingualism","upper ontologies","linguistic ontologies"],"Method":["Upper ontologies","equivalence formalisms"]},{"ID":"schuster-etal-2021-get","title":"Get Your Vitamin {C}! Robust Fact Verification with Contrastive Evidence","abstract":"Typical fact verification models use retrieved written evidence to verify claims. Evidence sources, however, often change over time as more information is gathered and revised. In order to adapt, models must be sensitive to subtle differences in supporting evidence. We present VitaminC, a benchmark infused with challenging cases that require fact verification models to discern and adjust to slight factual changes. We collect over 100,000 Wikipedia revisions that modify an underlying fact, and leverage these revisions, together with additional synthetically constructed ones, to create a total of over 400,000 claim-evidence pairs. Unlike previous resources, the examples in VitaminC are contrastive, i.e., they contain evidence pairs that are nearly identical in language and content, with the exception that one supports a given claim while the other does not. We show that training using this design increases robustness{---}improving accuracy by 10{\\%} on adversarial fact verification and 6{\\%} on adversarial natural language inference (NLI). Moreover, the structure of VitaminC leads us to define additional tasks for fact-checking resources: tagging relevant words in the evidence for verifying the claim, identifying factual revisions, and providing automatic edits via factually consistent text generation.","year":2021,"title_abstract":"Get Your Vitamin {C}! Robust Fact Verification with Contrastive Evidence Typical fact verification models use retrieved written evidence to verify claims. Evidence sources, however, often change over time as more information is gathered and revised. In order to adapt, models must be sensitive to subtle differences in supporting evidence. We present VitaminC, a benchmark infused with challenging cases that require fact verification models to discern and adjust to slight factual changes. We collect over 100,000 Wikipedia revisions that modify an underlying fact, and leverage these revisions, together with additional synthetically constructed ones, to create a total of over 400,000 claim-evidence pairs. Unlike previous resources, the examples in VitaminC are contrastive, i.e., they contain evidence pairs that are nearly identical in language and content, with the exception that one supports a given claim while the other does not. We show that training using this design increases robustness{---}improving accuracy by 10{\\%} on adversarial fact verification and 6{\\%} on adversarial natural language inference (NLI). Moreover, the structure of VitaminC leads us to define additional tasks for fact-checking resources: tagging relevant words in the evidence for verifying the claim, identifying factual revisions, and providing automatic edits via factually consistent text generation.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1579330117,"Goal":"Climate Action","Task":["Fact Verification","adversarial fact verification","adversarial natural language inference","fact - checking resources","automatic edits","factually consistent text generation"],"Method":["fact verification models","VitaminC","fact verification models","VitaminC","VitaminC"]},{"ID":"passonneau-etal-2012-masc","title":"The {MASC} Word Sense Corpus","abstract":"The MASC project has produced a multi-genre corpus with multiple layers of linguistic annotation, together with a sentence corpus containing WordNet 3.1 sense tags for 1000 occurrences of each of 100 words produced by multiple annotators, accompanied by indepth inter-annotator agreement data. Here we give an overview of the contents of MASC and then focus on the word sense sentence corpus, describing the characteristics that differentiate it from other word sense corpora and detailing the inter-annotator agreement studies that have been performed on the annotations. Finally, we discuss the potential to grow the word sense sentence corpus through crowdsourcing and the plan to enhance the content and annotations of MASC through a community-based collaborative effort.","year":2012,"title_abstract":"The {MASC} Word Sense Corpus The MASC project has produced a multi-genre corpus with multiple layers of linguistic annotation, together with a sentence corpus containing WordNet 3.1 sense tags for 1000 occurrences of each of 100 words produced by multiple annotators, accompanied by indepth inter-annotator agreement data. Here we give an overview of the contents of MASC and then focus on the word sense sentence corpus, describing the characteristics that differentiate it from other word sense corpora and detailing the inter-annotator agreement studies that have been performed on the annotations. Finally, we discuss the potential to grow the word sense sentence corpus through crowdsourcing and the plan to enhance the content and annotations of MASC through a community-based collaborative effort.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1579316854,"Goal":"Sustainable Cities and Communities","Task":["linguistic annotation"],"Method":["MASC","MASC"]},{"ID":"haruta-etal-2020-logical","title":"Logical Inferences with Comparatives and Generalized Quantifiers","abstract":"Comparative constructions pose a challenge in Natural Language Inference (NLI), which is the task of determining whether a text entails a hypothesis. Comparatives are structurally complex in that they interact with other linguistic phenomena such as quantifiers, numerals, and lexical antonyms. In formal semantics, there is a rich body of work on comparatives and gradable expressions using the notion of degree. However, a logical inference system for comparatives has not been sufficiently developed for use in the NLI task. In this paper, we present a compositional semantics that maps various comparative constructions in English to semantic representations via Combinatory Categorial Grammar (CCG) parsers and combine it with an inference system based on automated theorem proving. We evaluate our system on three NLI datasets that contain complex logical inferences with comparatives, generalized quantifiers, and numerals. We show that the system outperforms previous logic-based systems as well as recent deep learning-based models.","year":2020,"title_abstract":"Logical Inferences with Comparatives and Generalized Quantifiers Comparative constructions pose a challenge in Natural Language Inference (NLI), which is the task of determining whether a text entails a hypothesis. Comparatives are structurally complex in that they interact with other linguistic phenomena such as quantifiers, numerals, and lexical antonyms. In formal semantics, there is a rich body of work on comparatives and gradable expressions using the notion of degree. However, a logical inference system for comparatives has not been sufficiently developed for use in the NLI task. In this paper, we present a compositional semantics that maps various comparative constructions in English to semantic representations via Combinatory Categorial Grammar (CCG) parsers and combine it with an inference system based on automated theorem proving. We evaluate our system on three NLI datasets that contain complex logical inferences with comparatives, generalized quantifiers, and numerals. We show that the system outperforms previous logic-based systems as well as recent deep learning-based models.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1579159796,"Goal":"Reduced Inequalities","Task":["Logical Inferences","Natural Language Inference","formal semantics","comparatives","NLI","automated theorem proving","NLI"],"Method":["logical inference system","compositional semantics","semantic representations","Combinatory Categorial Grammar","parsers","inference system","logic - based systems","deep learning - based models"]},{"ID":"diwersy-luxardo-2020-querying","title":"Querying a large annotated corpus of parliamentary debates","abstract":"The TAPS corpus makes it possible to share a large volume of French parliamentary data. The TEI-compliant approach behind its design choices facilitates the publishing and the interoperability of data, but also the implementation of exploratory data analysis techniques in order to process institutional or political discourse. We demonstrate its application to the debates occurred in the context of a specific legislative process, which generated a strong opposition.","year":2020,"title_abstract":"Querying a large annotated corpus of parliamentary debates The TAPS corpus makes it possible to share a large volume of French parliamentary data. The TEI-compliant approach behind its design choices facilitates the publishing and the interoperability of data, but also the implementation of exploratory data analysis techniques in order to process institutional or political discourse. We demonstrate its application to the debates occurred in the context of a specific legislative process, which generated a strong opposition.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.157906279,"Goal":"Peace, Justice and Strong Institutions","Task":["publishing","legislative process"],"Method":["TEI - compliant approach","exploratory data analysis techniques"]},{"ID":"chen-etal-2020-exploring","title":"Exploring Text Specific and Blackbox Fairness Algorithms in Multimodal Clinical {NLP}","abstract":"Clinical machine learning is increasingly multimodal, collected in both structured tabular formats and unstructured forms such as free text. We propose a novel task of exploring \\textit{fairness} on a multimodal clinical dataset, adopting \\textit{equalized odds} for the downstream medical prediction tasks. To this end, we investigate a modality-agnostic fairness algorithm - equalized odds post processing - and compare it to a text-specific fairness algorithm: debiased clinical word embeddings. Despite the fact that debiased word embeddings do not explicitly address equalized odds of protected groups, we show that a text-specific approach to fairness may simultaneously achieve a good balance of performance classical notions of fairness. Our work opens the door for future work at the critical intersection of clinical NLP and fairness.","year":2020,"title_abstract":"Exploring Text Specific and Blackbox Fairness Algorithms in Multimodal Clinical {NLP} Clinical machine learning is increasingly multimodal, collected in both structured tabular formats and unstructured forms such as free text. We propose a novel task of exploring \\textit{fairness} on a multimodal clinical dataset, adopting \\textit{equalized odds} for the downstream medical prediction tasks. To this end, we investigate a modality-agnostic fairness algorithm - equalized odds post processing - and compare it to a text-specific fairness algorithm: debiased clinical word embeddings. Despite the fact that debiased word embeddings do not explicitly address equalized odds of protected groups, we show that a text-specific approach to fairness may simultaneously achieve a good balance of performance classical notions of fairness. Our work opens the door for future work at the critical intersection of clinical NLP and fairness.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1578786671,"Goal":"Reduced Inequalities","Task":["downstream medical prediction tasks","fairness","clinical NLP","fairness"],"Method":["Text Specific and Blackbox Fairness Algorithms","\\textit{fairness}","modality - agnostic fairness algorithm","equalized odds post processing","text - specific fairness algorithm","debiased clinical word embeddings","text - specific approach"]},{"ID":"drozd-etal-2016-word","title":"Word Embeddings, Analogies, and Machine Learning: Beyond king - man + woman = queen","abstract":"Solving word analogies became one of the most popular benchmarks for word embeddings on the assumption that linear relations between word pairs (such as \\textit{king}:\\textit{man} :: \\textit{woman}:\\textit{queen}) are indicative of the quality of the embedding. We question this assumption by showing that the information not detected by linear offset may still be recoverable by a more sophisticated search method, and thus is actually encoded in the embedding. The general problem with linear offset is its sensitivity to the idiosyncrasies of individual words. We show that simple averaging over multiple word pairs improves over the state-of-the-art. A further improvement in accuracy (up to 30{\\%} for some embeddings and relations) is achieved by combining cosine similarity with an estimation of the extent to which a candidate answer belongs to the correct word class. In addition to this practical contribution, this work highlights the problem of the interaction between word embeddings and analogy retrieval algorithms, and its implications for the evaluation of word embeddings and the use of analogies in extrinsic tasks.","year":2016,"title_abstract":"Word Embeddings, Analogies, and Machine Learning: Beyond king - man + woman = queen Solving word analogies became one of the most popular benchmarks for word embeddings on the assumption that linear relations between word pairs (such as \\textit{king}:\\textit{man} :: \\textit{woman}:\\textit{queen}) are indicative of the quality of the embedding. We question this assumption by showing that the information not detected by linear offset may still be recoverable by a more sophisticated search method, and thus is actually encoded in the embedding. The general problem with linear offset is its sensitivity to the idiosyncrasies of individual words. We show that simple averaging over multiple word pairs improves over the state-of-the-art. A further improvement in accuracy (up to 30{\\%} for some embeddings and relations) is achieved by combining cosine similarity with an estimation of the extent to which a candidate answer belongs to the correct word class. In addition to this practical contribution, this work highlights the problem of the interaction between word embeddings and analogy retrieval algorithms, and its implications for the evaluation of word embeddings and the use of analogies in extrinsic tasks.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1578737646,"Goal":"Gender Equality","Task":["Word Embeddings","Machine Learning","word embeddings","linear offset","evaluation of word embeddings","analogies","extrinsic tasks"],"Method":["Analogies","king - man + woman = queen","word analogies","search method","averaging","cosine similarity","word embeddings","analogy retrieval algorithms"]},{"ID":"filgueiras-etal-2019-complaint","title":"Complaint Analysis and Classification for Economic and Food Safety","abstract":"Governmental institutions are employing artificial intelligence techniques to deal with their specific problems and exploit their huge amounts of both structured and unstructured information. In particular, natural language processing and machine learning techniques are being used to process citizen feedback. In this paper, we report on the use of such techniques for analyzing and classifying complaints, in the context of the Portuguese Economic and Food Safety Authority. Grounded in its operational process, we address three different classification problems: target economic activity, implied infraction severity level, and institutional competence. We show promising results obtained using feature-based approaches and traditional classifiers, with accuracy scores above 70{\\%}, and analyze the shortcomings of our current results and avenues for further improvement, taking into account the intended use of our classifiers in helping human officers to cope with thousands of yearly complaints.","year":2019,"title_abstract":"Complaint Analysis and Classification for Economic and Food Safety Governmental institutions are employing artificial intelligence techniques to deal with their specific problems and exploit their huge amounts of both structured and unstructured information. In particular, natural language processing and machine learning techniques are being used to process citizen feedback. In this paper, we report on the use of such techniques for analyzing and classifying complaints, in the context of the Portuguese Economic and Food Safety Authority. Grounded in its operational process, we address three different classification problems: target economic activity, implied infraction severity level, and institutional competence. We show promising results obtained using feature-based approaches and traditional classifiers, with accuracy scores above 70{\\%}, and analyze the shortcomings of our current results and avenues for further improvement, taking into account the intended use of our classifiers in helping human officers to cope with thousands of yearly complaints.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1578679681,"Goal":"Climate Action","Task":["Complaint Analysis","Classification","Economic and Food Safety","citizen feedback","analyzing and classifying complaints","Portuguese Economic and Food Safety Authority","classification problems"],"Method":["artificial intelligence techniques","natural language processing","machine learning techniques","feature - based approaches","classifiers","classifiers"]},{"ID":"hajicova-2014-three","title":"Three dimensions of the so-called {``}interoperability{''} of annotation schemes{''}","abstract":"\u0093Interoperability\u0094 of annotation schemes is one of the key words in the discussions about annotation of corpora. In the present contribution, we propose to look at the so-called interoperability from (at least) three angles, namely (i) as a relation (and possible interaction or cooperation) of different annotation schemes for different layers or phenomena of a single language, (ii) the possibility to annotate different languages by a single (modified or not) annotation scheme, and (iii) the relation between different annotation schemes for a single language, or for a single phenomenon or layer of the same language. The pros and cons of each of these aspects are discussed as well as their contribution to linguistic studies and natural language processing. It is stressed that a communication and collaboration between different annotation schemes requires an explicit specification and consistency of each of the schemes.","year":2014,"title_abstract":"Three dimensions of the so-called {``}interoperability{''} of annotation schemes{''} \u0093Interoperability\u0094 of annotation schemes is one of the key words in the discussions about annotation of corpora. In the present contribution, we propose to look at the so-called interoperability from (at least) three angles, namely (i) as a relation (and possible interaction or cooperation) of different annotation schemes for different layers or phenomena of a single language, (ii) the possibility to annotate different languages by a single (modified or not) annotation scheme, and (iii) the relation between different annotation schemes for a single language, or for a single phenomenon or layer of the same language. The pros and cons of each of these aspects are discussed as well as their contribution to linguistic studies and natural language processing. It is stressed that a communication and collaboration between different annotation schemes requires an explicit specification and consistency of each of the schemes.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1578214914,"Goal":"Partnership for the Goals","Task":["annotation of corpora","linguistic studies","natural language processing"],"Method":["annotation schemes{''}","annotation schemes","annotation schemes","annotation scheme","annotation schemes","annotation schemes"]},{"ID":"tekumalla-banda-2020-characterizing","title":"Characterizing drug mentions in {COVID}-19 {T}witter Chatter","abstract":"Since the classification of COVID-19 as a global pandemic, there have been many attempts to treat and contain the virus. Although there is no specific antiviral treatment recommended for COVID-19, there are several drugs that can potentially help with symptoms. In this work, we mined a large twitter dataset of 424 million tweets of COVID-19 chatter to identify discourse around drug mentions. While seemingly a straightforward task, due to the informal nature of language use in Twitter, we demonstrate the need of machine learning alongside traditional automated methods to aid in this task. By applying these complementary methods, we are able to recover almost 15{\\%} additional data, making misspelling handling a needed task as a pre-processing step when dealing with social media data.","year":2020,"title_abstract":"Characterizing drug mentions in {COVID}-19 {T}witter Chatter Since the classification of COVID-19 as a global pandemic, there have been many attempts to treat and contain the virus. Although there is no specific antiviral treatment recommended for COVID-19, there are several drugs that can potentially help with symptoms. In this work, we mined a large twitter dataset of 424 million tweets of COVID-19 chatter to identify discourse around drug mentions. While seemingly a straightforward task, due to the informal nature of language use in Twitter, we demonstrate the need of machine learning alongside traditional automated methods to aid in this task. By applying these complementary methods, we are able to recover almost 15{\\%} additional data, making misspelling handling a needed task as a pre-processing step when dealing with social media data.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1578067541,"Goal":"Climate Action","Task":["Characterizing drug mentions","COVID - 19","COVID - 19","misspelling handling","pre - processing step"],"Method":["antiviral treatment","machine learning","automated methods"]},{"ID":"maamouri-etal-2008-enhancing","title":"Enhancing the {A}rabic Treebank: a Collaborative Effort toward New Annotation Guidelines","abstract":"The Arabic Treebank team at the Linguistic Data Consortium has significantly revised and enhanced its annotation guidelines and procedure over the past year. Improvements were made to both the morphological and syntactic annotation guidelines, and annotators were trained in the new guidelines, focusing on areas of low inter-annotator agreement. The revised guidelines are now being applied in annotation production, and the combination of the revised guidelines and a period of intensive annotator training has raised inter-annotator agreement f-measure scores already and has also improved parsing results.","year":2008,"title_abstract":"Enhancing the {A}rabic Treebank: a Collaborative Effort toward New Annotation Guidelines The Arabic Treebank team at the Linguistic Data Consortium has significantly revised and enhanced its annotation guidelines and procedure over the past year. Improvements were made to both the morphological and syntactic annotation guidelines, and annotators were trained in the new guidelines, focusing on areas of low inter-annotator agreement. The revised guidelines are now being applied in annotation production, and the combination of the revised guidelines and a period of intensive annotator training has raised inter-annotator agreement f-measure scores already and has also improved parsing results.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1577947587,"Goal":"Partnership for the Goals","Task":["annotation production","parsing"],"Method":["annotator training"]},{"ID":"lauscher-etal-2020-araweat","title":"{A}ra{WEAT}: Multidimensional Analysis of Biases in {A}rabic Word Embeddings","abstract":"Recent work has shown that distributional word vector spaces often encode human biases like sexism or racism. In this work, we conduct an extensive analysis of biases in Arabic word embeddings by applying a range of recently introduced bias tests on a variety of embedding spaces induced from corpora in Arabic. We measure the presence of biases across several dimensions, namely: embedding models (Skip-Gram, CBOW, and FastText) and vector sizes, types of text (encyclopedic text, and news vs. user-generated content), dialects (Egyptian Arabic vs. Modern Standard Arabic), and time (diachronic analyses over corpora from different time periods). Our analysis yields several interesting findings, e.g., that implicit gender bias in embeddings trained on Arabic news corpora steadily increases over time (between 2007 and 2017). We make the Arabic bias specifications (AraWEAT) publicly available.","year":2020,"title_abstract":"{A}ra{WEAT}: Multidimensional Analysis of Biases in {A}rabic Word Embeddings Recent work has shown that distributional word vector spaces often encode human biases like sexism or racism. In this work, we conduct an extensive analysis of biases in Arabic word embeddings by applying a range of recently introduced bias tests on a variety of embedding spaces induced from corpora in Arabic. We measure the presence of biases across several dimensions, namely: embedding models (Skip-Gram, CBOW, and FastText) and vector sizes, types of text (encyclopedic text, and news vs. user-generated content), dialects (Egyptian Arabic vs. Modern Standard Arabic), and time (diachronic analyses over corpora from different time periods). Our analysis yields several interesting findings, e.g., that implicit gender bias in embeddings trained on Arabic news corpora steadily increases over time (between 2007 and 2017). We make the Arabic bias specifications (AraWEAT) publicly available.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.15771617,"Goal":"Gender Equality","Task":["Multidimensional Analysis of Biases","Word Embeddings","Arabic word embeddings"],"Method":["bias tests","embedding models","CBOW","FastText)","vector sizes"]},{"ID":"thorne-vlachos-2021-evidence","title":"Evidence-based Factual Error Correction","abstract":"This paper introduces the task of factual error correction: performing edits to a claim so that the generated rewrite is better supported by evidence. This extends the well-studied task of fact verification by providing a mechanism to correct written texts that are refuted or only partially supported by evidence. We demonstrate that it is feasible to train factual error correction systems from existing fact checking datasets which only contain labeled claims accompanied by evidence, but not the correction. We achieve this by employing a two-stage distant supervision approach that incorporates evidence into masked claims when generating corrections. Our approach, based on the T5 transformer and using retrieved evidence, achieved better results than existing work which used a pointer copy network and gold evidence, producing accurate factual error corrections for 5x more instances in human evaluation and a .125 increase in SARI score. The evaluation is conducted on a dataset of 65,000 instances based on a recent fact verification shared task and we release it to enable further work on the task.","year":2021,"title_abstract":"Evidence-based Factual Error Correction This paper introduces the task of factual error correction: performing edits to a claim so that the generated rewrite is better supported by evidence. This extends the well-studied task of fact verification by providing a mechanism to correct written texts that are refuted or only partially supported by evidence. We demonstrate that it is feasible to train factual error correction systems from existing fact checking datasets which only contain labeled claims accompanied by evidence, but not the correction. We achieve this by employing a two-stage distant supervision approach that incorporates evidence into masked claims when generating corrections. Our approach, based on the T5 transformer and using retrieved evidence, achieved better results than existing work which used a pointer copy network and gold evidence, producing accurate factual error corrections for 5x more instances in human evaluation and a .125 increase in SARI score. The evaluation is conducted on a dataset of 65,000 instances based on a recent fact verification shared task and we release it to enable further work on the task.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1576531529,"Goal":"Climate Action","Task":["Evidence - based Factual Error Correction","factual error correction","edits","fact verification","correction","fact verification shared task"],"Method":["factual error correction systems","stage distant supervision approach","T5 transformer","pointer copy network"]},{"ID":"ohta-etal-2006-linguistic","title":"Linguistic and Biological Annotations of Biological Interaction Events","abstract":"This paper discusses an augmentation of a corpus ofresearch abstracts in biomedical domain (the GENIA corpus) with two kinds of annotations: tree annotation and event annotation. The tree annotation identifies the linguistic structure that encodes the relations among entities. The event annotation reveals the semantic structure of the biological interaction events encoded in the text. With these annotations we aim to provide a link between the clue and the target of biological event information extraction.","year":2006,"title_abstract":"Linguistic and Biological Annotations of Biological Interaction Events This paper discusses an augmentation of a corpus ofresearch abstracts in biomedical domain (the GENIA corpus) with two kinds of annotations: tree annotation and event annotation. The tree annotation identifies the linguistic structure that encodes the relations among entities. The event annotation reveals the semantic structure of the biological interaction events encoded in the text. With these annotations we aim to provide a link between the clue and the target of biological event information extraction.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1575508565,"Goal":"Life on Land","Task":["Linguistic and Biological Annotations of Biological Interaction Events","tree annotation","event annotation","biological event information extraction"],"Method":["tree annotation","event annotation"]},{"ID":"hotate-etal-2020-generating","title":"Generating Diverse Corrections with Local Beam Search for Grammatical Error Correction","abstract":"In this study, we propose a beam search method to obtain diverse outputs in a local sequence transduction task where most of the tokens in the source and target sentences overlap, such as in grammatical error correction (GEC). In GEC, it is advisable to rewrite only the local sequences that must be rewritten while leaving the correct sequences unchanged. However, existing methods of acquiring various outputs focus on revising all tokens of a sentence. Therefore, existing methods may either generate ungrammatical sentences because they force the entire sentence to be changed or produce non-diversified sentences by weakening the constraints to avoid generating ungrammatical sentences. Considering these issues, we propose a method that does not rewrite all the tokens in a text, but only rewrites those parts that need to be diversely corrected. Our beam search method adjusts the search token in the beam according to the probability that the prediction is copied from the source sentence. The experimental results show that our proposed method generates more diverse corrections than existing methods without losing accuracy in the GEC task.","year":2020,"title_abstract":"Generating Diverse Corrections with Local Beam Search for Grammatical Error Correction In this study, we propose a beam search method to obtain diverse outputs in a local sequence transduction task where most of the tokens in the source and target sentences overlap, such as in grammatical error correction (GEC). In GEC, it is advisable to rewrite only the local sequences that must be rewritten while leaving the correct sequences unchanged. However, existing methods of acquiring various outputs focus on revising all tokens of a sentence. Therefore, existing methods may either generate ungrammatical sentences because they force the entire sentence to be changed or produce non-diversified sentences by weakening the constraints to avoid generating ungrammatical sentences. Considering these issues, we propose a method that does not rewrite all the tokens in a text, but only rewrites those parts that need to be diversely corrected. Our beam search method adjusts the search token in the beam according to the probability that the prediction is copied from the source sentence. The experimental results show that our proposed method generates more diverse corrections than existing methods without losing accuracy in the GEC task.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1574905813,"Goal":"Gender Equality","Task":["Diverse Corrections","Grammatical Error Correction","local sequence transduction task","grammatical error correction","GEC","GEC task"],"Method":["Local Beam Search","beam search method","beam search method"]},{"ID":"palanikumar-etal-2022-de","title":"{DE}-{ABUSE}@{T}amil{NLP}-{ACL} 2022: Transliteration as Data Augmentation for Abuse Detection in {T}amil","abstract":"With the rise of social media and internet, thereis a necessity to provide an inclusive space andprevent the abusive topics against any gender,race or community. This paper describes thesystem submitted to the ACL-2022 shared taskon fine-grained abuse detection in Tamil. In ourapproach we transliterated code-mixed datasetas an augmentation technique to increase thesize of the data. Using this method we wereable to rank 3rd on the task with a 0.290 macroaverage F1 score and a 0.590 weighted F1score","year":2022,"title_abstract":"{DE}-{ABUSE}@{T}amil{NLP}-{ACL} 2022: Transliteration as Data Augmentation for Abuse Detection in {T}amil With the rise of social media and internet, thereis a necessity to provide an inclusive space andprevent the abusive topics against any gender,race or community. This paper describes thesystem submitted to the ACL-2022 shared taskon fine-grained abuse detection in Tamil. In ourapproach we transliterated code-mixed datasetas an augmentation technique to increase thesize of the data. Using this method we wereable to rank 3rd on the task with a 0.290 macroaverage F1 score and a 0.590 weighted F1score","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1574590206,"Goal":"Gender Equality","Task":["Transliteration","Data Augmentation","Abuse Detection","ACL - 2022 shared taskon fine - grained abuse detection"],"Method":["augmentation technique"]},{"ID":"berck-etal-2006-ontology","title":"Ontology-based Language Archive Utilization","abstract":"At the MPI for Psycholinguistics a large archive with language resources has been created with contributions from many different individual researchers and research projects. All of these resources, in particular annotated media streams and multimedia lexica, are accessible via the web and can be utilized with the help of web-based utilization frameworks. Therefore, the archive lends itself to motivate users to operate across the boundaries of single corpora and to support cross-language work. This, however, can only be done when the problems of interoperability, in particular at the level of linguistic encoding, can be solved in an efficient way. Two Max-Planck-Institutes are cooperating to build a framework that allows users to easily create their own practical ontologies and if wanted to relate their concepts to central ontologies.","year":2006,"title_abstract":"Ontology-based Language Archive Utilization At the MPI for Psycholinguistics a large archive with language resources has been created with contributions from many different individual researchers and research projects. All of these resources, in particular annotated media streams and multimedia lexica, are accessible via the web and can be utilized with the help of web-based utilization frameworks. Therefore, the archive lends itself to motivate users to operate across the boundaries of single corpora and to support cross-language work. This, however, can only be done when the problems of interoperability, in particular at the level of linguistic encoding, can be solved in an efficient way. Two Max-Planck-Institutes are cooperating to build a framework that allows users to easily create their own practical ontologies and if wanted to relate their concepts to central ontologies.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.1574332863,"Goal":"Life Below Water","Task":["Ontology - based Language Archive Utilization","Psycholinguistics","cross - language work","interoperability","linguistic encoding"],"Method":["web - based utilization frameworks"]},{"ID":"schouten-etal-2017-commit","title":"{COMMIT} at {S}em{E}val-2017 Task 5: Ontology-based Method for Sentiment Analysis of Financial Headlines","abstract":"This paper describes our submission to Task 5 of SemEval 2017, Fine-Grained Sentiment Analysis on Financial Microblogs and News, where we limit ourselves to performing sentiment analysis on news headlines only (track 2). The approach presented in this paper uses a Support Vector Machine to do the required regression, and besides unigrams and a sentiment tool, we use various ontology-based features. To this end we created a domain ontology that models various concepts from the financial domain. This allows us to model the sentiment of actions depending on which entity they are affecting (e.g., {`}decreasing debt{'} is positive, but {`}decreasing profit{'} is negative). The presented approach yielded a cosine distance of 0.6810 on the official test data, resulting in the 12th position.","year":2017,"title_abstract":"{COMMIT} at {S}em{E}val-2017 Task 5: Ontology-based Method for Sentiment Analysis of Financial Headlines This paper describes our submission to Task 5 of SemEval 2017, Fine-Grained Sentiment Analysis on Financial Microblogs and News, where we limit ourselves to performing sentiment analysis on news headlines only (track 2). The approach presented in this paper uses a Support Vector Machine to do the required regression, and besides unigrams and a sentiment tool, we use various ontology-based features. To this end we created a domain ontology that models various concepts from the financial domain. This allows us to model the sentiment of actions depending on which entity they are affecting (e.g., {`}decreasing debt{'} is positive, but {`}decreasing profit{'} is negative). The presented approach yielded a cosine distance of 0.6810 on the official test data, resulting in the 12th position.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1574151814,"Goal":"Climate Action","Task":["Sentiment Analysis of Financial Headlines","SemEval 2017","Fine - Grained Sentiment Analysis","sentiment analysis","regression"],"Method":["Ontology - based Method","Support Vector Machine","unigrams","sentiment tool","ontology - based features","domain ontology"]},{"ID":"wang-etal-2017-xmu-neural","title":"{XMU} Neural Machine Translation Systems for {WAT} 2017","abstract":"This paper describes the Neural Machine Translation systems of Xiamen University for the shared translation tasks of WAT 2017. Our systems are based on the Encoder-Decoder framework with attention. We participated in three subtasks. We experimented subword segmentation, synthetic training data and model ensembling. Experiments show that all these methods can give substantial improvements.","year":2017,"title_abstract":"{XMU} Neural Machine Translation Systems for {WAT} 2017 This paper describes the Neural Machine Translation systems of Xiamen University for the shared translation tasks of WAT 2017. Our systems are based on the Encoder-Decoder framework with attention. We participated in three subtasks. We experimented subword segmentation, synthetic training data and model ensembling. Experiments show that all these methods can give substantial improvements.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1573998034,"Goal":"Gender Equality","Task":["Neural Machine Translation Systems","Translation","shared translation tasks","WAT 2017","subword segmentation"],"Method":["Encoder - Decoder framework","attention","model ensembling"]},{"ID":"darbari-1999-computer","title":"Computer assisted translation system {--} an {I}ndian perspective","abstract":"Work in the area of Machine Translation has been going on for several decades and it was only during the early 90s that a promising translation technology began to emerge with advanced researches in the field of Artificial Intelligence and Computational Linguistics. This held the promise of successfully developing usable Machine Translation Systems in certain well-defined domains. C-DAC took up this challenge, as we felt that India, being a multi-lingual and multi-cultural country with a population of approximately 950 million people and 18 constitutionally recognized languages, needs a translation system for instant transfer of information and knowledge. The other groups who are working in this area of English to Hindi Translation are National Center for Software Technology (NCST), who are working on translation of News Stories and Electronics Research {\\&} Development Center of India (ER {\\&} DCI). who have developed the Machine Assisted Translation System for the Health Domain. A major project on Indian Languages to Indian Languages Translation (Anusaaraka) is also under development at University of Hyderabad.","year":1999,"title_abstract":"Computer assisted translation system {--} an {I}ndian perspective Work in the area of Machine Translation has been going on for several decades and it was only during the early 90s that a promising translation technology began to emerge with advanced researches in the field of Artificial Intelligence and Computational Linguistics. This held the promise of successfully developing usable Machine Translation Systems in certain well-defined domains. C-DAC took up this challenge, as we felt that India, being a multi-lingual and multi-cultural country with a population of approximately 950 million people and 18 constitutionally recognized languages, needs a translation system for instant transfer of information and knowledge. The other groups who are working in this area of English to Hindi Translation are National Center for Software Technology (NCST), who are working on translation of News Stories and Electronics Research {\\&} Development Center of India (ER {\\&} DCI). who have developed the Machine Assisted Translation System for the Health Domain. A major project on Indian Languages to Indian Languages Translation (Anusaaraka) is also under development at University of Hyderabad.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1573617309,"Goal":"Gender Equality","Task":["Computer assisted translation system","Machine Translation","Artificial Intelligence","Computational Linguistics","Machine Translation Systems","instant transfer of information","English to Hindi Translation","Software Technology","translation","Electronics Research","Development Center","Translation","Health Domain","Translation"],"Method":["translation technology","C - DAC","translation system"]},{"ID":"lawrence-riezler-2016-nlmaps","title":"{NL}maps: A Natural Language Interface to Query {O}pen{S}treet{M}ap","abstract":"We present a Natural Language Interface (nlmaps.cl.uni-heidelberg.de) to query OpenStreetMap. Natural language questions about geographical facts are parsed into database queries that can be executed against the OpenStreetMap (OSM) database. After parsing the question, the system provides a text based answer as well as an interactive map with all points of interest and their relevant information marked. Additionally, we provide several options for users to give feedback after a question has been parsed.","year":2016,"title_abstract":"{NL}maps: A Natural Language Interface to Query {O}pen{S}treet{M}ap We present a Natural Language Interface (nlmaps.cl.uni-heidelberg.de) to query OpenStreetMap. Natural language questions about geographical facts are parsed into database queries that can be executed against the OpenStreetMap (OSM) database. After parsing the question, the system provides a text based answer as well as an interactive map with all points of interest and their relevant information marked. Additionally, we provide several options for users to give feedback after a question has been parsed.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1573563814,"Goal":"Sustainable Cities and Communities","Task":["Query"],"Method":["Natural Language Interface","Natural Language Interface"]},{"ID":"ding-etal-2020-hit","title":"{HIT}-{SCIR} at {S}em{E}val-2020 Task 5: Training Pre-trained Language Model with Pseudo-labeling Data for Counterfactuals Detection","abstract":"We describe our system for Task 5 of SemEval 2020: Modelling Causal Reasoning in Language: Detecting Counterfactuals. Despite deep learning has achieved significant success in many fields, it still hardly drives today{'}s AI to strong AI, as it lacks of causation, which is a fundamental concept in human thinking and reasoning. In this task, we dedicate to detecting causation, especially counterfactuals from texts. We explore multiple pre-trained models to learn basic features and then fine-tune models with counterfactual data and pseudo-labeling data. Our team HIT-SCIR wins the first place (1st) in Sub-task 1 {---} Detecting Counterfactual Statements and is ranked 4th in Sub-task 2 {---} Detecting Antecedent and Consequence. In this paper we provide a detailed description of the approach, as well as the results obtained in this task.","year":2020,"title_abstract":"{HIT}-{SCIR} at {S}em{E}val-2020 Task 5: Training Pre-trained Language Model with Pseudo-labeling Data for Counterfactuals Detection We describe our system for Task 5 of SemEval 2020: Modelling Causal Reasoning in Language: Detecting Counterfactuals. Despite deep learning has achieved significant success in many fields, it still hardly drives today{'}s AI to strong AI, as it lacks of causation, which is a fundamental concept in human thinking and reasoning. In this task, we dedicate to detecting causation, especially counterfactuals from texts. We explore multiple pre-trained models to learn basic features and then fine-tune models with counterfactual data and pseudo-labeling data. Our team HIT-SCIR wins the first place (1st) in Sub-task 1 {---} Detecting Counterfactual Statements and is ranked 4th in Sub-task 2 {---} Detecting Antecedent and Consequence. In this paper we provide a detailed description of the approach, as well as the results obtained in this task.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1572789401,"Goal":"Climate Action","Task":["Counterfactuals Detection","SemEval 2020","Modelling Causal Reasoning in Language","Detecting Counterfactuals","AI","AI","human thinking and reasoning","detecting causation"],"Method":["Language Model","deep learning"]},{"ID":"hartmann-1999-next","title":"The next step: moving to an integrated {MT} system for high-volume environments","abstract":"Within the realm of small to medium-sized translation companies, the demands placed on MT in a high-volume production environment plagued with extremely demanding turn-around times and cost pressures are quite different from most other uses of MT. With the help of an analysis of a typical project the author shows the need for MT to become an integrated part of a translation application which will reduce the amount of extraneous processes to a minimum. In conclusion, a system is proposed which will streamline all the ancillary processes in order conform to customers' turn-around demands without jeopardizing post-editing quality.","year":1999,"title_abstract":"The next step: moving to an integrated {MT} system for high-volume environments Within the realm of small to medium-sized translation companies, the demands placed on MT in a high-volume production environment plagued with extremely demanding turn-around times and cost pressures are quite different from most other uses of MT. With the help of an analysis of a typical project the author shows the need for MT to become an integrated part of a translation application which will reduce the amount of extraneous processes to a minimum. In conclusion, a system is proposed which will streamline all the ancillary processes in order conform to customers' turn-around demands without jeopardizing post-editing quality.","social_need":"Responsible Consumption and Production Ensure sustainable consumption and production patterns","cosine_similarity":0.1572321504,"Goal":"Responsible Consumption and Production","Task":["high - volume environments","translation companies","MT","high - volume production environment","MT","MT","translation application"],"Method":["{MT} system","ancillary processes"]},{"ID":"baly-etal-2018-integrating","title":"Integrating Stance Detection and Fact Checking in a Unified Corpus","abstract":"A reasonable approach for fact checking a claim involves retrieving potentially relevant documents from different sources (e.g., news websites, social media, etc.), determining the stance of each document with respect to the claim, and finally making a prediction about the claim{'}s factuality by aggregating the strength of the stances, while taking the reliability of the source into account. Moreover, a fact checking system should be able to explain its decision by providing relevant extracts (rationales) from the documents. Yet, this setup is not directly supported by existing datasets, which treat fact checking, document retrieval, source credibility, stance detection and rationale extraction as independent tasks. In this paper, we support the interdependencies between these tasks as annotations in the same corpus. We implement this setup on an Arabic fact checking corpus, the first of its kind.","year":2018,"title_abstract":"Integrating Stance Detection and Fact Checking in a Unified Corpus A reasonable approach for fact checking a claim involves retrieving potentially relevant documents from different sources (e.g., news websites, social media, etc.), determining the stance of each document with respect to the claim, and finally making a prediction about the claim{'}s factuality by aggregating the strength of the stances, while taking the reliability of the source into account. Moreover, a fact checking system should be able to explain its decision by providing relevant extracts (rationales) from the documents. Yet, this setup is not directly supported by existing datasets, which treat fact checking, document retrieval, source credibility, stance detection and rationale extraction as independent tasks. In this paper, we support the interdependencies between these tasks as annotations in the same corpus. We implement this setup on an Arabic fact checking corpus, the first of its kind.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1572162807,"Goal":"Climate Action","Task":["Stance Detection","Fact Checking","fact checking a claim","fact checking","document retrieval","source credibility","stance detection","rationale extraction"],"Method":["fact checking system"]},{"ID":"kolkey-etal-2019-nlg","title":"An {NLG} System for Constituent Correspondence: Personality, Affect, and Alignment","abstract":"Roughly 30{\\%} of congressional staffers in the United States report spending a {``}great deal{''} of time writing responses to constituent letters. Letters often solicit an update on the status of legislation and a description of a congressman{'}s vote record or vote intention {---} structurable data that can be leveraged by a natural language generation (NLG) system to create a coherent letter response. This paper describes how PoliScribe, a pipeline-architectured NLG platform, constructs personalized responses to constituents inquiring about legislation. Emphasis will be placed on adapting NLG methodologies to the political domain, which entails special attention to affect, discursive variety, and rhetorical strategies that align a speaker with their interlocutor, even in cases of policy disagreement.","year":2019,"title_abstract":"An {NLG} System for Constituent Correspondence: Personality, Affect, and Alignment Roughly 30{\\%} of congressional staffers in the United States report spending a {``}great deal{''} of time writing responses to constituent letters. Letters often solicit an update on the status of legislation and a description of a congressman{'}s vote record or vote intention {---} structurable data that can be leveraged by a natural language generation (NLG) system to create a coherent letter response. This paper describes how PoliScribe, a pipeline-architectured NLG platform, constructs personalized responses to constituents inquiring about legislation. Emphasis will be placed on adapting NLG methodologies to the political domain, which entails special attention to affect, discursive variety, and rhetorical strategies that align a speaker with their interlocutor, even in cases of policy disagreement.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1571891606,"Goal":"Climate Action","Task":["Constituent Correspondence","Alignment","personalized responses"],"Method":["natural language generation (NLG) system","PoliScribe","pipeline - architectured NLG platform","NLG methodologies","rhetorical strategies"]},{"ID":"mboning-wandji-2021-construire","title":"Construire des ressources collaboratives pour les langues peu dot{\\'e}es: une mod{\\'e}lisation orient{\\'e}e communaut{\\'e} (Building collaborative resources for poorly endowed languages : community-oriented modeling )","abstract":"Les applications du traitement automatique des langues (TAL) nourrissent aujourd{'}hui une bonne partie des langues indo-europ{\\'e}ennes en raison des corpus linguistiques de qualit{\\'e} disponibles en grande quantit{\\'e} et vari{\\'e}t{\\'e}. Les corpus de donn{\\'e}es open sources en langues africaines {\\'e}tant quasi inexistants, comment arrimer les avanc{\\'e}es du TAL {\\`a} ces langues peu dot{\\'e}es ? Dans cet article, nous examinons le probl{\\`e}me de construction des ressources lexicographiques pour les langues peu dot{\\'e}es. Nous souhaitons introduire un mod{\\`e}le de construction des ressources lexicographiques en exploitant les comp{\\'e}tences socio-linguistiques des communaut{\\'e}s linguistiques locales. Au fil des sections, nous pr{\\'e}senterons le nouveau mod{\\`e}le de codification des dictionnaires issue de cette mod{\\'e}lisation orient{\\'e}e communaut{\\'e}.","year":2021,"title_abstract":"Construire des ressources collaboratives pour les langues peu dot{\\'e}es: une mod{\\'e}lisation orient{\\'e}e communaut{\\'e} (Building collaborative resources for poorly endowed languages : community-oriented modeling ) Les applications du traitement automatique des langues (TAL) nourrissent aujourd{'}hui une bonne partie des langues indo-europ{\\'e}ennes en raison des corpus linguistiques de qualit{\\'e} disponibles en grande quantit{\\'e} et vari{\\'e}t{\\'e}. Les corpus de donn{\\'e}es open sources en langues africaines {\\'e}tant quasi inexistants, comment arrimer les avanc{\\'e}es du TAL {\\`a} ces langues peu dot{\\'e}es ? Dans cet article, nous examinons le probl{\\`e}me de construction des ressources lexicographiques pour les langues peu dot{\\'e}es. Nous souhaitons introduire un mod{\\`e}le de construction des ressources lexicographiques en exploitant les comp{\\'e}tences socio-linguistiques des communaut{\\'e}s linguistiques locales. Au fil des sections, nous pr{\\'e}senterons le nouveau mod{\\`e}le de codification des dictionnaires issue de cette mod{\\'e}lisation orient{\\'e}e communaut{\\'e}.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1571567059,"Goal":"Sustainable Cities and Communities","Task":["ressources lexicographiques"],"Method":["community - oriented modeling","traitement automatique"]},{"ID":"tamari-etal-2019-playing","title":"Playing by the Book: An Interactive Game Approach for Action Graph Extraction from Text","abstract":"Understanding procedural text requires tracking entities, actions and effects as the narrative unfolds. We focus on the challenging real-world problem of action-graph extraction from materials science papers, where language is highly specialized and data annotation is expensive and scarce. We propose a novel approach, Text2Quest, where procedural text is interpreted as instructions for an interactive game. A learning agent completes the game by executing the procedure correctly in a text-based simulated lab environment. The framework can complement existing approaches and enables richer forms of learning compared to static texts. We discuss potential limitations and advantages of the approach, and release a prototype proof-of-concept, hoping to encourage research in this direction.","year":2019,"title_abstract":"Playing by the Book: An Interactive Game Approach for Action Graph Extraction from Text Understanding procedural text requires tracking entities, actions and effects as the narrative unfolds. We focus on the challenging real-world problem of action-graph extraction from materials science papers, where language is highly specialized and data annotation is expensive and scarce. We propose a novel approach, Text2Quest, where procedural text is interpreted as instructions for an interactive game. A learning agent completes the game by executing the procedure correctly in a text-based simulated lab environment. The framework can complement existing approaches and enables richer forms of learning compared to static texts. We discuss potential limitations and advantages of the approach, and release a prototype proof-of-concept, hoping to encourage research in this direction.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1571219414,"Goal":"Climate Action","Task":["Action Graph Extraction","Text Understanding","real - world problem of action - graph extraction","data annotation","interactive game","learning"],"Method":["Interactive Game Approach","Text2Quest","learning agent"]},{"ID":"itahashi-tseng-2008-2008","title":"The 2008 Oriental {COCOSDA} Book Project: in Commemoration of the First Decade of Sustained Activities in {A}sia","abstract":"The purpose of Oriental COCOSDA is to provide the Asian community a platform to exchange ideas, to share information and to discuss regional matters on creation, utilization, dissemination of spoken language corpora of oriental languages and also on the assessment methods of speech recognition\/synthesis systems as well as to promote speech research on oriental languages. Since its preparatory meeting in Hong Kong in 1997, annual workshops have been organized and held in Japan, Taiwan, China, Korea, Thailand, Singapore, India, Indonesia, Malaysia, and Vietnam from 1998 onwards. The organization is managed by a convener, three advisory members, and 26 committee members from 13 regions in Oriental area. In order to commemorate 10 years of continued activities, the members have decided to publish a book which covers a wide range of speech research. Special focus will be on speech resources or speech corpora in Oriental countries and standardization of speech input\/output systems performance evaluation methods on which key technologies for speech systems development are based. The book will also include linguistic outlines of oriental languages, annotation, labeling, and software tools for speech processing.","year":2008,"title_abstract":"The 2008 Oriental {COCOSDA} Book Project: in Commemoration of the First Decade of Sustained Activities in {A}sia The purpose of Oriental COCOSDA is to provide the Asian community a platform to exchange ideas, to share information and to discuss regional matters on creation, utilization, dissemination of spoken language corpora of oriental languages and also on the assessment methods of speech recognition\/synthesis systems as well as to promote speech research on oriental languages. Since its preparatory meeting in Hong Kong in 1997, annual workshops have been organized and held in Japan, Taiwan, China, Korea, Thailand, Singapore, India, Indonesia, Malaysia, and Vietnam from 1998 onwards. The organization is managed by a convener, three advisory members, and 26 committee members from 13 regions in Oriental area. In order to commemorate 10 years of continued activities, the members have decided to publish a book which covers a wide range of speech research. Special focus will be on speech resources or speech corpora in Oriental countries and standardization of speech input\/output systems performance evaluation methods on which key technologies for speech systems development are based. The book will also include linguistic outlines of oriental languages, annotation, labeling, and software tools for speech processing.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1570765674,"Goal":"Partnership for the Goals","Task":["speech research","speech research","speech input\/output systems performance evaluation methods","speech systems development","annotation","labeling","speech processing"],"Method":["assessment methods","speech recognition\/synthesis systems","software tools"]},{"ID":"broeder-etal-2020-lr4sshoc","title":"{LR}4{SSHOC}: The Future of Language Resources in the Context of the Social Sciences and Humanities Open Cloud","abstract":"This paper outlines the future of language resources and identifies their potential contribution for creating and sustaining the social sciences and humanities (SSH) component of the European Open Science Cloud (EOSC).","year":2020,"title_abstract":"{LR}4{SSHOC}: The Future of Language Resources in the Context of the Social Sciences and Humanities Open Cloud This paper outlines the future of language resources and identifies their potential contribution for creating and sustaining the social sciences and humanities (SSH) component of the European Open Science Cloud (EOSC).","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1570730209,"Goal":"Sustainable Cities and Communities","Task":["Language Resources","Social Sciences and Humanities Open Cloud","language resources"],"Method":["social sciences and humanities (SSH) component","European Open Science Cloud"]},{"ID":"saggion-szasz-2012-concisus","title":"The {CONCISUS} Corpus of Event Summaries","abstract":"Text summarization and information extraction systems require adaptation to new domains and languages. This adaptation usually depends on the availability of language resources such as corpora. In this paper we present a comparable corpus in Spanish and English for the study of cross-lingual information extraction and summarization: the CONCISUS Corpus. It is a rich human-annotated dataset composed of comparable event summaries in Spanish and English covering four different domains: aviation accidents, rail accidents, earthquakes, and terrorist attacks. In addition to the monolingual summaries in English and Spanish, we provide automatic translations and ``comparable'' full event reports of the events. The human annotations are concepts marked in the textual sources representing the key event information associated to the event type. The dataset has also been annotated using text processing pipelines. It is being made freely available to the research community for research purposes.","year":2012,"title_abstract":"The {CONCISUS} Corpus of Event Summaries Text summarization and information extraction systems require adaptation to new domains and languages. This adaptation usually depends on the availability of language resources such as corpora. In this paper we present a comparable corpus in Spanish and English for the study of cross-lingual information extraction and summarization: the CONCISUS Corpus. It is a rich human-annotated dataset composed of comparable event summaries in Spanish and English covering four different domains: aviation accidents, rail accidents, earthquakes, and terrorist attacks. In addition to the monolingual summaries in English and Spanish, we provide automatic translations and ``comparable'' full event reports of the events. The human annotations are concepts marked in the textual sources representing the key event information associated to the event type. The dataset has also been annotated using text processing pipelines. It is being made freely available to the research community for research purposes.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1570147425,"Goal":"Sustainable Cities and Communities","Task":["summarization","cross - lingual information extraction","summarization","aviation accidents"],"Method":["text processing pipelines"]},{"ID":"ahmad-etal-2018-centement","title":"{CENTEMENT} at {S}em{E}val-2018 Task 1: Classification of Tweets using Multiple Thresholds with Self-correction and Weighted Conditional Probabilities","abstract":"In this paper we present our contribution to SemEval-2018, a classifier for classifying multi-label emotions of Arabic and English tweets. We attempted {``}Affect in Tweets{''}, specifically Task E-c: Detecting Emotions (multi-label classification). Our method is based on preprocessing the tweets and creating word vectors combined with a self correction step to remove noise. We also make use of emotion specific thresholds. The final submission was selected upon the best performance achieved, selected when using a range of thresholds. Our system was evaluated on the Arabic and English datasets provided for the task by the competition organisers, where it ranked 2nd for the Arabic dataset (out of 14 entries) and 12th for the English dataset (out of 35 entries).","year":2018,"title_abstract":"{CENTEMENT} at {S}em{E}val-2018 Task 1: Classification of Tweets using Multiple Thresholds with Self-correction and Weighted Conditional Probabilities In this paper we present our contribution to SemEval-2018, a classifier for classifying multi-label emotions of Arabic and English tweets. We attempted {``}Affect in Tweets{''}, specifically Task E-c: Detecting Emotions (multi-label classification). Our method is based on preprocessing the tweets and creating word vectors combined with a self correction step to remove noise. We also make use of emotion specific thresholds. The final submission was selected upon the best performance achieved, selected when using a range of thresholds. Our system was evaluated on the Arabic and English datasets provided for the task by the competition organisers, where it ranked 2nd for the Arabic dataset (out of 14 entries) and 12th for the English dataset (out of 35 entries).","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.157012403,"Goal":"Reduced Inequalities","Task":["Classification of Tweets","classifying multi - label emotions of Arabic and English tweets","Detecting Emotions (multi - label classification)"],"Method":["Self - correction","classifier","self correction step"]},{"ID":"bossard-rodrigues-2015-robo","title":"{ROBO}, an edit distance for sentence comparison Application to automatic summarization","abstract":"Dans cet article, nous proposons une mesure de distance entre phrases fond{\\'e}e sur la distance de Levenshtein, doublement pond{\\'e}r{\\'e}e par la fr{\\'e}quence des mots et par le type d{'}op{\\'e}ration r{\\'e}alis{\\'e}e. Nous l{'}{\\'e}valuons au sein d{'}un syst{\\`e}me de r{\\'e}sum{\\'e} automatique dont la m{\\'e}thode de calcul est volontairement limit{\\'e}e {\\`a} une approche fond{\\'e}e sur la similarit{\\'e} entre phrases. Nous sommes donc ainsi en mesure d{'}{\\'e}valuer indirectement la performance de cette nouvelle mesure de distance.","year":2015,"title_abstract":"{ROBO}, an edit distance for sentence comparison Application to automatic summarization Dans cet article, nous proposons une mesure de distance entre phrases fond{\\'e}e sur la distance de Levenshtein, doublement pond{\\'e}r{\\'e}e par la fr{\\'e}quence des mots et par le type d{'}op{\\'e}ration r{\\'e}alis{\\'e}e. Nous l{'}{\\'e}valuons au sein d{'}un syst{\\`e}me de r{\\'e}sum{\\'e} automatique dont la m{\\'e}thode de calcul est volontairement limit{\\'e}e {\\`a} une approche fond{\\'e}e sur la similarit{\\'e} entre phrases. Nous sommes donc ainsi en mesure d{'}{\\'e}valuer indirectement la performance de cette nouvelle mesure de distance.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1569882333,"Goal":"Reduced Inequalities","Task":["sentence comparison","automatic summarization"],"Method":["edit distance"]},{"ID":"celikyilmaz-etal-2018-deep","title":"Deep Communicating Agents for Abstractive Summarization","abstract":"We present deep communicating agents in an encoder-decoder architecture to address the challenges of representing a long document for abstractive summarization. With deep communicating agents, the task of encoding a long text is divided across multiple collaborating agents, each in charge of a subsection of the input text. These encoders are connected to a single decoder, trained end-to-end using reinforcement learning to generate a focused and coherent summary. Empirical results demonstrate that multiple communicating encoders lead to a higher quality summary compared to several strong baselines, including those based on a single encoder or multiple non-communicating encoders.","year":2018,"title_abstract":"Deep Communicating Agents for Abstractive Summarization We present deep communicating agents in an encoder-decoder architecture to address the challenges of representing a long document for abstractive summarization. With deep communicating agents, the task of encoding a long text is divided across multiple collaborating agents, each in charge of a subsection of the input text. These encoders are connected to a single decoder, trained end-to-end using reinforcement learning to generate a focused and coherent summary. Empirical results demonstrate that multiple communicating encoders lead to a higher quality summary compared to several strong baselines, including those based on a single encoder or multiple non-communicating encoders.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.156974107,"Goal":"Partnership for the Goals","Task":["Abstractive Summarization","long document","abstractive summarization"],"Method":["Deep Communicating Agents","deep communicating agents","encoder - decoder architecture","deep communicating agents","decoder","reinforcement learning","communicating encoders","encoder","multiple non - communicating encoders"]},{"ID":"kugathasan-sumathipala-2021-neural","title":"Neural Machine Translation for {S}inhala-{E}nglish Code-Mixed Text","abstract":"Code-mixing has become a moving method of communication among multilingual speakers. Most of the social media content of the multilingual societies are written in code-mixed text. However, most of the current translation systems neglect to convert code-mixed texts to a standard language. Most of the user written code-mixed content in social media remains unprocessed due to the unavailability of linguistic resource such as parallel corpus. This paper proposes a Neural Machine Translation(NMT) model to translate the Sinhala-English code-mixed text to the Sinhala language. Due to the limited resources available for Sinhala-English code-mixed(SECM) text, a parallel corpus is created with SECM sentences and Sinhala sentences. Srilankan social media sites contain SECM texts more frequently than the standard languages. The model proposed for code-mixed text translation in this study is a combination of Encoder-Decoder framework with LSTM units and Teachers Forcing Algorithm. The translated sentences from the model are evaluated using BLEU(Bilingual Evaluation Understudy) metric. Our model achieved a remarkable BLEU score for the translation.","year":2021,"title_abstract":"Neural Machine Translation for {S}inhala-{E}nglish Code-Mixed Text Code-mixing has become a moving method of communication among multilingual speakers. Most of the social media content of the multilingual societies are written in code-mixed text. However, most of the current translation systems neglect to convert code-mixed texts to a standard language. Most of the user written code-mixed content in social media remains unprocessed due to the unavailability of linguistic resource such as parallel corpus. This paper proposes a Neural Machine Translation(NMT) model to translate the Sinhala-English code-mixed text to the Sinhala language. Due to the limited resources available for Sinhala-English code-mixed(SECM) text, a parallel corpus is created with SECM sentences and Sinhala sentences. Srilankan social media sites contain SECM texts more frequently than the standard languages. The model proposed for code-mixed text translation in this study is a combination of Encoder-Decoder framework with LSTM units and Teachers Forcing Algorithm. The translated sentences from the model are evaluated using BLEU(Bilingual Evaluation Understudy) metric. Our model achieved a remarkable BLEU score for the translation.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1569392979,"Goal":"Gender Equality","Task":["Neural Machine Translation","Code - mixing","communication","code - mixed text translation","translation"],"Method":["translation systems","Neural Machine Translation(NMT) model","Encoder - Decoder framework","LSTM units","Teachers Forcing Algorithm"]},{"ID":"xu-etal-2019-uds","title":"{U}d{S} Submission for the {WMT} 19 Automatic Post-Editing Task","abstract":"In this paper, we describe our submission to the English-German APE shared task at WMT 2019. We utilize and adapt an NMT architecture originally developed for exploiting context information to APE, implement this in our own transformer model and explore joint training of the APE task with a de-noising encoder.","year":2019,"title_abstract":"{U}d{S} Submission for the {WMT} 19 Automatic Post-Editing Task In this paper, we describe our submission to the English-German APE shared task at WMT 2019. We utilize and adapt an NMT architecture originally developed for exploiting context information to APE, implement this in our own transformer model and explore joint training of the APE task with a de-noising encoder.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1569365263,"Goal":"Gender Equality","Task":["Automatic Post - Editing Task","English - German","APE","APE task"],"Method":["NMT architecture","transformer model","joint training","de - noising encoder"]},{"ID":"gritta-etal-2017-vancouver","title":"{V}ancouver Welcomes You! Minimalist Location Metonymy Resolution","abstract":"Named entities are frequently used in a metonymic manner. They serve as references to related entities such as people and organisations. Accurate identification and interpretation of metonymy can be directly beneficial to various NLP applications, such as Named Entity Recognition and Geographical Parsing. Until now, metonymy resolution (MR) methods mainly relied on parsers, taggers, dictionaries, external word lists and other handcrafted lexical resources. We show how a minimalist neural approach combined with a novel predicate window method can achieve competitive results on the SemEval 2007 task on Metonymy Resolution. Additionally, we contribute with a new Wikipedia-based MR dataset called RelocaR, which is tailored towards locations as well as improving previous deficiencies in annotation guidelines.","year":2017,"title_abstract":"{V}ancouver Welcomes You! Minimalist Location Metonymy Resolution Named entities are frequently used in a metonymic manner. They serve as references to related entities such as people and organisations. Accurate identification and interpretation of metonymy can be directly beneficial to various NLP applications, such as Named Entity Recognition and Geographical Parsing. Until now, metonymy resolution (MR) methods mainly relied on parsers, taggers, dictionaries, external word lists and other handcrafted lexical resources. We show how a minimalist neural approach combined with a novel predicate window method can achieve competitive results on the SemEval 2007 task on Metonymy Resolution. Additionally, we contribute with a new Wikipedia-based MR dataset called RelocaR, which is tailored towards locations as well as improving previous deficiencies in annotation guidelines.","social_need":"Clean Water and Sanitation Ensure availability and sustainable management of water and sanitation for all","cosine_similarity":0.1569085419,"Goal":"Clean Water and Sanitation","Task":["Accurate identification and interpretation of metonymy","NLP applications","Named Entity Recognition","Geographical Parsing","SemEval 2007 task","Metonymy Resolution","annotation"],"Method":["metonymy resolution","parsers","taggers","dictionaries","minimalist neural approach","predicate window method"]},{"ID":"hlaing-etal-2021-nectecs","title":"{NECTEC}{'}s Participation in {WAT}-2021","abstract":"In this paper, we report the experimental results of Machine Translation models conducted by a NECTEC team for the translation tasks of WAT-2021. Basically, our models are based on neural methods for both directions of English-Myanmar and Myanmar-English language pairs. Most of the existing Neural Machine Translation (NMT) models mainly focus on the conversion of sequential data and do not directly use syntactic information. However, we conduct multi-source neural machine translation (NMT) models using the multilingual corpora such as string data corpus, tree data corpus, or POS-tagged data corpus. The multi-source translation is an approach to exploit multiple inputs (e.g. in two different formats) to increase translation accuracy. The RNN-based encoder-decoder model with attention mechanism and transformer architectures have been carried out for our experiment. The experimental results showed that the proposed models of RNN-based architecture outperform the baseline model for English-to-Myanmar translation task, and the multi-source and shared-multi-source transformer models yield better translation results than the baseline.","year":2021,"title_abstract":"{NECTEC}{'}s Participation in {WAT}-2021 In this paper, we report the experimental results of Machine Translation models conducted by a NECTEC team for the translation tasks of WAT-2021. Basically, our models are based on neural methods for both directions of English-Myanmar and Myanmar-English language pairs. Most of the existing Neural Machine Translation (NMT) models mainly focus on the conversion of sequential data and do not directly use syntactic information. However, we conduct multi-source neural machine translation (NMT) models using the multilingual corpora such as string data corpus, tree data corpus, or POS-tagged data corpus. The multi-source translation is an approach to exploit multiple inputs (e.g. in two different formats) to increase translation accuracy. The RNN-based encoder-decoder model with attention mechanism and transformer architectures have been carried out for our experiment. The experimental results showed that the proposed models of RNN-based architecture outperform the baseline model for English-to-Myanmar translation task, and the multi-source and shared-multi-source transformer models yield better translation results than the baseline.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1568948328,"Goal":"Gender Equality","Task":["translation tasks","WAT - 2021","Neural Machine Translation","multi - source translation","translation","English - to - Myanmar translation task","translation"],"Method":["Machine Translation models","NECTEC team","neural methods","multi - source neural machine translation (NMT) models","RNN - based encoder - decoder model","attention mechanism","transformer architectures","RNN - based architecture","multi - source and shared - multi - source transformer models"]},{"ID":"wang-etal-2017-winning","title":"Winning on the Merits: The Joint Effects of Content and Style on Debate Outcomes","abstract":"Debate and deliberation play essential roles in politics and government, but most models presume that debates are won mainly via superior style or agenda control. Ideally, however, debates would be won on the merits, as a function of which side has the stronger arguments. We propose a predictive model of debate that estimates the effects of linguistic features and the latent persuasive strengths of different topics, as well as the interactions between the two. Using a dataset of 118 Oxford-style debates, our model{'}s combination of content (as latent topics) and style (as linguistic features) allows us to predict audience-adjudicated winners with 74{\\%} accuracy, significantly outperforming linguistic features alone (66{\\%}). Our model finds that winning sides employ stronger arguments, and allows us to identify the linguistic features associated with strong or weak arguments.","year":2017,"title_abstract":"Winning on the Merits: The Joint Effects of Content and Style on Debate Outcomes Debate and deliberation play essential roles in politics and government, but most models presume that debates are won mainly via superior style or agenda control. Ideally, however, debates would be won on the merits, as a function of which side has the stronger arguments. We propose a predictive model of debate that estimates the effects of linguistic features and the latent persuasive strengths of different topics, as well as the interactions between the two. Using a dataset of 118 Oxford-style debates, our model{'}s combination of content (as latent topics) and style (as linguistic features) allows us to predict audience-adjudicated winners with 74{\\%} accuracy, significantly outperforming linguistic features alone (66{\\%}). Our model finds that winning sides employ stronger arguments, and allows us to identify the linguistic features associated with strong or weak arguments.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1568561643,"Goal":"Reduced Inequalities","Task":["Debate","government"],"Method":["predictive model of debate"]},{"ID":"jiang-etal-2018-interpretable","title":"Interpretable Rationale Augmented Charge Prediction System","abstract":"This paper proposes a neural based system to solve the essential interpretability problem existing in text classification, especially in charge prediction task. First, we use a deep reinforcement learning method to extract rationales which mean short, readable and decisive snippets from input text. Then a rationale augmented classification model is proposed to elevate the prediction accuracy. Naturally, the extracted rationales serve as the introspection explanation for the prediction result of the model, enhancing the transparency of the model. Experimental results demonstrate that our system is able to extract readable rationales in a high consistency with manual annotation and is comparable with the attention model in prediction accuracy.","year":2018,"title_abstract":"Interpretable Rationale Augmented Charge Prediction System This paper proposes a neural based system to solve the essential interpretability problem existing in text classification, especially in charge prediction task. First, we use a deep reinforcement learning method to extract rationales which mean short, readable and decisive snippets from input text. Then a rationale augmented classification model is proposed to elevate the prediction accuracy. Naturally, the extracted rationales serve as the introspection explanation for the prediction result of the model, enhancing the transparency of the model. Experimental results demonstrate that our system is able to extract readable rationales in a high consistency with manual annotation and is comparable with the attention model in prediction accuracy.","social_need":"Affordable and Clean Energy Ensure access to affordable, reliable, sustainable and modern energy for all","cosine_similarity":0.1567354798,"Goal":"Affordable and Clean Energy","Task":["interpretability problem","text classification","charge prediction task"],"Method":["Interpretable Rationale Augmented Charge Prediction System","neural based system","deep reinforcement learning method","rationale augmented classification model","attention model"]},{"ID":"jayaram-allaway-2021-human","title":"Human Rationales as Attribution Priors for Explainable Stance Detection","abstract":"As NLP systems become better at detecting opinions and beliefs from text, it is important to ensure not only that models are accurate but also that they arrive at their predictions in ways that align with human reasoning. In this work, we present a method for imparting human-like rationalization to a stance detection model using crowdsourced annotations on a small fraction of the training data. We show that in a data-scarce setting, our approach can improve the reasoning of a state-of-the-art classifier{---}particularly for inputs containing challenging phenomena such as sarcasm{---}at no cost in predictive performance. Furthermore, we demonstrate that attention weights surpass a leading attribution method in providing faithful explanations of our model{'}s predictions, thus serving as a computationally cheap and reliable source of attributions for our model.","year":2021,"title_abstract":"Human Rationales as Attribution Priors for Explainable Stance Detection As NLP systems become better at detecting opinions and beliefs from text, it is important to ensure not only that models are accurate but also that they arrive at their predictions in ways that align with human reasoning. In this work, we present a method for imparting human-like rationalization to a stance detection model using crowdsourced annotations on a small fraction of the training data. We show that in a data-scarce setting, our approach can improve the reasoning of a state-of-the-art classifier{---}particularly for inputs containing challenging phenomena such as sarcasm{---}at no cost in predictive performance. Furthermore, we demonstrate that attention weights surpass a leading attribution method in providing faithful explanations of our model{'}s predictions, thus serving as a computationally cheap and reliable source of attributions for our model.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1567285359,"Goal":"Climate Action","Task":["Explainable Stance Detection","detecting opinions and beliefs","data - scarce setting","attributions"],"Method":["Attribution Priors","NLP systems","human reasoning","stance detection model","classifier{ - - - }particularly","attribution method"]},{"ID":"shin-doyle-2018-alignment","title":"Alignment, Acceptance, and Rejection of Group Identities in Online Political Discourse","abstract":"Conversation is a joint social process, with participants cooperating to exchange information. This process is helped along through linguistic alignment: participants{'} adoption of each other{'}s word use. This alignment is robust, appearing many settings, and is nearly always positive. We create an alignment model for examining alignment in Twitter conversations across antagonistic groups. This model finds that some word categories, specifically pronouns used to establish group identity and common ground, are negatively aligned. This negative alignment is observed despite other categories, which are less related to the group dynamics, showing the standard positive alignment. This suggests that alignment is strongly biased toward cooperative alignment, but that different linguistic features can show substantially different behaviors.","year":2018,"title_abstract":"Alignment, Acceptance, and Rejection of Group Identities in Online Political Discourse Conversation is a joint social process, with participants cooperating to exchange information. This process is helped along through linguistic alignment: participants{'} adoption of each other{'}s word use. This alignment is robust, appearing many settings, and is nearly always positive. We create an alignment model for examining alignment in Twitter conversations across antagonistic groups. This model finds that some word categories, specifically pronouns used to establish group identity and common ground, are negatively aligned. This negative alignment is observed despite other categories, which are less related to the group dynamics, showing the standard positive alignment. This suggests that alignment is strongly biased toward cooperative alignment, but that different linguistic features can show substantially different behaviors.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.1567254663,"Goal":"Peace, Justice and Strong Institutions","Task":["Alignment","Online Political Discourse Conversation","joint social process","linguistic alignment","alignment","Twitter conversations","alignment","cooperative alignment"],"Method":["alignment model"]},{"ID":"hashempour-2019-deep","title":"A Deep Learning Approach to Language-independent Gender Prediction on {T}witter","abstract":"This work presents a set of experiments conducted to predict the gender of Twitter users based on language-independent features extracted from the text of the users{'} tweets. The experiments were performed on a version of TwiSty dataset including tweets written by the users of six different languages: Portuguese, French, Dutch, English, German, and Italian. Logistic regression (LR), and feed-forward neural networks (FFNN) with back-propagation were used to build models in two different settings: Inter-Lingual (IL) and Cross-Lingual (CL). In the IL setting, the training and testing were performed on the same language whereas in the CL, Italian and German datasets were set aside and only used as test sets and the rest were combined to compose training and development sets. In the IL, the highest accuracy score belongs to LR whereas, in the CL, FFNN with three hidden layers yields the highest score. The results show that neural network based models underperform traditional models when the size of the training set is small; however, they beat traditional models by a non-trivial margin, when they are fed with large enough data. Finally, the feature analysis confirms that men and women have different writing styles independent of their language.","year":2019,"title_abstract":"A Deep Learning Approach to Language-independent Gender Prediction on {T}witter This work presents a set of experiments conducted to predict the gender of Twitter users based on language-independent features extracted from the text of the users{'} tweets. The experiments were performed on a version of TwiSty dataset including tweets written by the users of six different languages: Portuguese, French, Dutch, English, German, and Italian. Logistic regression (LR), and feed-forward neural networks (FFNN) with back-propagation were used to build models in two different settings: Inter-Lingual (IL) and Cross-Lingual (CL). In the IL setting, the training and testing were performed on the same language whereas in the CL, Italian and German datasets were set aside and only used as test sets and the rest were combined to compose training and development sets. In the IL, the highest accuracy score belongs to LR whereas, in the CL, FFNN with three hidden layers yields the highest score. The results show that neural network based models underperform traditional models when the size of the training set is small; however, they beat traditional models by a non-trivial margin, when they are fed with large enough data. Finally, the feature analysis confirms that men and women have different writing styles independent of their language.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1566833556,"Goal":"Gender Equality","Task":["Language - independent Gender Prediction","IL setting"],"Method":["Deep Learning Approach","Logistic regression","feed - forward neural networks","back - propagation","IL","LR","CL","FFNN","neural network based models","feature analysis"]},{"ID":"wang-etal-2020-evidenceminer","title":"{EVIDENCEMINER}: Textual Evidence Discovery for Life Sciences","abstract":"Traditional search engines for life sciences (e.g., PubMed) are designed for document retrieval and do not allow direct retrieval of specific statements. Some of these statements may serve as textual evidence that is key to tasks such as hypothesis generation and new finding validation. We present EVIDENCEMINER, a web-based system that lets users query a natural language statement and automatically retrieves textual evidence from a background corpora for life sciences. EVIDENCEMINER is constructed in a completely automated way without any human effort for training data annotation. It is supported by novel data-driven methods for distantly supervised named entity recognition and open information extraction. The entities and patterns are pre-computed and indexed offline to support fast online evidence retrieval. The annotation results are also highlighted in the original document for better visualization. EVIDENCEMINER also includes analytic functionalities such as the most frequent entity and relation summarization. EVIDENCEMINER can help scientists uncover important research issues, leading to more effective research and more in-depth quantitative analysis. The system of EVIDENCEMINER is available at https:\/\/evidenceminer.firebaseapp.com\/.","year":2020,"title_abstract":"{EVIDENCEMINER}: Textual Evidence Discovery for Life Sciences Traditional search engines for life sciences (e.g., PubMed) are designed for document retrieval and do not allow direct retrieval of specific statements. Some of these statements may serve as textual evidence that is key to tasks such as hypothesis generation and new finding validation. We present EVIDENCEMINER, a web-based system that lets users query a natural language statement and automatically retrieves textual evidence from a background corpora for life sciences. EVIDENCEMINER is constructed in a completely automated way without any human effort for training data annotation. It is supported by novel data-driven methods for distantly supervised named entity recognition and open information extraction. The entities and patterns are pre-computed and indexed offline to support fast online evidence retrieval. The annotation results are also highlighted in the original document for better visualization. EVIDENCEMINER also includes analytic functionalities such as the most frequent entity and relation summarization. EVIDENCEMINER can help scientists uncover important research issues, leading to more effective research and more in-depth quantitative analysis. The system of EVIDENCEMINER is available at https:\/\/evidenceminer.firebaseapp.com\/.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1565950215,"Goal":"Life on Land","Task":["Textual Evidence Discovery","Life Sciences","document retrieval","direct retrieval of specific statements","hypothesis generation","new finding validation","training data annotation","distantly supervised named entity recognition","open information extraction","online evidence retrieval","summarization","quantitative analysis"],"Method":["search engines","EVIDENCEMINER","web - based system","EVIDENCEMINER","data - driven methods","EVIDENCEMINER","analytic functionalities","EVIDENCEMINER","EVIDENCEMINER"]},{"ID":"dominguez-etal-2020-themepro","title":"{T}heme{P}ro: A Toolkit for the Analysis of Thematic Progression","abstract":"This paper introduces ThemePro, a toolkit for the automatic analysis of thematic progression. Thematic progression is relevant to natural language processing (NLP) applications dealing, among others, with discourse structure, argumentation structure, natural language generation, summarization and topic detection. A web platform demonstrates the potential of this toolkit and provides a visualization of the results including syntactic trees, hierarchical thematicity over propositions and thematic progression over whole texts.","year":2020,"title_abstract":"{T}heme{P}ro: A Toolkit for the Analysis of Thematic Progression This paper introduces ThemePro, a toolkit for the automatic analysis of thematic progression. Thematic progression is relevant to natural language processing (NLP) applications dealing, among others, with discourse structure, argumentation structure, natural language generation, summarization and topic detection. A web platform demonstrates the potential of this toolkit and provides a visualization of the results including syntactic trees, hierarchical thematicity over propositions and thematic progression over whole texts.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1565878242,"Goal":"Partnership for the Goals","Task":["Analysis of Thematic Progression","automatic analysis of thematic progression","Thematic progression","natural language processing","natural language generation","summarization","topic detection"],"Method":["ThemePro","web platform"]},{"ID":"soraluze-etal-2017-enriching","title":"Enriching {B}asque Coreference Resolution System using Semantic Knowledge sources","abstract":"In this paper we present a Basque coreference resolution system enriched with semantic knowledge. An error analysis carried out revealed the deficiencies that the system had in resolving coreference cases in which semantic or world knowledge is needed. We attempt to improve the deficiencies using two semantic knowledge sources, specifically Wikipedia and WordNet.","year":2017,"title_abstract":"Enriching {B}asque Coreference Resolution System using Semantic Knowledge sources In this paper we present a Basque coreference resolution system enriched with semantic knowledge. An error analysis carried out revealed the deficiencies that the system had in resolving coreference cases in which semantic or world knowledge is needed. We attempt to improve the deficiencies using two semantic knowledge sources, specifically Wikipedia and WordNet.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.1565793604,"Goal":"Life Below Water","Task":["error analysis","resolving coreference cases"],"Method":["{B}asque Coreference Resolution System","Semantic Knowledge sources","Basque coreference resolution system"]},{"ID":"jimenez-gutierrez-etal-2020-document","title":"Document Classification for {COVID}-19 Literature","abstract":"The global pandemic has made it more important than ever to quickly and accurately retrieve relevant scientific literature for effective consumption by researchers in a wide range of fields. We provide an analysis of several multi-label document classification models on the LitCovid dataset, a growing collection of 23,000 research papers regarding the novel 2019 coronavirus. We find that pre-trained language models fine-tuned on this dataset outperform all other baselines and that BioBERT surpasses the others by a small margin with micro-F1 and accuracy scores of around 86{\\%} and 75{\\%} respectively on the test set. We evaluate the data efficiency and generalizability of these models as essential features of any system prepared to deal with an urgent situation like the current health crisis. We perform a data ablation study to determine how important article titles are for achieving reasonable performance on this dataset. Finally, we explore 50 errors made by the best performing models on LitCovid documents and find that they often (1) correlate certain labels too closely together and (2) fail to focus on discriminative sections of the articles; both of which are important issues to address in future work. Both data and code are available on GitHub.","year":2020,"title_abstract":"Document Classification for {COVID}-19 Literature The global pandemic has made it more important than ever to quickly and accurately retrieve relevant scientific literature for effective consumption by researchers in a wide range of fields. We provide an analysis of several multi-label document classification models on the LitCovid dataset, a growing collection of 23,000 research papers regarding the novel 2019 coronavirus. We find that pre-trained language models fine-tuned on this dataset outperform all other baselines and that BioBERT surpasses the others by a small margin with micro-F1 and accuracy scores of around 86{\\%} and 75{\\%} respectively on the test set. We evaluate the data efficiency and generalizability of these models as essential features of any system prepared to deal with an urgent situation like the current health crisis. We perform a data ablation study to determine how important article titles are for achieving reasonable performance on this dataset. Finally, we explore 50 errors made by the best performing models on LitCovid documents and find that they often (1) correlate certain labels too closely together and (2) fail to focus on discriminative sections of the articles; both of which are important issues to address in future work. Both data and code are available on GitHub.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1565789282,"Goal":"Climate Action","Task":["Document Classification","data ablation study"],"Method":["multi - label document classification models","language models","BioBERT"]},{"ID":"friedrich-etal-2021-debie","title":"{D}eb{IE}: A Platform for Implicit and Explicit Debiasing of Word Embedding Spaces","abstract":"Recent research efforts in NLP have demonstrated that distributional word vector spaces often encode stereotypical human biases, such as racism and sexism. With word representations ubiquitously used in NLP models and pipelines, this raises ethical issues and jeopardizes the fairness of language technologies. While there exists a large body of work on bias measures and debiasing methods, to date, there is no platform that would unify these research efforts and make bias measuring and debiasing of representation spaces widely accessible. In this work, we present DebIE, the first integrated platform for (1) measuring and (2) mitigating bias in word embeddings. Given an (i) embedding space (users can choose between the predefined spaces or upload their own) and (ii) a bias specification (users can choose between existing bias specifications or create their own), DebIE can (1) compute several measures of implicit and explicit bias and modify the embedding space by executing two (mutually composable) debiasing models. DebIE{'}s functionality can be accessed through four different interfaces: (a) a web application, (b) a desktop application, (c) a REST-ful API, and (d) as a command-line application. DebIE is available at: debie.informatik.uni-mannheim.de.","year":2021,"title_abstract":"{D}eb{IE}: A Platform for Implicit and Explicit Debiasing of Word Embedding Spaces Recent research efforts in NLP have demonstrated that distributional word vector spaces often encode stereotypical human biases, such as racism and sexism. With word representations ubiquitously used in NLP models and pipelines, this raises ethical issues and jeopardizes the fairness of language technologies. While there exists a large body of work on bias measures and debiasing methods, to date, there is no platform that would unify these research efforts and make bias measuring and debiasing of representation spaces widely accessible. In this work, we present DebIE, the first integrated platform for (1) measuring and (2) mitigating bias in word embeddings. Given an (i) embedding space (users can choose between the predefined spaces or upload their own) and (ii) a bias specification (users can choose between existing bias specifications or create their own), DebIE can (1) compute several measures of implicit and explicit bias and modify the embedding space by executing two (mutually composable) debiasing models. DebIE{'}s functionality can be accessed through four different interfaces: (a) a web application, (b) a desktop application, (c) a REST-ful API, and (d) as a command-line application. DebIE is available at: debie.informatik.uni-mannheim.de.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1565464437,"Goal":"Gender Equality","Task":["Implicit and Explicit Debiasing of Word Embedding Spaces","NLP","bias measures","bias measuring","debiasing of representation spaces","(1) measuring and (2) mitigating bias in word embeddings"],"Method":["word representations","NLP models","pipelines","language technologies","debiasing methods","DebIE","DebIE","composable) debiasing models","DebIE{'}s functionality","web application","desktop application","REST - ful API","command - line application","DebIE"]},{"ID":"panchenko-etal-2019-categorizing","title":"Categorizing Comparative Sentences","abstract":"We tackle the tasks of automatically identifying comparative sentences and categorizing the intended preference (e.g., {``}Python has better NLP libraries than MATLAB{''} \u2192 Python, better, MATLAB). To this end, we manually annotate 7,199 sentences for 217 distinct target item pairs from several domains (27{\\%} of the sentences contain an oriented comparison in the sense of {``}better{''} or {``}worse{''}). A gradient boosting model based on pre-trained sentence embeddings reaches an F1 score of 85{\\%} in our experimental evaluation. The model can be used to extract comparative sentences for pro\/con argumentation in comparative \/ argument search engines or debating technologies.","year":2019,"title_abstract":"Categorizing Comparative Sentences We tackle the tasks of automatically identifying comparative sentences and categorizing the intended preference (e.g., {``}Python has better NLP libraries than MATLAB{''} \u2192 Python, better, MATLAB). To this end, we manually annotate 7,199 sentences for 217 distinct target item pairs from several domains (27{\\%} of the sentences contain an oriented comparison in the sense of {``}better{''} or {``}worse{''}). A gradient boosting model based on pre-trained sentence embeddings reaches an F1 score of 85{\\%} in our experimental evaluation. The model can be used to extract comparative sentences for pro\/con argumentation in comparative \/ argument search engines or debating technologies.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1564559639,"Goal":"Reduced Inequalities","Task":["Categorizing Comparative Sentences","automatically identifying comparative sentences","argumentation","comparative \/ argument search engines","debating technologies"],"Method":["gradient boosting model","sentence embeddings"]},{"ID":"trushkina-etal-2008-sentence","title":"Sentence Alignment in {DPC}: Maximizing Precision, Minimizing Human Effort","abstract":"A wide spectrum of multilingual applications have aligned parallel corpora as their prerequisite. The aim of the project described in this paper is to build a multilingual corpus where all sentences are aligned at very high precision with a minimal human effort involved. The experiments on a combination of sentence aligners with different underlying algorithms described in this paper showed that by verifying only those links which were not recognized by at least two aligners, an error rate can be reduced by 93.76{\\%} as compared to the performance of the best aligner. Such manual involvement concerned only a small portion of all data (6{\\%}). This significantly reduces a load of manual work necessary to achieve nearly 100{\\%} accuracy of alignment.","year":2008,"title_abstract":"Sentence Alignment in {DPC}: Maximizing Precision, Minimizing Human Effort A wide spectrum of multilingual applications have aligned parallel corpora as their prerequisite. The aim of the project described in this paper is to build a multilingual corpus where all sentences are aligned at very high precision with a minimal human effort involved. The experiments on a combination of sentence aligners with different underlying algorithms described in this paper showed that by verifying only those links which were not recognized by at least two aligners, an error rate can be reduced by 93.76{\\%} as compared to the performance of the best aligner. Such manual involvement concerned only a small portion of all data (6{\\%}). This significantly reduces a load of manual work necessary to achieve nearly 100{\\%} accuracy of alignment.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1564278156,"Goal":"Gender Equality","Task":["Sentence Alignment","Minimizing Human Effort","multilingual applications","alignment"],"Method":["{DPC}","aligners","aligner"]},{"ID":"r-l-m-2020-nitk","title":"{NITK} {NLP} at {F}in{C}ausal-2020 Task 1 Using {BERT} and Linear models.","abstract":"FinCausal-2020 is the shared task which focuses on the causality detection of factual data for financial analysis. The financial data facts don{'}t provide much explanation on the variability of these data. This paper aims to propose an efficient method to classify the data into one which is having any financial cause or not. Many models were used to classify the data, out of which SVM model gave an F-Score of 0.9435, BERT with specific fine-tuning achieved best results with F-Score of 0.9677.","year":2020,"title_abstract":"{NITK} {NLP} at {F}in{C}ausal-2020 Task 1 Using {BERT} and Linear models. FinCausal-2020 is the shared task which focuses on the causality detection of factual data for financial analysis. The financial data facts don{'}t provide much explanation on the variability of these data. This paper aims to propose an efficient method to classify the data into one which is having any financial cause or not. Many models were used to classify the data, out of which SVM model gave an F-Score of 0.9435, BERT with specific fine-tuning achieved best results with F-Score of 0.9677.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1564166546,"Goal":"Climate Action","Task":["causality detection of factual data","financial analysis"],"Method":["{BERT} and Linear models","FinCausal - 2020","SVM model","fine - tuning"]},{"ID":"bicici-2021-rtm","title":"{RTM} Super Learner Results at Quality Estimation Task","abstract":"We obtain new results using referential translation machines (RTMs) with predictions mixed to obtain a better mixture of experts prediction. Our super learner results improve the results and provide a robust combination model.","year":2021,"title_abstract":"{RTM} Super Learner Results at Quality Estimation Task We obtain new results using referential translation machines (RTMs) with predictions mixed to obtain a better mixture of experts prediction. Our super learner results improve the results and provide a robust combination model.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1564155817,"Goal":"Quality Education","Task":["Quality Estimation Task"],"Method":["Super Learner","referential translation machines","mixture of experts prediction","super learner","combination model"]},{"ID":"park-jeoung-2022-raison","title":"Raison d{'}{\\^e}tre of the benchmark dataset: A Survey of Current Practices of Benchmark Dataset Sharing Platforms","abstract":"This paper critically examines the current practices of benchmark dataset sharing in NLP and suggests a better way to inform reusers of the benchmark dataset. As the dataset sharing platform plays a key role not only in distributing the dataset but also in informing the potential reusers about the dataset, we believe data-sharing platforms should provide a comprehensive context of the datasets. We survey four benchmark dataset sharing platforms: HuggingFace, PaperswithCode, Tensorflow, and Pytorch to diagnose the current practices of how the dataset is shared which metadata is shared and omitted. To be specific, drawing on the concept of data curation which considers the future reuse when the data is made public, we advance the direction that benchmark dataset sharing platforms should take into consideration. We identify that four benchmark platforms have different practices of using metadata and there is a lack of consensus on what social impact metadata is. We believe the problem of missing a discussion around social impact in the dataset sharing platforms has to do with the failed agreement on who should be in charge. We propose that the benchmark dataset should develop social impact metadata and data curator should take a role in managing the social impact metadata.","year":2022,"title_abstract":"Raison d{'}{\\^e}tre of the benchmark dataset: A Survey of Current Practices of Benchmark Dataset Sharing Platforms This paper critically examines the current practices of benchmark dataset sharing in NLP and suggests a better way to inform reusers of the benchmark dataset. As the dataset sharing platform plays a key role not only in distributing the dataset but also in informing the potential reusers about the dataset, we believe data-sharing platforms should provide a comprehensive context of the datasets. We survey four benchmark dataset sharing platforms: HuggingFace, PaperswithCode, Tensorflow, and Pytorch to diagnose the current practices of how the dataset is shared which metadata is shared and omitted. To be specific, drawing on the concept of data curation which considers the future reuse when the data is made public, we advance the direction that benchmark dataset sharing platforms should take into consideration. We identify that four benchmark platforms have different practices of using metadata and there is a lack of consensus on what social impact metadata is. We believe the problem of missing a discussion around social impact in the dataset sharing platforms has to do with the failed agreement on who should be in charge. We propose that the benchmark dataset should develop social impact metadata and data curator should take a role in managing the social impact metadata.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1563733965,"Goal":"Partnership for the Goals","Task":["benchmark dataset sharing","NLP","data curation","dataset sharing platforms","social impact metadata"],"Method":["Benchmark Dataset Sharing Platforms","dataset sharing platform","data - sharing platforms","HuggingFace","Tensorflow","Pytorch","data curator"]},{"ID":"rouvier-2017-lia","title":"{LIA} at {S}em{E}val-2017 Task 4: An Ensemble of Neural Networks for Sentiment Classification","abstract":"This paper describes the system developed at LIA for the SemEval-2017 evaluation campaign. The goal of Task 4.A was to identify sentiment polarity in tweets. The system is an ensemble of Deep Neural Network (DNN) models: Convolutional Neural Network (CNN) and Recurrent Neural Network Long Short-Term Memory (RNN-LSTM). We initialize the input representation of DNN with different sets of embeddings trained on large datasets. The ensemble of DNNs are combined using a score-level fusion approach. The system ranked 2nd at SemEval-2017 and obtained an average recall of 67.6{\\%}.","year":2017,"title_abstract":"{LIA} at {S}em{E}val-2017 Task 4: An Ensemble of Neural Networks for Sentiment Classification This paper describes the system developed at LIA for the SemEval-2017 evaluation campaign. The goal of Task 4.A was to identify sentiment polarity in tweets. The system is an ensemble of Deep Neural Network (DNN) models: Convolutional Neural Network (CNN) and Recurrent Neural Network Long Short-Term Memory (RNN-LSTM). We initialize the input representation of DNN with different sets of embeddings trained on large datasets. The ensemble of DNNs are combined using a score-level fusion approach. The system ranked 2nd at SemEval-2017 and obtained an average recall of 67.6{\\%}.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.1562845409,"Goal":"Peace, Justice and Strong Institutions","Task":["Sentiment Classification","sentiment polarity"],"Method":["Ensemble of Neural Networks","ensemble of Deep Neural Network (DNN) models","Convolutional Neural Network","Recurrent Neural Network Long Short - Term Memory (RNN - LSTM)","input representation","DNN","ensemble of DNNs","score - level fusion approach"]},{"ID":"chen-etal-2019-learning-link","title":"Learning to Link Grammar and Encyclopedic Information of Assist {ESL} Learners","abstract":"We introduce a system aimed at improving and expanding second language learners{'} English vocabulary. In addition to word definitions, we provide rich lexical information such as collocations and grammar patterns for target words. We present Linggle Booster that takes an article, identifies target vocabulary, provides lexical information, and generates a quiz on target words. Linggle Booster also links named-entity to corresponding Wikipedia pages. Evaluation on a set of target words shows that the method have reasonably good performance in terms of generating useful and information for learning vocabulary.","year":2019,"title_abstract":"Learning to Link Grammar and Encyclopedic Information of Assist {ESL} Learners We introduce a system aimed at improving and expanding second language learners{'} English vocabulary. In addition to word definitions, we provide rich lexical information such as collocations and grammar patterns for target words. We present Linggle Booster that takes an article, identifies target vocabulary, provides lexical information, and generates a quiz on target words. Linggle Booster also links named-entity to corresponding Wikipedia pages. Evaluation on a set of target words shows that the method have reasonably good performance in terms of generating useful and information for learning vocabulary.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1562570781,"Goal":"Quality Education","Task":["second language learners{'} English vocabulary","learning vocabulary"],"Method":["Linggle Booster","Linggle Booster"]},{"ID":"kordjamshidi-etal-2020-representation","title":"Representation, Learning and Reasoning on Spatial Language for Downstream {NLP} Tasks","abstract":"Understating spatial semantics expressed in natural language can become highly complex in real-world applications. This includes applications of language grounding, navigation, visual question answering, and more generic human-machine interaction and dialogue systems. In many of such downstream tasks, explicit representation of spatial concepts and relationships can improve the capabilities of machine learning models in reasoning and deep language understanding. In this tutorial, we overview the cutting-edge research results and existing challenges related to spatial language understanding including semantic annotations, existing corpora, symbolic and sub-symbolic representations, qualitative spatial reasoning, spatial common sense, deep and structured learning models. We discuss the recent results on the above-mentioned applications {--}that need spatial language learning and reasoning {--} and highlight the research gaps and future directions.","year":2020,"title_abstract":"Representation, Learning and Reasoning on Spatial Language for Downstream {NLP} Tasks Understating spatial semantics expressed in natural language can become highly complex in real-world applications. This includes applications of language grounding, navigation, visual question answering, and more generic human-machine interaction and dialogue systems. In many of such downstream tasks, explicit representation of spatial concepts and relationships can improve the capabilities of machine learning models in reasoning and deep language understanding. In this tutorial, we overview the cutting-edge research results and existing challenges related to spatial language understanding including semantic annotations, existing corpora, symbolic and sub-symbolic representations, qualitative spatial reasoning, spatial common sense, deep and structured learning models. We discuss the recent results on the above-mentioned applications {--}that need spatial language learning and reasoning {--} and highlight the research gaps and future directions.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1562539786,"Goal":"Sustainable Cities and Communities","Task":["Representation","Learning and Reasoning","Spatial Language","Downstream {NLP} Tasks","Understating spatial semantics","real - world applications","language grounding","navigation","visual question answering","human - machine interaction","dialogue systems","downstream tasks","explicit representation of spatial concepts","reasoning","deep language understanding","spatial language understanding","spatial language learning","reasoning"],"Method":["machine learning models","symbolic and sub - symbolic representations","qualitative spatial reasoning","spatial common sense","deep and structured learning models"]},{"ID":"rashkin-etal-2018-event2mind","title":"{E}vent2{M}ind: Commonsense Inference on Events, Intents, and Reactions","abstract":"We investigate a new commonsense inference task: given an event described in a short free-form text ({``}X drinks coffee in the morning{''}), a system reasons about the likely intents ({``}X wants to stay awake{''}) and reactions ({``}X feels alert{''}) of the event{'}s participants. To support this study, we construct a new crowdsourced corpus of 25,000 event phrases covering a diverse range of everyday events and situations. We report baseline performance on this task, demonstrating that neural encoder-decoder models can successfully compose embedding representations of previously unseen events and reason about the likely intents and reactions of the event participants. In addition, we demonstrate how commonsense inference on people{'}s intents and reactions can help unveil the implicit gender inequality prevalent in modern movie scripts.","year":2018,"title_abstract":"{E}vent2{M}ind: Commonsense Inference on Events, Intents, and Reactions We investigate a new commonsense inference task: given an event described in a short free-form text ({``}X drinks coffee in the morning{''}), a system reasons about the likely intents ({``}X wants to stay awake{''}) and reactions ({``}X feels alert{''}) of the event{'}s participants. To support this study, we construct a new crowdsourced corpus of 25,000 event phrases covering a diverse range of everyday events and situations. We report baseline performance on this task, demonstrating that neural encoder-decoder models can successfully compose embedding representations of previously unseen events and reason about the likely intents and reactions of the event participants. In addition, we demonstrate how commonsense inference on people{'}s intents and reactions can help unveil the implicit gender inequality prevalent in modern movie scripts.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1562070251,"Goal":"Gender Equality","Task":["Commonsense Inference","commonsense inference task"],"Method":["encoder - decoder models","embedding representations","commonsense inference"]},{"ID":"hamdan-2017-senti17","title":"{S}enti17 at {S}em{E}val-2017 Task 4: Ten Convolutional Neural Network Voters for Tweet Polarity Classification","abstract":"This paper presents Senti17 system which uses ten convolutional neural networks (ConvNet) to assign a sentiment label to a tweet. The network consists of a convolutional layer followed by a fully-connected layer and a Softmax on top. Ten instances of this network are initialized with the same word embeddings as inputs but with different initializations for the network weights. We combine the results of all instances by selecting the sentiment label given by the majority of the ten voters. This system is ranked fourth in SemEval-2017 Task4 over 38 systems with 67.4{\\%} average recall.","year":2017,"title_abstract":"{S}enti17 at {S}em{E}val-2017 Task 4: Ten Convolutional Neural Network Voters for Tweet Polarity Classification This paper presents Senti17 system which uses ten convolutional neural networks (ConvNet) to assign a sentiment label to a tweet. The network consists of a convolutional layer followed by a fully-connected layer and a Softmax on top. Ten instances of this network are initialized with the same word embeddings as inputs but with different initializations for the network weights. We combine the results of all instances by selecting the sentiment label given by the majority of the ten voters. This system is ranked fourth in SemEval-2017 Task4 over 38 systems with 67.4{\\%} average recall.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.1561452448,"Goal":"Peace, Justice and Strong Institutions","Task":["Tweet Polarity Classification"],"Method":["Convolutional Neural Network Voters","Senti17 system","convolutional neural networks","convolutional layer","fully - connected layer","Softmax"]},{"ID":"stranisci-etal-2016-annotating","title":"Annotating Sentiment and Irony in the Online {I}talian Political Debate on {\\#}labuonascuola","abstract":"In this paper we present the TWitterBuonaScuola corpus (TW-BS), a novel Italian linguistic resource for Sentiment Analysis, developed with the main aim of analyzing the online debate on the controversial Italian political reform {``}Buona Scuola{''} (Good school), aimed at reorganizing the national educational and training systems. We describe the methodologies applied in the collection and annotation of data. The collection has been driven by the detection of the hashtags mainly used by the participants to the debate, while the annotation has been focused on sentiment polarity and irony, but also extended to mark the aspects of the reform that were mainly discussed in the debate. An in-depth study of the disagreement among annotators is included. We describe the collection and annotation stages, and the in-depth analysis of disagreement made with Crowdflower, a crowdsourcing annotation platform.","year":2016,"title_abstract":"Annotating Sentiment and Irony in the Online {I}talian Political Debate on {\\#}labuonascuola In this paper we present the TWitterBuonaScuola corpus (TW-BS), a novel Italian linguistic resource for Sentiment Analysis, developed with the main aim of analyzing the online debate on the controversial Italian political reform {``}Buona Scuola{''} (Good school), aimed at reorganizing the national educational and training systems. We describe the methodologies applied in the collection and annotation of data. The collection has been driven by the detection of the hashtags mainly used by the participants to the debate, while the annotation has been focused on sentiment polarity and irony, but also extended to mark the aspects of the reform that were mainly discussed in the debate. An in-depth study of the disagreement among annotators is included. We describe the collection and annotation stages, and the in-depth analysis of disagreement made with Crowdflower, a crowdsourcing annotation platform.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1560361385,"Goal":"Reduced Inequalities","Task":["Annotating Sentiment and Irony","Sentiment Analysis","national educational and training systems","annotation of data","annotation","irony","annotation stages","in - depth analysis of disagreement"],"Method":["Crowdflower","crowdsourcing annotation platform"]},{"ID":"hipson-mohammad-2020-poki","title":"{P}o{K}i: A Large Dataset of Poems by Children","abstract":"Child language studies are crucial in improving our understanding of child well-being; especially in determining the factors that impact happiness, the sources of anxiety, techniques of emotion regulation, and the mechanisms to cope with stress. However, much of this research is stymied by the lack of availability of large child-written texts. We present a new corpus of child-written text, PoKi, which includes about 62 thousand poems written by children from grades 1 to 12. PoKi is especially useful in studying child language because it comes with information about the age of the child authors (their grade). We analyze the words in PoKi along several emotion dimensions (valence, arousal, dominance) and discrete emotions (anger, fear, sadness, joy). We use non-parametric regressions to model developmental differences from early childhood to late-adolescence. Results show decreases in valence that are especially pronounced during mid-adolescence, while arousal and dominance peaked during adolescence. Gender differences in the developmental trajectory of emotions are also observed. Our results support and extend the current state of emotion development research.","year":2020,"title_abstract":"{P}o{K}i: A Large Dataset of Poems by Children Child language studies are crucial in improving our understanding of child well-being; especially in determining the factors that impact happiness, the sources of anxiety, techniques of emotion regulation, and the mechanisms to cope with stress. However, much of this research is stymied by the lack of availability of large child-written texts. We present a new corpus of child-written text, PoKi, which includes about 62 thousand poems written by children from grades 1 to 12. PoKi is especially useful in studying child language because it comes with information about the age of the child authors (their grade). We analyze the words in PoKi along several emotion dimensions (valence, arousal, dominance) and discrete emotions (anger, fear, sadness, joy). We use non-parametric regressions to model developmental differences from early childhood to late-adolescence. Results show decreases in valence that are especially pronounced during mid-adolescence, while arousal and dominance peaked during adolescence. Gender differences in the developmental trajectory of emotions are also observed. Our results support and extend the current state of emotion development research.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.1560250521,"Goal":"Good Health and Well-Being","Task":["language studies","child well - being;","emotion regulation","emotion development"],"Method":["non - parametric regressions"]},{"ID":"strassel-tracey-2016-lorelei","title":"{LORELEI} Language Packs: Data, Tools, and Resources for Technology Development in Low Resource Languages","abstract":"In this paper, we describe the textual linguistic resources in nearly 3 dozen languages being produced by Linguistic Data Consortium for DARPA{'}s LORELEI (Low Resource Languages for Emergent Incidents) Program. The goal of LORELEI is to improve the performance of human language technologies for low-resource languages and enable rapid re-training of such technologies for new languages, with a focus on the use case of deployment of resources in sudden emergencies such as natural disasters. Representative languages have been selected to provide broad typological coverage for training, and surprise incident languages for testing will be selected over the course of the program. Our approach treats the full set of language packs as a coherent whole, maintaining LORELEI-wide specifications, tagsets, and guidelines, while allowing for adaptation to the specific needs created by each language. Each representative language corpus, therefore, both stands on its own as a resource for the specific language and forms part of a large multilingual resource for broader cross-language technology development.","year":2016,"title_abstract":"{LORELEI} Language Packs: Data, Tools, and Resources for Technology Development in Low Resource Languages In this paper, we describe the textual linguistic resources in nearly 3 dozen languages being produced by Linguistic Data Consortium for DARPA{'}s LORELEI (Low Resource Languages for Emergent Incidents) Program. The goal of LORELEI is to improve the performance of human language technologies for low-resource languages and enable rapid re-training of such technologies for new languages, with a focus on the use case of deployment of resources in sudden emergencies such as natural disasters. Representative languages have been selected to provide broad typological coverage for training, and surprise incident languages for testing will be selected over the course of the program. Our approach treats the full set of language packs as a coherent whole, maintaining LORELEI-wide specifications, tagsets, and guidelines, while allowing for adaptation to the specific needs created by each language. Each representative language corpus, therefore, both stands on its own as a resource for the specific language and forms part of a large multilingual resource for broader cross-language technology development.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1559887826,"Goal":"Sustainable Cities and Communities","Task":["Technology Development","re - training","deployment of resources","sudden emergencies","training","cross - language technology development"],"Method":["Language Packs","LORELEI","human language technologies"]},{"ID":"kartal-kutlu-2020-trclaim","title":"{T}r{C}laim-19: The First Collection for {T}urkish Check-Worthy Claim Detection with Annotator Rationales","abstract":"Massive misinformation spread over Internet has many negative impacts on our lives. While spreading a claim is easy, investigating its veracity is hard and time consuming, Therefore, we urgently need systems to help human fact-checkers. However, available data resources to develop effective systems are limited and the vast majority of them is for English. In this work, we introduce TrClaim-19, which is the very first labeled dataset for Turkish check-worthy claims. TrClaim-19 consists of labeled 2287 Turkish tweets with annotator rationales, enabling us to better understand the characteristics of check-worthy claims. The rationales we collected suggest that claims{'} topics and their possible negative impacts are the main factors affecting their check-worthiness.","year":2020,"title_abstract":"{T}r{C}laim-19: The First Collection for {T}urkish Check-Worthy Claim Detection with Annotator Rationales Massive misinformation spread over Internet has many negative impacts on our lives. While spreading a claim is easy, investigating its veracity is hard and time consuming, Therefore, we urgently need systems to help human fact-checkers. However, available data resources to develop effective systems are limited and the vast majority of them is for English. In this work, we introduce TrClaim-19, which is the very first labeled dataset for Turkish check-worthy claims. TrClaim-19 consists of labeled 2287 Turkish tweets with annotator rationales, enabling us to better understand the characteristics of check-worthy claims. The rationales we collected suggest that claims{'} topics and their possible negative impacts are the main factors affecting their check-worthiness.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.15594998,"Goal":"Climate Action","Task":["{T}urkish Check - Worthy Claim Detection","human fact - checkers"],"Method":["TrClaim - 19"]},{"ID":"aranberri-etal-2014-comparison","title":"Comparison of post-editing productivity between professional translators and lay users","abstract":"This work compares the post-editing productivity of professional translators and lay users. We integrate an English to Basque MT system within Bologna Translation Service, an end-to-end translation management platform, and perform a producitivity experiment in a real working environment. Six translators and six lay users translate or post-edit two texts from English into Basque. Results suggest that overall, post-editing increases translation throughput for both translators and users, although the latter seem to benefit more from the MT output. We observe that translators and users perceive MT differently. Additionally, a preliminary analysis seems to suggest that familiarity with the domain, source text complexity and MT quality might affect potential productivity gain.","year":2014,"title_abstract":"Comparison of post-editing productivity between professional translators and lay users This work compares the post-editing productivity of professional translators and lay users. We integrate an English to Basque MT system within Bologna Translation Service, an end-to-end translation management platform, and perform a producitivity experiment in a real working environment. Six translators and six lay users translate or post-edit two texts from English into Basque. Results suggest that overall, post-editing increases translation throughput for both translators and users, although the latter seem to benefit more from the MT output. We observe that translators and users perceive MT differently. Additionally, a preliminary analysis seems to suggest that familiarity with the domain, source text complexity and MT quality might affect potential productivity gain.","social_need":"Decent Work and Economic Growth Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all","cosine_similarity":0.1559480429,"Goal":"Decent Work and Economic Growth","Task":["MT","translation management platform","post - editing","MT"],"Method":["Translation Service"]},{"ID":"yang-wang-2019-blcu","title":"The {BLCU} System in the {BEA} 2019 Shared Task","abstract":"This paper describes the BLCU Group submissions to the Building Educational Applications (BEA) 2019 Shared Task on Grammatical Error Correction (GEC). The task is to detect and correct grammatical errors that occurred in essays. We participate in 2 tracks including the Restricted Track and the Unrestricted Track. Our system is based on a Transformer model architecture. We integrate many effective methods proposed in recent years. Such as, Byte Pair Encoding, model ensemble, checkpoints average and spell checker. We also corrupt the public monolingual data to further improve the performance of the model. On the test data of the BEA 2019 Shared Task, our system yields F0.5 = 58.62 and 59.50, ranking twelfth and fourth respectively.","year":2019,"title_abstract":"The {BLCU} System in the {BEA} 2019 Shared Task This paper describes the BLCU Group submissions to the Building Educational Applications (BEA) 2019 Shared Task on Grammatical Error Correction (GEC). The task is to detect and correct grammatical errors that occurred in essays. We participate in 2 tracks including the Restricted Track and the Unrestricted Track. Our system is based on a Transformer model architecture. We integrate many effective methods proposed in recent years. Such as, Byte Pair Encoding, model ensemble, checkpoints average and spell checker. We also corrupt the public monolingual data to further improve the performance of the model. On the test data of the BEA 2019 Shared Task, our system yields F0.5 = 58.62 and 59.50, ranking twelfth and fourth respectively.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1559209228,"Goal":"Gender Equality","Task":["Building Educational Applications","Grammatical Error Correction"],"Method":["BLCU Group submissions","Transformer model architecture","Byte Pair Encoding","model ensemble","checkpoints average","spell checker"]},{"ID":"plaza-del-arco-etal-2019-sinai-semeval","title":"{SINAI} at {S}em{E}val-2019 Task 5: Ensemble learning to detect hate speech against inmigrants and women in {E}nglish and {S}panish tweets","abstract":"Misogyny and xenophobia are some of the most important social problems. With the in- crease in the use of social media, this feeling ofhatred towards women and immigrants can be more easily expressed, therefore it can cause harmful effects on social media users. For this reason, it is important to develop systems ca- pable of detecting hateful comments automatically. In this paper, we describe our system to analyze the hate speech in English and Spanish tweets against Immigrants and Women as part of our participation in SemEval-2019 Task 5: hatEval. Our main contribution is the integration of three individual algorithms of predic- tion in a model based on Vote ensemble classifier.","year":2019,"title_abstract":"{SINAI} at {S}em{E}val-2019 Task 5: Ensemble learning to detect hate speech against inmigrants and women in {E}nglish and {S}panish tweets Misogyny and xenophobia are some of the most important social problems. With the in- crease in the use of social media, this feeling ofhatred towards women and immigrants can be more easily expressed, therefore it can cause harmful effects on social media users. For this reason, it is important to develop systems ca- pable of detecting hateful comments automatically. In this paper, we describe our system to analyze the hate speech in English and Spanish tweets against Immigrants and Women as part of our participation in SemEval-2019 Task 5: hatEval. Our main contribution is the integration of three individual algorithms of predic- tion in a model based on Vote ensemble classifier.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1559124589,"Goal":"Gender Equality","Task":["social problems","detecting hateful comments","hatEval","predic - tion"],"Method":["Ensemble learning","Vote ensemble classifier"]},{"ID":"pancur-erjavec-2020-siparl","title":"The si{P}arl corpus of {S}lovene parliamentary proceedings","abstract":"The paper describes the process of acquisition, up-translation, encoding, annotation, and distribution of siParl, a collection of the parliamentary debates from the Assembly of the Republic of Slovenia from 1990{--}2018, covering the period from just before Slovenia became an independent country in 1991, and almost up to the present. The entire corpus, comprising over 8 thousand sessions, 1 million speeches and 200 million words was uniformly encoded in accordance with the TEI-based Parla-CLARIN schema for encoding corpora of parliamentary debates, and contains extensive meta-data about the speakers, a typology of sessions etc. and structural and editorial annotations. The corpus was also part-of-speech tagged and lemmatised using state-of-the-art tools. The corpus is maintained on GitHub with its major versions archived in the CLARIN.SI repository and is available for linguistic analysis in the scope of the on-line CLARIN.SI concordancers, thus offering an invaluable resource for scholars studying Slovenian political history.","year":2020,"title_abstract":"The si{P}arl corpus of {S}lovene parliamentary proceedings The paper describes the process of acquisition, up-translation, encoding, annotation, and distribution of siParl, a collection of the parliamentary debates from the Assembly of the Republic of Slovenia from 1990{--}2018, covering the period from just before Slovenia became an independent country in 1991, and almost up to the present. The entire corpus, comprising over 8 thousand sessions, 1 million speeches and 200 million words was uniformly encoded in accordance with the TEI-based Parla-CLARIN schema for encoding corpora of parliamentary debates, and contains extensive meta-data about the speakers, a typology of sessions etc. and structural and editorial annotations. The corpus was also part-of-speech tagged and lemmatised using state-of-the-art tools. The corpus is maintained on GitHub with its major versions archived in the CLARIN.SI repository and is available for linguistic analysis in the scope of the on-line CLARIN.SI concordancers, thus offering an invaluable resource for scholars studying Slovenian political history.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.1558997333,"Goal":"Peace, Justice and Strong Institutions","Task":["acquisition","up - translation","encoding","annotation","distribution","encoding corpora of parliamentary debates","linguistic analysis","SI concordancers"],"Method":["TEI - based Parla - CLARIN schema"]},{"ID":"medic-snajder-2020-improved","title":"Improved Local Citation Recommendation Based on Context Enhanced with Global Information","abstract":"Local citation recommendation aims at finding articles relevant for given citation context. While most previous approaches represent context using solely text surrounding the citation, we propose enhancing context representation with global information. Specifically, we include citing article{'}s title and abstract into context representation. We evaluate our model on datasets with different citation context sizes and demonstrate improvements with globally-enhanced context representations when citation contexts are smaller.","year":2020,"title_abstract":"Improved Local Citation Recommendation Based on Context Enhanced with Global Information Local citation recommendation aims at finding articles relevant for given citation context. While most previous approaches represent context using solely text surrounding the citation, we propose enhancing context representation with global information. Specifically, we include citing article{'}s title and abstract into context representation. We evaluate our model on datasets with different citation context sizes and demonstrate improvements with globally-enhanced context representations when citation contexts are smaller.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1558883488,"Goal":"Sustainable Cities and Communities","Task":["Local Citation Recommendation","Local citation recommendation"],"Method":["context representation","context representation","globally - enhanced context representations"]},{"ID":"magge-etal-2020-upennhlp","title":"{UP}enn{HLP} at {WNUT}-2020 Task 2 : Transformer models for classification of {COVID}19 posts on {T}witter","abstract":"Increasing usage of social media presents new non-traditional avenues for monitoring disease outbreaks, virus transmissions and disease progressions through user posts describing test results or disease symptoms. However, the discussions on the topic of infectious diseases that are informative in nature also span various topics such as news, politics and humor which makes the data mining challenging. We present a system to identify tweets about the COVID19 disease outbreak that are deemed to be informative on Twitter for use in downstream applications. The system scored a F1-score of 0.8941, Precision of 0.9028, Recall of 0.8856 and Accuracy of 0.9010. In the shared task organized as part of the 6th Workshop of Noisy User-generated Text (WNUT), the system was ranked 18th by F1-score and 13th by Accuracy.","year":2020,"title_abstract":"{UP}enn{HLP} at {WNUT}-2020 Task 2 : Transformer models for classification of {COVID}19 posts on {T}witter Increasing usage of social media presents new non-traditional avenues for monitoring disease outbreaks, virus transmissions and disease progressions through user posts describing test results or disease symptoms. However, the discussions on the topic of infectious diseases that are informative in nature also span various topics such as news, politics and humor which makes the data mining challenging. We present a system to identify tweets about the COVID19 disease outbreak that are deemed to be informative on Twitter for use in downstream applications. The system scored a F1-score of 0.8941, Precision of 0.9028, Recall of 0.8856 and Accuracy of 0.9010. In the shared task organized as part of the 6th Workshop of Noisy User-generated Text (WNUT), the system was ranked 18th by F1-score and 13th by Accuracy.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1558765322,"Goal":"Climate Action","Task":["classification of {COVID}19 posts","monitoring disease outbreaks","virus transmissions","disease progressions","infectious diseases","data mining","downstream applications"],"Method":["Transformer models"]},{"ID":"roy-etal-2022-iiitsurat","title":"{IIITS}urat@{LT}-{EDI}-{ACL}2022: Hope Speech Detection using Machine Learning","abstract":"This paper addresses the issue of Hope Speech detection using machine learning techniques. Designing a robust model that helps in predicting the target class with higher accuracy is a challenging task in machine learning, especially when the distribution of the class labels is highly imbalanced. This study uses and compares the experimental outcomes of the different oversampling techniques. Many models are implemented to classify the comments into Hope and Non-Hope speech, and it found that machine learning algorithms perform better than deep learning models. The English language dataset used in this research was developed by collecting YouTube comments and is part of the task {``}ACL-2022:Hope Speech Detection for Equality, Diversity, and Inclusion{''}. The proposed model achieved a weighted F1-score of 0.55 on the test dataset and secured the first rank among the participated teams.","year":2022,"title_abstract":"{IIITS}urat@{LT}-{EDI}-{ACL}2022: Hope Speech Detection using Machine Learning This paper addresses the issue of Hope Speech detection using machine learning techniques. Designing a robust model that helps in predicting the target class with higher accuracy is a challenging task in machine learning, especially when the distribution of the class labels is highly imbalanced. This study uses and compares the experimental outcomes of the different oversampling techniques. Many models are implemented to classify the comments into Hope and Non-Hope speech, and it found that machine learning algorithms perform better than deep learning models. The English language dataset used in this research was developed by collecting YouTube comments and is part of the task {``}ACL-2022:Hope Speech Detection for Equality, Diversity, and Inclusion{''}. The proposed model achieved a weighted F1-score of 0.55 on the test dataset and secured the first rank among the participated teams.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1558346897,"Goal":"Gender Equality","Task":["Hope Speech Detection","Hope Speech detection","machine learning","Hope Speech Detection"],"Method":["Machine Learning","machine learning techniques","robust model","oversampling techniques","machine learning algorithms","deep learning models"]},{"ID":"prabhakaran-etal-2021-releasing","title":"On Releasing Annotator-Level Labels and Information in Datasets","abstract":"A common practice in building NLP datasets, especially using crowd-sourced annotations, involves obtaining multiple annotator judgements on the same data instances, which are then flattened to produce a single {``}ground truth{''} label or score, through majority voting, averaging, or adjudication. While these approaches may be appropriate in certain annotation tasks, such aggregations overlook the socially constructed nature of human perceptions that annotations for relatively more subjective tasks are meant to capture. In particular, systematic disagreements between annotators owing to their socio-cultural backgrounds and\/or lived experiences are often obfuscated through such aggregations. In this paper, we empirically demonstrate that label aggregation may introduce representational biases of individual and group perspectives. Based on this finding, we propose a set of recommendations for increased utility and transparency of datasets for downstream use cases.","year":2021,"title_abstract":"On Releasing Annotator-Level Labels and Information in Datasets A common practice in building NLP datasets, especially using crowd-sourced annotations, involves obtaining multiple annotator judgements on the same data instances, which are then flattened to produce a single {``}ground truth{''} label or score, through majority voting, averaging, or adjudication. While these approaches may be appropriate in certain annotation tasks, such aggregations overlook the socially constructed nature of human perceptions that annotations for relatively more subjective tasks are meant to capture. In particular, systematic disagreements between annotators owing to their socio-cultural backgrounds and\/or lived experiences are often obfuscated through such aggregations. In this paper, we empirically demonstrate that label aggregation may introduce representational biases of individual and group perspectives. Based on this finding, we propose a set of recommendations for increased utility and transparency of datasets for downstream use cases.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1558339,"Goal":"Sustainable Cities and Communities","Task":["Releasing Annotator - Level Labels","NLP","annotation tasks","label aggregation","downstream use cases"],"Method":["majority voting","averaging","adjudication"]},{"ID":"sridharan-tr-2019-amrita","title":"Amrita School of Engineering - {CSE} at {S}em{E}val-2019 Task 6: Manipulating Attention with Temporal Convolutional Neural Network for Offense Identification and Classification","abstract":"With the proliferation and ubiquity of smart gadgets and smart devices, across the world, data generated by them has been growing at exponential rates; in particular social media platforms like Facebook, Twitter and Instagram have been generating voluminous data on a daily basis. According to Twitter{'}s usage statistics, about 500 million tweets are generated each day. While the tweets reflect the users{'} opinions on several events across the world, there are tweets which are offensive in nature that need to be tagged under the hateful conduct policy of Twitter. Offensive tweets have to be identified, captured and processed further, for a variety of reasons, which include i) identifying offensive tweets in order to prevent violent\/abusive behavior in Twitter (or any social media for that matter), ii) creating and maintaining a history of offensive tweets for individual users (would be helpful in creating meta-data for user profile), iii) inferring the sentiment of the users on particular event\/issue\/topic . We have employed neural network models which manipulate attention with Temporal Convolutional Neural Network for the three shared sub-tasks i) ATT-TCN (ATTention based Temporal Convolutional Neural Network) employed for shared sub-task A that yielded a best macro-F1 score of 0.46, ii) SAE-ATT-TCN(Self Attentive Embedding-ATTention based Temporal Convolutional Neural Network) employed for shared sub-task B and sub-task C that yielded best macro-F1 score of 0.61 and 0.51 respectively. Among the two variants ATT-TCN and SAE-ATT-TCN, the latter performed better.","year":2019,"title_abstract":"Amrita School of Engineering - {CSE} at {S}em{E}val-2019 Task 6: Manipulating Attention with Temporal Convolutional Neural Network for Offense Identification and Classification With the proliferation and ubiquity of smart gadgets and smart devices, across the world, data generated by them has been growing at exponential rates; in particular social media platforms like Facebook, Twitter and Instagram have been generating voluminous data on a daily basis. According to Twitter{'}s usage statistics, about 500 million tweets are generated each day. While the tweets reflect the users{'} opinions on several events across the world, there are tweets which are offensive in nature that need to be tagged under the hateful conduct policy of Twitter. Offensive tweets have to be identified, captured and processed further, for a variety of reasons, which include i) identifying offensive tweets in order to prevent violent\/abusive behavior in Twitter (or any social media for that matter), ii) creating and maintaining a history of offensive tweets for individual users (would be helpful in creating meta-data for user profile), iii) inferring the sentiment of the users on particular event\/issue\/topic . We have employed neural network models which manipulate attention with Temporal Convolutional Neural Network for the three shared sub-tasks i) ATT-TCN (ATTention based Temporal Convolutional Neural Network) employed for shared sub-task A that yielded a best macro-F1 score of 0.46, ii) SAE-ATT-TCN(Self Attentive Embedding-ATTention based Temporal Convolutional Neural Network) employed for shared sub-task B and sub-task C that yielded best macro-F1 score of 0.61 and 0.51 respectively. Among the two variants ATT-TCN and SAE-ATT-TCN, the latter performed better.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.155820787,"Goal":"Climate Action","Task":["Manipulating Attention","Offense Identification and Classification","user profile)","shared sub - tasks","shared sub - task","shared sub - task B","sub - task C"],"Method":["Temporal Convolutional Neural Network","neural network models","attention","Temporal Convolutional Neural Network","ATT - TCN (ATTention based Temporal Convolutional Neural Network)","SAE - ATT - TCN(Self Attentive Embedding - ATTention based Temporal Convolutional Neural Network)","ATT - TCN","SAE - ATT - TCN"]},{"ID":"thorne-vlachos-2018-automated","title":"Automated Fact Checking: Task Formulations, Methods and Future Directions","abstract":"The recently increased focus on misinformation has stimulated research in fact checking, the task of assessing the truthfulness of a claim. Research in automating this task has been conducted in a variety of disciplines including natural language processing, machine learning, knowledge representation, databases, and journalism. While there has been substantial progress, relevant papers and articles have been published in research communities that are often unaware of each other and use inconsistent terminology, thus impeding understanding and further progress. In this paper we survey automated fact checking research stemming from natural language processing and related disciplines, unifying the task formulations and methodologies across papers and authors. Furthermore, we highlight the use of evidence as an important distinguishing factor among them cutting across task formulations and methods. We conclude with proposing avenues for future NLP research on automated fact checking.","year":2018,"title_abstract":"Automated Fact Checking: Task Formulations, Methods and Future Directions The recently increased focus on misinformation has stimulated research in fact checking, the task of assessing the truthfulness of a claim. Research in automating this task has been conducted in a variety of disciplines including natural language processing, machine learning, knowledge representation, databases, and journalism. While there has been substantial progress, relevant papers and articles have been published in research communities that are often unaware of each other and use inconsistent terminology, thus impeding understanding and further progress. In this paper we survey automated fact checking research stemming from natural language processing and related disciplines, unifying the task formulations and methodologies across papers and authors. Furthermore, we highlight the use of evidence as an important distinguishing factor among them cutting across task formulations and methods. We conclude with proposing avenues for future NLP research on automated fact checking.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1558142006,"Goal":"Climate Action","Task":["Automated Fact Checking","Task Formulations","fact checking","truthfulness of a claim","natural language processing","machine learning","knowledge representation","databases","journalism","automated fact checking research","natural language processing","NLP","automated fact checking"],"Method":["task formulations","task formulations"]},{"ID":"cardellino-etal-2017-legal","title":"Legal {NERC} with ontologies, {W}ikipedia and curriculum learning","abstract":"In this paper, we present a Wikipedia-based approach to develop resources for the legal domain. We establish a mapping between a legal domain ontology, LKIF (Hoekstra et al. 2007), and a Wikipedia-based ontology, YAGO (Suchanek et al. 2007), and through that we populate LKIF. Moreover, we use the mentions of those entities in Wikipedia text to train a specific Named Entity Recognizer and Classifier. We find that this classifier works well in the Wikipedia, but, as could be expected, performance decreases in a corpus of judgments of the European Court of Human Rights. However, this tool will be used as a preprocess for human annotation. We resort to a technique called {``}curriculum learning{''} aimed to overcome problems of overfitting by learning increasingly more complex concepts. However, we find that in this particular setting, the method works best by learning from most specific to most general concepts, not the other way round.","year":2017,"title_abstract":"Legal {NERC} with ontologies, {W}ikipedia and curriculum learning In this paper, we present a Wikipedia-based approach to develop resources for the legal domain. We establish a mapping between a legal domain ontology, LKIF (Hoekstra et al. 2007), and a Wikipedia-based ontology, YAGO (Suchanek et al. 2007), and through that we populate LKIF. Moreover, we use the mentions of those entities in Wikipedia text to train a specific Named Entity Recognizer and Classifier. We find that this classifier works well in the Wikipedia, but, as could be expected, performance decreases in a corpus of judgments of the European Court of Human Rights. However, this tool will be used as a preprocess for human annotation. We resort to a technique called {``}curriculum learning{''} aimed to overcome problems of overfitting by learning increasingly more complex concepts. However, we find that in this particular setting, the method works best by learning from most specific to most general concepts, not the other way round.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1558080614,"Goal":"Quality Education","Task":["curriculum learning","preprocess","human annotation"],"Method":["Wikipedia - based approach","LKIF","LKIF","Named Entity Recognizer and Classifier"]},{"ID":"chiarcos-etal-2020-annotation","title":"Annotation Interoperability for the Post-{ISOC}at Era","abstract":"With this paper, we provide an overview over ISOCat successor solutions and annotation standardization efforts since 2010, and we describe the low-cost harmonization of post-ISOCat vocabularies by means of modular, linked ontologies: The CLARIN Concept Registry, LexInfo, Universal Parts of Speech, Universal Dependencies and UniMorph are linked with the Ontologies of Linguistic Annotation and through it with ISOCat, the GOLD ontology, the Typological Database Systems ontology and a large number of annotation schemes.","year":2020,"title_abstract":"Annotation Interoperability for the Post-{ISOC}at Era With this paper, we provide an overview over ISOCat successor solutions and annotation standardization efforts since 2010, and we describe the low-cost harmonization of post-ISOCat vocabularies by means of modular, linked ontologies: The CLARIN Concept Registry, LexInfo, Universal Parts of Speech, Universal Dependencies and UniMorph are linked with the Ontologies of Linguistic Annotation and through it with ISOCat, the GOLD ontology, the Typological Database Systems ontology and a large number of annotation schemes.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1557879448,"Goal":"Partnership for the Goals","Task":["Annotation Interoperability","Post - {ISOC}at Era","annotation","Linguistic Annotation"],"Method":["ISOCat successor solutions","CLARIN Concept Registry","ISOCat","Typological Database Systems ontology","annotation schemes"]},{"ID":"schneider-1991-metal","title":"The {METAL} System. Status 1991","abstract":"The METAL system which originally evolved from a cooperation between the University of Texas and Siemens became a product in 1988. METAL is implemented on multi-user worksta- tions with a LISP server in the background. It is integrated into the office environment and permits automatic deformatting and reformatting of documents. METAL is characterized by recursive grammars, best paths parsing and a modular lexicon structure. Recent changes in system design have focussed both on internal structure and on user interface. Experiences with productive use have proven METAL{'}s cost-effectiveness but have also shown the need for increased cooperation between developers and end-users.","year":1991,"title_abstract":"The {METAL} System. Status 1991 The METAL system which originally evolved from a cooperation between the University of Texas and Siemens became a product in 1988. METAL is implemented on multi-user worksta- tions with a LISP server in the background. It is integrated into the office environment and permits automatic deformatting and reformatting of documents. METAL is characterized by recursive grammars, best paths parsing and a modular lexicon structure. Recent changes in system design have focussed both on internal structure and on user interface. Experiences with productive use have proven METAL{'}s cost-effectiveness but have also shown the need for increased cooperation between developers and end-users.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1557351649,"Goal":"Partnership for the Goals","Task":["automatic deformatting and reformatting of documents","system design"],"Method":["{METAL} System","METAL system","METAL","LISP server","METAL","recursive grammars","best paths parsing","METAL{'}s"]},{"ID":"todirascu-rousselot-2001-ontologies","title":"Ontologies for Information Retrieval","abstract":"The paper presents a system for querying (in natural language) a set of text documents from a limited domain. The domain knowledge, represented in description logics (DL), is used for filtering the documents returned as answer and it is extended dynamically (when new concepts are identified in the texts), as result of DL inference mechanisms. The conceptual hierarchy is built semi-automatically from the texts. Concept instances are identified using shallow natural language parsing techniques.","year":2001,"title_abstract":"Ontologies for Information Retrieval The paper presents a system for querying (in natural language) a set of text documents from a limited domain. The domain knowledge, represented in description logics (DL), is used for filtering the documents returned as answer and it is extended dynamically (when new concepts are identified in the texts), as result of DL inference mechanisms. The conceptual hierarchy is built semi-automatically from the texts. Concept instances are identified using shallow natural language parsing techniques.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.1557216048,"Goal":"Life Below Water","Task":["Information Retrieval","querying (in natural language)"],"Method":["Ontologies","description logics","DL inference mechanisms","shallow natural language parsing techniques"]},{"ID":"dan-etal-2020-spatial","title":"From Spatial Relations to Spatial Configurations","abstract":"Spatial Reasoning from language is essential for natural language understanding. Supporting it requires a representation scheme that can capture spatial phenomena encountered in language as well as in images and videos.Existing spatial representations are not sufficient for describing spatial configurations used in complex tasks. This paper extends the capabilities of existing spatial representation languages and increases coverage of the semantic aspects that are needed to ground spatial meaning of natural language text in the world. Our spatial relation language is able to represent a large, comprehensive set of spatial concepts crucial for reasoning and is designed to support composition of static and dynamic spatial configurations. We integrate this language with the Abstract Meaning Representation (AMR) annotation schema and present a corpus annotated by this extended AMR. To exhibit the applicability of our representation scheme, we annotate text taken from diverse datasets and show how we extend the capabilities of existing spatial representation languages with fine-grained decomposition of semantics and blend it seamlessly with AMRs of sentences and discourse representations as a whole.","year":2020,"title_abstract":"From Spatial Relations to Spatial Configurations Spatial Reasoning from language is essential for natural language understanding. Supporting it requires a representation scheme that can capture spatial phenomena encountered in language as well as in images and videos.Existing spatial representations are not sufficient for describing spatial configurations used in complex tasks. This paper extends the capabilities of existing spatial representation languages and increases coverage of the semantic aspects that are needed to ground spatial meaning of natural language text in the world. Our spatial relation language is able to represent a large, comprehensive set of spatial concepts crucial for reasoning and is designed to support composition of static and dynamic spatial configurations. We integrate this language with the Abstract Meaning Representation (AMR) annotation schema and present a corpus annotated by this extended AMR. To exhibit the applicability of our representation scheme, we annotate text taken from diverse datasets and show how we extend the capabilities of existing spatial representation languages with fine-grained decomposition of semantics and blend it seamlessly with AMRs of sentences and discourse representations as a whole.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1557177901,"Goal":"Life on Land","Task":["Spatial Configurations","Spatial Reasoning","natural language understanding","spatial configurations","reasoning","composition of static and dynamic spatial configurations"],"Method":["representation scheme","spatial representations","spatial representation languages","spatial relation language","Abstract Meaning Representation","annotation schema","AMR","representation scheme","spatial representation languages","AMRs","discourse representations"]},{"ID":"eckert-neves-2018-semantic","title":"Semantic role labeling tools for biomedical question answering: a study of selected tools on the {B}io{ASQ} datasets","abstract":"Question answering (QA) systems usually rely on advanced natural language processing components to precisely understand the questions and extract the answers. Semantic role labeling (SRL) is known to boost performance for QA, but its use for biomedical texts has not yet been fully studied. We analyzed the performance of three SRL tools (BioKIT, BIOSMILE and PathLSTM) on 1776 questions from the BioASQ challenge. We compared the systems regarding the coverage of the questions and snippets, as well as based on pre-defined criteria, such as easiness of installation, supported formats and usability. Finally, we integrated two of the tools in a simple QA system to further evaluate their performance over the official BioASQ test sets.","year":2018,"title_abstract":"Semantic role labeling tools for biomedical question answering: a study of selected tools on the {B}io{ASQ} datasets Question answering (QA) systems usually rely on advanced natural language processing components to precisely understand the questions and extract the answers. Semantic role labeling (SRL) is known to boost performance for QA, but its use for biomedical texts has not yet been fully studied. We analyzed the performance of three SRL tools (BioKIT, BIOSMILE and PathLSTM) on 1776 questions from the BioASQ challenge. We compared the systems regarding the coverage of the questions and snippets, as well as based on pre-defined criteria, such as easiness of installation, supported formats and usability. Finally, we integrated two of the tools in a simple QA system to further evaluate their performance over the official BioASQ test sets.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1557095647,"Goal":"Life on Land","Task":["biomedical question answering","Question answering","QA","SRL","installation","QA"],"Method":["Semantic role labeling tools","natural language processing components","Semantic role labeling","BIOSMILE","PathLSTM)"]},{"ID":"renner-etal-2021-end","title":"An End-to-End Approach for Full Bridging Resolution","abstract":"In this article, we describe our submission to the CODI-CRAC 2021 Shared Task on Anaphora Resolution in Dialogues {--} Track BR (Gold). We demonstrate the performance of an end-to-end transformer-based higher-order coreference model finetuned for the task of full bridging. We find that while our approach is not effective at modeling the complexities of the task, it performs well on bridging resolution, suggesting a need for investigations into a robust anaphor identification model for future improvements.","year":2021,"title_abstract":"An End-to-End Approach for Full Bridging Resolution In this article, we describe our submission to the CODI-CRAC 2021 Shared Task on Anaphora Resolution in Dialogues {--} Track BR (Gold). We demonstrate the performance of an end-to-end transformer-based higher-order coreference model finetuned for the task of full bridging. We find that while our approach is not effective at modeling the complexities of the task, it performs well on bridging resolution, suggesting a need for investigations into a robust anaphor identification model for future improvements.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1557011306,"Goal":"Partnership for the Goals","Task":["Full Bridging Resolution","CODI - CRAC 2021 Shared Task","Anaphora Resolution","full bridging","bridging resolution"],"Method":["End Approach","transformer - based higher - order coreference model","anaphor identification model"]},{"ID":"villegas-etal-2012-using","title":"Using Language Resources in Humanities research","abstract":"In this paper we present two real cases, in the fields of discourse analysis of newspapers and communication research which demonstrate the impact of Language Resources (LR) and NLP in the humanities. We describe our collaboration with (i) the Feminario research group from the UAB which has been investigating androcentric practices in Spanish general press since the 80s and whose research suggests that Spanish general press has undergone a dehumanization process that excludes women and men and (ii) the \u0093Municipals'11 online\u0094 project which investigates the Spanish local election campaign in the blogosphere. We will see how NLP tools and LRs make possible the so called \u0091e-Humanities research' as they provide Humanities with tools to perform intensive and automatic text analyses. Language technologies have evolved a lot and are mature enough to provide useful tools to researchers dealing with large amount of textual data. The language resources that have been developed within the field of NLP have proven to be useful for other disciplines that are unaware of their existence and nevertheless would greatly benefit from them as they provide (i) exhaustiveness -to guarantee that data coverage is wide and representative enough- and (ii) reliable and significant results -to guarantee that the reported results are statistically significant.","year":2012,"title_abstract":"Using Language Resources in Humanities research In this paper we present two real cases, in the fields of discourse analysis of newspapers and communication research which demonstrate the impact of Language Resources (LR) and NLP in the humanities. We describe our collaboration with (i) the Feminario research group from the UAB which has been investigating androcentric practices in Spanish general press since the 80s and whose research suggests that Spanish general press has undergone a dehumanization process that excludes women and men and (ii) the \u0093Municipals'11 online\u0094 project which investigates the Spanish local election campaign in the blogosphere. We will see how NLP tools and LRs make possible the so called \u0091e-Humanities research' as they provide Humanities with tools to perform intensive and automatic text analyses. Language technologies have evolved a lot and are mature enough to provide useful tools to researchers dealing with large amount of textual data. The language resources that have been developed within the field of NLP have proven to be useful for other disciplines that are unaware of their existence and nevertheless would greatly benefit from them as they provide (i) exhaustiveness -to guarantee that data coverage is wide and representative enough- and (ii) reliable and significant results -to guarantee that the reported results are statistically significant.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1556925774,"Goal":"Sustainable Cities and Communities","Task":["Humanities research","discourse analysis of newspapers","communication research","intensive and automatic text analyses","NLP"],"Method":["Language Resources","Language Resources","NLP","dehumanization process","NLP tools","LRs","Language technologies","language resources"]},{"ID":"glavas-etal-2014-hieve","title":"{H}i{E}ve: A Corpus for Extracting Event Hierarchies from News Stories","abstract":"In news stories, event mentions denote real-world events of different spatial and temporal granularity. Narratives in news stories typically describe some real-world event of coarse spatial and temporal granularity along with its subevents. In this work, we present HiEve, a corpus for recognizing relations of spatiotemporal containment between events. In HiEve, the narratives are represented as hierarchies of events based on relations of spatiotemporal containment (i.e., superevent\u2015subevent relations). We describe the process of manual annotation of HiEve. Furthermore, we build a supervised classifier for recognizing spatiotemporal containment between events to serve as a baseline for future research. Preliminary experimental results are encouraging, with classifier performance reaching 58{\\%} F1-score, only 11{\\%} less than the inter annotator agreement.","year":2014,"title_abstract":"{H}i{E}ve: A Corpus for Extracting Event Hierarchies from News Stories In news stories, event mentions denote real-world events of different spatial and temporal granularity. Narratives in news stories typically describe some real-world event of coarse spatial and temporal granularity along with its subevents. In this work, we present HiEve, a corpus for recognizing relations of spatiotemporal containment between events. In HiEve, the narratives are represented as hierarchies of events based on relations of spatiotemporal containment (i.e., superevent\u2015subevent relations). We describe the process of manual annotation of HiEve. Furthermore, we build a supervised classifier for recognizing spatiotemporal containment between events to serve as a baseline for future research. Preliminary experimental results are encouraging, with classifier performance reaching 58{\\%} F1-score, only 11{\\%} less than the inter annotator agreement.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1556451023,"Goal":"Sustainable Cities and Communities","Task":["Extracting Event Hierarchies","recognizing relations of spatiotemporal containment between events","manual annotation","HiEve","recognizing spatiotemporal containment between events","classifier"],"Method":["HiEve","HiEve","supervised classifier"]},{"ID":"sultan-etal-2020-importance","title":"On the Importance of Diversity in Question Generation for {QA}","abstract":"Automatic question generation (QG) has shown promise as a source of synthetic training data for question answering (QA). In this paper we ask: Is textual diversity in QG beneficial for downstream QA? Using top-p nucleus sampling to derive samples from a transformer-based question generator, we show that diversity-promoting QG indeed provides better QA training than likelihood maximization approaches such as beam search. We also show that standard QG evaluation metrics such as BLEU, ROUGE and METEOR are inversely correlated with diversity, and propose a diversity-aware intrinsic measure of overall QG quality that correlates well with extrinsic evaluation on QA.","year":2020,"title_abstract":"On the Importance of Diversity in Question Generation for {QA} Automatic question generation (QG) has shown promise as a source of synthetic training data for question answering (QA). In this paper we ask: Is textual diversity in QG beneficial for downstream QA? Using top-p nucleus sampling to derive samples from a transformer-based question generator, we show that diversity-promoting QG indeed provides better QA training than likelihood maximization approaches such as beam search. We also show that standard QG evaluation metrics such as BLEU, ROUGE and METEOR are inversely correlated with diversity, and propose a diversity-aware intrinsic measure of overall QG quality that correlates well with extrinsic evaluation on QA.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.155644387,"Goal":"Quality Education","Task":["Question Generation","{QA} Automatic question generation","question answering","downstream QA?","QA training","QA"],"Method":["QG","top - p nucleus sampling","transformer - based question generator","diversity - promoting QG","likelihood maximization approaches","beam search"]},{"ID":"zhao-etal-2019-improving","title":"Improving Grammatical Error Correction via Pre-Training a Copy-Augmented Architecture with Unlabeled Data","abstract":"Neural machine translation systems have become state-of-the-art approaches for Grammatical Error Correction (GEC) task. In this paper, we propose a copy-augmented architecture for the GEC task by copying the unchanged words from the source sentence to the target sentence. Since the GEC suffers from not having enough labeled training data to achieve high accuracy. We pre-train the copy-augmented architecture with a denoising auto-encoder using the unlabeled One Billion Benchmark and make comparisons between the fully pre-trained model and a partially pre-trained model. It is the first time copying words from the source context and fully pre-training a sequence to sequence model are experimented on the GEC task. Moreover, We add token-level and sentence-level multi-task learning for the GEC task. The evaluation results on the CoNLL-2014 test set show that our approach outperforms all recently published state-of-the-art results by a large margin.","year":2019,"title_abstract":"Improving Grammatical Error Correction via Pre-Training a Copy-Augmented Architecture with Unlabeled Data Neural machine translation systems have become state-of-the-art approaches for Grammatical Error Correction (GEC) task. In this paper, we propose a copy-augmented architecture for the GEC task by copying the unchanged words from the source sentence to the target sentence. Since the GEC suffers from not having enough labeled training data to achieve high accuracy. We pre-train the copy-augmented architecture with a denoising auto-encoder using the unlabeled One Billion Benchmark and make comparisons between the fully pre-trained model and a partially pre-trained model. It is the first time copying words from the source context and fully pre-training a sequence to sequence model are experimented on the GEC task. Moreover, We add token-level and sentence-level multi-task learning for the GEC task. The evaluation results on the CoNLL-2014 test set show that our approach outperforms all recently published state-of-the-art results by a large margin.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.155615747,"Goal":"Gender Equality","Task":["Grammatical Error Correction","translation","Grammatical Error Correction","GEC","GEC","GEC task","GEC"],"Method":["Pre - Training","Copy - Augmented Architecture","copy - augmented architecture","copy - augmented architecture","denoising auto - encoder","fully pre - trained model","partially pre - trained model","sequence to sequence model","token - level","sentence - level multi - task learning"]},{"ID":"krebs-etal-2018-semeval","title":"{S}em{E}val-2018 Task 10: Capturing Discriminative Attributes","abstract":"This paper describes the SemEval 2018 Task 10 on Capturing Discriminative Attributes. Participants were asked to identify whether an attribute could help discriminate between two concepts. For example, a successful system should determine that {`}urine{'} is a discriminating feature in the word pair {`}kidney{'}, {`}bone{'}. The aim of the task is to better evaluate the capabilities of state of the art semantic models, beyond pure semantic similarity. The task attracted submissions from 21 teams, and the best system achieved a 0.75 F1 score.","year":2018,"title_abstract":"{S}em{E}val-2018 Task 10: Capturing Discriminative Attributes This paper describes the SemEval 2018 Task 10 on Capturing Discriminative Attributes. Participants were asked to identify whether an attribute could help discriminate between two concepts. For example, a successful system should determine that {`}urine{'} is a discriminating feature in the word pair {`}kidney{'}, {`}bone{'}. The aim of the task is to better evaluate the capabilities of state of the art semantic models, beyond pure semantic similarity. The task attracted submissions from 21 teams, and the best system achieved a 0.75 F1 score.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1556116045,"Goal":"Gender Equality","Task":["Capturing Discriminative Attributes","Capturing Discriminative Attributes"],"Method":["semantic models"]},{"ID":"pan-2019-chinese","title":"The {C}hinese\/{E}nglish Political Interpreting Corpus ({CEPIC}): A New Electronic Resource for Translators and Interpreters","abstract":"The Chinese\/English Political Interpreting Corpus (CEPIC) is a new electronic and open access resource developed for translators and interpreters, especially those working with political text types. Over 6 million word tokens in size, the online corpus consists of transcripts of Chinese (Cantonese {\\&} Putonghua) \/ English political speeches and their translated and interpreted texts. It includes rich meta-data and is POS-tagged and annotated with prosodic and paralinguistic features that are of concern to spoken language and interpreting. The online platform of the CEPIC features main functions including Keyword Search, Word Collocation and Expanded Keyword in Context, which are illustrated in the paper. The CEPIC can shed light on online translation and interpreting corpora development in the future.","year":2019,"title_abstract":"The {C}hinese\/{E}nglish Political Interpreting Corpus ({CEPIC}): A New Electronic Resource for Translators and Interpreters The Chinese\/English Political Interpreting Corpus (CEPIC) is a new electronic and open access resource developed for translators and interpreters, especially those working with political text types. Over 6 million word tokens in size, the online corpus consists of transcripts of Chinese (Cantonese {\\&} Putonghua) \/ English political speeches and their translated and interpreted texts. It includes rich meta-data and is POS-tagged and annotated with prosodic and paralinguistic features that are of concern to spoken language and interpreting. The online platform of the CEPIC features main functions including Keyword Search, Word Collocation and Expanded Keyword in Context, which are illustrated in the paper. The CEPIC can shed light on online translation and interpreting corpora development in the future.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.155582726,"Goal":"Peace, Justice and Strong Institutions","Task":["interpreters","interpreting","Keyword Search","online translation and interpreting corpora development"],"Method":["CEPIC","CEPIC"]},{"ID":"karadzhov-etal-2017-built","title":"We Built a Fake News \/ Click Bait Filter: What Happened Next Will Blow Your Mind!","abstract":"It is completely amazing! Fake news and {``}click baits{''} have totally invaded the cyberspace. Let us face it: everybody hates them for three simple reasons. Reason {\\#}2 will absolutely amaze you. What these can achieve at the time of election will completely blow your mind! Now, we all agree, this cannot go on, you know, somebody has to stop it. So, we did this research, and trust us, it is totally great research, it really is! Make no mistake. This is the best research ever! Seriously, come have a look, we have it all: neural networks, attention mechanism, sentiment lexicons, author profiling, you name it. Lexical features, semantic features, we absolutely have it all. And we have totally tested it, trust us! We have results, and numbers, really big numbers. The best numbers ever! Oh, and analysis, absolutely top notch analysis. Interested? Come read the shocking truth about fake news and clickbait in the Bulgarian cyberspace. You won{'}t believe what we have found!","year":2017,"title_abstract":"We Built a Fake News \/ Click Bait Filter: What Happened Next Will Blow Your Mind! It is completely amazing! Fake news and {``}click baits{''} have totally invaded the cyberspace. Let us face it: everybody hates them for three simple reasons. Reason {\\#}2 will absolutely amaze you. What these can achieve at the time of election will completely blow your mind! Now, we all agree, this cannot go on, you know, somebody has to stop it. So, we did this research, and trust us, it is totally great research, it really is! Make no mistake. This is the best research ever! Seriously, come have a look, we have it all: neural networks, attention mechanism, sentiment lexicons, author profiling, you name it. Lexical features, semantic features, we absolutely have it all. And we have totally tested it, trust us! We have results, and numbers, really big numbers. The best numbers ever! Oh, and analysis, absolutely top notch analysis. Interested? Come read the shocking truth about fake news and clickbait in the Bulgarian cyberspace. You won{'}t believe what we have found!","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1555763483,"Goal":"Climate Action","Task":["author profiling","analysis"],"Method":["Fake News \/ Click Bait Filter","neural networks","attention mechanism","sentiment lexicons","notch analysis"]},{"ID":"lan-etal-2021-neural","title":"Neural semi-{M}arkov {CRF} for Monolingual Word Alignment","abstract":"Monolingual word alignment is important for studying fine-grained editing operations (i.e., deletion, addition, and substitution) in text-to-text generation tasks, such as paraphrase generation, text simplification, neutralizing biased language, etc. In this paper, we present a novel neural semi-Markov CRF alignment model, which unifies word and phrase alignments through variable-length spans. We also create a new benchmark with human annotations that cover four different text genres to evaluate monolingual word alignment models in more realistic settings. Experimental results show that our proposed model outperforms all previous approaches for monolingual word alignment as well as a competitive QA-based baseline, which was previously only applied to bilingual data. Our model demonstrates good generalizability to three out-of-domain datasets and shows great utility in two downstream applications: automatic text simplification and sentence pair classification tasks.","year":2021,"title_abstract":"Neural semi-{M}arkov {CRF} for Monolingual Word Alignment Monolingual word alignment is important for studying fine-grained editing operations (i.e., deletion, addition, and substitution) in text-to-text generation tasks, such as paraphrase generation, text simplification, neutralizing biased language, etc. In this paper, we present a novel neural semi-Markov CRF alignment model, which unifies word and phrase alignments through variable-length spans. We also create a new benchmark with human annotations that cover four different text genres to evaluate monolingual word alignment models in more realistic settings. Experimental results show that our proposed model outperforms all previous approaches for monolingual word alignment as well as a competitive QA-based baseline, which was previously only applied to bilingual data. Our model demonstrates good generalizability to three out-of-domain datasets and shows great utility in two downstream applications: automatic text simplification and sentence pair classification tasks.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1555523574,"Goal":"Gender Equality","Task":["Monolingual Word Alignment","Monolingual word alignment","fine - grained editing operations","deletion","addition","substitution)","text - to - text generation tasks","paraphrase generation","text simplification","neutralizing biased language","monolingual word alignment","automatic text simplification","sentence pair classification tasks"],"Method":["Neural semi - {M}arkov","neural semi - Markov CRF alignment model","monolingual word alignment models","QA - based baseline"]},{"ID":"mcnamee-etal-2020-tagging","title":"Tagging Location Phrases in Text","abstract":"For over thirty years researchers have studied the problem of automatically detecting named entities in written language. Throughout this time the majority of such work has focused on detection and classification of entities into coarse-grained types like: PERSON, ORGANIZATION, and LOCATION. Less attention has been focused on non-named mentions of entities, including non-named location phrases such as {``}the medical clinic in Telonge{''} or {``}2 km below the Dolin Maniche bridge{''}. In this work we describe the Location Phrase Detection task to identify such spans. Our key accomplishments include: developing a sequential tagging approach; crafting annotation guidelines; building annotated datasets for English and Russian news; and, conducting experiments in automated detection of location phrases with both statistical and neural taggers. This work is motivated by extracting rich location information to support situational awareness during humanitarian crises such as natural disasters.","year":2020,"title_abstract":"Tagging Location Phrases in Text For over thirty years researchers have studied the problem of automatically detecting named entities in written language. Throughout this time the majority of such work has focused on detection and classification of entities into coarse-grained types like: PERSON, ORGANIZATION, and LOCATION. Less attention has been focused on non-named mentions of entities, including non-named location phrases such as {``}the medical clinic in Telonge{''} or {``}2 km below the Dolin Maniche bridge{''}. In this work we describe the Location Phrase Detection task to identify such spans. Our key accomplishments include: developing a sequential tagging approach; crafting annotation guidelines; building annotated datasets for English and Russian news; and, conducting experiments in automated detection of location phrases with both statistical and neural taggers. This work is motivated by extracting rich location information to support situational awareness during humanitarian crises such as natural disasters.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1554582119,"Goal":"Sustainable Cities and Communities","Task":["Tagging Location Phrases","automatically detecting named entities","detection and classification of entities","Location Phrase Detection task","annotation guidelines;","automated detection of location phrases","situational awareness","humanitarian crises"],"Method":["sequential tagging approach;","statistical and neural taggers"]},{"ID":"colic-etal-2020-annotating","title":"Annotating the Pandemic: Named Entity Recognition and Normalisation in {COVID}-19 Literature","abstract":"The COVID-19 pandemic has been accompanied by such an explosive increase in media coverage and scientific publications that researchers find it difficult to keep up. We are presenting a publicly available pipeline to perform named entity recognition and normalisation in parallel to help find relevant publications and to aid in downstream NLP tasks such as text summarisation. In our approach, we are using a dictionary-based system for its high recall in conjunction with two models based on BioBERT for their accuracy. Their outputs are combined according to different strategies depending on the entity type. In addition, we are using a manually crafted dictionary to increase performance for new concepts related to COVID-19. We have previously evaluated our work on the CRAFT corpus, and make the output of our pipeline available on two visualisation platforms.","year":2020,"title_abstract":"Annotating the Pandemic: Named Entity Recognition and Normalisation in {COVID}-19 Literature The COVID-19 pandemic has been accompanied by such an explosive increase in media coverage and scientific publications that researchers find it difficult to keep up. We are presenting a publicly available pipeline to perform named entity recognition and normalisation in parallel to help find relevant publications and to aid in downstream NLP tasks such as text summarisation. In our approach, we are using a dictionary-based system for its high recall in conjunction with two models based on BioBERT for their accuracy. Their outputs are combined according to different strategies depending on the entity type. In addition, we are using a manually crafted dictionary to increase performance for new concepts related to COVID-19. We have previously evaluated our work on the CRAFT corpus, and make the output of our pipeline available on two visualisation platforms.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1553775966,"Goal":"Climate Action","Task":["Annotating the Pandemic","Named Entity Recognition","Normalisation","named entity recognition","normalisation","downstream NLP tasks","text summarisation"],"Method":["dictionary - based system","BioBERT"]},{"ID":"park-2021-bridging","title":"Bridging Multi-disciplinary Collaboration Challenges in {ML} Development via Domain Knowledge Elicitation","abstract":"Building a machine learning model in a sophisticated domain is a time-consuming process, partially due to the steep learning curve of domain knowledge for data scientists. We introduce Ziva, an interface for supporting domain knowledge from domain experts to data scientists in two ways: (1) a concept creation interface where domain experts extract important concept of the domain and (2) five kinds of justification elicitation interfaces that solicit elicitation how the domain concept are expressed in data instances.","year":2021,"title_abstract":"Bridging Multi-disciplinary Collaboration Challenges in {ML} Development via Domain Knowledge Elicitation Building a machine learning model in a sophisticated domain is a time-consuming process, partially due to the steep learning curve of domain knowledge for data scientists. We introduce Ziva, an interface for supporting domain knowledge from domain experts to data scientists in two ways: (1) a concept creation interface where domain experts extract important concept of the domain and (2) five kinds of justification elicitation interfaces that solicit elicitation how the domain concept are expressed in data instances.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1553762108,"Goal":"Partnership for the Goals","Task":["Multi - disciplinary Collaboration Challenges","{ML} Development","Domain Knowledge Elicitation","data scientists","domain knowledge"],"Method":["machine learning model","Ziva","concept creation interface","justification elicitation interfaces"]},{"ID":"elsaadany-suter-2020-grapheme","title":"Grapheme-to-Phoneme Conversion with a Multilingual Transformer Model","abstract":"In this paper, we describe our three submissions to the SIGMORPHON 2020 shared task 1 on grapheme-to-phoneme conversion for 15 languages. We experimented with a single multilingual transformer model. We observed that the multilingual model achieves results on par with our separately trained monolingual models and is even able to avoid a few of the errors made by the monolingual models.","year":2020,"title_abstract":"Grapheme-to-Phoneme Conversion with a Multilingual Transformer Model In this paper, we describe our three submissions to the SIGMORPHON 2020 shared task 1 on grapheme-to-phoneme conversion for 15 languages. We experimented with a single multilingual transformer model. We observed that the multilingual model achieves results on par with our separately trained monolingual models and is even able to avoid a few of the errors made by the monolingual models.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1553557813,"Goal":"Gender Equality","Task":["Grapheme - to - Phoneme Conversion","SIGMORPHON 2020 shared task","grapheme - to - phoneme conversion"],"Method":["Multilingual Transformer Model","multilingual transformer model","multilingual model","monolingual models","monolingual models"]},{"ID":"chekalina-etal-2021-better","title":"Which is Better for Deep Learning: Python or {MATLAB}? Answering Comparative Questions in Natural Language","abstract":"We present a system for answering comparative questions (Is X better than Y with respect to Z?) in natural language. Answering such questions is important for assisting humans in making informed decisions. The key component of our system is a natural language interface for comparative QA that can be used in personal assistants, chatbots, and similar NLP devices. Comparative QA is a challenging NLP task, since it requires collecting support evidence from many different sources, and direct comparisons of rare objects may be not available even on the entire Web. We take the first step towards a solution for such a task offering a testbed for comparative QA in natural language by probing several methods, making the three best ones available as an online demo.","year":2021,"title_abstract":"Which is Better for Deep Learning: Python or {MATLAB}? Answering Comparative Questions in Natural Language We present a system for answering comparative questions (Is X better than Y with respect to Z?) in natural language. Answering such questions is important for assisting humans in making informed decisions. The key component of our system is a natural language interface for comparative QA that can be used in personal assistants, chatbots, and similar NLP devices. Comparative QA is a challenging NLP task, since it requires collecting support evidence from many different sources, and direct comparisons of rare objects may be not available even on the entire Web. We take the first step towards a solution for such a task offering a testbed for comparative QA in natural language by probing several methods, making the three best ones available as an online demo.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1553483605,"Goal":"Reduced Inequalities","Task":["Answering Comparative Questions","answering comparative questions","informed decisions","comparative QA","personal assistants","NLP devices","Comparative QA","NLP task","comparative QA"],"Method":["Deep Learning","Python","natural language interface"]},{"ID":"joseph-etal-2017-constance","title":"{C}on{S}tance: Modeling Annotation Contexts to Improve Stance Classification","abstract":"Manual annotations are a prerequisite for many applications of machine learning. However, weaknesses in the annotation process itself are easy to overlook. In particular, scholars often choose what information to give to annotators without examining these decisions empirically. For subjective tasks such as sentiment analysis, sarcasm, and stance detection, such choices can impact results. Here, for the task of political stance detection on Twitter, we show that providing too little context can result in noisy and uncertain annotations, whereas providing too strong a context may cause it to outweigh other signals. To characterize and reduce these biases, we develop ConStance, a general model for reasoning about annotations across information conditions. Given conflicting labels produced by multiple annotators seeing the same instances with different contexts, ConStance simultaneously estimates gold standard labels and also learns a classifier for new instances. We show that the classifier learned by ConStance outperforms a variety of baselines at predicting political stance, while the model{'}s interpretable parameters shed light on the effects of each context.","year":2017,"title_abstract":"{C}on{S}tance: Modeling Annotation Contexts to Improve Stance Classification Manual annotations are a prerequisite for many applications of machine learning. However, weaknesses in the annotation process itself are easy to overlook. In particular, scholars often choose what information to give to annotators without examining these decisions empirically. For subjective tasks such as sentiment analysis, sarcasm, and stance detection, such choices can impact results. Here, for the task of political stance detection on Twitter, we show that providing too little context can result in noisy and uncertain annotations, whereas providing too strong a context may cause it to outweigh other signals. To characterize and reduce these biases, we develop ConStance, a general model for reasoning about annotations across information conditions. Given conflicting labels produced by multiple annotators seeing the same instances with different contexts, ConStance simultaneously estimates gold standard labels and also learns a classifier for new instances. We show that the classifier learned by ConStance outperforms a variety of baselines at predicting political stance, while the model{'}s interpretable parameters shed light on the effects of each context.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1553436369,"Goal":"Climate Action","Task":["Modeling Annotation Contexts","Stance Classification","Manual annotations","annotation process","subjective tasks","sentiment analysis","sarcasm","stance detection","political stance detection","annotations","predicting political stance"],"Method":["machine learning","ConStance","general model","ConStance","classifier","classifier","ConStance"]},{"ID":"ding-etal-2021-levenshtein","title":"{L}evenshtein Training for Word-level Quality Estimation","abstract":"We propose a novel scheme to use the Levenshtein Transformer to perform the task of word-level quality estimation. A Levenshtein Transformer is a natural fit for this task: trained to perform decoding in an iterative manner, a Levenshtein Transformer can learn to post-edit without explicit supervision. To further minimize the mismatch between the translation task and the word-level QE task, we propose a two-stage transfer learning procedure on both augmented data and human post-editing data. We also propose heuristics to construct reference labels that are compatible with subword-level finetuning and inference. Results on WMT 2020 QE shared task dataset show that our proposed method has superior data efficiency under the data-constrained setting and competitive performance under the unconstrained setting.","year":2021,"title_abstract":"{L}evenshtein Training for Word-level Quality Estimation We propose a novel scheme to use the Levenshtein Transformer to perform the task of word-level quality estimation. A Levenshtein Transformer is a natural fit for this task: trained to perform decoding in an iterative manner, a Levenshtein Transformer can learn to post-edit without explicit supervision. To further minimize the mismatch between the translation task and the word-level QE task, we propose a two-stage transfer learning procedure on both augmented data and human post-editing data. We also propose heuristics to construct reference labels that are compatible with subword-level finetuning and inference. Results on WMT 2020 QE shared task dataset show that our proposed method has superior data efficiency under the data-constrained setting and competitive performance under the unconstrained setting.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1552264094,"Goal":"Quality Education","Task":["Word - level Quality Estimation","word - level quality estimation","decoding","translation task","word - level QE task","inference","data - constrained setting","unconstrained setting"],"Method":["Levenshtein Transformer","Levenshtein Transformer","Levenshtein Transformer","stage transfer learning procedure"]},{"ID":"ramos-soto-etal-2018-meteorologists","title":"Meteorologists and Students: A resource for language grounding of geographical descriptors","abstract":"We present a data resource which can be useful for research purposes on language grounding tasks in the context of geographical referring expression generation. The resource is composed of two data sets that encompass 25 different geographical descriptors and a set of associated graphical representations, drawn as polygons on a map by two groups of human subjects: teenage students and expert meteorologists.","year":2018,"title_abstract":"Meteorologists and Students: A resource for language grounding of geographical descriptors We present a data resource which can be useful for research purposes on language grounding tasks in the context of geographical referring expression generation. The resource is composed of two data sets that encompass 25 different geographical descriptors and a set of associated graphical representations, drawn as polygons on a map by two groups of human subjects: teenage students and expert meteorologists.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1552164853,"Goal":"Sustainable Cities and Communities","Task":["language grounding of geographical descriptors","language grounding tasks","geographical referring expression generation"],"Method":["data resource","graphical representations"]},{"ID":"abdul-mageed-etal-2020-aranet","title":"{A}ra{N}et: A Deep Learning Toolkit for {A}rabic Social Media","abstract":"We describe AraNet, a collection of deep learning Arabic social media processing tools. Namely, we exploit an extensive host of both publicly available and novel social media datasets to train bidirectional encoders from transformers (BERT) focused at social meaning extraction. AraNet models predict age, dialect, gender, emotion, irony, and sentiment. AraNet either delivers state-of-the-art performance on a number of these tasks and performs competitively on others. AraNet is exclusively based on a deep learning framework, giving it the advantage of being feature-engineering free. To the best of our knowledge, AraNet is the first to performs predictions across such a wide range of tasks for Arabic NLP. As such, AraNet has the potential to meet critical needs. We publicly release AraNet to accelerate research, and to facilitate model-based comparisons across the different tasks","year":2020,"title_abstract":"{A}ra{N}et: A Deep Learning Toolkit for {A}rabic Social Media We describe AraNet, a collection of deep learning Arabic social media processing tools. Namely, we exploit an extensive host of both publicly available and novel social media datasets to train bidirectional encoders from transformers (BERT) focused at social meaning extraction. AraNet models predict age, dialect, gender, emotion, irony, and sentiment. AraNet either delivers state-of-the-art performance on a number of these tasks and performs competitively on others. AraNet is exclusively based on a deep learning framework, giving it the advantage of being feature-engineering free. To the best of our knowledge, AraNet is the first to performs predictions across such a wide range of tasks for Arabic NLP. As such, AraNet has the potential to meet critical needs. We publicly release AraNet to accelerate research, and to facilitate model-based comparisons across the different tasks","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1551778316,"Goal":"Gender Equality","Task":["social meaning extraction","Arabic NLP"],"Method":["Deep Learning Toolkit","AraNet","deep learning Arabic social media processing tools","bidirectional encoders","transformers","AraNet models","AraNet","AraNet","deep learning framework","feature - engineering","AraNet","AraNet","AraNet"]},{"ID":"hartholt-etal-2008-common","title":"A Common Ground for Virtual Humans: Using an Ontology in a Natural Language Oriented Virtual Human Architecture","abstract":"When dealing with large, distributed systems that use state-of-the-art components, individual components are usually developed in parallel. As development continues, the decoupling invariably leads to a mismatch between how these components internally represent concepts and how they communicate these representations to other components: representations can get out of synch, contain localized errors, or become manageable only by a small group of experts for each module. In this paper, we describe the use of an ontology as part of a complex distributed virtual human architecture in order to enable better communication between modules while improving the overall flexibility needed to change or extend the system. We focus on the natural language understanding capabilities of this architecture and the relationship between language and concepts within the entire system in general and the ontology in particular.","year":2008,"title_abstract":"A Common Ground for Virtual Humans: Using an Ontology in a Natural Language Oriented Virtual Human Architecture When dealing with large, distributed systems that use state-of-the-art components, individual components are usually developed in parallel. As development continues, the decoupling invariably leads to a mismatch between how these components internally represent concepts and how they communicate these representations to other components: representations can get out of synch, contain localized errors, or become manageable only by a small group of experts for each module. In this paper, we describe the use of an ontology as part of a complex distributed virtual human architecture in order to enable better communication between modules while improving the overall flexibility needed to change or extend the system. We focus on the natural language understanding capabilities of this architecture and the relationship between language and concepts within the entire system in general and the ontology in particular.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1550934762,"Goal":"Sustainable Cities and Communities","Task":["Virtual Humans","large , distributed systems","natural language understanding capabilities"],"Method":["Natural Language Oriented Virtual Human Architecture","ontology","distributed virtual human architecture"]},{"ID":"mihaylova-etal-2019-semeval","title":"{S}em{E}val-2019 Task 8: Fact Checking in Community Question Answering Forums","abstract":"We present SemEval-2019 Task 8 on Fact Checking in Community Question Answering Forums, which features two subtasks. Subtask A is about deciding whether a question asks for factual information vs. an opinion\/advice vs. just socializing. Subtask B asks to predict whether an answer to a factual question is true, false or not a proper answer. We received 17 official submissions for subtask A and 11 official submissions for Subtask B. For subtask A, all systems improved over the majority class baseline. For Subtask B, all systems were below a majority class baseline, but several systems were very close to it. The leaderboard and the data from the competition can be found at http:\/\/competitions.codalab.org\/competitions\/20022.","year":2019,"title_abstract":"{S}em{E}val-2019 Task 8: Fact Checking in Community Question Answering Forums We present SemEval-2019 Task 8 on Fact Checking in Community Question Answering Forums, which features two subtasks. Subtask A is about deciding whether a question asks for factual information vs. an opinion\/advice vs. just socializing. Subtask B asks to predict whether an answer to a factual question is true, false or not a proper answer. We received 17 official submissions for subtask A and 11 official submissions for Subtask B. For subtask A, all systems improved over the majority class baseline. For Subtask B, all systems were below a majority class baseline, but several systems were very close to it. The leaderboard and the data from the competition can be found at http:\/\/competitions.codalab.org\/competitions\/20022.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1550467908,"Goal":"Climate Action","Task":["Fact Checking","Community Question Answering Forums","Fact Checking","Community Question Answering Forums"],"Method":["majority class baseline"]},{"ID":"kordjamshidi-etal-2010-spatial","title":"Spatial Role Labeling: Task Definition and Annotation Scheme","abstract":"One of the essential functions of natural language is to talk about spatial relationships between objects. Linguistic constructs can express highly complex, relational structures of objects, spatial relations between them, and patterns of motion through spaces relative to some reference point. Learning how to map this information onto a formal representation from a text is a challenging problem. At present no well-defined framework for automatic spatial information extraction exists that can handle all of these issues. In this paper we introduce the task of spatial role labeling and propose an annotation scheme that is language-independent and facilitates the application of machine learning techniques. Our framework consists of a set of spatial roles based on the theory of holistic spatial semantics with the intent of covering all aspects of spatial concepts, including both static and dynamic spatial relations. We illustrate our annotation scheme with many examples throughout the paper, and in addition we highlight how to connect to spatial calculi such as region connection calculus and also how our approach fits into related work.","year":2010,"title_abstract":"Spatial Role Labeling: Task Definition and Annotation Scheme One of the essential functions of natural language is to talk about spatial relationships between objects. Linguistic constructs can express highly complex, relational structures of objects, spatial relations between them, and patterns of motion through spaces relative to some reference point. Learning how to map this information onto a formal representation from a text is a challenging problem. At present no well-defined framework for automatic spatial information extraction exists that can handle all of these issues. In this paper we introduce the task of spatial role labeling and propose an annotation scheme that is language-independent and facilitates the application of machine learning techniques. Our framework consists of a set of spatial roles based on the theory of holistic spatial semantics with the intent of covering all aspects of spatial concepts, including both static and dynamic spatial relations. We illustrate our annotation scheme with many examples throughout the paper, and in addition we highlight how to connect to spatial calculi such as region connection calculus and also how our approach fits into related work.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1550285816,"Goal":"Sustainable Cities and Communities","Task":["Spatial Role Labeling","Task Definition","automatic spatial information extraction","spatial role labeling"],"Method":["Annotation Scheme","formal representation","annotation scheme","machine learning techniques","annotation scheme","spatial calculi","region connection calculus"]},{"ID":"syed-etal-2021-summary","title":"Summary Explorer: Visualizing the State of the Art in Text Summarization","abstract":"This paper introduces Summary Explorer, a new tool to support the manual inspection of text summarization systems by compiling the outputs of 55 state-of-the-art single document summarization approaches on three benchmark datasets, and visually exploring them during a qualitative assessment. The underlying design of the tool considers three well-known summary quality criteria (coverage, faithfulness, and position bias), encapsulated in a guided assessment based on tailored visualizations. The tool complements existing approaches for locally debugging summarization models and improves upon them. The tool is available at https:\/\/tldr.webis.de\/","year":2021,"title_abstract":"Summary Explorer: Visualizing the State of the Art in Text Summarization This paper introduces Summary Explorer, a new tool to support the manual inspection of text summarization systems by compiling the outputs of 55 state-of-the-art single document summarization approaches on three benchmark datasets, and visually exploring them during a qualitative assessment. The underlying design of the tool considers three well-known summary quality criteria (coverage, faithfulness, and position bias), encapsulated in a guided assessment based on tailored visualizations. The tool complements existing approaches for locally debugging summarization models and improves upon them. The tool is available at https:\/\/tldr.webis.de\/","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1549974084,"Goal":"Partnership for the Goals","Task":["Text Summarization","manual inspection of text summarization systems","summarization","qualitative assessment","guided assessment","locally debugging summarization models"],"Method":["Summary Explorer","Summary Explorer","tailored visualizations"]},{"ID":"prasad-etal-2020-opinion","title":"Opinion Mining System for Processing {H}indi Text for Home Remedies Domain","abstract":"Opinion Mining (OM) is a field of study in Computer Science that deals with development of software applications related to text classifications and summarizations. Researchers working in this field contribute lexical resources, computing methodologies, text classification approaches, and summarization modules to perform OM tasks across various domains and different languages. Lexical and computational components developed for an Opinion Mining System that processes Hindi text taken from weblogs are presented in the paper for the demonstration. Text chosen for processing are the ones demonstrating cause and effect relationship between related entities {`}Food{'} and {`}Health Issues{'}. The work is novel and lexical resources developed are useful in current research and may be of importance for future research in the field. The resources are developed for an algorithm {`}A{'} such that for a sentence {`}Y{'} which is a domain specific sentence from weblogs in Hindi, A(Y) returns a set F, HI, p, s such that F is a subset of set, FOOD=set of word or phrases in Hindi used for an edible item and HI is a subset of set, HEALTH{\\_}ISSUE= set of word or phrases in Hindi used for a part of body composition {`}BODY{\\_}COMPONENT{'} UNION set of word or phrases in Hindi used for a health problem a human being face {`}HEALTH{\\_}PROBLEM{'}. Element {`}p{'} takes numeric value {`}1{'} or {`}-1{'} where value {`}1{'} means that from the text {`}Y{'}, algorithm {`}A{'} computationally derived that the food entities mentioned in set {`}F{'} have a positive effect in health issues mentioned in set {`}HI{'} and the numeric value {`}-1{'} means that the food entities in set {`}F{'} have a negative effect in health issues in set {`}HI{'}. The element{`}s{'} may take value {`}1{'} or {`}2{'} indicating that the strength of polarity {`}p{'} is medium or strong.","year":2020,"title_abstract":"Opinion Mining System for Processing {H}indi Text for Home Remedies Domain Opinion Mining (OM) is a field of study in Computer Science that deals with development of software applications related to text classifications and summarizations. Researchers working in this field contribute lexical resources, computing methodologies, text classification approaches, and summarization modules to perform OM tasks across various domains and different languages. Lexical and computational components developed for an Opinion Mining System that processes Hindi text taken from weblogs are presented in the paper for the demonstration. Text chosen for processing are the ones demonstrating cause and effect relationship between related entities {`}Food{'} and {`}Health Issues{'}. The work is novel and lexical resources developed are useful in current research and may be of importance for future research in the field. The resources are developed for an algorithm {`}A{'} such that for a sentence {`}Y{'} which is a domain specific sentence from weblogs in Hindi, A(Y) returns a set F, HI, p, s such that F is a subset of set, FOOD=set of word or phrases in Hindi used for an edible item and HI is a subset of set, HEALTH{\\_}ISSUE= set of word or phrases in Hindi used for a part of body composition {`}BODY{\\_}COMPONENT{'} UNION set of word or phrases in Hindi used for a health problem a human being face {`}HEALTH{\\_}PROBLEM{'}. Element {`}p{'} takes numeric value {`}1{'} or {`}-1{'} where value {`}1{'} means that from the text {`}Y{'}, algorithm {`}A{'} computationally derived that the food entities mentioned in set {`}F{'} have a positive effect in health issues mentioned in set {`}HI{'} and the numeric value {`}-1{'} means that the food entities in set {`}F{'} have a negative effect in health issues in set {`}HI{'}. The element{`}s{'} may take value {`}1{'} or {`}2{'} indicating that the strength of polarity {`}p{'} is medium or strong.","social_need":"Clean Water and Sanitation Ensure availability and sustainable management of water and sanitation for all","cosine_similarity":0.1549807489,"Goal":"Clean Water and Sanitation","Task":["Home Remedies","Domain Opinion Mining","Computer Science","software applications","text classifications and summarizations","OM tasks","health problem"],"Method":["Opinion Mining System","lexical resources","computing methodologies","text classification approaches","summarization modules","Lexical and computational components","Opinion Mining System"]},{"ID":"tenney-etal-2020-language","title":"The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for {NLP} Models","abstract":"We present the Language Interpretability Tool (LIT), an open-source platform for visualization and understanding of NLP models. We focus on core questions about model behavior: Why did my model make this prediction? When does it perform poorly? What happens under a controlled change in the input? LIT integrates local explanations, aggregate analysis, and counterfactual generation into a streamlined, browser-based interface to enable rapid exploration and error analysis. We include case studies for a diverse set of workflows, including exploring counterfactuals for sentiment analysis, measuring gender bias in coreference systems, and exploring local behavior in text generation. LIT supports a wide range of models{---}including classification, seq2seq, and structured prediction{---}and is highly extensible through a declarative, framework-agnostic API. LIT is under active development, with code and full documentation available at https:\/\/github.com\/pair-code\/lit.","year":2020,"title_abstract":"The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for {NLP} Models We present the Language Interpretability Tool (LIT), an open-source platform for visualization and understanding of NLP models. We focus on core questions about model behavior: Why did my model make this prediction? When does it perform poorly? What happens under a controlled change in the input? LIT integrates local explanations, aggregate analysis, and counterfactual generation into a streamlined, browser-based interface to enable rapid exploration and error analysis. We include case studies for a diverse set of workflows, including exploring counterfactuals for sentiment analysis, measuring gender bias in coreference systems, and exploring local behavior in text generation. LIT supports a wide range of models{---}including classification, seq2seq, and structured prediction{---}and is highly extensible through a declarative, framework-agnostic API. LIT is under active development, with code and full documentation available at https:\/\/github.com\/pair-code\/lit.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1549342424,"Goal":"Gender Equality","Task":["Extensible , Interactive Visualizations and Analysis","visualization and understanding of NLP models","model behavior","rapid exploration and error analysis","counterfactuals","sentiment analysis","gender bias","coreference systems","local behavior","text generation","classification","seq2seq","structured prediction{"],"Method":["Language Interpretability Tool","{NLP} Models","Language Interpretability Tool","LIT","local explanations","aggregate analysis","counterfactual generation","browser - based interface","LIT","declarative , framework - agnostic API","LIT"]},{"ID":"alkhouli-etal-2018-alignment","title":"On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation","abstract":"This work investigates the alignment problem in state-of-the-art multi-head attention models based on the transformer architecture. We demonstrate that alignment extraction in transformer models can be improved by augmenting an additional alignment head to the multi-head source-to-target attention component. This is used to compute sharper attention weights. We describe how to use the alignment head to achieve competitive performance. To study the effect of adding the alignment head, we simulate a dictionary-guided translation task, where the user wants to guide translation using pre-defined dictionary entries. Using the proposed approach, we achieve up to 3.8{\\%} BLEU improvement when using the dictionary, in comparison to 2.4{\\%} BLEU in the baseline case. We also propose alignment pruning to speed up decoding in alignment-based neural machine translation (ANMT), which speeds up translation by a factor of 1.8 without loss in translation performance. We carry out experiments on the shared WMT 2016 English\u2192Romanian news task and the BOLT Chinese\u2192English discussion forum task.","year":2018,"title_abstract":"On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation This work investigates the alignment problem in state-of-the-art multi-head attention models based on the transformer architecture. We demonstrate that alignment extraction in transformer models can be improved by augmenting an additional alignment head to the multi-head source-to-target attention component. This is used to compute sharper attention weights. We describe how to use the alignment head to achieve competitive performance. To study the effect of adding the alignment head, we simulate a dictionary-guided translation task, where the user wants to guide translation using pre-defined dictionary entries. Using the proposed approach, we achieve up to 3.8{\\%} BLEU improvement when using the dictionary, in comparison to 2.4{\\%} BLEU in the baseline case. We also propose alignment pruning to speed up decoding in alignment-based neural machine translation (ANMT), which speeds up translation by a factor of 1.8 without loss in translation performance. We carry out experiments on the shared WMT 2016 English\u2192Romanian news task and the BOLT Chinese\u2192English discussion forum task.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1548934877,"Goal":"Gender Equality","Task":["Alignment Problem","Multi - Head Attention - Based Neural Machine Translation","alignment problem","alignment extraction","dictionary - guided translation task","translation","decoding","alignment - based neural machine translation","translation","translation"],"Method":["multi - head attention models","transformer architecture","transformer models","multi - head source - to - target attention component","alignment pruning"]},{"ID":"mohammad-2022-ethics","title":"Ethics Sheets for {AI} Tasks","abstract":"Several high-profile events, such as the mass testing of emotion recognition systems on vulnerable sub-populations and using question answering systems to make moral judgments, have highlighted how technology will often lead to more adverse outcomes for those that are already marginalized. At issue here are not just individual systems and datasets, but also the AI tasks themselves. In this position paper, I make a case for thinking about ethical considerations not just at the level of individual models and datasets, but also at the level of AI tasks. I will present a new form of such an effort, Ethics Sheets for AI Tasks, dedicated to fleshing out the assumptions and ethical considerations hidden in how a task is commonly framed and in the choices we make regarding the data, method, and evaluation. I will also present a template for ethics sheets with 50 ethical considerations, using the task of emotion recognition as a running example. Ethics sheets are a mechanism to engage with and document ethical considerations before building datasets and systems. Similar to survey articles, a small number of carefully created ethics sheets can serve numerous researchers and developers.","year":2022,"title_abstract":"Ethics Sheets for {AI} Tasks Several high-profile events, such as the mass testing of emotion recognition systems on vulnerable sub-populations and using question answering systems to make moral judgments, have highlighted how technology will often lead to more adverse outcomes for those that are already marginalized. At issue here are not just individual systems and datasets, but also the AI tasks themselves. In this position paper, I make a case for thinking about ethical considerations not just at the level of individual models and datasets, but also at the level of AI tasks. I will present a new form of such an effort, Ethics Sheets for AI Tasks, dedicated to fleshing out the assumptions and ethical considerations hidden in how a task is commonly framed and in the choices we make regarding the data, method, and evaluation. I will also present a template for ethics sheets with 50 ethical considerations, using the task of emotion recognition as a running example. Ethics sheets are a mechanism to engage with and document ethical considerations before building datasets and systems. Similar to survey articles, a small number of carefully created ethics sheets can serve numerous researchers and developers.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1548410356,"Goal":"Gender Equality","Task":["{AI} Tasks","AI tasks","AI tasks","AI Tasks","evaluation","emotion recognition"],"Method":["emotion recognition systems","question answering systems"]},{"ID":"solano-etal-2018-development","title":"Development of Natural Language Processing Tools for {C}ook {I}slands {M}{\\=a}ori","abstract":"This paper presents three ongoing projects for NLP in Cook Islands Maori: Untrained Forced Alignment (approx. 9{\\%} error when detecting the center of words), speech-to-text (37{\\%} WER in the best trained models) and POS tagging (92{\\%} accuracy for the best performing model). Included as part of these projects are new resources filling in a gap in Australasian languages, including gold standard POS-tagged written corpora, transcribed speech corpora, time-aligned corpora down to the level of phonemes. These are part of efforts to accelerate the documentation of Cook Islands Maori and to increase its vitality amongst its users.","year":2018,"title_abstract":"Development of Natural Language Processing Tools for {C}ook {I}slands {M}{\\=a}ori This paper presents three ongoing projects for NLP in Cook Islands Maori: Untrained Forced Alignment (approx. 9{\\%} error when detecting the center of words), speech-to-text (37{\\%} WER in the best trained models) and POS tagging (92{\\%} accuracy for the best performing model). Included as part of these projects are new resources filling in a gap in Australasian languages, including gold standard POS-tagged written corpora, transcribed speech corpora, time-aligned corpora down to the level of phonemes. These are part of efforts to accelerate the documentation of Cook Islands Maori and to increase its vitality amongst its users.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1548208594,"Goal":"Life on Land","Task":["NLP"],"Method":["Natural Language Processing Tools","Untrained Forced Alignment","POS"]},{"ID":"zmiycharov-etal-2021-comparative","title":"A Comparative Study on Abstractive and Extractive Approaches in Summarization of {E}uropean Legislation Documents","abstract":"Extracting the most important part of legislation documents has great business value because the texts are usually very long and hard to understand. The aim of this article is to evaluate different algorithms for text summarization on EU legislation documents. The content contains domain-specific words. We collected a text summarization dataset of EU legal documents consisting of 1563 documents, in which the mean length of summaries is 424 words. Experiments were conducted with different algorithms using the new dataset. A simple extractive algorithm was selected as a baseline. Advanced extractive algorithms, which use encoders show better results than baseline. The best result measured by ROUGE scores was achieved by a fine-tuned abstractive T5 model, which was adapted to work with long texts.","year":2021,"title_abstract":"A Comparative Study on Abstractive and Extractive Approaches in Summarization of {E}uropean Legislation Documents Extracting the most important part of legislation documents has great business value because the texts are usually very long and hard to understand. The aim of this article is to evaluate different algorithms for text summarization on EU legislation documents. The content contains domain-specific words. We collected a text summarization dataset of EU legal documents consisting of 1563 documents, in which the mean length of summaries is 424 words. Experiments were conducted with different algorithms using the new dataset. A simple extractive algorithm was selected as a baseline. Advanced extractive algorithms, which use encoders show better results than baseline. The best result measured by ROUGE scores was achieved by a fine-tuned abstractive T5 model, which was adapted to work with long texts.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1547822952,"Goal":"Reduced Inequalities","Task":["Summarization","text summarization","summarization"],"Method":["Abstractive and Extractive Approaches","extractive algorithm","extractive algorithms","encoders","fine - tuned abstractive T5 model"]},{"ID":"rajagopal-etal-2022-curie","title":"{CURIE}: An Iterative Querying Approach for Reasoning About Situations","abstract":"Predicting the effects of unexpected situations is an important reasoning task, e.g., would cloudy skies help or hinder plant growth? Given a context, the goal of such situational reasoning is to elicit the consequences of a new situation (st) that arises in that context. We propose CURIE, a method to iteratively build a graph of relevant consequences explicitly in a structured situational graph (st graph) using natural language queries over a finetuned language model. Across multiple domains, CURIE generates st graphs that humans find relevant and meaningful in eliciting the consequences of a new situation (75{\\%} of the graphs were judged correct by humans). We present a case study of a situation reasoning end task (WIQA-QA), where simply augmenting their input with st graphs improves accuracy by 3 points. We show that these improvements mainly come from a hard subset of the data, that requires background knowledge and multi-hop reasoning.","year":2022,"title_abstract":"{CURIE}: An Iterative Querying Approach for Reasoning About Situations Predicting the effects of unexpected situations is an important reasoning task, e.g., would cloudy skies help or hinder plant growth? Given a context, the goal of such situational reasoning is to elicit the consequences of a new situation (st) that arises in that context. We propose CURIE, a method to iteratively build a graph of relevant consequences explicitly in a structured situational graph (st graph) using natural language queries over a finetuned language model. Across multiple domains, CURIE generates st graphs that humans find relevant and meaningful in eliciting the consequences of a new situation (75{\\%} of the graphs were judged correct by humans). We present a case study of a situation reasoning end task (WIQA-QA), where simply augmenting their input with st graphs improves accuracy by 3 points. We show that these improvements mainly come from a hard subset of the data, that requires background knowledge and multi-hop reasoning.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1547691226,"Goal":"Climate Action","Task":["Reasoning About Situations","Predicting the effects of unexpected situations","reasoning task","situation reasoning end task","- QA)"],"Method":["Iterative Querying Approach","situational reasoning","CURIE","finetuned language model","CURIE","st graphs","st graphs","multi - hop reasoning"]},{"ID":"garg-etal-2019-jointly","title":"Jointly Learning to Align and Translate with Transformer Models","abstract":"The state of the art in machine translation (MT) is governed by neural approaches, which typically provide superior translation accuracy over statistical approaches. However, on the closely related task of word alignment, traditional statistical word alignment models often remain the go-to solution. In this paper, we present an approach to train a Transformer model to produce both accurate translations and alignments. We extract discrete alignments from the attention probabilities learnt during regular neural machine translation model training and leverage them in a multi-task framework to optimize towards translation and alignment objectives. We demonstrate that our approach produces competitive results compared to GIZA++ trained IBM alignment models without sacrificing translation accuracy and outperforms previous attempts on Transformer model based word alignment. Finally, by incorporating IBM model alignments into our multi-task training, we report significantly better alignment accuracies compared to GIZA++ on three publicly available data sets.","year":2019,"title_abstract":"Jointly Learning to Align and Translate with Transformer Models The state of the art in machine translation (MT) is governed by neural approaches, which typically provide superior translation accuracy over statistical approaches. However, on the closely related task of word alignment, traditional statistical word alignment models often remain the go-to solution. In this paper, we present an approach to train a Transformer model to produce both accurate translations and alignments. We extract discrete alignments from the attention probabilities learnt during regular neural machine translation model training and leverage them in a multi-task framework to optimize towards translation and alignment objectives. We demonstrate that our approach produces competitive results compared to GIZA++ trained IBM alignment models without sacrificing translation accuracy and outperforms previous attempts on Transformer model based word alignment. Finally, by incorporating IBM model alignments into our multi-task training, we report significantly better alignment accuracies compared to GIZA++ on three publicly available data sets.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1546801925,"Goal":"Gender Equality","Task":["Jointly Learning","Align","Translate","machine translation","word alignment","translation and alignment objectives","word alignment"],"Method":["Transformer Models","neural approaches","statistical approaches","statistical word alignment models","Transformer model","regular neural machine translation model training","multi - task framework","GIZA++","IBM alignment models","Transformer model","IBM model alignments","multi - task training","GIZA++"]},{"ID":"soler-wanner-2016-semi","title":"A Semi-Supervised Approach for Gender Identification","abstract":"In most of the research studies on Author Profiling, large quantities of correctly labeled data are used to train the models. However, this does not reflect the reality in forensic scenarios: in practical linguistic forensic investigations, the resources that are available to profile the author of a text are usually scarce. To pay tribute to this fact, we implemented a Semi-Supervised Learning variant of the k nearest neighbors algorithm that uses small sets of labeled data and a larger amount of unlabeled data to classify the authors of texts by gender (man vs woman). We describe the enriched KNN algorithm and show that the use of unlabeled instances improves the accuracy of our gender identification model. We also present a feature set that facilitates the use of a very small number of instances, reaching accuracies higher than 70{\\%} with only 113 instances to train the model. It is also shown that the algorithm also performs well using publicly available data.","year":2016,"title_abstract":"A Semi-Supervised Approach for Gender Identification In most of the research studies on Author Profiling, large quantities of correctly labeled data are used to train the models. However, this does not reflect the reality in forensic scenarios: in practical linguistic forensic investigations, the resources that are available to profile the author of a text are usually scarce. To pay tribute to this fact, we implemented a Semi-Supervised Learning variant of the k nearest neighbors algorithm that uses small sets of labeled data and a larger amount of unlabeled data to classify the authors of texts by gender (man vs woman). We describe the enriched KNN algorithm and show that the use of unlabeled instances improves the accuracy of our gender identification model. We also present a feature set that facilitates the use of a very small number of instances, reaching accuracies higher than 70{\\%} with only 113 instances to train the model. It is also shown that the algorithm also performs well using publicly available data.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1546143293,"Goal":"Gender Equality","Task":["Gender Identification","Author Profiling","forensic scenarios","linguistic forensic investigations"],"Method":["Semi - Supervised Approach","Semi - Supervised Learning variant","k nearest neighbors algorithm","enriched KNN algorithm","gender identification model"]},{"ID":"alex-etal-2021-online","title":"The Online Pivot: Lessons Learned from Teaching a Text and Data Mining Course in Lockdown, Enhancing online Teaching with Pair Programming and Digital Badges","abstract":"In this paper we provide an account of how we ported a text and data mining course online in summer 2020 as a result of the COVID-19 pandemic and how we improved it in a second pilot run. We describe the course, how we adapted it over the two pilot runs and what teaching techniques we used to improve students{'} learning and community building online. We also provide information on the relentless feedback collected during the course which helped us to adapt our teaching from one session to the next and one pilot to the next. We discuss the lessons learned and promote the use of innovative teaching techniques applied to the digital such as digital badges and pair programming in break-out rooms for teaching Natural Language Processing courses to beginners and students with different backgrounds.","year":2021,"title_abstract":"The Online Pivot: Lessons Learned from Teaching a Text and Data Mining Course in Lockdown, Enhancing online Teaching with Pair Programming and Digital Badges In this paper we provide an account of how we ported a text and data mining course online in summer 2020 as a result of the COVID-19 pandemic and how we improved it in a second pilot run. We describe the course, how we adapted it over the two pilot runs and what teaching techniques we used to improve students{'} learning and community building online. We also provide information on the relentless feedback collected during the course which helped us to adapt our teaching from one session to the next and one pilot to the next. We discuss the lessons learned and promote the use of innovative teaching techniques applied to the digital such as digital badges and pair programming in break-out rooms for teaching Natural Language Processing courses to beginners and students with different backgrounds.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.154528141,"Goal":"Quality Education","Task":["Text and Data Mining Course","online Teaching","Pair Programming","text and data mining course","community building","Natural Language Processing courses"],"Method":["Online Pivot","teaching techniques","teaching techniques","pair programming"]},{"ID":"hwang-etal-2020-sprucing","title":"Sprucing up Supersenses: Untangling the Semantic Clusters of Accompaniment and Purpose","abstract":"We reevaluate an existing adpositional annotation scheme with respect to two thorny semantic domains: accompaniment and purpose. {`}Accompaniment{'} broadly speaking includes two entities situated together or participating in the same event, while {`}purpose{'} broadly speaking covers the desired outcome of an action, the intended use or evaluated use of an entity, and more. We argue the policy in the SNACS scheme for English should be recalibrated with respect to these clusters of interrelated meanings without adding complexity to the overall scheme. Our analysis highlights tradeoffs in lumping vs. splitting decisions as well as the flexibility afforded by the construal analysis.","year":2020,"title_abstract":"Sprucing up Supersenses: Untangling the Semantic Clusters of Accompaniment and Purpose We reevaluate an existing adpositional annotation scheme with respect to two thorny semantic domains: accompaniment and purpose. {`}Accompaniment{'} broadly speaking includes two entities situated together or participating in the same event, while {`}purpose{'} broadly speaking covers the desired outcome of an action, the intended use or evaluated use of an entity, and more. We argue the policy in the SNACS scheme for English should be recalibrated with respect to these clusters of interrelated meanings without adding complexity to the overall scheme. Our analysis highlights tradeoffs in lumping vs. splitting decisions as well as the flexibility afforded by the construal analysis.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1545037478,"Goal":"Partnership for the Goals","Task":["splitting decisions"],"Method":["adpositional annotation scheme","SNACS scheme","construal analysis"]},{"ID":"simonjetz-2020-reference","title":"Reference to Discourse Topics: Introducing {``}Global{''} Shell Nouns","abstract":"Shell nouns (SNs) are abstract nouns like {``}fact{''}, {``}issue{''}, and {``}decision{''}, which are capable of refer- ring to non-nominal antecedents, much like anaphoric pronouns. As an extension of classical anaphora resolution, the automatic detection of SNs alongside their respective antecedents has received a growing research interest in recent years but proved to be a challenging task. This paper critically examines the assumption prevalent in previous research that SNs are typically accompanied by a specific antecedent, arguing that SNs like {``}issue{''} and {``}decision{''} are frequently used to refer, not to specific antecedents, but to global discourse topics, in which case they are out of reach of previously proposed resolution strategies that are tailored to SNs with explicit antecedents. The contribution of this work is three-fold. First, the notion of global SNs is defined; second, their qualitative and quantitative impact on previous SN research is investigated; and third, implications for previous and future approaches to SN resolution are discussed.","year":2020,"title_abstract":"Reference to Discourse Topics: Introducing {``}Global{''} Shell Nouns Shell nouns (SNs) are abstract nouns like {``}fact{''}, {``}issue{''}, and {``}decision{''}, which are capable of refer- ring to non-nominal antecedents, much like anaphoric pronouns. As an extension of classical anaphora resolution, the automatic detection of SNs alongside their respective antecedents has received a growing research interest in recent years but proved to be a challenging task. This paper critically examines the assumption prevalent in previous research that SNs are typically accompanied by a specific antecedent, arguing that SNs like {``}issue{''} and {``}decision{''} are frequently used to refer, not to specific antecedents, but to global discourse topics, in which case they are out of reach of previously proposed resolution strategies that are tailored to SNs with explicit antecedents. The contribution of this work is three-fold. First, the notion of global SNs is defined; second, their qualitative and quantitative impact on previous SN research is investigated; and third, implications for previous and future approaches to SN resolution are discussed.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1544882655,"Goal":"Partnership for the Goals","Task":["automatic detection of SNs","SN","SN resolution"],"Method":["anaphora resolution","resolution strategies"]},{"ID":"ogrodniczuk-kopec-2017-lexical","title":"Lexical Correction of {P}olish {T}witter Political Data","abstract":"Language processing architectures are often evaluated in near-to-perfect conditions with respect to processed content. The tools which perform sufficiently well on electronic press, books and other type of non-interactive content may poorly handle littered, colloquial and multilingual textual data which make the majority of communication today. This paper aims at investigating how Polish Twitter data (in a slightly controlled {`}political{'} flavour) differs from expectation of linguistic tools and how they could be corrected to be ready for processing by standard language processing chains available for Polish. The setting includes specialised components for spelling correction of tweets as well as hashtag and username decoding.","year":2017,"title_abstract":"Lexical Correction of {P}olish {T}witter Political Data Language processing architectures are often evaluated in near-to-perfect conditions with respect to processed content. The tools which perform sufficiently well on electronic press, books and other type of non-interactive content may poorly handle littered, colloquial and multilingual textual data which make the majority of communication today. This paper aims at investigating how Polish Twitter data (in a slightly controlled {`}political{'} flavour) differs from expectation of linguistic tools and how they could be corrected to be ready for processing by standard language processing chains available for Polish. The setting includes specialised components for spelling correction of tweets as well as hashtag and username decoding.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1544758081,"Goal":"Climate Action","Task":["Lexical Correction","communication","spelling correction of tweets","hashtag and username decoding"],"Method":["Language processing architectures","linguistic tools","language processing chains"]},{"ID":"guest-etal-2021-expert","title":"An Expert Annotated Dataset for the Detection of Online Misogyny","abstract":"Online misogyny is a pernicious social problem that risks making online platforms toxic and unwelcoming to women. We present a new hierarchical taxonomy for online misogyny, as well as an expert labelled dataset to enable automatic classification of misogynistic content. The dataset consists of 6567 labels for Reddit posts and comments. As previous research has found untrained crowdsourced annotators struggle with identifying misogyny, we hired and trained annotators and provided them with robust annotation guidelines. We report baseline classification performance on the binary classification task, achieving accuracy of 0.93 and F1 of 0.43. The codebook and datasets are made freely available for future researchers.","year":2021,"title_abstract":"An Expert Annotated Dataset for the Detection of Online Misogyny Online misogyny is a pernicious social problem that risks making online platforms toxic and unwelcoming to women. We present a new hierarchical taxonomy for online misogyny, as well as an expert labelled dataset to enable automatic classification of misogynistic content. The dataset consists of 6567 labels for Reddit posts and comments. As previous research has found untrained crowdsourced annotators struggle with identifying misogyny, we hired and trained annotators and provided them with robust annotation guidelines. We report baseline classification performance on the binary classification task, achieving accuracy of 0.93 and F1 of 0.43. The codebook and datasets are made freely available for future researchers.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1544683129,"Goal":"Gender Equality","Task":["Detection of Online Misogyny Online misogyny","pernicious social problem","online misogyny","automatic classification of misogynistic content","identifying misogyny","classification","binary classification task"],"Method":["hierarchical taxonomy"]},{"ID":"vorakitphan-etal-2020-regrexit","title":"Regrexit or not Regrexit: Aspect-based Sentiment Analysis in Polarized Contexts","abstract":"Emotion analysis in polarized contexts represents a challenge for Natural Language Processing modeling. As a step in the aforementioned direction, we present a methodology to extend the task of Aspect-based Sentiment Analysis (ABSA) toward the affect and emotion representation in polarized settings. In particular, we adopt the three-dimensional model of affect based on Valence, Arousal, and Dominance (VAD). We then present a Brexit scenario that proves how affect varies toward the same aspect when politically polarized stances are presented. Our approach captures aspect-based polarization from newspapers regarding the Brexit scenario of 1.2m entities at sentence-level. We demonstrate how basic constituents of emotions can be mapped to the VAD model, along with their interactions respecting the polarized context in ABSA settings using biased key-concepts (e.g., {``}stop Brexit{''} vs. {``}support Brexit{''}). Quite intriguingly, the framework achieves to produce coherent aspect evidences of Brexit{'}s stance from key-concepts, showing that VAD influence the support and opposition aspects.","year":2020,"title_abstract":"Regrexit or not Regrexit: Aspect-based Sentiment Analysis in Polarized Contexts Emotion analysis in polarized contexts represents a challenge for Natural Language Processing modeling. As a step in the aforementioned direction, we present a methodology to extend the task of Aspect-based Sentiment Analysis (ABSA) toward the affect and emotion representation in polarized settings. In particular, we adopt the three-dimensional model of affect based on Valence, Arousal, and Dominance (VAD). We then present a Brexit scenario that proves how affect varies toward the same aspect when politically polarized stances are presented. Our approach captures aspect-based polarization from newspapers regarding the Brexit scenario of 1.2m entities at sentence-level. We demonstrate how basic constituents of emotions can be mapped to the VAD model, along with their interactions respecting the polarized context in ABSA settings using biased key-concepts (e.g., {``}stop Brexit{''} vs. {``}support Brexit{''}). Quite intriguingly, the framework achieves to produce coherent aspect evidences of Brexit{'}s stance from key-concepts, showing that VAD influence the support and opposition aspects.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1544304639,"Goal":"Climate Action","Task":["Aspect - based Sentiment Analysis","Emotion analysis","Natural Language Processing modeling","Aspect - based Sentiment Analysis","affect and emotion representation","aspect - based polarization","Brexit scenario"],"Method":["three - dimensional model of affect","VAD","VAD"]},{"ID":"cuadros-etal-2010-integrating","title":"Integrating a Large Domain Ontology of Species into {W}ord{N}et","abstract":"With the proliferation of applications sharing information represented in multiple ontologies, the development of automatic methods for robust and accurate ontology matching will be crucial to their success. Connecting and merging already existing semantic networks is perhaps one of the most challenging task related to knowledge engineering. This paper presents a new approach for aligning automatically a very large domain ontology of Species to WordNet in the framework of the KYOTO project. The approach relies on the use of knowledge-based Word Sense Disambiguation algorithm which accurately assigns WordNet synsets to the concepts represented in Species 2000.","year":2010,"title_abstract":"Integrating a Large Domain Ontology of Species into {W}ord{N}et With the proliferation of applications sharing information represented in multiple ontologies, the development of automatic methods for robust and accurate ontology matching will be crucial to their success. Connecting and merging already existing semantic networks is perhaps one of the most challenging task related to knowledge engineering. This paper presents a new approach for aligning automatically a very large domain ontology of Species to WordNet in the framework of the KYOTO project. The approach relies on the use of knowledge-based Word Sense Disambiguation algorithm which accurately assigns WordNet synsets to the concepts represented in Species 2000.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1543900073,"Goal":"Life on Land","Task":["robust and accurate ontology matching","semantic networks","knowledge engineering","KYOTO project"],"Method":["{W}ord{N}et","automatic methods","knowledge - based Word Sense Disambiguation algorithm"]},{"ID":"dreuw-etal-2010-signspeak","title":"The {S}ign{S}peak Project - Bridging the Gap Between Signers and Speakers","abstract":"The SignSpeak project will be the first step to approach sign language recognition and translation at a scientific level already reached in similar research fields such as automatic speech recognition or statistical machine translation of spoken languages. Deaf communities revolve around sign languages as they are their natural means of communication. Although deaf, hard of hearing and hearing signers can communicate without problems amongst themselves, there is a serious challenge for the deaf community in trying to integrate into educational, social and work environments. The overall goal of SignSpeak is to develop a new vision-based technology for recognizing and translating continuous sign language to text. New knowledge about the nature of sign language structure from the perspective of machine recognition of continuous sign language will allow a subsequent breakthrough in the development of a new vision-based technology for continuous sign language recognition and translation. Existing and new publicly available corpora will be used to evaluate the research progress throughout the whole project.","year":2010,"title_abstract":"The {S}ign{S}peak Project - Bridging the Gap Between Signers and Speakers The SignSpeak project will be the first step to approach sign language recognition and translation at a scientific level already reached in similar research fields such as automatic speech recognition or statistical machine translation of spoken languages. Deaf communities revolve around sign languages as they are their natural means of communication. Although deaf, hard of hearing and hearing signers can communicate without problems amongst themselves, there is a serious challenge for the deaf community in trying to integrate into educational, social and work environments. The overall goal of SignSpeak is to develop a new vision-based technology for recognizing and translating continuous sign language to text. New knowledge about the nature of sign language structure from the perspective of machine recognition of continuous sign language will allow a subsequent breakthrough in the development of a new vision-based technology for continuous sign language recognition and translation. Existing and new publicly available corpora will be used to evaluate the research progress throughout the whole project.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1543798447,"Goal":"Sustainable Cities and Communities","Task":["SignSpeak project","sign language recognition","translation","automatic speech recognition","statistical machine translation of spoken languages","communication","SignSpeak","translating continuous sign language","machine recognition of continuous sign language","continuous sign language recognition","translation"],"Method":["vision - based technology","vision - based technology"]},{"ID":"meng-etal-2021-bringing","title":"Bringing Structure into Summaries: a Faceted Summarization Dataset for Long Scientific Documents","abstract":"Faceted summarization provides briefings of a document from different perspectives. Readers can quickly comprehend the main points of a long document with the help of a structured outline. However, little research has been conducted on this subject, partially due to the lack of large-scale faceted summarization datasets. In this study, we present FacetSum, a faceted summarization benchmark built on Emerald journal articles, covering a diverse range of domains. Different from traditional document-summary pairs, FacetSum provides multiple summaries, each targeted at specific sections of a long document, including the purpose, method, findings, and value. Analyses and empirical results on our dataset reveal the importance of bringing structure into summaries. We believe FacetSum will spur further advances in summarization research and foster the development of NLP systems that can leverage the structured information in both long texts and summaries.","year":2021,"title_abstract":"Bringing Structure into Summaries: a Faceted Summarization Dataset for Long Scientific Documents Faceted summarization provides briefings of a document from different perspectives. Readers can quickly comprehend the main points of a long document with the help of a structured outline. However, little research has been conducted on this subject, partially due to the lack of large-scale faceted summarization datasets. In this study, we present FacetSum, a faceted summarization benchmark built on Emerald journal articles, covering a diverse range of domains. Different from traditional document-summary pairs, FacetSum provides multiple summaries, each targeted at specific sections of a long document, including the purpose, method, findings, and value. Analyses and empirical results on our dataset reveal the importance of bringing structure into summaries. We believe FacetSum will spur further advances in summarization research and foster the development of NLP systems that can leverage the structured information in both long texts and summaries.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1543773711,"Goal":"Partnership for the Goals","Task":["Summarization","Long Scientific Documents","summarization","summarization","summarization","summarization research"],"Method":["FacetSum","FacetSum","FacetSum","NLP systems"]},{"ID":"tuan-nguyen-2020-tatl","title":"{TATL} at {WNUT}-2020 Task 2: A Transformer-based Baseline System for Identification of Informative {COVID}-19 {E}nglish Tweets","abstract":"As the COVID-19 outbreak continues to spread throughout the world, more and more information about the pandemic has been shared publicly on social media. For example, there are a huge number of COVID-19 English Tweets daily on Twitter. However, the majority of those Tweets are uninformative, and hence it is important to be able to automatically select only the informative ones for downstream applications. In this short paper, we present our participation in the W-NUT 2020 Shared Task 2: Identification of Informative COVID-19 English Tweets. Inspired by the recent advances in pretrained Transformer language models, we propose a simple yet effective baseline for the task. Despite its simplicity, our proposed approach shows very competitive results in the leaderboard as we ranked 8 over 56 teams participated in total.","year":2020,"title_abstract":"{TATL} at {WNUT}-2020 Task 2: A Transformer-based Baseline System for Identification of Informative {COVID}-19 {E}nglish Tweets As the COVID-19 outbreak continues to spread throughout the world, more and more information about the pandemic has been shared publicly on social media. For example, there are a huge number of COVID-19 English Tweets daily on Twitter. However, the majority of those Tweets are uninformative, and hence it is important to be able to automatically select only the informative ones for downstream applications. In this short paper, we present our participation in the W-NUT 2020 Shared Task 2: Identification of Informative COVID-19 English Tweets. Inspired by the recent advances in pretrained Transformer language models, we propose a simple yet effective baseline for the task. Despite its simplicity, our proposed approach shows very competitive results in the leaderboard as we ranked 8 over 56 teams participated in total.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1543442607,"Goal":"Climate Action","Task":["Identification of Informative {COVID} - 19","COVID - 19","downstream applications","W - NUT","Identification of Informative COVID - 19 English Tweets"],"Method":["Transformer - based Baseline System","pretrained Transformer language models"]},{"ID":"mubarak-hassan-2021-ul2c","title":"{UL}2{C}: Mapping User Locations to Countries on {A}rabic {T}witter","abstract":"Mapping user locations to countries can be useful for many applications such as dialect identification, author profiling, recommendation system, etc. Twitter allows users to declare their locations as free text, and these user-declared locations are often noisy and hard to decipher automatically. In this paper, we present the largest manually labeled dataset for mapping user locations on Arabic Twitter to their corresponding countries. We build effective machine learning models that can automate this mapping with significantly better efficiency compared to libraries such as geopy. We also show that our dataset is more effective than data extracted from GeoNames geographical database in this task as the latter covers only locations written in formal ways.","year":2021,"title_abstract":"{UL}2{C}: Mapping User Locations to Countries on {A}rabic {T}witter Mapping user locations to countries can be useful for many applications such as dialect identification, author profiling, recommendation system, etc. Twitter allows users to declare their locations as free text, and these user-declared locations are often noisy and hard to decipher automatically. In this paper, we present the largest manually labeled dataset for mapping user locations on Arabic Twitter to their corresponding countries. We build effective machine learning models that can automate this mapping with significantly better efficiency compared to libraries such as geopy. We also show that our dataset is more effective than data extracted from GeoNames geographical database in this task as the latter covers only locations written in formal ways.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.154320091,"Goal":"Sustainable Cities and Communities","Task":["Mapping User Locations","dialect identification","author profiling","recommendation system","mapping user locations","mapping"],"Method":["machine learning models","geopy"]},{"ID":"kasunic-kaufman-2018-learning","title":"Learning to Listen: Critically Considering the Role of {AI} in Human Storytelling and Character Creation","abstract":"In this opinion piece, we argue that there is a need for alternative design directions to complement existing AI efforts in narrative and character generation and algorithm development. To make our argument, we a) outline the predominant roles and goals of AI research in storytelling; b) present existing discourse on the benefits and harms of narratives; and c) highlight the pain points in character creation revealed by semi-structured interviews we conducted with 14 individuals deeply involved in some form of character creation. We conclude by proffering several specific design avenues that we believe can seed fruitful research collaborations. In our vision, AI collaborates with humans during creative processes and narrative generation, helps amplify voices and perspectives that are currently marginalized or misrepresented, and engenders experiences of narrative that support spectatorship and listening roles.","year":2018,"title_abstract":"Learning to Listen: Critically Considering the Role of {AI} in Human Storytelling and Character Creation In this opinion piece, we argue that there is a need for alternative design directions to complement existing AI efforts in narrative and character generation and algorithm development. To make our argument, we a) outline the predominant roles and goals of AI research in storytelling; b) present existing discourse on the benefits and harms of narratives; and c) highlight the pain points in character creation revealed by semi-structured interviews we conducted with 14 individuals deeply involved in some form of character creation. We conclude by proffering several specific design avenues that we believe can seed fruitful research collaborations. In our vision, AI collaborates with humans during creative processes and narrative generation, helps amplify voices and perspectives that are currently marginalized or misrepresented, and engenders experiences of narrative that support spectatorship and listening roles.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.154317677,"Goal":"Sustainable Cities and Communities","Task":["Human Storytelling","Character Creation","AI","narrative and character generation","algorithm development","AI","storytelling;","character creation","character creation","creative processes","narrative generation"],"Method":["{AI}","AI"]},{"ID":"alonso-etal-2017-linguistic","title":"Linguistic Description of Complex Phenomena with the r{LDCP} {R} Package","abstract":"Monitoring and analysis of complex phenomena attract the attention of both academy and industry. Dealing with data produced by complex phenomena requires the use of advance computational intelligence techniques. Namely, linguistic description of complex phenomena constitutes a mature research line. It is supported by the Computational Theory of Perceptions grounded on the Fuzzy Sets Theory. Its aim is the development of computational systems with the ability to generate vague descriptions of the world in a similar way how humans do. This is a human-centric and multi-disciplinary research work. Moreover, its success is a matter of careful design; thus, developers play a key role. The rLDCP R package was designed to facilitate the development of new applications. This demo introduces the use of rLDCP, for both beginners and advance developers, in practical use cases.","year":2017,"title_abstract":"Linguistic Description of Complex Phenomena with the r{LDCP} {R} Package Monitoring and analysis of complex phenomena attract the attention of both academy and industry. Dealing with data produced by complex phenomena requires the use of advance computational intelligence techniques. Namely, linguistic description of complex phenomena constitutes a mature research line. It is supported by the Computational Theory of Perceptions grounded on the Fuzzy Sets Theory. Its aim is the development of computational systems with the ability to generate vague descriptions of the world in a similar way how humans do. This is a human-centric and multi-disciplinary research work. Moreover, its success is a matter of careful design; thus, developers play a key role. The rLDCP R package was designed to facilitate the development of new applications. This demo introduces the use of rLDCP, for both beginners and advance developers, in practical use cases.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1542757004,"Goal":"Sustainable Cities and Communities","Task":["Linguistic Description of Complex Phenomena","Monitoring and analysis of complex phenomena","linguistic description of complex phenomena"],"Method":["computational intelligence techniques","Computational Theory of Perceptions","Fuzzy Sets Theory","computational systems","rLDCP R package","rLDCP"]},{"ID":"an-etal-2022-learning","title":"Learning Bias-reduced Word Embeddings Using Dictionary Definitions","abstract":"Pre-trained word embeddings, such as GloVe, have shown undesirable gender, racial, and religious biases. To address this problem, we propose DD-GloVe, a train-time debiasing algorithm to learn word embeddings by leveraging $\\underline{d}$ictionary $\\underline{d}$efinitions. We introduce dictionary-guided loss functions that encourage word embeddings to be similar to their relatively neutral dictionary definition representations. Existing debiasing algorithms typically need a pre-compiled list of seed words to represent the bias direction, along which biased information gets removed. Producing this list involves subjective decisions and it might be difficult to obtain for some types of biases. We automate the process of finding seed words: our algorithm starts from a single pair of initial seed words and automatically finds more words whose definitions display similar attributes traits. We demonstrate the effectiveness of our approach with benchmark evaluations and empirical analyses. Our code is available at https:\/\/github.com\/haozhe-an\/DD-GloVe.","year":2022,"title_abstract":"Learning Bias-reduced Word Embeddings Using Dictionary Definitions Pre-trained word embeddings, such as GloVe, have shown undesirable gender, racial, and religious biases. To address this problem, we propose DD-GloVe, a train-time debiasing algorithm to learn word embeddings by leveraging $\\underline{d}$ictionary $\\underline{d}$efinitions. We introduce dictionary-guided loss functions that encourage word embeddings to be similar to their relatively neutral dictionary definition representations. Existing debiasing algorithms typically need a pre-compiled list of seed words to represent the bias direction, along which biased information gets removed. Producing this list involves subjective decisions and it might be difficult to obtain for some types of biases. We automate the process of finding seed words: our algorithm starts from a single pair of initial seed words and automatically finds more words whose definitions display similar attributes traits. We demonstrate the effectiveness of our approach with benchmark evaluations and empirical analyses. Our code is available at https:\/\/github.com\/haozhe-an\/DD-GloVe.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1542565972,"Goal":"Gender Equality","Task":["Learning Bias - reduced Word Embeddings","word embeddings"],"Method":["Dictionary Definitions","word embeddings","GloVe","DD - GloVe","train - time debiasing algorithm","dictionary - guided loss functions","neutral dictionary definition representations","debiasing algorithms"]},{"ID":"janzso-2021-disambiguating-grammatical","title":"Disambiguating Grammatical Number and Gender With {BERT}","abstract":"Accurately dealing with any type of ambiguity is a major task in Natural Language Processing, with great advances recently reached due to the development of context dependent language models and the use of word or sentence embeddings. In this context, our work aimed at determining how the popular language representation model BERT handle ambiguity of nouns in grammatical number and gender in different languages. We show that models trained on one specific language achieve better results for the disambiguation process than multilingual models. Also, ambiguity is generally better dealt with in grammatical number than it is in grammatical gender, reaching greater distance values from one to another in direct comparisons of individual senses. The overall results show also that the amount of data needed for training monolingual models as well as application should not be underestimated.","year":2021,"title_abstract":"Disambiguating Grammatical Number and Gender With {BERT} Accurately dealing with any type of ambiguity is a major task in Natural Language Processing, with great advances recently reached due to the development of context dependent language models and the use of word or sentence embeddings. In this context, our work aimed at determining how the popular language representation model BERT handle ambiguity of nouns in grammatical number and gender in different languages. We show that models trained on one specific language achieve better results for the disambiguation process than multilingual models. Also, ambiguity is generally better dealt with in grammatical number than it is in grammatical gender, reaching greater distance values from one to another in direct comparisons of individual senses. The overall results show also that the amount of data needed for training monolingual models as well as application should not be underestimated.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.154248789,"Goal":"Gender Equality","Task":["Disambiguating Grammatical Number","Natural Language Processing","disambiguation process","grammatical number"],"Method":["context dependent language models","word or sentence embeddings","language representation model","multilingual models","monolingual models"]},{"ID":"shen-feng-2020-cdl","title":"{CDL}: Curriculum Dual Learning for Emotion-Controllable Response Generation","abstract":"Emotion-controllable response generation is an attractive and valuable task that aims to make open-domain conversations more empathetic and engaging. Existing methods mainly enhance the emotion expression by adding regularization terms to standard cross-entropy loss and thus influence the training process. However, due to the lack of further consideration of content consistency, the common problem of response generation tasks, safe response, is intensified. Besides, query emotions that can help model the relationship between query and response are simply ignored in previous models, which would further hurt the coherence. To alleviate these problems, we propose a novel framework named Curriculum Dual Learning (CDL) which extends the emotion-controllable response generation to a dual task to generate emotional responses and emotional queries alternatively. CDL utilizes two rewards focusing on emotion and content to improve the duality. Additionally, it applies curriculum learning to gradually generate high-quality responses based on the difficulties of expressing various emotions. Experimental results show that CDL significantly outperforms the baselines in terms of coherence, diversity, and relation to emotion factors.","year":2020,"title_abstract":"{CDL}: Curriculum Dual Learning for Emotion-Controllable Response Generation Emotion-controllable response generation is an attractive and valuable task that aims to make open-domain conversations more empathetic and engaging. Existing methods mainly enhance the emotion expression by adding regularization terms to standard cross-entropy loss and thus influence the training process. However, due to the lack of further consideration of content consistency, the common problem of response generation tasks, safe response, is intensified. Besides, query emotions that can help model the relationship between query and response are simply ignored in previous models, which would further hurt the coherence. To alleviate these problems, we propose a novel framework named Curriculum Dual Learning (CDL) which extends the emotion-controllable response generation to a dual task to generate emotional responses and emotional queries alternatively. CDL utilizes two rewards focusing on emotion and content to improve the duality. Additionally, it applies curriculum learning to gradually generate high-quality responses based on the difficulties of expressing various emotions. Experimental results show that CDL significantly outperforms the baselines in terms of coherence, diversity, and relation to emotion factors.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.15422225,"Goal":"Quality Education","Task":["Emotion - Controllable Response Generation","Emotion - controllable response generation","training process","response generation tasks","safe response","emotion - controllable response generation","dual task"],"Method":["Curriculum Dual Learning","Curriculum Dual Learning","CDL","curriculum learning","CDL"]},{"ID":"rei-etal-2021-references","title":"Are References Really Needed? Unbabel-{IST} 2021 Submission for the Metrics Shared Task","abstract":"In this paper, we present the joint contribution of Unbabel and IST to the WMT 2021 Metrics Shared Task. With this year{'}s focus on Multidimensional Quality Metric (MQM) as the ground-truth human assessment, our aim was to steer COMET towards higher correlations with MQM. We do so by first pre-training on Direct Assessments and then fine-tuning on z-normalized MQM scores. In our experiments we also show that reference-free COMET models are becoming competitive with reference-based models, even outperforming the best COMET model from 2020 on this year{'}s development data. Additionally, we present COMETinho, a lightweight COMET model that is 19x faster on CPU than the original model, while also achieving state-of-the-art correlations with MQM. Finally, in the {``}QE as a metric{''} track, we also participated with a QE model trained using the OpenKiwi framework leveraging MQM scores and word-level annotations.","year":2021,"title_abstract":"Are References Really Needed? Unbabel-{IST} 2021 Submission for the Metrics Shared Task In this paper, we present the joint contribution of Unbabel and IST to the WMT 2021 Metrics Shared Task. With this year{'}s focus on Multidimensional Quality Metric (MQM) as the ground-truth human assessment, our aim was to steer COMET towards higher correlations with MQM. We do so by first pre-training on Direct Assessments and then fine-tuning on z-normalized MQM scores. In our experiments we also show that reference-free COMET models are becoming competitive with reference-based models, even outperforming the best COMET model from 2020 on this year{'}s development data. Additionally, we present COMETinho, a lightweight COMET model that is 19x faster on CPU than the original model, while also achieving state-of-the-art correlations with MQM. Finally, in the {``}QE as a metric{''} track, we also participated with a QE model trained using the OpenKiwi framework leveraging MQM scores and word-level annotations.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1541764885,"Goal":"Quality Education","Task":["Metrics Shared Task"],"Method":["Unbabel","IST","COMET","reference - free COMET models","reference - based models","COMET model","COMETinho","COMET model","QE model","OpenKiwi framework","MQM","word - level annotations"]},{"ID":"abdul-mageed-etal-2021-dialex","title":"{D}ia{L}ex: A Benchmark for Evaluating Multidialectal {A}rabic Word Embeddings","abstract":"Word embeddings are a core component of modern natural language processing systems, making the ability to thoroughly evaluate them a vital task. We describe DiaLex, a benchmark for intrinsic evaluation of dialectal Arabic word embeddings. DiaLex covers five important Arabic dialects: Algerian, Egyptian, Lebanese, Syrian, and Tunisian. Across these dialects, DiaLex provides a testbank for six syntactic and semantic relations, namely male to female, singular to dual, singular to plural, antonym, comparative, and genitive to past tense. DiaLex thus consists of a collection of word pairs representing each of the six relations in each of the five dialects. To demonstrate the utility of DiaLex, we use it to evaluate a set of existing and new Arabic word embeddings that we developed. Beyond evaluation of word embeddings, DiaLex supports efforts to integrate dialects into the Arabic language curriculum. It can be easily translated into Modern Standard Arabic and English, which can be useful for evaluating word translation. Our benchmark, evaluation code, and new word embedding models will be publicly available.","year":2021,"title_abstract":"{D}ia{L}ex: A Benchmark for Evaluating Multidialectal {A}rabic Word Embeddings Word embeddings are a core component of modern natural language processing systems, making the ability to thoroughly evaluate them a vital task. We describe DiaLex, a benchmark for intrinsic evaluation of dialectal Arabic word embeddings. DiaLex covers five important Arabic dialects: Algerian, Egyptian, Lebanese, Syrian, and Tunisian. Across these dialects, DiaLex provides a testbank for six syntactic and semantic relations, namely male to female, singular to dual, singular to plural, antonym, comparative, and genitive to past tense. DiaLex thus consists of a collection of word pairs representing each of the six relations in each of the five dialects. To demonstrate the utility of DiaLex, we use it to evaluate a set of existing and new Arabic word embeddings that we developed. Beyond evaluation of word embeddings, DiaLex supports efforts to integrate dialects into the Arabic language curriculum. It can be easily translated into Modern Standard Arabic and English, which can be useful for evaluating word translation. Our benchmark, evaluation code, and new word embedding models will be publicly available.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1541662961,"Goal":"Gender Equality","Task":["Multidialectal {A}rabic Word Embeddings","Word embeddings","intrinsic evaluation of dialectal Arabic word embeddings","evaluation of word embeddings","word translation"],"Method":["natural language processing systems","DiaLex","DiaLex","DiaLex","DiaLex","DiaLex","Arabic word embeddings","DiaLex","word embedding models"]},{"ID":"zhang-etal-2020-sema","title":"{SEMA}: Text Simplification Evaluation through Semantic Alignment","abstract":"Text simplification is an important branch of natural language processing. At present, methods used to evaluate the semantic retention of text simplification are mostly based on string matching. We propose the SEMA (text Simplification Evaluation Measure through Semantic Alignment), which is based on semantic alignment. Semantic alignments include complete alignment, partial alignment and hyponymy alignment. Our experiments show that the evaluation results of SEMA have a high consistency with human evaluation for the simplified corpus of Chinese and English news texts.","year":2020,"title_abstract":"{SEMA}: Text Simplification Evaluation through Semantic Alignment Text simplification is an important branch of natural language processing. At present, methods used to evaluate the semantic retention of text simplification are mostly based on string matching. We propose the SEMA (text Simplification Evaluation Measure through Semantic Alignment), which is based on semantic alignment. Semantic alignments include complete alignment, partial alignment and hyponymy alignment. Our experiments show that the evaluation results of SEMA have a high consistency with human evaluation for the simplified corpus of Chinese and English news texts.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1541296989,"Goal":"Gender Equality","Task":["Text Simplification Evaluation","Semantic Alignment","Text simplification","natural language processing","semantic retention","text simplification","Semantic Alignment)","semantic alignment","Semantic alignments","complete alignment"],"Method":["string matching","SEMA","SEMA"]},{"ID":"wang-etal-2021-enhanced","title":"Enhanced {U}niversal {D}ependency Parsing with Automated Concatenation of Embeddings","abstract":"This paper describe the system used in our submission to the \\textit{IWPT 2021 Shared Task}. Our system is a graph-based parser with the technique of Automated Concatenation of Embeddings (ACE). Because recent work found that better word representations can be obtained by concatenating different types of embeddings, we use ACE to automatically find the better concatenation of embeddings for the task of enhanced universal dependencies. According to official results averaged on 17 languages, our system rank 2nd over 9 teams.","year":2021,"title_abstract":"Enhanced {U}niversal {D}ependency Parsing with Automated Concatenation of Embeddings This paper describe the system used in our submission to the \\textit{IWPT 2021 Shared Task}. Our system is a graph-based parser with the technique of Automated Concatenation of Embeddings (ACE). Because recent work found that better word representations can be obtained by concatenating different types of embeddings, we use ACE to automatically find the better concatenation of embeddings for the task of enhanced universal dependencies. According to official results averaged on 17 languages, our system rank 2nd over 9 teams.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1540032625,"Goal":"Partnership for the Goals","Task":["enhanced universal dependencies"],"Method":["Enhanced {U}niversal {D}ependency Parsing","Automated Concatenation of Embeddings","graph - based parser","Automated Concatenation of Embeddings","word representations","ACE"]},{"ID":"anselma-mazzei-2018-designing","title":"Designing and testing the messages produced by a virtual dietitian","abstract":"This paper presents a project about the automatic generation of persuasive messages in the context of the diet management. In the first part of the paper we introduce the basic mechanisms related to data interpretation and content selection for a numerical data-to-text generation architecture. In the second part of the paper we discuss a number of factors influencing the design of the messages. In particular, we consider the design of the aggregation procedure. Finally, we present the results of a human-based evaluation concerning this design factor.","year":2018,"title_abstract":"Designing and testing the messages produced by a virtual dietitian This paper presents a project about the automatic generation of persuasive messages in the context of the diet management. In the first part of the paper we introduce the basic mechanisms related to data interpretation and content selection for a numerical data-to-text generation architecture. In the second part of the paper we discuss a number of factors influencing the design of the messages. In particular, we consider the design of the aggregation procedure. Finally, we present the results of a human-based evaluation concerning this design factor.","social_need":"Responsible Consumption and Production Ensure sustainable consumption and production patterns","cosine_similarity":0.153995201,"Goal":"Responsible Consumption and Production","Task":["automatic generation of persuasive messages","diet management","data interpretation","content selection","numerical data - to - text generation architecture"],"Method":["virtual dietitian","aggregation procedure"]},{"ID":"son-etal-2018-causal","title":"Causal Explanation Analysis on Social Media","abstract":"Understanding causal explanations - reasons given for happenings in one{'}s life - has been found to be an important psychological factor linked to physical and mental health. Causal explanations are often studied through manual identification of phrases over limited samples of personal writing. Automatic identification of causal explanations in social media, while challenging in relying on contextual and sequential cues, offers a larger-scale alternative to expensive manual ratings and opens the door for new applications (e.g. studying prevailing beliefs about causes, such as climate change). Here, we explore automating causal explanation analysis, building on discourse parsing, and presenting two novel subtasks: causality detection (determining whether a causal explanation exists at all) and causal explanation identification (identifying the specific phrase that is the explanation). We achieve strong accuracies for both tasks but find different approaches best: an SVM for causality prediction (F1 = 0.791) and a hierarchy of Bidirectional LSTMs for causal explanation identification (F1 = 0.853). Finally, we explore applications of our complete pipeline (F1 = 0.868), showing demographic differences in mentions of causal explanation and that the association between a word and sentiment can change when it is used within a causal explanation.","year":2018,"title_abstract":"Causal Explanation Analysis on Social Media Understanding causal explanations - reasons given for happenings in one{'}s life - has been found to be an important psychological factor linked to physical and mental health. Causal explanations are often studied through manual identification of phrases over limited samples of personal writing. Automatic identification of causal explanations in social media, while challenging in relying on contextual and sequential cues, offers a larger-scale alternative to expensive manual ratings and opens the door for new applications (e.g. studying prevailing beliefs about causes, such as climate change). Here, we explore automating causal explanation analysis, building on discourse parsing, and presenting two novel subtasks: causality detection (determining whether a causal explanation exists at all) and causal explanation identification (identifying the specific phrase that is the explanation). We achieve strong accuracies for both tasks but find different approaches best: an SVM for causality prediction (F1 = 0.791) and a hierarchy of Bidirectional LSTMs for causal explanation identification (F1 = 0.853). Finally, we explore applications of our complete pipeline (F1 = 0.868), showing demographic differences in mentions of causal explanation and that the association between a word and sentiment can change when it is used within a causal explanation.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.15392524,"Goal":"Climate Action","Task":["Causal Explanation Analysis","Understanding causal explanations","physical and mental health","Causal explanations","manual identification of phrases","Automatic identification of causal explanations","manual ratings","automating causal explanation analysis","discourse parsing","causality detection","causal explanation identification","causality prediction","causal explanation identification"],"Method":["SVM","Bidirectional LSTMs","causal explanation"]},{"ID":"tablan-etal-2006-user","title":"User-friendly ontology authoring using a controlled language","abstract":"In recent years, following the rapid development in the Semantic Web and Knowledge Management research, ontologies have become more in demand in Natural Language Processing. An increasing number of systems use ontologies either internally, for modelling the domain of the application, or as data structures that hold the output resulting from the work of the system, in the form of knowledge bases. While there are many ontology editing tools aimed at expert users, there are very few which are accessible to users wishing to create simple structures without delving into the intricacies of knowledge representation languages. The approach described in this paper allows users to create and edit ontologies simply by using a restricted version of the English language. The controlled language described within is based on an open vocabulary and a restricted set of grammatical constructs. Sentences written in this language unambiguously map into a number of knowledge representation formats including OWL and RDF-S to allow round-trip ontology management.","year":2006,"title_abstract":"User-friendly ontology authoring using a controlled language In recent years, following the rapid development in the Semantic Web and Knowledge Management research, ontologies have become more in demand in Natural Language Processing. An increasing number of systems use ontologies either internally, for modelling the domain of the application, or as data structures that hold the output resulting from the work of the system, in the form of knowledge bases. While there are many ontology editing tools aimed at expert users, there are very few which are accessible to users wishing to create simple structures without delving into the intricacies of knowledge representation languages. The approach described in this paper allows users to create and edit ontologies simply by using a restricted version of the English language. The controlled language described within is based on an open vocabulary and a restricted set of grammatical constructs. Sentences written in this language unambiguously map into a number of knowledge representation formats including OWL and RDF-S to allow round-trip ontology management.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.1539167315,"Goal":"Life Below Water","Task":["User - friendly ontology authoring","Semantic Web","Knowledge Management research","Natural Language Processing","round - trip ontology management"],"Method":["ontology editing tools","knowledge representation languages","knowledge representation formats","OWL","RDF - S"]},{"ID":"rosso-2020-profiling","title":"Profiling Bots, Fake News Spreaders and Haters","abstract":"Author profiling studies how language is shared by people. Stylometry techniques help in identifying aspects such as gender, age, native language, or even personality. Author profiling is a problem of growing importance, not only in marketing and forensics, but also in cybersecurity. The aim is not only to identify users whose messages are potential threats from a terrorism viewpoint but also those whose messages are a threat from a social exclusion perspective because containing hate speech, cyberbullying etc. Bots often play a key role in spreading hate speech, as well as fake news, with the purpose of polarizing the public opinion with respect to controversial issues like Brexit or the Catalan referendum. For instance, the authors of a recent study about the 1 Oct 2017 Catalan referendum, showed that in a dataset with 3.6 million tweets, about 23.6{\\%} of tweets were produced by bots. The target of these bots were pro-independence influencers that were sent negative, emotional and aggressive hateful tweets with hashtags such as {\\#}sonunesbesties (i.e. {\\#}theyareanimals). Since 2013 at the PAN Lab at CLEF (https:\/\/pan.webis.de\/) we have addressed several aspects of author profiling in social media. In 2019 we investigated the feasibility of distinguishing whether the author of a Twitter feed is a bot, while this year we are addressing the problem of profiling those authors that are more likely to spread fake news in Twitter because they did in the past. We aim at identifying possible fake news spreaders as a first step towards preventing fake news from being propagated among online users (fake news aim to polarize the public opinion and may contain hate speech). In 2021 we specifically aim at addressing the challenging problem of profiling haters in social media in order to monitor abusive language and prevent cases of social exclusion in order to combat, for instance, racism, xenophobia and misogyny. Although we already started addressing the problem of detecting hate speech when targets are immigrants or women at the HatEval shared task in SemEval-2019, and when targets are women also in the Automatic Misogyny Identification tasks at IberEval-2018, Evalita-2018 and Evalita-2020, it was not done from an author profiling perspective. At the end of the keynote, I will present some insights in order to stress the importance of monitoring abusive language in social media, for instance, in foreseeing sexual crimes. In fact, previous studies confirmed that a correlation might lay between the yearly per capita rate of rape and the misogynistic language used in Twitter.","year":2020,"title_abstract":"Profiling Bots, Fake News Spreaders and Haters Author profiling studies how language is shared by people. Stylometry techniques help in identifying aspects such as gender, age, native language, or even personality. Author profiling is a problem of growing importance, not only in marketing and forensics, but also in cybersecurity. The aim is not only to identify users whose messages are potential threats from a terrorism viewpoint but also those whose messages are a threat from a social exclusion perspective because containing hate speech, cyberbullying etc. Bots often play a key role in spreading hate speech, as well as fake news, with the purpose of polarizing the public opinion with respect to controversial issues like Brexit or the Catalan referendum. For instance, the authors of a recent study about the 1 Oct 2017 Catalan referendum, showed that in a dataset with 3.6 million tweets, about 23.6{\\%} of tweets were produced by bots. The target of these bots were pro-independence influencers that were sent negative, emotional and aggressive hateful tweets with hashtags such as {\\#}sonunesbesties (i.e. {\\#}theyareanimals). Since 2013 at the PAN Lab at CLEF (https:\/\/pan.webis.de\/) we have addressed several aspects of author profiling in social media. In 2019 we investigated the feasibility of distinguishing whether the author of a Twitter feed is a bot, while this year we are addressing the problem of profiling those authors that are more likely to spread fake news in Twitter because they did in the past. We aim at identifying possible fake news spreaders as a first step towards preventing fake news from being propagated among online users (fake news aim to polarize the public opinion and may contain hate speech). In 2021 we specifically aim at addressing the challenging problem of profiling haters in social media in order to monitor abusive language and prevent cases of social exclusion in order to combat, for instance, racism, xenophobia and misogyny. Although we already started addressing the problem of detecting hate speech when targets are immigrants or women at the HatEval shared task in SemEval-2019, and when targets are women also in the Automatic Misogyny Identification tasks at IberEval-2018, Evalita-2018 and Evalita-2020, it was not done from an author profiling perspective. At the end of the keynote, I will present some insights in order to stress the importance of monitoring abusive language in social media, for instance, in foreseeing sexual crimes. In fact, previous studies confirmed that a correlation might lay between the yearly per capita rate of rape and the misogynistic language used in Twitter.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1538847387,"Goal":"Climate Action","Task":["Profiling Bots","Fake News Spreaders","Author profiling","Author profiling","marketing","forensics","cybersecurity","spreading hate speech","author profiling","HatEval shared task","Automatic Misogyny Identification tasks","foreseeing sexual crimes"],"Method":["Stylometry techniques","Bots","author profiling perspective"]},{"ID":"xu-etal-2022-joint","title":"Joint Generation of Captions and Subtitles with Dual Decoding","abstract":"As the amount of audio-visual content increases, the need to develop automatic captioning and subtitling solutions to match the expectations of a growing international audience appears as the only viable way to boost throughput and lower the related post-production costs. Automatic captioning and subtitling often need to be tightly intertwined to achieve an appropriate level of consistency and synchronization with each other and with the video signal. In this work, we assess a dual decoding scheme to achieve a strong coupling between these two tasks and show how adequacy and consistency are increased, with virtually no additional cost in terms of model size and training complexity.","year":2022,"title_abstract":"Joint Generation of Captions and Subtitles with Dual Decoding As the amount of audio-visual content increases, the need to develop automatic captioning and subtitling solutions to match the expectations of a growing international audience appears as the only viable way to boost throughput and lower the related post-production costs. Automatic captioning and subtitling often need to be tightly intertwined to achieve an appropriate level of consistency and synchronization with each other and with the video signal. In this work, we assess a dual decoding scheme to achieve a strong coupling between these two tasks and show how adequacy and consistency are increased, with virtually no additional cost in terms of model size and training complexity.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1538237184,"Goal":"Partnership for the Goals","Task":["Joint Generation of Captions and Subtitles","automatic captioning and subtitling solutions","Automatic captioning","subtitling"],"Method":["Dual Decoding","dual decoding scheme"]},{"ID":"kolachina-etal-2012-evaluation","title":"Evaluation of Discourse Relation Annotation in the {H}indi Discourse Relation Bank","abstract":"We describe our experiments on evaluating recently proposed modifications to the discourse relation annotation scheme of the Penn Discourse Treebank (PDTB), in the context of annotating discourse relations in Hindi Discourse Relation Bank (HDRB). While the proposed modifications were driven by the desire to introduce greater conceptual clarity in the PDTB scheme and to facilitate better annotation quality, our findings indicate that overall, some of the changes render the annotation task much more difficult for the annotators, as also reflected in lower inter-annotator agreement for the relevant sub-tasks. Our study emphasizes the importance of best practices in annotation task design and guidelines, given that a major goal of an annotation effort should be to achieve maximally high agreement between annotators. Based on our study, we suggest modifications to the current version of the HDRB, to be incorporated in our future annotation work.","year":2012,"title_abstract":"Evaluation of Discourse Relation Annotation in the {H}indi Discourse Relation Bank We describe our experiments on evaluating recently proposed modifications to the discourse relation annotation scheme of the Penn Discourse Treebank (PDTB), in the context of annotating discourse relations in Hindi Discourse Relation Bank (HDRB). While the proposed modifications were driven by the desire to introduce greater conceptual clarity in the PDTB scheme and to facilitate better annotation quality, our findings indicate that overall, some of the changes render the annotation task much more difficult for the annotators, as also reflected in lower inter-annotator agreement for the relevant sub-tasks. Our study emphasizes the importance of best practices in annotation task design and guidelines, given that a major goal of an annotation effort should be to achieve maximally high agreement between annotators. Based on our study, we suggest modifications to the current version of the HDRB, to be incorporated in our future annotation work.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1536991596,"Goal":"Partnership for the Goals","Task":["Discourse Relation Annotation","annotating discourse relations","annotation task","annotation task design","guidelines","annotation","annotation"],"Method":["discourse relation annotation scheme","PDTB scheme","HDRB"]},{"ID":"bai-etal-2021-whose-heritage","title":"{WHOS}e {H}eritage: {C}lassification of {UNESCO} {W}orld {H}eritage Statements of ''{O}utstanding {U}niversal {V}alue{''} with Soft Labels","abstract":"The UNESCO World Heritage List (WHL) includes the exceptionally valuable cultural and natural heritage to be preserved for mankind. Evaluating and justifying the Outstanding Universal Value (OUV) is essential for each site inscribed in the WHL, and yet a complex task, even for experts, since the selection criteria of OUV are not mutually exclusive. Furthermore, manual annotation of heritage values and attributes from multi-source textual data, which is currently dominant in heritage studies, is knowledge-demanding and time-consuming, impeding systematic analysis of such authoritative documents in terms of their implications on heritage management. This study applies state-of-the-art NLP models to build a classifier on a new dataset containing Statements of OUV, seeking an explainable and scalable automation tool to facilitate the nomination, evaluation, research, and monitoring processes of World Heritage sites. Label smoothing is innovatively adapted to improve the model performance by adding prior inter-class relationship knowledge to generate soft labels. The study shows that the best models fine-tuned from BERT and ULMFiT can reach 94.3{\\%} top-3 accuracy. A human study with expert evaluation on the model prediction shows that the models are sufficiently generalizable. The study is promising to be further developed and applied in heritage research and practice.","year":2021,"title_abstract":"{WHOS}e {H}eritage: {C}lassification of {UNESCO} {W}orld {H}eritage Statements of ''{O}utstanding {U}niversal {V}alue{''} with Soft Labels The UNESCO World Heritage List (WHL) includes the exceptionally valuable cultural and natural heritage to be preserved for mankind. Evaluating and justifying the Outstanding Universal Value (OUV) is essential for each site inscribed in the WHL, and yet a complex task, even for experts, since the selection criteria of OUV are not mutually exclusive. Furthermore, manual annotation of heritage values and attributes from multi-source textual data, which is currently dominant in heritage studies, is knowledge-demanding and time-consuming, impeding systematic analysis of such authoritative documents in terms of their implications on heritage management. This study applies state-of-the-art NLP models to build a classifier on a new dataset containing Statements of OUV, seeking an explainable and scalable automation tool to facilitate the nomination, evaluation, research, and monitoring processes of World Heritage sites. Label smoothing is innovatively adapted to improve the model performance by adding prior inter-class relationship knowledge to generate soft labels. The study shows that the best models fine-tuned from BERT and ULMFiT can reach 94.3{\\%} top-3 accuracy. A human study with expert evaluation on the model prediction shows that the models are sufficiently generalizable. The study is promising to be further developed and applied in heritage research and practice.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1536744535,"Goal":"Life on Land","Task":["manual annotation","heritage studies","heritage management","nomination","evaluation","research , and monitoring processes of World Heritage sites","model prediction","heritage research and practice"],"Method":["OUV","NLP models","classifier","automation tool","Label smoothing","BERT","ULMFiT"]},{"ID":"ahmad-etal-2019-multi","title":"Multi-linguality helps: Event-Argument Extraction for Disaster Domain in Cross-lingual and Multi-lingual setting","abstract":"Automatic extraction of disaster-related events and their arguments from natural language text is vital for building a decision support system for crisis management. Event extraction from various news sources is a well-explored area for this objective. However, extracting events alone, without any context, provides only partial help for this purpose. Extracting related arguments like Time, Place, Casualties, etc., provides a complete picture of the disaster event. In this paper, we create a disaster domain dataset in Hindi by annotating disaster-related event and arguments. We also obtain equivalent datasets for Bengali and English from a collaboration. We build a multi-lingual deep learning model for argument extraction in all the three languages. We also compare our multi-lingual system with a similar baseline mono-lingual system trained for each language separately. It is observed that a single multi-lingual system is able to compensate for lack of training data, by using joint training of dataset from different languages in shared space, thus giving a better overall result.","year":2019,"title_abstract":"Multi-linguality helps: Event-Argument Extraction for Disaster Domain in Cross-lingual and Multi-lingual setting Automatic extraction of disaster-related events and their arguments from natural language text is vital for building a decision support system for crisis management. Event extraction from various news sources is a well-explored area for this objective. However, extracting events alone, without any context, provides only partial help for this purpose. Extracting related arguments like Time, Place, Casualties, etc., provides a complete picture of the disaster event. In this paper, we create a disaster domain dataset in Hindi by annotating disaster-related event and arguments. We also obtain equivalent datasets for Bengali and English from a collaboration. We build a multi-lingual deep learning model for argument extraction in all the three languages. We also compare our multi-lingual system with a similar baseline mono-lingual system trained for each language separately. It is observed that a single multi-lingual system is able to compensate for lack of training data, by using joint training of dataset from different languages in shared space, thus giving a better overall result.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1536560059,"Goal":"Climate Action","Task":["Event - Argument Extraction","Disaster Domain","Cross - lingual and Multi - lingual setting","Automatic extraction of disaster - related events","decision support system","crisis management","Event extraction","argument extraction"],"Method":["multi - lingual deep learning model","multi - lingual system","baseline mono - lingual system","multi - lingual system"]},{"ID":"indurthi-etal-2019-fermi","title":"{FERMI} at {S}em{E}val-2019 Task 5: Using Sentence embeddings to Identify Hate Speech Against Immigrants and Women in {T}witter","abstract":"This paper describes our system (Fermi) for Task 5 of SemEval-2019: HatEval: Multilingual Detection of Hate Speech Against Immigrants and Women on Twitter. We participated in the subtask A for English and ranked first in the evaluation on the test set. We evaluate the quality of multiple sentence embeddings and explore multiple training models to evaluate the performance of simple yet effective embedding-ML combination algorithms. Our team - Fermi{'}s model achieved an accuracy of 65.00{\\%} for English language in task A. Our models, which use pretrained Universal Encoder sentence embeddings for transforming the input and SVM (with RBF kernel) for classification, scored first position (among 68) in the leaderboard on the test set for Subtask A in English language. In this paper we provide a detailed description of the approach, as well as the results obtained in the task.","year":2019,"title_abstract":"{FERMI} at {S}em{E}val-2019 Task 5: Using Sentence embeddings to Identify Hate Speech Against Immigrants and Women in {T}witter This paper describes our system (Fermi) for Task 5 of SemEval-2019: HatEval: Multilingual Detection of Hate Speech Against Immigrants and Women on Twitter. We participated in the subtask A for English and ranked first in the evaluation on the test set. We evaluate the quality of multiple sentence embeddings and explore multiple training models to evaluate the performance of simple yet effective embedding-ML combination algorithms. Our team - Fermi{'}s model achieved an accuracy of 65.00{\\%} for English language in task A. Our models, which use pretrained Universal Encoder sentence embeddings for transforming the input and SVM (with RBF kernel) for classification, scored first position (among 68) in the leaderboard on the test set for Subtask A in English language. In this paper we provide a detailed description of the approach, as well as the results obtained in the task.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1535238326,"Goal":"Gender Equality","Task":["HatEval","Multilingual Detection of Hate Speech","classification"],"Method":["Sentence embeddings","training models","embedding - ML combination algorithms","team - Fermi{'}s model","pretrained Universal Encoder sentence embeddings","SVM (with RBF kernel)"]},{"ID":"garain-etal-2020-junlp","title":"{JUNLP} at {S}em{E}val-2020 Task 9: Sentiment Analysis of {H}indi-{E}nglish Code Mixed Data Using Grid Search Cross Validation","abstract":"Code-mixing is a phenomenon which arises mainly in multilingual societies. Multilingual people, who are well versed in their native languages and also English speakers, tend to code-mix using English-based phonetic typing and the insertion of anglicisms in their main language. This linguistic phenomenon poses a great challenge to conventional NLP domains such as Sentiment Analysis, Machine Translation, and Text Summarization, to name a few. In this work, we focus on working out a plausible solution to the domain of Code-Mixed Sentiment Analysis. This work was done as participation in the SemEval-2020 Sentimix Task, where we focused on the sentiment analysis of English-Hindi code-mixed sentences. our username for the submission was {``}sainik.mahata{''} and team name was {``}JUNLP{''}. We used feature extraction algorithms in conjunction with traditional machine learning algorithms such as SVR and Grid Search in an attempt to solve the task. Our approach garnered an f1-score of 66.2{\\%} when tested using metrics prepared by the organizers of the task.","year":2020,"title_abstract":"{JUNLP} at {S}em{E}val-2020 Task 9: Sentiment Analysis of {H}indi-{E}nglish Code Mixed Data Using Grid Search Cross Validation Code-mixing is a phenomenon which arises mainly in multilingual societies. Multilingual people, who are well versed in their native languages and also English speakers, tend to code-mix using English-based phonetic typing and the insertion of anglicisms in their main language. This linguistic phenomenon poses a great challenge to conventional NLP domains such as Sentiment Analysis, Machine Translation, and Text Summarization, to name a few. In this work, we focus on working out a plausible solution to the domain of Code-Mixed Sentiment Analysis. This work was done as participation in the SemEval-2020 Sentimix Task, where we focused on the sentiment analysis of English-Hindi code-mixed sentences. our username for the submission was {``}sainik.mahata{''} and team name was {``}JUNLP{''}. We used feature extraction algorithms in conjunction with traditional machine learning algorithms such as SVR and Grid Search in an attempt to solve the task. Our approach garnered an f1-score of 66.2{\\%} when tested using metrics prepared by the organizers of the task.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1534965038,"Goal":"Gender Equality","Task":["Sentiment Analysis","Code - mixing","NLP domains","Sentiment Analysis","Machine Translation","Text Summarization","Code - Mixed Sentiment Analysis","Sentimix Task","sentiment analysis"],"Method":["Grid Search Cross Validation","English - based phonetic typing","feature extraction algorithms","machine learning algorithms","SVR","Grid Search"]},{"ID":"tsekouras-etal-2020-social","title":"Social Web Observatory: A Platform and Method for Gathering Knowledge on Entities from Different Textual Sources","abstract":"Within this work we describe a framework for the collection and summarization of information from the Web in an entity-driven manner. The framework consists of a set of appropriate workflows and the Social Web Observatory platform, which implements those workflows, supporting them through a language analysis pipeline. The pipeline includes text collection\/crawling, identification of different entities, clustering of texts into events related to entities, entity-centric sentiment analysis, but also text analytics and visualization functionalities. The latter allow the user to take advantage of the gathered information as actionable knowledge: to understand the dynamics of the public opinion for a given entity over time and across real-world events. We describe the platform and the analysis functionality and evaluate the performance of the system, by allowing human users to score how the system fares in its intended purpose of summarizing entity-centered information from different sources in the Web.","year":2020,"title_abstract":"Social Web Observatory: A Platform and Method for Gathering Knowledge on Entities from Different Textual Sources Within this work we describe a framework for the collection and summarization of information from the Web in an entity-driven manner. The framework consists of a set of appropriate workflows and the Social Web Observatory platform, which implements those workflows, supporting them through a language analysis pipeline. The pipeline includes text collection\/crawling, identification of different entities, clustering of texts into events related to entities, entity-centric sentiment analysis, but also text analytics and visualization functionalities. The latter allow the user to take advantage of the gathered information as actionable knowledge: to understand the dynamics of the public opinion for a given entity over time and across real-world events. We describe the platform and the analysis functionality and evaluate the performance of the system, by allowing human users to score how the system fares in its intended purpose of summarizing entity-centered information from different sources in the Web.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1534532905,"Goal":"Sustainable Cities and Communities","Task":["Social Web Observatory","collection and summarization of information","text collection\/crawling","identification of different entities","clustering of texts","entity - centric sentiment analysis","summarizing entity - centered information"],"Method":["Social Web Observatory platform","language analysis pipeline","text analytics","visualization functionalities","analysis functionality"]},{"ID":"zilio-etal-2020-lexical","title":"A Lexical Simplification Tool for Promoting Health Literacy","abstract":"This paper presents MedSimples, an authoring tool that combines Natural Language Processing, Corpus Linguistics and Terminology to help writers to convert health-related information into a more accessible version for people with low literacy skills. MedSimples applies parsing methods associated with lexical resources to automatically evaluate a text and present simplification suggestions that are more suitable for the target audience. Using the suggestions provided by the tool, the author can adapt the original text and make it more accessible. The focus of MedSimples lies on texts for special purposes, so that it not only deals with general vocabulary, but also with specialized terms. The tool is currently under development, but an online working prototype exists and can be tested freely. An assessment of MedSimples was carried out aiming at evaluating its current performance with some promising results, especially for informing the future developments that are planned for the tool.","year":2020,"title_abstract":"A Lexical Simplification Tool for Promoting Health Literacy This paper presents MedSimples, an authoring tool that combines Natural Language Processing, Corpus Linguistics and Terminology to help writers to convert health-related information into a more accessible version for people with low literacy skills. MedSimples applies parsing methods associated with lexical resources to automatically evaluate a text and present simplification suggestions that are more suitable for the target audience. Using the suggestions provided by the tool, the author can adapt the original text and make it more accessible. The focus of MedSimples lies on texts for special purposes, so that it not only deals with general vocabulary, but also with specialized terms. The tool is currently under development, but an online working prototype exists and can be tested freely. An assessment of MedSimples was carried out aiming at evaluating its current performance with some promising results, especially for informing the future developments that are planned for the tool.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.1534028351,"Goal":"Good Health and Well-Being","Task":["Promoting Health Literacy"],"Method":["Lexical Simplification Tool","MedSimples","authoring tool","Natural Language Processing","Corpus Linguistics","MedSimples","parsing methods","MedSimples","MedSimples"]},{"ID":"wang-etal-2021-qemind","title":"{QEM}ind: {A}libaba{'}s Submission to the {WMT}21 Quality Estimation Shared Task","abstract":"Quality Estimation, as a crucial step of quality control for machine translation, has been explored for years. The goal is to to investigate automatic methods for estimating the quality of machine translation results without reference translations. In this year{'}s WMT QE shared task, we utilize the large-scale XLM-Roberta pre-trained model and additionally propose several useful features to evaluate the uncertainty of the translations to build our QE system, named \\textit{ \\textbf{QEMind} }. The system has been applied to the sentence-level scoring task of Direct Assessment and the binary score prediction task of Critical Error Detection. In this paper, we present our submissions to the WMT 2021 QE shared task and an extensive set of experimental results have shown us that our multilingual systems outperform the best system in the Direct Assessment QE task of WMT 2020.","year":2021,"title_abstract":"{QEM}ind: {A}libaba{'}s Submission to the {WMT}21 Quality Estimation Shared Task Quality Estimation, as a crucial step of quality control for machine translation, has been explored for years. The goal is to to investigate automatic methods for estimating the quality of machine translation results without reference translations. In this year{'}s WMT QE shared task, we utilize the large-scale XLM-Roberta pre-trained model and additionally propose several useful features to evaluate the uncertainty of the translations to build our QE system, named \\textit{ \\textbf{QEMind} }. The system has been applied to the sentence-level scoring task of Direct Assessment and the binary score prediction task of Critical Error Detection. In this paper, we present our submissions to the WMT 2021 QE shared task and an extensive set of experimental results have shown us that our multilingual systems outperform the best system in the Direct Assessment QE task of WMT 2020.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1533774734,"Goal":"Quality Education","Task":["Quality Estimation Shared Task Quality Estimation","quality control","machine translation","translation","WMT QE shared task","sentence - level scoring task","Direct Assessment","binary score prediction task","Critical Error Detection","WMT 2021 QE shared task","Direct Assessment QE task","WMT 2020"],"Method":["automatic methods","XLM - Roberta pre - trained model","QE system","multilingual systems"]},{"ID":"dayrell-etal-2012-rhetorical","title":"Rhetorical Move Detection in {E}nglish Abstracts: Multi-label Sentence Classifiers and their Annotated Corpora","abstract":"The relevance of automatically identifying rhetorical moves in scientific texts has been widely acknowledged in the literature. This study focuses on abstracts of standard research papers written in English and aims to tackle a fundamental limitation of current machine-learning classifiers: they are mono-labeled, that is, a sentence can only be assigned one single label. However, such approach does not adequately reflect actual language use since a move can be realized by a clause, a sentence, or even several sentences. Here, we present MAZEA (Multi-label Argumentative Zoning for English Abstracts), a multi-label classifier which automatically identifies rhetorical moves in abstracts but allows for a given sentence to be assigned as many labels as appropriate. We have resorted to various other NLP tools and used two large training corpora: (i) one corpus consists of 645 abstracts from physical sciences and engineering (PE) and (ii) the other corpus is made up of 690 from life and health sciences (LH). This paper presents our preliminary results and also discusses the various challenges involved in multi-label tagging and works towards satisfactory solutions. In addition, we also make our two training corpora publicly available so that they may serve as benchmark for this new task.","year":2012,"title_abstract":"Rhetorical Move Detection in {E}nglish Abstracts: Multi-label Sentence Classifiers and their Annotated Corpora The relevance of automatically identifying rhetorical moves in scientific texts has been widely acknowledged in the literature. This study focuses on abstracts of standard research papers written in English and aims to tackle a fundamental limitation of current machine-learning classifiers: they are mono-labeled, that is, a sentence can only be assigned one single label. However, such approach does not adequately reflect actual language use since a move can be realized by a clause, a sentence, or even several sentences. Here, we present MAZEA (Multi-label Argumentative Zoning for English Abstracts), a multi-label classifier which automatically identifies rhetorical moves in abstracts but allows for a given sentence to be assigned as many labels as appropriate. We have resorted to various other NLP tools and used two large training corpora: (i) one corpus consists of 645 abstracts from physical sciences and engineering (PE) and (ii) the other corpus is made up of 690 from life and health sciences (LH). This paper presents our preliminary results and also discusses the various challenges involved in multi-label tagging and works towards satisfactory solutions. In addition, we also make our two training corpora publicly available so that they may serve as benchmark for this new task.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1533660889,"Goal":"Climate Action","Task":["Rhetorical Move Detection","automatically identifying rhetorical moves","multi - label tagging"],"Method":["Multi - label Sentence Classifiers","machine - learning classifiers","MAZEA (Multi - label Argumentative Zoning","multi - label classifier","NLP tools"]},{"ID":"r-kamath-etal-2020-exploration","title":"Exploration of Cross-lingual Summarization for {K}annada-{E}nglish{L}anguage Pair","abstract":"Cross-lingual summarization(CLS) is the process of generating a summary in one particular language for a source document in a different language. Low resource languages like Kannada greatly benefit from such systems because they help in delivering a concise representation of the same information in a different popular language. We propose a novel dataset generation pipeline and a first of its kind dataset that will aid in CLS for Kannada-English language pair. This work is also an attempt to inspect the existing systems and extend them to the Kannada-English language pair using our dataset.","year":2020,"title_abstract":"Exploration of Cross-lingual Summarization for {K}annada-{E}nglish{L}anguage Pair Cross-lingual summarization(CLS) is the process of generating a summary in one particular language for a source document in a different language. Low resource languages like Kannada greatly benefit from such systems because they help in delivering a concise representation of the same information in a different popular language. We propose a novel dataset generation pipeline and a first of its kind dataset that will aid in CLS for Kannada-English language pair. This work is also an attempt to inspect the existing systems and extend them to the Kannada-English language pair using our dataset.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.153351739,"Goal":"Partnership for the Goals","Task":["Cross - lingual Summarization","{K}annada - {E}nglish{L}anguage Pair","Cross - lingual summarization(CLS)","CLS"],"Method":["concise representation","dataset generation pipeline"]},{"ID":"federmann-2018-appraise","title":"Appraise Evaluation Framework for Machine Translation","abstract":"We present Appraise, an open-source framework for crowd-based annotation tasks, notably for evaluation of machine translation output. This is the software used to run the yearly evaluation campaigns for shared tasks at the WMT Conference on Machine Translation. It has also been used at IWSLT 2017 and, recently, to measure human parity for machine translation for Chinese to English news text. The demo will present the full end-to-end lifecycle of an Appraise evaluation campaign, from task creation to annotation and interpretation of results.","year":2018,"title_abstract":"Appraise Evaluation Framework for Machine Translation We present Appraise, an open-source framework for crowd-based annotation tasks, notably for evaluation of machine translation output. This is the software used to run the yearly evaluation campaigns for shared tasks at the WMT Conference on Machine Translation. It has also been used at IWSLT 2017 and, recently, to measure human parity for machine translation for Chinese to English news text. The demo will present the full end-to-end lifecycle of an Appraise evaluation campaign, from task creation to annotation and interpretation of results.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1533212364,"Goal":"Gender Equality","Task":["Machine Translation","crowd - based annotation tasks","evaluation of machine translation output","shared tasks","Machine Translation","machine translation","Appraise evaluation campaign","task creation","annotation","interpretation of results"],"Method":["Appraise Evaluation Framework","Appraise","open - source framework"]},{"ID":"ghanem-etal-2018-stance","title":"Stance Detection in Fake News A Combined Feature Representation","abstract":"With the uncontrolled increasing of fake news and rumors over the Web, different approaches have been proposed to address the problem. In this paper, we present an approach that combines lexical, word embeddings and n-gram features to detect the stance in fake news. Our approach has been tested on the Fake News Challenge (FNC-1) dataset. Given a news title-article pair, the FNC-1 task aims at determining the relevance of the article and the title. Our proposed approach has achieved an accurate result (59.6 {\\%} Macro F1) that is close to the state-of-the-art result with 0.013 difference using a simple feature representation. Furthermore, we have investigated the importance of different lexicons in the detection of the classification labels.","year":2018,"title_abstract":"Stance Detection in Fake News A Combined Feature Representation With the uncontrolled increasing of fake news and rumors over the Web, different approaches have been proposed to address the problem. In this paper, we present an approach that combines lexical, word embeddings and n-gram features to detect the stance in fake news. Our approach has been tested on the Fake News Challenge (FNC-1) dataset. Given a news title-article pair, the FNC-1 task aims at determining the relevance of the article and the title. Our proposed approach has achieved an accurate result (59.6 {\\%} Macro F1) that is close to the state-of-the-art result with 0.013 difference using a simple feature representation. Furthermore, we have investigated the importance of different lexicons in the detection of the classification labels.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1533149183,"Goal":"Climate Action","Task":["Stance Detection","detection of the classification labels"],"Method":["Feature Representation","n - gram features","feature representation"]},{"ID":"amoyal-etal-2020-paco","title":"{PACO}: a Corpus to Analyze the Impact of Common Ground in Spontaneous Face-to-Face Interaction","abstract":"PAC0 is a French audio-video conversational corpus made of 15 face-to-face dyadic interactions, lasting around 20 min each. This compared corpus has been created in order to explore the impact of the lack of personal common ground (Clark, 1996) on participants collaboration during conversation and specifically on their smile during topic transitions. We have constituted this conversational corpus '' PACO{''} by replicating the experimental protocol of {``}Cheese!{''} (Priego-valverde {\\&} al.,2018). The only difference that distinguishes these two corpora is the degree of CG of the interlocutors: in Cheese! interlocutors are friends, while in PACO they do not know each other. This experimental protocol allows to analyze how the participants are getting acquainted. This study brings two main contributions. First, the PACO conversational corpus enables to compare the impact of the interlocutors{'} common ground. Second, the semi-automatic smile annotation protocol allows to obtain reliable and reproducible smile annotations while reducing the annotation time by a factor 10. Keywords : Common ground, spontaneous interaction, smile, automatic detection.","year":2020,"title_abstract":"{PACO}: a Corpus to Analyze the Impact of Common Ground in Spontaneous Face-to-Face Interaction PAC0 is a French audio-video conversational corpus made of 15 face-to-face dyadic interactions, lasting around 20 min each. This compared corpus has been created in order to explore the impact of the lack of personal common ground (Clark, 1996) on participants collaboration during conversation and specifically on their smile during topic transitions. We have constituted this conversational corpus '' PACO{''} by replicating the experimental protocol of {``}Cheese!{''} (Priego-valverde {\\&} al.,2018). The only difference that distinguishes these two corpora is the degree of CG of the interlocutors: in Cheese! interlocutors are friends, while in PACO they do not know each other. This experimental protocol allows to analyze how the participants are getting acquainted. This study brings two main contributions. First, the PACO conversational corpus enables to compare the impact of the interlocutors{'} common ground. Second, the semi-automatic smile annotation protocol allows to obtain reliable and reproducible smile annotations while reducing the annotation time by a factor 10. Keywords : Common ground, spontaneous interaction, smile, automatic detection.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.15330562,"Goal":"Partnership for the Goals","Task":["Spontaneous Face - to - Face Interaction","spontaneous interaction","smile","automatic detection"],"Method":["semi - automatic smile annotation protocol"]},{"ID":"boekestein-etal-2006-functioning","title":"Functioning of the Centre for {D}utch Language and Speech Technology","abstract":"The TST Centre manages a broad collection of Dutch digital language resources. It is an initiative of the Dutch Language Union (Nederlandse Taalunie), and is meant to reinforce research in the area of language and speech technology. It does this by stimulating the reuse of these language resources. The TST Centre keeps these resources up to date, facilitates their availability, and offers services such as providing information, documentation, online access, offering catalogues, custom-made data, etc. Also, the TST Centre strives for a uniformised, if not standardised, treatment of language resources of the same nature. A well-thought, structured administration system is needed to manage the various language resources, their updates, derived products, IPR, user administration, etc. We will discuss the organisation, tasks and services of the TST Centre, and the language resources it maintains. Also, we will look into practical data management solutions, IPR issues, and our activities in standardisation and linking language resources.","year":2006,"title_abstract":"Functioning of the Centre for {D}utch Language and Speech Technology The TST Centre manages a broad collection of Dutch digital language resources. It is an initiative of the Dutch Language Union (Nederlandse Taalunie), and is meant to reinforce research in the area of language and speech technology. It does this by stimulating the reuse of these language resources. The TST Centre keeps these resources up to date, facilitates their availability, and offers services such as providing information, documentation, online access, offering catalogues, custom-made data, etc. Also, the TST Centre strives for a uniformised, if not standardised, treatment of language resources of the same nature. A well-thought, structured administration system is needed to manage the various language resources, their updates, derived products, IPR, user administration, etc. We will discuss the organisation, tasks and services of the TST Centre, and the language resources it maintains. Also, we will look into practical data management solutions, IPR issues, and our activities in standardisation and linking language resources.","social_need":"Clean Water and Sanitation Ensure availability and sustainable management of water and sanitation for all","cosine_similarity":0.1532984525,"Goal":"Clean Water and Sanitation","Task":["{D}utch Language and Speech Technology","language and speech technology","user administration","data management solutions","IPR issues","linking language resources"],"Method":["TST Centre","TST Centre","TST Centre","administration system","TST Centre"]},{"ID":"elgabou-kazakov-2017-building","title":"Building Dialectal {A}rabic Corpora","abstract":"The aim of this research is to identify local Arabic dialects in texts from social media (Twitter) and link them to specific geographic areas. Dialect identification is studied as a subset of the task of language identification. The proposed method is based on unsupervised learning using simultaneously lexical and geographic distance. While this study focusses on Libyan dialects, the approach is general, and could produce resources to support human translators and interpreters when dealing with vernaculars rather than standard Arabic.","year":2017,"title_abstract":"Building Dialectal {A}rabic Corpora The aim of this research is to identify local Arabic dialects in texts from social media (Twitter) and link them to specific geographic areas. Dialect identification is studied as a subset of the task of language identification. The proposed method is based on unsupervised learning using simultaneously lexical and geographic distance. While this study focusses on Libyan dialects, the approach is general, and could produce resources to support human translators and interpreters when dealing with vernaculars rather than standard Arabic.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1532871872,"Goal":"Sustainable Cities and Communities","Task":["Dialect identification","language identification"],"Method":["unsupervised learning"]},{"ID":"mani-etal-2008-spatialml","title":"{S}patial{ML}: Annotation Scheme, Corpora, and Tools","abstract":"SpatialML is an annotation scheme for marking up references to places in natural language. It covers both named and nominal references to places, grounding them where possible with geo-coordinates, including both relative and absolute locations, and characterizes relationships among places in terms of a region calculus. A freely available annotation editor has been developed for SpatialML, along with a corpus of annotated documents released by the Linguistic Data Consortium. Inter-annotator agreement on SpatialML is 77.0 F-measure for extents on that corpus. An automatic tagger for SpatialML extents scores 78.5 F-measure. A disambiguator scores 93.0 F-measure and 93.4 Predictive Accuracy. In adapting the extent tagger to new domains, merging the training data from the above corpus with annotated data in the new domain provides the best performance.","year":2008,"title_abstract":"{S}patial{ML}: Annotation Scheme, Corpora, and Tools SpatialML is an annotation scheme for marking up references to places in natural language. It covers both named and nominal references to places, grounding them where possible with geo-coordinates, including both relative and absolute locations, and characterizes relationships among places in terms of a region calculus. A freely available annotation editor has been developed for SpatialML, along with a corpus of annotated documents released by the Linguistic Data Consortium. Inter-annotator agreement on SpatialML is 77.0 F-measure for extents on that corpus. An automatic tagger for SpatialML extents scores 78.5 F-measure. A disambiguator scores 93.0 F-measure and 93.4 Predictive Accuracy. In adapting the extent tagger to new domains, merging the training data from the above corpus with annotated data in the new domain provides the best performance.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1531962156,"Goal":"Life on Land","Task":["marking up references to places","SpatialML extents","disambiguator"],"Method":["Annotation Scheme","SpatialML","annotation scheme","region calculus","annotation editor","SpatialML","SpatialML","automatic tagger","extent tagger"]},{"ID":"calvo-figueras-etal-2022-semantics","title":"A Semantics-Aware Approach to Automated Claim Verification","abstract":"The influence of fake news in the perception of reality has become a mainstream topic in the last years due to the fast propagation of misleading information. In order to help in the fight against misinformation, automated solutions to fact-checking are being actively developed within the research community. In this context, the task of Automated Claim Verification is defined as assessing the truthfulness of a claim by finding evidence about its veracity. In this work we empirically demonstrate that enriching a BERT model with explicit semantic information such as Semantic Role Labelling helps to improve results in claim verification as proposed by the FEVER benchmark. Furthermore, we perform a number of explainability tests that suggest that the semantically-enriched model is better at handling complex cases, such as those including passive forms or multiple propositions.","year":2022,"title_abstract":"A Semantics-Aware Approach to Automated Claim Verification The influence of fake news in the perception of reality has become a mainstream topic in the last years due to the fast propagation of misleading information. In order to help in the fight against misinformation, automated solutions to fact-checking are being actively developed within the research community. In this context, the task of Automated Claim Verification is defined as assessing the truthfulness of a claim by finding evidence about its veracity. In this work we empirically demonstrate that enriching a BERT model with explicit semantic information such as Semantic Role Labelling helps to improve results in claim verification as proposed by the FEVER benchmark. Furthermore, we perform a number of explainability tests that suggest that the semantically-enriched model is better at handling complex cases, such as those including passive forms or multiple propositions.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.153136,"Goal":"Climate Action","Task":["Automated Claim Verification","perception of reality","fact - checking","Automated Claim Verification","claim verification"],"Method":["Semantics - Aware Approach","BERT model","Semantic Role Labelling","explainability tests","semantically - enriched model"]},{"ID":"sarioglu-kayi-etal-2020-detecting","title":"Detecting Urgency Status of Crisis Tweets: A Transfer Learning Approach for Low Resource Languages","abstract":"We release an urgency dataset that consists of English tweets relating to natural crises, along with annotations of their corresponding urgency status. Additionally, we release evaluation datasets for two low-resource languages, i.e. Sinhala and Odia, and demonstrate an effective zero-shot transfer from English to these two languages by training cross-lingual classifiers. We adopt cross-lingual embeddings constructed using different methods to extract features of the tweets, including a few state-of-the-art contextual embeddings such as BERT, RoBERTa and XLM-R. We train classifiers of different architectures on the extracted features. We also explore semi-supervised approaches by utilizing unlabeled tweets and experiment with ensembling different classifiers. With very limited amounts of labeled data in English and zero data in the low resource languages, we show a successful framework of training monolingual and cross-lingual classifiers using deep learning methods which are known to be data hungry. Specifically, we show that the recent deep contextual embeddings are also helpful when dealing with very small-scale datasets. Classifiers that incorporate RoBERTa yield the best performance for English urgency detection task, with F1 scores that are more than 25 points over our baseline classifier. For the zero-shot transfer to low resource languages, classifiers that use LASER features perform the best for Sinhala transfer while XLM-R features benefit the Odia transfer the most.","year":2020,"title_abstract":"Detecting Urgency Status of Crisis Tweets: A Transfer Learning Approach for Low Resource Languages We release an urgency dataset that consists of English tweets relating to natural crises, along with annotations of their corresponding urgency status. Additionally, we release evaluation datasets for two low-resource languages, i.e. Sinhala and Odia, and demonstrate an effective zero-shot transfer from English to these two languages by training cross-lingual classifiers. We adopt cross-lingual embeddings constructed using different methods to extract features of the tweets, including a few state-of-the-art contextual embeddings such as BERT, RoBERTa and XLM-R. We train classifiers of different architectures on the extracted features. We also explore semi-supervised approaches by utilizing unlabeled tweets and experiment with ensembling different classifiers. With very limited amounts of labeled data in English and zero data in the low resource languages, we show a successful framework of training monolingual and cross-lingual classifiers using deep learning methods which are known to be data hungry. Specifically, we show that the recent deep contextual embeddings are also helpful when dealing with very small-scale datasets. Classifiers that incorporate RoBERTa yield the best performance for English urgency detection task, with F1 scores that are more than 25 points over our baseline classifier. For the zero-shot transfer to low resource languages, classifiers that use LASER features perform the best for Sinhala transfer while XLM-R features benefit the Odia transfer the most.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1531101465,"Goal":"Climate Action","Task":["Detecting Urgency Status of Crisis Tweets","English urgency detection task","zero - shot transfer","Sinhala transfer","Odia transfer"],"Method":["Transfer Learning Approach","zero - shot transfer","cross - lingual classifiers","cross - lingual embeddings","contextual embeddings","BERT","RoBERTa","XLM - R","classifiers","semi - supervised approaches","classifiers","monolingual and cross - lingual classifiers","deep learning methods","deep contextual embeddings","Classifiers","RoBERTa","baseline classifier","classifiers"]},{"ID":"barry-etal-2020-adapt","title":"The {ADAPT} Enhanced Dependency Parser at the {IWPT} 2020 Shared Task","abstract":"We describe the ADAPT system for the 2020 IWPT Shared Task on parsing enhanced Universal Dependencies in 17 languages. We implement a pipeline approach using UDPipe and UDPipe-future to provide initial levels of annotation. The enhanced dependency graph is either produced by a graph-based semantic dependency parser or is built from the basic tree using a small set of heuristics. Our results show that, for the majority of languages, a semantic dependency parser can be successfully applied to the task of parsing enhanced dependencies. Unfortunately, we did not ensure a connected graph as part of our pipeline approach and our competition submission relied on a last-minute fix to pass the validation script which harmed our official evaluation scores significantly. Our submission ranked eighth in the official evaluation with a macro-averaged coarse ELAS F1 of 67.23 and a treebank average of 67.49. We later implemented our own graph-connecting fix which resulted in a score of 79.53 (language average) or 79.76 (treebank average), which would have placed fourth in the competition evaluation.","year":2020,"title_abstract":"The {ADAPT} Enhanced Dependency Parser at the {IWPT} 2020 Shared Task We describe the ADAPT system for the 2020 IWPT Shared Task on parsing enhanced Universal Dependencies in 17 languages. We implement a pipeline approach using UDPipe and UDPipe-future to provide initial levels of annotation. The enhanced dependency graph is either produced by a graph-based semantic dependency parser or is built from the basic tree using a small set of heuristics. Our results show that, for the majority of languages, a semantic dependency parser can be successfully applied to the task of parsing enhanced dependencies. Unfortunately, we did not ensure a connected graph as part of our pipeline approach and our competition submission relied on a last-minute fix to pass the validation script which harmed our official evaluation scores significantly. Our submission ranked eighth in the official evaluation with a macro-averaged coarse ELAS F1 of 67.23 and a treebank average of 67.49. We later implemented our own graph-connecting fix which resulted in a score of 79.53 (language average) or 79.76 (treebank average), which would have placed fourth in the competition evaluation.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1530953795,"Goal":"Partnership for the Goals","Task":["2020 IWPT Shared Task","parsing enhanced dependencies"],"Method":["Enhanced Dependency Parser","ADAPT","pipeline approach","UDPipe","UDPipe - future","graph - based semantic dependency parser","semantic dependency parser","pipeline approach","graph - connecting fix"]},{"ID":"maveli-2020-edinburghnlp","title":"{E}dinburgh{NLP} at {WNUT}-2020 Task 2: Leveraging Transformers with Generalized Augmentation for Identifying Informativeness in {COVID}-19 Tweets","abstract":"Twitter has become an important communication channel in times of emergency. The ubiquitousness of smartphones enables people to announce an emergency they{'}re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (disaster relief organizations and news agencies) and therefore recognizing the informativeness of a tweet can help filter noise from large volumes of data. In this paper, we present our submission for WNUT-2020 Task 2: Identification of informative COVID-19 English Tweets. Our most successful model is an ensemble of transformers including RoBERTa, XLNet, and BERTweet trained in a Semi-Supervised Learning (SSL) setting. The proposed system achieves a F1 score of 0.9011 on the test set (ranking 7th on the leaderboard), and shows significant gains in performance compared to a baseline system using fasttext embeddings.","year":2020,"title_abstract":"{E}dinburgh{NLP} at {WNUT}-2020 Task 2: Leveraging Transformers with Generalized Augmentation for Identifying Informativeness in {COVID}-19 Tweets Twitter has become an important communication channel in times of emergency. The ubiquitousness of smartphones enables people to announce an emergency they{'}re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (disaster relief organizations and news agencies) and therefore recognizing the informativeness of a tweet can help filter noise from large volumes of data. In this paper, we present our submission for WNUT-2020 Task 2: Identification of informative COVID-19 English Tweets. Our most successful model is an ensemble of transformers including RoBERTa, XLNet, and BERTweet trained in a Semi-Supervised Learning (SSL) setting. The proposed system achieves a F1 score of 0.9011 on the test set (ranking 7th on the leaderboard), and shows significant gains in performance compared to a baseline system using fasttext embeddings.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1530858874,"Goal":"Climate Action","Task":["Identifying Informativeness","Twitter","communication channel","WNUT - 2020 Task","Identification of informative COVID - 19","Semi - Supervised Learning"],"Method":["Transformers","Generalized Augmentation","ensemble of transformers","RoBERTa","XLNet","BERTweet","fasttext embeddings"]},{"ID":"li-etal-2017-bibi","title":"{BIBI} System Description: Building with {CNN}s and Breaking with Deep Reinforcement Learning","abstract":"This paper describes our submission to the sentiment analysis sub-task of {``}Build It, Break It: The Language Edition (BIBI){''}, on both the builder and breaker sides. As a builder, we use convolutional neural nets, trained on both phrase and sentence data. As a breaker, we use Q-learning to learn minimal change pairs, and apply a token substitution method automatically. We analyse the results to gauge the robustness of NLP systems.","year":2017,"title_abstract":"{BIBI} System Description: Building with {CNN}s and Breaking with Deep Reinforcement Learning This paper describes our submission to the sentiment analysis sub-task of {``}Build It, Break It: The Language Edition (BIBI){''}, on both the builder and breaker sides. As a builder, we use convolutional neural nets, trained on both phrase and sentence data. As a breaker, we use Q-learning to learn minimal change pairs, and apply a token substitution method automatically. We analyse the results to gauge the robustness of NLP systems.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.1530522853,"Goal":"Peace, Justice and Strong Institutions","Task":["sentiment analysis sub - task","Language Edition"],"Method":["{CNN}s","Deep Reinforcement Learning","convolutional neural nets","Q - learning","token substitution method","NLP systems"]},{"ID":"hahn-etal-2016-uima","title":"{UIMA}-Based {JC}o{R}e 2.0 Goes {G}it{H}ub and Maven Central \u2015 State-of-the-Art Software Resource Engineering and Distribution of {NLP} Pipelines","abstract":"We introduce JCoRe 2.0, the relaunch of a UIMA-based open software repository for full-scale natural language processing originating from the Jena University Language {\\&} Information Engineering (JULIE) Lab. In an attempt to put the new release of JCoRe on firm software engineering ground, we uploaded it to GitHub, a social coding platform, with an underlying source code versioning system and various means to support collaboration for software development and code modification management. In order to automate the builds of complex NLP pipelines and properly represent and track dependencies of the underlying Java code, we incorporated Maven as part of our software configuration management efforts. In the meantime, we have deployed our artifacts on Maven Central, as well. JCoRe 2.0 offers a broad range of text analytics functionality (mostly) for English-language scientific abstracts and full-text articles, especially from the life sciences domain.","year":2016,"title_abstract":"{UIMA}-Based {JC}o{R}e 2.0 Goes {G}it{H}ub and Maven Central \u2015 State-of-the-Art Software Resource Engineering and Distribution of {NLP} Pipelines We introduce JCoRe 2.0, the relaunch of a UIMA-based open software repository for full-scale natural language processing originating from the Jena University Language {\\&} Information Engineering (JULIE) Lab. In an attempt to put the new release of JCoRe on firm software engineering ground, we uploaded it to GitHub, a social coding platform, with an underlying source code versioning system and various means to support collaboration for software development and code modification management. In order to automate the builds of complex NLP pipelines and properly represent and track dependencies of the underlying Java code, we incorporated Maven as part of our software configuration management efforts. In the meantime, we have deployed our artifacts on Maven Central, as well. JCoRe 2.0 offers a broad range of text analytics functionality (mostly) for English-language scientific abstracts and full-text articles, especially from the life sciences domain.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1530467272,"Goal":"Life on Land","Task":["Software Resource Engineering","Distribution of {NLP} Pipelines","full - scale natural language processing","software engineering ground","collaboration","software development","code modification management","software configuration management","text analytics functionality"],"Method":["JCoRe 2","UIMA - based open software repository","JCoRe","social coding platform","source code versioning system","NLP","Maven","JCoRe 2"]},{"ID":"vo-etal-2016-disco","title":"{DISCO}: A System Leveraging Semantic Search in Document Review","abstract":"This paper presents Disco, a prototype for supporting knowledge workers in exploring, reviewing and sorting collections of textual data. The goal is to facilitate, accelerate and improve the discovery of information. To this end, it combines Semantic Relatedness techniques with a review workflow developed in a tangible environment. Disco uses a semantic model that is leveraged on-line in the course of search sessions, and accessed through natural hand-gesture, in a simple and intuitive way.","year":2016,"title_abstract":"{DISCO}: A System Leveraging Semantic Search in Document Review This paper presents Disco, a prototype for supporting knowledge workers in exploring, reviewing and sorting collections of textual data. The goal is to facilitate, accelerate and improve the discovery of information. To this end, it combines Semantic Relatedness techniques with a review workflow developed in a tangible environment. Disco uses a semantic model that is leveraged on-line in the course of search sessions, and accessed through natural hand-gesture, in a simple and intuitive way.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.1529335082,"Goal":"Life Below Water","Task":["Semantic Search","Document Review","knowledge workers","reviewing and sorting collections of textual data","discovery of information","search sessions"],"Method":["Disco","Semantic Relatedness techniques","review workflow","Disco","semantic model"]},{"ID":"cattoni-etal-2012-knowledgestore","title":"The {K}nowledge{S}tore: an Entity-Based Storage System","abstract":"This paper describes the KnowledgeStore, a large-scale infrastructure for the combined storage and interlinking of multimedia resources and ontological knowledge. Information in the KnowledgeStore is organized around entities, such as persons, organizations and locations. The system allows (i) to import background knowledge about entities, in form of annotated RDF triples; (ii) to associate resources to entities by automatically recognizing, coreferring and linking mentions of named entities; and (iii) to derive new entities based on knowledge extracted from mentions. The KnowledgeStore builds on state of art technologies for language processing, including document tagging, named entity extraction and cross-document coreference. Its design provides for a tight integration of linguistic and semantic features, and eases the further processing of information by explicitly representing the contexts where knowledge and mentions are valid or relevant. We describe the system and report about the creation of a large-scale KnowledgeStore instance for storing and integrating multimedia contents and background knowledge relevant to the Italian Trentino region.","year":2012,"title_abstract":"The {K}nowledge{S}tore: an Entity-Based Storage System This paper describes the KnowledgeStore, a large-scale infrastructure for the combined storage and interlinking of multimedia resources and ontological knowledge. Information in the KnowledgeStore is organized around entities, such as persons, organizations and locations. The system allows (i) to import background knowledge about entities, in form of annotated RDF triples; (ii) to associate resources to entities by automatically recognizing, coreferring and linking mentions of named entities; and (iii) to derive new entities based on knowledge extracted from mentions. The KnowledgeStore builds on state of art technologies for language processing, including document tagging, named entity extraction and cross-document coreference. Its design provides for a tight integration of linguistic and semantic features, and eases the further processing of information by explicitly representing the contexts where knowledge and mentions are valid or relevant. We describe the system and report about the creation of a large-scale KnowledgeStore instance for storing and integrating multimedia contents and background knowledge relevant to the Italian Trentino region.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.1529265642,"Goal":"Life Below Water","Task":["combined storage and interlinking of multimedia resources","coreferring and linking mentions of named entities;","language processing","document tagging","named entity extraction","cross - document coreference","processing of information","KnowledgeStore instance","storing and integrating multimedia contents"],"Method":["Entity - Based Storage System","KnowledgeStore","KnowledgeStore"]},{"ID":"yoshimi-sata-1999-improvement","title":"Improvement of translation quality of {E}nglish newspaper headlines by automatic preediting","abstract":"Since the headlines of English news articles have a characteristic style, different from the styles which prevail in ordinary sentences, it is difficult for MT systems to generate high quality translations for headlines. We try to solve this problem by adding to an existing system a preediting module which rewrites the headlines to ordinary expressions. Rewriting of headlines makes it possible to generate better translations which would not otherwise be generated, with little or no changes to the existing parts of the system. Focusing on the absence of a form of the verb of 'be', we have described rewriting rules for putting properly the verb 'be' into the headlines.","year":1999,"title_abstract":"Improvement of translation quality of {E}nglish newspaper headlines by automatic preediting Since the headlines of English news articles have a characteristic style, different from the styles which prevail in ordinary sentences, it is difficult for MT systems to generate high quality translations for headlines. We try to solve this problem by adding to an existing system a preediting module which rewrites the headlines to ordinary expressions. Rewriting of headlines makes it possible to generate better translations which would not otherwise be generated, with little or no changes to the existing parts of the system. Focusing on the absence of a form of the verb of 'be', we have described rewriting rules for putting properly the verb 'be' into the headlines.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1528623998,"Goal":"Climate Action","Task":["MT","Rewriting of headlines"],"Method":["automatic preediting","preediting module","rewriting rules"]},{"ID":"abrate-etal-2014-sharing","title":"Sharing Cultural Heritage: the Clavius on the Web Project","abstract":"In the last few years the amount of manuscripts digitized and made available on the Web has been constantly increasing. However, there is still a considarable lack of results concerning both the explicitation of their content and the tools developed to make it available. The objective of the Clavius on the Web project is to develop a Web platform exposing a selection of Christophorus Clavius letters along with three different levels of analysis: linguistic, lexical and semantic. The multilayered annotation of the corpus involves a XML-TEI encoding followed by a tokenization step where each token is univocally identified through a CTS urn notation and then associated to a part-of-speech and a lemma. The text is lexically and semantically annotated on the basis of a lexicon and a domain ontology, the former structuring the most relevant terms occurring in the text and the latter representing the domain entities of interest (e.g. people, places, etc.). Moreover, each entity is connected to linked and non linked resources, including DBpedia and VIAF. Finally, the results of the three layers of analysis are gathered and shown through interactive visualization and storytelling techniques. A demo version of the integrated architecture was developed.","year":2014,"title_abstract":"Sharing Cultural Heritage: the Clavius on the Web Project In the last few years the amount of manuscripts digitized and made available on the Web has been constantly increasing. However, there is still a considarable lack of results concerning both the explicitation of their content and the tools developed to make it available. The objective of the Clavius on the Web project is to develop a Web platform exposing a selection of Christophorus Clavius letters along with three different levels of analysis: linguistic, lexical and semantic. The multilayered annotation of the corpus involves a XML-TEI encoding followed by a tokenization step where each token is univocally identified through a CTS urn notation and then associated to a part-of-speech and a lemma. The text is lexically and semantically annotated on the basis of a lexicon and a domain ontology, the former structuring the most relevant terms occurring in the text and the latter representing the domain entities of interest (e.g. people, places, etc.). Moreover, each entity is connected to linked and non linked resources, including DBpedia and VIAF. Finally, the results of the three layers of analysis are gathered and shown through interactive visualization and storytelling techniques. A demo version of the integrated architecture was developed.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1528500617,"Goal":"Sustainable Cities and Communities","Task":["Sharing Cultural Heritage","Web project","analysis","multilayered annotation","analysis"],"Method":["Web platform","XML - TEI encoding","tokenization step","CTS urn notation","lexicon","interactive visualization","storytelling techniques","integrated architecture"]},{"ID":"kodner-2022-modeling","title":"Modeling the Relationship between Input Distributions and Learning Trajectories with the Tolerance Principle","abstract":"Child language learners develop with remarkable uniformity, both in their learning trajectories and ultimate outcomes, despite major differences in their learning environments. In this paper, we explore the role that the frequencies and distributions of irregular lexical items in the input plays in driving learning trajectories. We conclude that while the Tolerance Principle, a type-based model of productivity learning, accounts for inter-learner uniformity, it also interacts with input distributions to drive cross-linguistic variation in learning trajectories.","year":2022,"title_abstract":"Modeling the Relationship between Input Distributions and Learning Trajectories with the Tolerance Principle Child language learners develop with remarkable uniformity, both in their learning trajectories and ultimate outcomes, despite major differences in their learning environments. In this paper, we explore the role that the frequencies and distributions of irregular lexical items in the input plays in driving learning trajectories. We conclude that while the Tolerance Principle, a type-based model of productivity learning, accounts for inter-learner uniformity, it also interacts with input distributions to drive cross-linguistic variation in learning trajectories.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.15282318,"Goal":"Quality Education","Task":["Learning Trajectories","learning trajectories","productivity learning","learning trajectories"],"Method":["Tolerance Principle","Tolerance Principle","type - based model"]},{"ID":"broeder-etal-2006-technologies","title":"Technologies for a Federation of Language Resource Archives","abstract":"The DAM-LR project aims at virtually integrating various European language resource archives that allow users to navigate and operate in a single unified domain of language resources. This type of integration introduces Grid technology to the humanities disciplines and forms a federation of archives. It is the basis for establishing a research infrastructure for language resources which will finally enable eHumanities. Currently, the complete architecture is designed based on a few well-known components and some components are already tested. Based on the technological insights gathered and due to discussions within the international DELAMAN network the ethical and organizational basis for such a federation is defined.","year":2006,"title_abstract":"Technologies for a Federation of Language Resource Archives The DAM-LR project aims at virtually integrating various European language resource archives that allow users to navigate and operate in a single unified domain of language resources. This type of integration introduces Grid technology to the humanities disciplines and forms a federation of archives. It is the basis for establishing a research infrastructure for language resources which will finally enable eHumanities. Currently, the complete architecture is designed based on a few well-known components and some components are already tested. Based on the technological insights gathered and due to discussions within the international DELAMAN network the ethical and organizational basis for such a federation is defined.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.1528008282,"Goal":"Peace, Justice and Strong Institutions","Task":["Federation of Language Resource Archives","language resources","eHumanities"],"Method":["DAM - LR","Grid technology"]},{"ID":"tabak-purver-2020-temporal","title":"Temporal Mental Health Dynamics on Social Media","abstract":"We describe a set of experiments for building a temporal mental health dynamics system. We utilise a pre-existing methodology for distant- supervision of mental health data mining from social media platforms and deploy the system during the global COVID-19 pandemic as a case study. Despite the challenging nature of the task, we produce encouraging results, both explicit to the global pandemic and implicit to a global phenomenon, Christmas Depres- sion, supported by the literature. We propose a methodology for providing insight into tem- poral mental health dynamics to be utilised for strategic decision-making.","year":2020,"title_abstract":"Temporal Mental Health Dynamics on Social Media We describe a set of experiments for building a temporal mental health dynamics system. We utilise a pre-existing methodology for distant- supervision of mental health data mining from social media platforms and deploy the system during the global COVID-19 pandemic as a case study. Despite the challenging nature of the task, we produce encouraging results, both explicit to the global pandemic and implicit to a global phenomenon, Christmas Depres- sion, supported by the literature. We propose a methodology for providing insight into tem- poral mental health dynamics to be utilised for strategic decision-making.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1527944207,"Goal":"Climate Action","Task":["Temporal Mental Health Dynamics","distant - supervision of mental health data mining","global COVID - 19 pandemic","tem - poral mental health dynamics","strategic decision - making"],"Method":["temporal mental health dynamics system"]},{"ID":"schwarzer-etal-2021-improving","title":"Improving Human Text Simplification with Sentence Fusion","abstract":"The quality of fully automated text simplification systems is not good enough for use in real-world settings; instead, human simplifications are used. In this paper, we examine how to improve the cost and quality of human simplifications by leveraging crowdsourcing. We introduce a graph-based sentence fusion approach to augment human simplifications and a reranking approach to both select high quality simplifications and to allow for targeting simplifications with varying levels of simplicity. Using the Newsela dataset (Xu et al., 2015) we show consistent improvements over experts at varying simplification levels and find that the additional sentence fusion simplifications allow for simpler output than the human simplifications alone.","year":2021,"title_abstract":"Improving Human Text Simplification with Sentence Fusion The quality of fully automated text simplification systems is not good enough for use in real-world settings; instead, human simplifications are used. In this paper, we examine how to improve the cost and quality of human simplifications by leveraging crowdsourcing. We introduce a graph-based sentence fusion approach to augment human simplifications and a reranking approach to both select high quality simplifications and to allow for targeting simplifications with varying levels of simplicity. Using the Newsela dataset (Xu et al., 2015) we show consistent improvements over experts at varying simplification levels and find that the additional sentence fusion simplifications allow for simpler output than the human simplifications alone.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1527284384,"Goal":"Sustainable Cities and Communities","Task":["Human Text Simplification","Sentence Fusion","real - world settings;","human simplifications","crowdsourcing","human simplifications"],"Method":["text simplification systems","human simplifications","graph - based sentence fusion approach","reranking approach","sentence fusion simplifications","human simplifications"]},{"ID":"zhang-etal-2020-rapidly","title":"Rapidly Deploying a Neural Search Engine for the {COVID-19} {Open} {Research} {Dataset}","abstract":"The Neural Covidex is a search engine that exploits the latest neural ranking architectures to provide information access to the COVID-19 Open Research Dataset (CORD-19) curated by the Allen Institute for AI. It exists as part of a suite of tools we have developed to help domain experts tackle the ongoing global pandemic. We hope that improved information access capabilities to the scientific literature can inform evidence-based decision making and insight generation.","year":2020,"title_abstract":"Rapidly Deploying a Neural Search Engine for the {COVID-19} {Open} {Research} {Dataset} The Neural Covidex is a search engine that exploits the latest neural ranking architectures to provide information access to the COVID-19 Open Research Dataset (CORD-19) curated by the Allen Institute for AI. It exists as part of a suite of tools we have developed to help domain experts tackle the ongoing global pandemic. We hope that improved information access capabilities to the scientific literature can inform evidence-based decision making and insight generation.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1527281404,"Goal":"Climate Action","Task":["AI","evidence - based decision making","insight generation"],"Method":["Neural Search Engine","Neural Covidex","search engine","neural ranking architectures"]},{"ID":"kevin-etal-2018-information","title":"Information Nutrition Labels: A Plugin for Online News Evaluation","abstract":"In this paper we present a browser plugin \\textit{NewsScan} that assists online news readers in evaluating the quality of online content they read by providing \\textit{information nutrition labels} for online news articles. In analogy to groceries, where nutrition labels help consumers make choices that they consider best for themselves, information nutrition labels tag online news articles with data that help readers judge the articles they engage with. This paper discusses the choice of the labels, their implementation and visualization.","year":2018,"title_abstract":"Information Nutrition Labels: A Plugin for Online News Evaluation In this paper we present a browser plugin \\textit{NewsScan} that assists online news readers in evaluating the quality of online content they read by providing \\textit{information nutrition labels} for online news articles. In analogy to groceries, where nutrition labels help consumers make choices that they consider best for themselves, information nutrition labels tag online news articles with data that help readers judge the articles they engage with. This paper discusses the choice of the labels, their implementation and visualization.","social_need":"No Hunger End hunger, achieve food security and improved nutrition and promote sustainable agriculture","cosine_similarity":0.1527245939,"Goal":"No Hunger","Task":["Online News Evaluation","visualization"],"Method":["Information Nutrition Labels","browser plugin"]},{"ID":"celli-etal-2016-predicting","title":"Predicting {B}rexit: Classifying Agreement is Better than Sentiment and Pollsters","abstract":"On June 23rd 2016, UK held the referendum which ratified the exit from the EU. While most of the traditional pollsters failed to forecast the final vote, there were online systems that hit the result with high accuracy using opinion mining techniques and big data. Starting one month before, we collected and monitored millions of posts about the referendum from social media conversations, and exploited Natural Language Processing techniques to predict the referendum outcome. In this paper we discuss the methods used by traditional pollsters and compare it to the predictions based on different opinion mining techniques. We find that opinion mining based on agreement\/disagreement classification works better than opinion mining based on polarity classification in the forecast of the referendum outcome.","year":2016,"title_abstract":"Predicting {B}rexit: Classifying Agreement is Better than Sentiment and Pollsters On June 23rd 2016, UK held the referendum which ratified the exit from the EU. While most of the traditional pollsters failed to forecast the final vote, there were online systems that hit the result with high accuracy using opinion mining techniques and big data. Starting one month before, we collected and monitored millions of posts about the referendum from social media conversations, and exploited Natural Language Processing techniques to predict the referendum outcome. In this paper we discuss the methods used by traditional pollsters and compare it to the predictions based on different opinion mining techniques. We find that opinion mining based on agreement\/disagreement classification works better than opinion mining based on polarity classification in the forecast of the referendum outcome.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1527229548,"Goal":"Climate Action","Task":["Predicting {B}rexit","Classifying Agreement","opinion mining","forecast of the referendum outcome"],"Method":["online systems","opinion mining techniques","Natural Language Processing techniques","pollsters","opinion mining techniques","agreement\/disagreement classification","opinion mining","polarity classification"]},{"ID":"imani-etal-2022-graph","title":"Graph Neural Networks for Multiparallel Word Alignment","abstract":"After a period of decrease, interest in word alignments is increasing again for their usefulness in domains such as typological research, cross-lingual annotation projection and machine translation. Generally, alignment algorithms only use bitext and do not make use of the fact that many parallel corpora are multiparallel. Here, we compute high-quality word alignments between multiple language pairs by considering all language pairs together. First, we create a multiparallel word alignment graph, joining all bilingual word alignment pairs in one graph. Next, we use graph neural networks (GNNs) to exploit the graph structure. Our GNN approach (i) utilizes information about the meaning, position and language of the input words, (ii) incorporates information from multiple parallel sentences, (iii) adds and removes edges from the initial alignments, and (iv) yields a prediction model that can generalize beyond the training sentences. We show that community detection algorithms can provide valuable information for multiparallel word alignment. Our method outperforms previous work on three word alignment datasets and on a downstream task.","year":2022,"title_abstract":"Graph Neural Networks for Multiparallel Word Alignment After a period of decrease, interest in word alignments is increasing again for their usefulness in domains such as typological research, cross-lingual annotation projection and machine translation. Generally, alignment algorithms only use bitext and do not make use of the fact that many parallel corpora are multiparallel. Here, we compute high-quality word alignments between multiple language pairs by considering all language pairs together. First, we create a multiparallel word alignment graph, joining all bilingual word alignment pairs in one graph. Next, we use graph neural networks (GNNs) to exploit the graph structure. Our GNN approach (i) utilizes information about the meaning, position and language of the input words, (ii) incorporates information from multiple parallel sentences, (iii) adds and removes edges from the initial alignments, and (iv) yields a prediction model that can generalize beyond the training sentences. We show that community detection algorithms can provide valuable information for multiparallel word alignment. Our method outperforms previous work on three word alignment datasets and on a downstream task.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1527211517,"Goal":"Gender Equality","Task":["Multiparallel Word Alignment","word alignments","typological research","cross - lingual annotation projection","machine translation","word alignments","multiparallel word alignment graph","multiparallel word alignment","downstream task"],"Method":["Graph Neural Networks","alignment algorithms","graph neural networks","GNN approach","prediction model","community detection algorithms"]},{"ID":"singh-etal-2021-hold","title":"{``}Hold on honey, men at work{''}: A semi-supervised approach to detecting sexism in sitcoms","abstract":"Television shows play an important role inpropagating societal norms. Owing to the popularity of the situational comedy (sitcom) genre, it contributes significantly to the over-all development of society. In an effort to analyze the content of television shows belong-ing to this genre, we present a dataset of dialogue turns from popular sitcoms annotated for the presence of sexist remarks. We train a text classification model to detect sexism using domain adaptive learning. We apply the model to our dataset to analyze the evolution of sexist content over the years. We propose a domain-specific semi-supervised architecture for the aforementioned detection of sexism.Through extensive experiments, we show that our model often yields better classification performance over generic deep learn-ing based sentence classification that does not employ domain-specific training. We find that while sexism decreases over time on average,the proportion of sexist dialogue for the most sexist sitcom actually increases. A quantitative analysis along with a detailed error analysis presents the case for our proposed methodology","year":2021,"title_abstract":"{``}Hold on honey, men at work{''}: A semi-supervised approach to detecting sexism in sitcoms Television shows play an important role inpropagating societal norms. Owing to the popularity of the situational comedy (sitcom) genre, it contributes significantly to the over-all development of society. In an effort to analyze the content of television shows belong-ing to this genre, we present a dataset of dialogue turns from popular sitcoms annotated for the presence of sexist remarks. We train a text classification model to detect sexism using domain adaptive learning. We apply the model to our dataset to analyze the evolution of sexist content over the years. We propose a domain-specific semi-supervised architecture for the aforementioned detection of sexism.Through extensive experiments, we show that our model often yields better classification performance over generic deep learn-ing based sentence classification that does not employ domain-specific training. We find that while sexism decreases over time on average,the proportion of sexist dialogue for the most sexist sitcom actually increases. A quantitative analysis along with a detailed error analysis presents the case for our proposed methodology","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1526867151,"Goal":"Gender Equality","Task":["detecting sexism","sexism","detection of sexism","classification"],"Method":["semi - supervised approach","text classification model","domain adaptive learning","domain - specific semi - supervised architecture","deep learn - ing based sentence classification","domain - specific training","error analysis"]},{"ID":"manuvinakurike-etal-2021-incremental","title":"Incremental temporal summarization in multi-party meetings","abstract":"In this work, we develop a dataset for incremental temporal summarization in a multiparty dialogue. We use crowd-sourcing paradigm with a model-in-loop approach for collecting the summaries and compare the data with the expert summaries. We leverage the question generation paradigm to automatically generate questions from the dialogue, which can be used to validate the user participation and potentially also draw attention of the user towards the contents then need to summarize. We then develop several models for abstractive summary generation in the Incremental temporal scenario. We perform a detailed analysis of the results and show that including the past context into the summary generation yields better summaries.","year":2021,"title_abstract":"Incremental temporal summarization in multi-party meetings In this work, we develop a dataset for incremental temporal summarization in a multiparty dialogue. We use crowd-sourcing paradigm with a model-in-loop approach for collecting the summaries and compare the data with the expert summaries. We leverage the question generation paradigm to automatically generate questions from the dialogue, which can be used to validate the user participation and potentially also draw attention of the user towards the contents then need to summarize. We then develop several models for abstractive summary generation in the Incremental temporal scenario. We perform a detailed analysis of the results and show that including the past context into the summary generation yields better summaries.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1526795924,"Goal":"Partnership for the Goals","Task":["Incremental temporal summarization","multi - party meetings","incremental temporal summarization","abstractive summary generation","Incremental temporal scenario","summary generation"],"Method":["crowd - sourcing paradigm","model - in - loop approach","question generation paradigm"]},{"ID":"daudert-2020-web","title":"A Web-based Collaborative Annotation and Consolidation Tool","abstract":"Annotation tools are a valuable asset for the construction of labelled textual datasets. However, they tend to have a rigid structure, closed back-end and front-end, and are built in a non-user-friendly way. These downfalls difficult their use in annotation tasks requiring varied text formats, prevent researchers to optimise the tool to the annotation task, and impede people with little programming knowledge to easily modify the tool rendering it unusable for a large cohort. Targeting these needs, we present a web-based collaborative annotation and consolidation tool (AWOCATo), capable of supporting varied textual formats. AWOCATo is based on three pillars: (1) Simplicity, built with a modular architecture employing easy to use technologies; (2) Flexibility, the JSON configuration file allows an easy adaption to the annotation task; (3) Customizability, parameters such as labels, colours, or consolidation features can be easily customized. These features allow AWOCATo to support a range of tasks and domains, filling the gap left by the absence of annotation tools that can be used by people with and without programming knowledge, including those who wish to easily adapt a tool to less common tasks. AWOCATo is available for download at https:\/\/github.com\/TDaudert\/AWOCATo.","year":2020,"title_abstract":"A Web-based Collaborative Annotation and Consolidation Tool Annotation tools are a valuable asset for the construction of labelled textual datasets. However, they tend to have a rigid structure, closed back-end and front-end, and are built in a non-user-friendly way. These downfalls difficult their use in annotation tasks requiring varied text formats, prevent researchers to optimise the tool to the annotation task, and impede people with little programming knowledge to easily modify the tool rendering it unusable for a large cohort. Targeting these needs, we present a web-based collaborative annotation and consolidation tool (AWOCATo), capable of supporting varied textual formats. AWOCATo is based on three pillars: (1) Simplicity, built with a modular architecture employing easy to use technologies; (2) Flexibility, the JSON configuration file allows an easy adaption to the annotation task; (3) Customizability, parameters such as labels, colours, or consolidation features can be easily customized. These features allow AWOCATo to support a range of tasks and domains, filling the gap left by the absence of annotation tools that can be used by people with and without programming knowledge, including those who wish to easily adapt a tool to less common tasks. AWOCATo is available for download at https:\/\/github.com\/TDaudert\/AWOCATo.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1526557207,"Goal":"Sustainable Cities and Communities","Task":["construction of labelled textual datasets","annotation tasks","annotation task","annotation task;"],"Method":["Web - based Collaborative Annotation and Consolidation Tool","Annotation tools","web - based collaborative annotation and consolidation tool","AWOCATo","modular architecture","annotation tools","AWOCATo"]},{"ID":"sanguinetti-etal-2020-treebanking","title":"Treebanking User-Generated Content: A Proposal for a Unified Representation in {U}niversal {D}ependencies","abstract":"The paper presents a discussion on the main linguistic phenomena of user-generated texts found in web and social media, and proposes a set of annotation guidelines for their treatment within the Universal Dependencies (UD) framework. Given on the one hand the increasing number of treebanks featuring user-generated content, and its somewhat inconsistent treatment in these resources on the other, the aim of this paper is twofold: (1) to provide a short, though comprehensive, overview of such treebanks - based on available literature - along with their main features and a comparative analysis of their annotation criteria, and (2) to propose a set of tentative UD-based annotation guidelines, to promote consistent treatment of the particular phenomena found in these types of texts. The main goal of this paper is to provide a common framework for those teams interested in developing similar resources in UD, thus enabling cross-linguistic consistency, which is a principle that has always been in the spirit of UD.","year":2020,"title_abstract":"Treebanking User-Generated Content: A Proposal for a Unified Representation in {U}niversal {D}ependencies The paper presents a discussion on the main linguistic phenomena of user-generated texts found in web and social media, and proposes a set of annotation guidelines for their treatment within the Universal Dependencies (UD) framework. Given on the one hand the increasing number of treebanks featuring user-generated content, and its somewhat inconsistent treatment in these resources on the other, the aim of this paper is twofold: (1) to provide a short, though comprehensive, overview of such treebanks - based on available literature - along with their main features and a comparative analysis of their annotation criteria, and (2) to propose a set of tentative UD-based annotation guidelines, to promote consistent treatment of the particular phenomena found in these types of texts. The main goal of this paper is to provide a common framework for those teams interested in developing similar resources in UD, thus enabling cross-linguistic consistency, which is a principle that has always been in the spirit of UD.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1526324749,"Goal":"Partnership for the Goals","Task":["annotation","annotation","UD","cross - linguistic consistency"],"Method":["Unified Representation","Universal Dependencies","treebanks","UD"]},{"ID":"vazquez-etal-2021-helsinki","title":"The {H}elsinki submission to the {A}mericas{NLP} shared task","abstract":"The University of Helsinki participated in the AmericasNLP shared task for all ten language pairs. Our multilingual NMT models reached the first rank on all language pairs in track 1, and first rank on nine out of ten language pairs in track 2. We focused our efforts on three aspects: (1) the collection of additional data from various sources such as Bibles and political constitutions, (2) the cleaning and filtering of training data with the OpusFilter toolkit, and (3) different multilingual training techniques enabled by the latest version of the OpenNMT-py toolkit to make the most efficient use of the scarce data. This paper describes our efforts in detail.","year":2021,"title_abstract":"The {H}elsinki submission to the {A}mericas{NLP} shared task The University of Helsinki participated in the AmericasNLP shared task for all ten language pairs. Our multilingual NMT models reached the first rank on all language pairs in track 1, and first rank on nine out of ten language pairs in track 2. We focused our efforts on three aspects: (1) the collection of additional data from various sources such as Bibles and political constitutions, (2) the cleaning and filtering of training data with the OpusFilter toolkit, and (3) different multilingual training techniques enabled by the latest version of the OpenNMT-py toolkit to make the most efficient use of the scarce data. This paper describes our efforts in detail.","social_need":"No Poverty End poverty in all its forms everywhere","cosine_similarity":0.1524982005,"Goal":"No Poverty","Task":["cleaning and filtering of training data"],"Method":["NMT","OpusFilter toolkit","multilingual training techniques","OpenNMT - py toolkit"]},{"ID":"arnold-tilton-2018-cross","title":"Cross-Discourse and Multilingual Exploration of Textual Corpora with the {D}ual{N}eighbors Algorithm","abstract":"Word choice is dependent on the cultural context of writers and their subjects. Different words are used to describe similar actions, objects, and features based on factors such as class, race, gender, geography and political affinity. Exploratory techniques based on locating and counting words may, therefore, lead to conclusions that reinforce culturally inflected boundaries. We offer a new method, the DualNeighbors algorithm, for linking thematically similar documents both within and across discursive and linguistic barriers to reveal cross-cultural connections. Qualitative and quantitative evaluations of this technique are shown as applied to two cultural datasets of interest to researchers across the humanities and social sciences. An open-source implementation of the DualNeighbors algorithm is provided to assist in its application.","year":2018,"title_abstract":"Cross-Discourse and Multilingual Exploration of Textual Corpora with the {D}ual{N}eighbors Algorithm Word choice is dependent on the cultural context of writers and their subjects. Different words are used to describe similar actions, objects, and features based on factors such as class, race, gender, geography and political affinity. Exploratory techniques based on locating and counting words may, therefore, lead to conclusions that reinforce culturally inflected boundaries. We offer a new method, the DualNeighbors algorithm, for linking thematically similar documents both within and across discursive and linguistic barriers to reveal cross-cultural connections. Qualitative and quantitative evaluations of this technique are shown as applied to two cultural datasets of interest to researchers across the humanities and social sciences. An open-source implementation of the DualNeighbors algorithm is provided to assist in its application.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1524669826,"Goal":"Reduced Inequalities","Task":["Cross - Discourse and Multilingual Exploration of Textual Corpora","linking thematically similar documents"],"Method":["{D}ual{N}eighbors Algorithm","Exploratory techniques","DualNeighbors algorithm","DualNeighbors algorithm"]},{"ID":"eskevich-etal-2020-clarin","title":"{CLARIN}: Distributed Language Resources and Technology in a {E}uropean Infrastructure","abstract":"CLARIN is a European Research Infrastructure providing access to digital language resources and tools from across Europe and beyond to researchers in the humanities and social sciences. This paper focuses on CLARIN as a platform for the sharing of language resources. It zooms in on the service offer for the aggregation of language repositories and the value proposition for a number of communities that benefit from the enhanced visibility of their data and services as a result of integration in CLARIN. The enhanced findability of language resources is serving the social sciences and humanities (SSH) community at large and supports research communities that aim to collaborate based on virtual collections for a specific domain. The paper also addresses the wider landscape of service platforms based on language technologies which has the potential of becoming a powerful set of interoperable facilities to a variety of communities of use.","year":2020,"title_abstract":"{CLARIN}: Distributed Language Resources and Technology in a {E}uropean Infrastructure CLARIN is a European Research Infrastructure providing access to digital language resources and tools from across Europe and beyond to researchers in the humanities and social sciences. This paper focuses on CLARIN as a platform for the sharing of language resources. It zooms in on the service offer for the aggregation of language repositories and the value proposition for a number of communities that benefit from the enhanced visibility of their data and services as a result of integration in CLARIN. The enhanced findability of language resources is serving the social sciences and humanities (SSH) community at large and supports research communities that aim to collaborate based on virtual collections for a specific domain. The paper also addresses the wider landscape of service platforms based on language technologies which has the potential of becoming a powerful set of interoperable facilities to a variety of communities of use.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.1524220109,"Goal":"Life Below Water","Task":["Distributed Language Resources","sharing of language resources","aggregation of language repositories","findability of language resources","social sciences and humanities"],"Method":["CLARIN","CLARIN","CLARIN","service platforms","language technologies"]},{"ID":"hutchinson-etal-2020-social","title":"Social Biases in {NLP} Models as Barriers for Persons with Disabilities","abstract":"Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.","year":2020,"title_abstract":"Social Biases in {NLP} Models as Barriers for Persons with Disabilities Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1524104029,"Goal":"Reduced Inequalities","Task":["equitable and inclusive NLP technologies","toxicity prediction","sentiment analysis","NLP"],"Method":["{NLP} Models","ML models","English language models","neural embeddings"]},{"ID":"dayanik-etal-2021-using","title":"Using Hierarchical Class Structure to Improve Fine-Grained Claim Classification","abstract":"The analysis of public debates crucially requires the classification of political demands according to hierarchical \\textit{claim ontologies} (e.g. for immigration, a supercategory {``}Controlling Migration{''} might have subcategories {``}Asylum limit{''} or {``}Border installations{''}). A major challenge for automatic claim classification is the large number and low frequency of such subclasses. We address it by jointly predicting pairs of matching super- and subcategories. We operationalize this idea by (a) encoding soft constraints in the claim classifier and (b) imposing hard constraints via Integer Linear Programming. Our experiments with different claim classifiers on a German immigration newspaper corpus show consistent performance increases for joint prediction, in particular for infrequent categories and discuss the complementarity of the two approaches.","year":2021,"title_abstract":"Using Hierarchical Class Structure to Improve Fine-Grained Claim Classification The analysis of public debates crucially requires the classification of political demands according to hierarchical \\textit{claim ontologies} (e.g. for immigration, a supercategory {``}Controlling Migration{''} might have subcategories {``}Asylum limit{''} or {``}Border installations{''}). A major challenge for automatic claim classification is the large number and low frequency of such subclasses. We address it by jointly predicting pairs of matching super- and subcategories. We operationalize this idea by (a) encoding soft constraints in the claim classifier and (b) imposing hard constraints via Integer Linear Programming. Our experiments with different claim classifiers on a German immigration newspaper corpus show consistent performance increases for joint prediction, in particular for infrequent categories and discuss the complementarity of the two approaches.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1524083465,"Goal":"Reduced Inequalities","Task":["Fine - Grained Claim Classification","analysis of public debates","classification of political demands","immigration","automatic claim classification","joint prediction"],"Method":["claim classifier","Integer Linear Programming","claim classifiers"]},{"ID":"ramanathan-visweswariah-2012-study","title":"A Study of Word-Classing for {MT} Reordering","abstract":"MT systems typically use parsers to help reorder constituents. However most languages do not have adequate treebank data to learn good parsers, and such training data is extremely time-consuming to annotate. Our earlier work has shown that a reordering model learned from word-alignments using POS tags as features can improve MT performance (Visweswariah et al., 2011). In this paper, we investigate the effect of word-classing on reordering performance using this model. We show that unsupervised word clusters perform somewhat worse but still reasonably well, compared to a part-of-speech (POS) tagger built with a small amount of annotated data; while a richer tag set including case and gender-number-person further improves reordering performance by around 1.2 monolingual BLEU points. While annotating this richer tagset is more complicated than annotating the base tagset, it is much easier than annotating treebank data.","year":2012,"title_abstract":"A Study of Word-Classing for {MT} Reordering MT systems typically use parsers to help reorder constituents. However most languages do not have adequate treebank data to learn good parsers, and such training data is extremely time-consuming to annotate. Our earlier work has shown that a reordering model learned from word-alignments using POS tags as features can improve MT performance (Visweswariah et al., 2011). In this paper, we investigate the effect of word-classing on reordering performance using this model. We show that unsupervised word clusters perform somewhat worse but still reasonably well, compared to a part-of-speech (POS) tagger built with a small amount of annotated data; while a richer tag set including case and gender-number-person further improves reordering performance by around 1.2 monolingual BLEU points. While annotating this richer tagset is more complicated than annotating the base tagset, it is much easier than annotating treebank data.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1523997784,"Goal":"Gender Equality","Task":["MT","MT","reordering","reordering"],"Method":["Word - Classing","parsers","parsers","reordering model","word - alignments","word - classing","unsupervised word clusters","part - of - speech","tagger"]},{"ID":"liu-etal-2020-exploring","title":"Exploring and Evaluating Attributes, Values, and Structures for Entity Alignment","abstract":"Entity alignment (EA) aims at building a unified Knowledge Graph (KG) of rich content by linking the equivalent entities from various KGs. GNN-based EA methods present promising performance by modeling the KG structure defined by relation triples. However, attribute triples can also provide crucial alignment signal but have not been well explored yet. In this paper, we propose to utilize an attributed value encoder and partition the KG into subgraphs to model the various types of attribute triples efficiently. Besides, the performances of current EA methods are overestimated because of the name-bias of existing EA datasets. To make an objective evaluation, we propose a hard experimental setting where we select equivalent entity pairs with very different names as the test set. Under both the regular and hard settings, our method achieves significant improvements (5.10{\\%} on average Hits@1 in DBP15k) over 12 baselines in cross-lingual and monolingual datasets. Ablation studies on different subgraphs and a case study about attribute types further demonstrate the effectiveness of our method. Source code and data can be found at \\url{https:\/\/github.com\/thunlp\/explore-and-evaluate}.","year":2020,"title_abstract":"Exploring and Evaluating Attributes, Values, and Structures for Entity Alignment Entity alignment (EA) aims at building a unified Knowledge Graph (KG) of rich content by linking the equivalent entities from various KGs. GNN-based EA methods present promising performance by modeling the KG structure defined by relation triples. However, attribute triples can also provide crucial alignment signal but have not been well explored yet. In this paper, we propose to utilize an attributed value encoder and partition the KG into subgraphs to model the various types of attribute triples efficiently. Besides, the performances of current EA methods are overestimated because of the name-bias of existing EA datasets. To make an objective evaluation, we propose a hard experimental setting where we select equivalent entity pairs with very different names as the test set. Under both the regular and hard settings, our method achieves significant improvements (5.10{\\%} on average Hits@1 in DBP15k) over 12 baselines in cross-lingual and monolingual datasets. Ablation studies on different subgraphs and a case study about attribute types further demonstrate the effectiveness of our method. Source code and data can be found at \\url{https:\/\/github.com\/thunlp\/explore-and-evaluate}.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1523975134,"Goal":"Gender Equality","Task":["Entity Alignment","Entity alignment","unified Knowledge Graph"],"Method":["GNN - based EA methods","attributed value encoder","EA methods","regular and hard settings"]},{"ID":"paetzold-etal-2017-massalign","title":"{MASSA}lign: Alignment and Annotation of Comparable Documents","abstract":"We introduce MASSAlign: a Python library for the alignment and annotation of monolingual comparable documents. MASSAlign offers easy-to-use access to state of the art algorithms for paragraph and sentence-level alignment, as well as novel algorithms for word-level annotation of transformation operations between aligned sentences. In addition, MASSAlign provides a visualization module to display and analyze the alignments and annotations performed.","year":2017,"title_abstract":"{MASSA}lign: Alignment and Annotation of Comparable Documents We introduce MASSAlign: a Python library for the alignment and annotation of monolingual comparable documents. MASSAlign offers easy-to-use access to state of the art algorithms for paragraph and sentence-level alignment, as well as novel algorithms for word-level annotation of transformation operations between aligned sentences. In addition, MASSAlign provides a visualization module to display and analyze the alignments and annotations performed.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1523736119,"Goal":"Gender Equality","Task":["Alignment","Annotation","alignment and annotation of monolingual comparable documents","paragraph and sentence - level alignment","word - level annotation of transformation operations"],"Method":["MASSAlign","Python library","MASSAlign","MASSAlign","visualization module"]},{"ID":"roesiger-2016-scicorp","title":"{S}ci{C}orp: A Corpus of {E}nglish Scientific Articles Annotated for Information Status Analysis","abstract":"This paper presents SciCorp, a corpus of full-text English scientific papers of two disciplines, genetics and computational linguistics. The corpus comprises co-reference and bridging information as well as information status labels. Since SciCorp is annotated with both labels and the respective co-referent and bridging links, we believe it is a valuable resource for NLP researchers working on scientific articles or on applications such as co-reference resolution, bridging resolution or information status classification. The corpus has been reliably annotated by independent human coders with moderate inter-annotator agreement (average kappa = 0.71). In total, we have annotated 14 full papers containing 61,045 tokens and marked 8,708 definite noun phrases. The paper describes in detail the annotation scheme as well as the resulting corpus. The corpus is available for download in two different formats: in an offset-based format and for the co-reference annotations in the widely-used, tabular CoNLL-2012 format.","year":2016,"title_abstract":"{S}ci{C}orp: A Corpus of {E}nglish Scientific Articles Annotated for Information Status Analysis This paper presents SciCorp, a corpus of full-text English scientific papers of two disciplines, genetics and computational linguistics. The corpus comprises co-reference and bridging information as well as information status labels. Since SciCorp is annotated with both labels and the respective co-referent and bridging links, we believe it is a valuable resource for NLP researchers working on scientific articles or on applications such as co-reference resolution, bridging resolution or information status classification. The corpus has been reliably annotated by independent human coders with moderate inter-annotator agreement (average kappa = 0.71). In total, we have annotated 14 full papers containing 61,045 tokens and marked 8,708 definite noun phrases. The paper describes in detail the annotation scheme as well as the resulting corpus. The corpus is available for download in two different formats: in an offset-based format and for the co-reference annotations in the widely-used, tabular CoNLL-2012 format.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1523244977,"Goal":"Life on Land","Task":["Information Status Analysis","computational linguistics","NLP researchers","co - reference resolution","bridging resolution","information status classification","co - reference annotations"],"Method":["SciCorp","SciCorp","annotation scheme"]},{"ID":"zhang-etal-2021-generic","title":"Generic Mechanism for Reducing Repetitions in Encoder-Decoder Models","abstract":"Encoder-decoder models have been commonly used for many tasks such as machine translation and response generation. As previous research reported, these models suffer from generating redundant repetition. In this research, we propose a new mechanism for encoder-decoder models that estimates the semantic difference of a source sentence before and after being fed into the encoder-decoder model to capture the consistency between two sides. This mechanism helps reduce repeatedly generated tokens for a variety of tasks. Evaluation results on publicly available machine translation and response generation datasets demonstrate the effectiveness of our proposal.","year":2021,"title_abstract":"Generic Mechanism for Reducing Repetitions in Encoder-Decoder Models Encoder-decoder models have been commonly used for many tasks such as machine translation and response generation. As previous research reported, these models suffer from generating redundant repetition. In this research, we propose a new mechanism for encoder-decoder models that estimates the semantic difference of a source sentence before and after being fed into the encoder-decoder model to capture the consistency between two sides. This mechanism helps reduce repeatedly generated tokens for a variety of tasks. Evaluation results on publicly available machine translation and response generation datasets demonstrate the effectiveness of our proposal.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1523145288,"Goal":"Gender Equality","Task":["Reducing Repetitions","machine translation","response generation","machine translation"],"Method":["Generic Mechanism","Encoder - Decoder Models","Encoder - decoder models","encoder - decoder models","encoder - decoder model"]},{"ID":"chiang-etal-2021-improved","title":"Improved Text Classification of Long-term Care Materials","abstract":"Aging populations have posed a challenge to many countries including Taiwan, and with them come the issue of long-term care. Given the current context, the aim of this study was to explore the hotly-discussed subtopics in the field of long-term care, and identify its features through NLP. This study applied TF-IDF, the Logistic Regression model, and the Naive Bayes classifier to process data. In sum, the results showed that it reached a best F1-score of 0.920 in identification, and a best accuracy of 0.708 in classification. The results of this study could be used as a reference for future long-term care related applications.","year":2021,"title_abstract":"Improved Text Classification of Long-term Care Materials Aging populations have posed a challenge to many countries including Taiwan, and with them come the issue of long-term care. Given the current context, the aim of this study was to explore the hotly-discussed subtopics in the field of long-term care, and identify its features through NLP. This study applied TF-IDF, the Logistic Regression model, and the Naive Bayes classifier to process data. In sum, the results showed that it reached a best F1-score of 0.920 in identification, and a best accuracy of 0.708 in classification. The results of this study could be used as a reference for future long-term care related applications.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.1523032933,"Goal":"Good Health and Well-Being","Task":["Text Classification of Long - term Care Materials Aging populations","long - term care","long - term care","identification","classification","long - term care related applications"],"Method":["NLP","TF - IDF","Logistic Regression model","Naive Bayes classifier"]},{"ID":"shekarpour-etal-2020-qa2explanation","title":"{QA}2{E}xplanation: Generating and Evaluating Explanations for Question Answering Systems over Knowledge Graph","abstract":"In the era of Big Knowledge Graphs, Question Answering (QA) systems have reached a milestone in their performance and feasibility. However, their applicability, particularly in specific domains such as the biomedical domain, has not gained wide acceptance due to their {``}black box{''} nature, which hinders transparency, fairness, and accountability of QA systems. Therefore, users are unable to understand how and why particular questions have been answered, whereas some others fail. To address this challenge, in this paper, we develop an automatic approach for generating explanations during various stages of a pipeline-based QA system. Our approach is a supervised and automatic approach which considers three classes (i.e., success, no answer, and wrong answer) for annotating the output of involved QA components. Upon our prediction, a template explanation is chosen and integrated into the output of the corresponding component. To measure the effectiveness of the approach, we conducted a user survey as to how non-expert users perceive our generated explanations. The results of our study show a significant increase in the four dimensions of the human factor from the Human-computer interaction community.","year":2020,"title_abstract":"{QA}2{E}xplanation: Generating and Evaluating Explanations for Question Answering Systems over Knowledge Graph In the era of Big Knowledge Graphs, Question Answering (QA) systems have reached a milestone in their performance and feasibility. However, their applicability, particularly in specific domains such as the biomedical domain, has not gained wide acceptance due to their {``}black box{''} nature, which hinders transparency, fairness, and accountability of QA systems. Therefore, users are unable to understand how and why particular questions have been answered, whereas some others fail. To address this challenge, in this paper, we develop an automatic approach for generating explanations during various stages of a pipeline-based QA system. Our approach is a supervised and automatic approach which considers three classes (i.e., success, no answer, and wrong answer) for annotating the output of involved QA components. Upon our prediction, a template explanation is chosen and integrated into the output of the corresponding component. To measure the effectiveness of the approach, we conducted a user survey as to how non-expert users perceive our generated explanations. The results of our study show a significant increase in the four dimensions of the human factor from the Human-computer interaction community.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1522862315,"Goal":"Quality Education","Task":["Generating and Evaluating Explanations","Question Answering Systems","Big Knowledge Graphs","Question Answering","QA","generating explanations","QA","QA","prediction","Human - computer interaction community"],"Method":["Knowledge Graph","automatic approach","supervised and automatic approach","template explanation"]},{"ID":"adouane-etal-2019-normalising","title":"Normalising Non-standardised Orthography in {A}lgerian Code-switched User-generated Data","abstract":"We work with Algerian, an under-resourced non-standardised Arabic variety, for which we compile a new parallel corpus consisting of user-generated textual data matched with normalised and corrected human annotations following data-driven and our linguistically motivated standard. We use an end-to-end deep neural model designed to deal with context-dependent spelling correction and normalisation. Results indicate that a model with two CNN sub-network encoders and an LSTM decoder performs the best, and that word context matters. Additionally, pre-processing data token-by-token with an edit-distance based aligner significantly improves the performance. We get promising results for the spelling correction and normalisation, as a pre-processing step for downstream tasks, on detecting binary Semantic Textual Similarity.","year":2019,"title_abstract":"Normalising Non-standardised Orthography in {A}lgerian Code-switched User-generated Data We work with Algerian, an under-resourced non-standardised Arabic variety, for which we compile a new parallel corpus consisting of user-generated textual data matched with normalised and corrected human annotations following data-driven and our linguistically motivated standard. We use an end-to-end deep neural model designed to deal with context-dependent spelling correction and normalisation. Results indicate that a model with two CNN sub-network encoders and an LSTM decoder performs the best, and that word context matters. Additionally, pre-processing data token-by-token with an edit-distance based aligner significantly improves the performance. We get promising results for the spelling correction and normalisation, as a pre-processing step for downstream tasks, on detecting binary Semantic Textual Similarity.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1522854865,"Goal":"Gender Equality","Task":["Normalising Non - standardised Orthography","context - dependent spelling correction","normalisation","spelling correction","normalisation","pre - processing step","downstream tasks","detecting binary Semantic Textual Similarity"],"Method":["end - to - end deep neural model","CNN sub - network encoders","LSTM decoder","edit - distance based aligner"]},{"ID":"morland-2002-getting","title":"Getting the message in: a global company{'}s experience with the new generation of low-cost,high-performance machine translation systems","abstract":"Most large companies are very good at {``}getting the message out{''} {--}publishing reams of announcements and documentation to their employees and customers. More challenging by far is {``}getting the message in{''} {--} ensuring that these messages are read, understood, and acted upon by the recipients. This paper describes NCR Corporation{'}s experience with the selection and implementation of a machine translation (MT) system in the Global Learning division of Human Resources. The author summarizes NCR{`}s vision for the use of MT, the competitive {``}fly-off{''} evaluation process he conducted in the spring of 2000, the current MT production environment, and the reactions of the MT users. Although the vision is not yet fulfilled, progress is being made. The author describes NCR{'}s plans to extend its current MT architecture to provide real-time translation of web pages and other intranet resources.","year":2002,"title_abstract":"Getting the message in: a global company{'}s experience with the new generation of low-cost,high-performance machine translation systems Most large companies are very good at {``}getting the message out{''} {--}publishing reams of announcements and documentation to their employees and customers. More challenging by far is {``}getting the message in{''} {--} ensuring that these messages are read, understood, and acted upon by the recipients. This paper describes NCR Corporation{'}s experience with the selection and implementation of a machine translation (MT) system in the Global Learning division of Human Resources. The author summarizes NCR{`}s vision for the use of MT, the competitive {``}fly-off{''} evaluation process he conducted in the spring of 2000, the current MT production environment, and the reactions of the MT users. Although the vision is not yet fulfilled, progress is being made. The author describes NCR{'}s plans to extend its current MT architecture to provide real-time translation of web pages and other intranet resources.","social_need":"Decent Work and Economic Growth Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all","cosine_similarity":0.1522453874,"Goal":"Decent Work and Economic Growth","Task":["low - cost , high - performance machine translation systems","machine translation","Global Learning division of Human Resources","MT","competitive {``}fly - off{''} evaluation process","MT production environment","MT","MT","real - time translation"],"Method":["NCR","NCR{`}s vision"]},{"ID":"prabhumoye-etal-2019-principled","title":"Principled Frameworks for Evaluating Ethics in {NLP} Systems","abstract":"We critique recent work on ethics in natural language processing. Those discussions have focused on data collection, experimental design, and interventions in modeling. But we argue that we ought to first understand the frameworks of ethics that are being used to evaluate the fairness and justice of algorithmic systems. Here, we begin that discussion by outlining deontological and consequentialist ethics, and make predictions on the research agenda prioritized by each.","year":2019,"title_abstract":"Principled Frameworks for Evaluating Ethics in {NLP} Systems We critique recent work on ethics in natural language processing. Those discussions have focused on data collection, experimental design, and interventions in modeling. But we argue that we ought to first understand the frameworks of ethics that are being used to evaluate the fairness and justice of algorithmic systems. Here, we begin that discussion by outlining deontological and consequentialist ethics, and make predictions on the research agenda prioritized by each.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.1522165835,"Goal":"Peace, Justice and Strong Institutions","Task":["Evaluating Ethics","{NLP} Systems","ethics","natural language processing","data collection","experimental design","interventions","modeling","algorithmic systems"],"Method":["Principled Frameworks"]},{"ID":"jaradat-etal-2018-claimrank","title":"{C}laim{R}ank: Detecting Check-Worthy Claims in {A}rabic and {E}nglish","abstract":"We present ClaimRank, an online system for detecting check-worthy claims. While originally trained on political debates, the system can work for any kind of text, e.g., interviews or just regular news articles. Its aim is to facilitate manual fact-checking efforts by prioritizing the claims that fact-checkers should consider first. ClaimRank supports both Arabic and English, it is trained on actual annotations from nine reputable fact-checking organizations (PolitiFact, FactCheck, ABC, CNN, NPR, NYT, Chicago Tribune, The Guardian, and Washington Post), and thus it can mimic the claim selection strategies for each and any of them, as well as for the union of them all.","year":2018,"title_abstract":"{C}laim{R}ank: Detecting Check-Worthy Claims in {A}rabic and {E}nglish We present ClaimRank, an online system for detecting check-worthy claims. While originally trained on political debates, the system can work for any kind of text, e.g., interviews or just regular news articles. Its aim is to facilitate manual fact-checking efforts by prioritizing the claims that fact-checkers should consider first. ClaimRank supports both Arabic and English, it is trained on actual annotations from nine reputable fact-checking organizations (PolitiFact, FactCheck, ABC, CNN, NPR, NYT, Chicago Tribune, The Guardian, and Washington Post), and thus it can mimic the claim selection strategies for each and any of them, as well as for the union of them all.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1522054374,"Goal":"Climate Action","Task":["Detecting Check - Worthy Claims","detecting check - worthy claims","manual fact - checking efforts"],"Method":["ClaimRank","online system","ClaimRank","claim selection strategies"]},{"ID":"blodgett-etal-2021-stereotyping","title":"Stereotyping {N}orwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets","abstract":"Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal. Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system{'}s behavior on these pairs into measurements of harms. We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution. We apply a measurement modeling lens{---}originating from the social sciences{---}to inventory a range of pitfalls that threaten these benchmarks{'} validity as measurement models for stereotyping. We find that these benchmarks frequently lack clear articulations of what is being measured, and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping.","year":2021,"title_abstract":"Stereotyping {N}orwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal. Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system{'}s behavior on these pairs into measurements of harms. We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution. We apply a measurement modeling lens{---}originating from the social sciences{---}to inventory a range of pitfalls that threaten these benchmarks{'} validity as measurement models for stereotyping. We find that these benchmarks frequently lack clear articulations of what is being measured, and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1521994621,"Goal":"Reduced Inequalities","Task":["NLP","computational harms","surfacing stereotypes","NLP","NLP","language modeling","coreference resolution","stereotyping"],"Method":["measurement modeling lens{","measurement models"]},{"ID":"yang-etal-2018-hotpotqa","title":"{H}otpot{QA}: A Dataset for Diverse, Explainable Multi-hop Question Answering","abstract":"Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems{'} ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.","year":2018,"title_abstract":"{H}otpot{QA}: A Dataset for Diverse, Explainable Multi-hop Question Answering Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems{'} ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1521907151,"Goal":"Quality Education","Task":["Diverse , Explainable Multi - hop Question Answering","question answering","QA","reasoning","reasoning","QA","QA","QA"],"Method":["HotpotQA","HotpotQA"]},{"ID":"milde-etal-2016-demonstrating","title":"Demonstrating Ambient Search: Implicit Document Retrieval for Speech Streams","abstract":"In this demonstration paper we describe Ambient Search, a system that displays and retrieves documents in real time based on speech input. The system operates continuously in ambient mode, i.e. it generates speech transcriptions and identifies main keywords and keyphrases, while also querying its index to display relevant documents without explicit query. Without user intervention, the results are dynamically updated; users can choose to interact with the system at any time, employing a conversation protocol that is enriched with the ambient information gathered continuously. Our evaluation shows that Ambient Search outperforms another implicit speech-based information retrieval system. Ambient search is available as open source software.","year":2016,"title_abstract":"Demonstrating Ambient Search: Implicit Document Retrieval for Speech Streams In this demonstration paper we describe Ambient Search, a system that displays and retrieves documents in real time based on speech input. The system operates continuously in ambient mode, i.e. it generates speech transcriptions and identifies main keywords and keyphrases, while also querying its index to display relevant documents without explicit query. Without user intervention, the results are dynamically updated; users can choose to interact with the system at any time, employing a conversation protocol that is enriched with the ambient information gathered continuously. Our evaluation shows that Ambient Search outperforms another implicit speech-based information retrieval system. Ambient search is available as open source software.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.1521885246,"Goal":"Life Below Water","Task":["Ambient Search","Implicit Document Retrieval","Speech Streams","Ambient search"],"Method":["Ambient Search","conversation protocol","Ambient Search","implicit speech - based information retrieval system"]},{"ID":"sharma-etal-2022-findings","title":"Findings of the {CONSTRAINT} 2022 Shared Task on Detecting the Hero, the Villain, and the Victim in Memes","abstract":"We present the findings of the shared task at the CONSTRAINT 2022 Workshop: Hero, Villain, and Victim: Dissecting harmful memes for Semantic role labeling of entities. The task aims to delve deeper into the domain of meme comprehension by deciphering the connotations behind the entities present in a meme. In more nuanced terms, the shared task focuses on determining the victimizing, glorifying, and vilifying intentions embedded in meme entities to explicate their connotations. To this end, we curate HVVMemes, a novel meme dataset of about 7000 memes spanning the domains of COVID-19 and US Politics, each containing entities and their associated roles: hero, villain, victim, or none. The shared task attracted 105 participants, but eventually only 6 submissions were made. Most of the successful submissions relied on fine-tuning pre-trained language and multimodal models along with ensembles. The best submission achieved an F1-score of 58.67.","year":2022,"title_abstract":"Findings of the {CONSTRAINT} 2022 Shared Task on Detecting the Hero, the Villain, and the Victim in Memes We present the findings of the shared task at the CONSTRAINT 2022 Workshop: Hero, Villain, and Victim: Dissecting harmful memes for Semantic role labeling of entities. The task aims to delve deeper into the domain of meme comprehension by deciphering the connotations behind the entities present in a meme. In more nuanced terms, the shared task focuses on determining the victimizing, glorifying, and vilifying intentions embedded in meme entities to explicate their connotations. To this end, we curate HVVMemes, a novel meme dataset of about 7000 memes spanning the domains of COVID-19 and US Politics, each containing entities and their associated roles: hero, villain, victim, or none. The shared task attracted 105 participants, but eventually only 6 submissions were made. Most of the successful submissions relied on fine-tuning pre-trained language and multimodal models along with ensembles. The best submission achieved an F1-score of 58.67.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1521736532,"Goal":"Climate Action","Task":["{CONSTRAINT} 2022 Shared Task","CONSTRAINT 2022 Workshop","Semantic role labeling of entities","meme comprehension","shared task"],"Method":["HVVMemes","language and multimodal models","ensembles"]},{"ID":"bosch-griesel-2018-african","title":"{A}frican {W}ordnet: facilitating language learning in {A}frican languages","abstract":"The development of the African Wordnet (AWN) has reached a stage of maturity where the first steps towards an application can be attempted. The AWN is based on the expand method, and to compensate for the general resource scarceness of the African languages, various development strategies were used. The aim of this paper is to investigate the usefulness of the current isiZulu Wordnet in an application such as language learning. The advantage of incorporating the wordnet of a language into a language learning system is that it provides learners with an integrated application to enhance their learning experience by means of the unique sense identification features of wordnets. In this paper it will be demonstrated by means of a variety of examples within the context of a basic free online course how the isiZulu Wordnet can offer the language learner improved decision support.","year":2018,"title_abstract":"{A}frican {W}ordnet: facilitating language learning in {A}frican languages The development of the African Wordnet (AWN) has reached a stage of maturity where the first steps towards an application can be attempted. The AWN is based on the expand method, and to compensate for the general resource scarceness of the African languages, various development strategies were used. The aim of this paper is to investigate the usefulness of the current isiZulu Wordnet in an application such as language learning. The advantage of incorporating the wordnet of a language into a language learning system is that it provides learners with an integrated application to enhance their learning experience by means of the unique sense identification features of wordnets. In this paper it will be demonstrated by means of a variety of examples within the context of a basic free online course how the isiZulu Wordnet can offer the language learner improved decision support.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1521546841,"Goal":"Quality Education","Task":["language learning","language learning","decision support"],"Method":["AWN","expand method","development strategies","language learning system","wordnets","language learner"]},{"ID":"field-tsvetkov-2019-entity","title":"Entity-Centric Contextual Affective Analysis","abstract":"While contextualized word representations have improved state-of-the-art benchmarks in many NLP tasks, their potential usefulness for social-oriented tasks remains largely unexplored. We show how contextualized word embeddings can be used to capture affect dimensions in portrayals of people. We evaluate our methodology quantitatively, on held-out affect lexicons, and qualitatively, through case examples. We find that contextualized word representations do encode meaningful affect information, but they are heavily biased towards their training data, which limits their usefulness to in-domain analyses. We ultimately use our method to examine differences in portrayals of men and women.","year":2019,"title_abstract":"Entity-Centric Contextual Affective Analysis While contextualized word representations have improved state-of-the-art benchmarks in many NLP tasks, their potential usefulness for social-oriented tasks remains largely unexplored. We show how contextualized word embeddings can be used to capture affect dimensions in portrayals of people. We evaluate our methodology quantitatively, on held-out affect lexicons, and qualitatively, through case examples. We find that contextualized word representations do encode meaningful affect information, but they are heavily biased towards their training data, which limits their usefulness to in-domain analyses. We ultimately use our method to examine differences in portrayals of men and women.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1521427631,"Goal":"Gender Equality","Task":["NLP tasks","social - oriented tasks","in - domain analyses"],"Method":["Entity - Centric Contextual Affective Analysis","contextualized word representations","contextualized word embeddings","contextualized word representations"]},{"ID":"kreutz-daelemans-2018-enhancing","title":"Enhancing General Sentiment Lexicons for Domain-Specific Use","abstract":"Lexicon based methods for sentiment analysis rely on high quality polarity lexicons. In recent years, automatic methods for inducing lexicons have increased the viability of lexicon based methods for polarity classification. SentProp is a framework for inducing domain-specific polarities from word embeddings. We elaborate on SentProp by evaluating its use for enhancing DuOMan, a general-purpose lexicon, for use in the political domain. By adding only top sentiment bearing words from the vocabulary and applying small polarity shifts in the general-purpose lexicon, we increase accuracy in an in-domain classification task. The enhanced lexicon performs worse than the original lexicon in an out-domain task, showing that the words we added and the polarity shifts we applied are domain-specific and do not translate well to an out-domain setting.","year":2018,"title_abstract":"Enhancing General Sentiment Lexicons for Domain-Specific Use Lexicon based methods for sentiment analysis rely on high quality polarity lexicons. In recent years, automatic methods for inducing lexicons have increased the viability of lexicon based methods for polarity classification. SentProp is a framework for inducing domain-specific polarities from word embeddings. We elaborate on SentProp by evaluating its use for enhancing DuOMan, a general-purpose lexicon, for use in the political domain. By adding only top sentiment bearing words from the vocabulary and applying small polarity shifts in the general-purpose lexicon, we increase accuracy in an in-domain classification task. The enhanced lexicon performs worse than the original lexicon in an out-domain task, showing that the words we added and the polarity shifts we applied are domain-specific and do not translate well to an out-domain setting.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1521296352,"Goal":"Reduced Inequalities","Task":["Domain - Specific Use","sentiment analysis","inducing lexicons","polarity classification","inducing domain - specific polarities","word embeddings","in - domain classification task","out - domain task"],"Method":["General Sentiment Lexicons","Lexicon based methods","automatic methods","lexicon based methods","SentProp","SentProp","DuOMan","enhanced lexicon"]},{"ID":"torsi-morante-2018-annotating","title":"Annotating Claims in the Vaccination Debate","abstract":"In this paper we present annotation experiments with three different annotation schemes for the identification of argument components in texts related to the vaccination debate. Identifying claims about vaccinations made by participants in the debate is of great societal interest, as the decision to vaccinate or not has impact in public health and safety. Since most corpora that have been annotated with argumentation information contain texts that belong to a specific genre and have a well defined argumentation structure, we needed to adjust the annotation schemes to our corpus, which contains heterogeneous texts from the Web. We started with a complex annotation scheme that had to be simplified due to low IAA. In our final experiment, which focused on annotating claims, annotators reached 57.3{\\%} IAA.","year":2018,"title_abstract":"Annotating Claims in the Vaccination Debate In this paper we present annotation experiments with three different annotation schemes for the identification of argument components in texts related to the vaccination debate. Identifying claims about vaccinations made by participants in the debate is of great societal interest, as the decision to vaccinate or not has impact in public health and safety. Since most corpora that have been annotated with argumentation information contain texts that belong to a specific genre and have a well defined argumentation structure, we needed to adjust the annotation schemes to our corpus, which contains heterogeneous texts from the Web. We started with a complex annotation scheme that had to be simplified due to low IAA. In our final experiment, which focused on annotating claims, annotators reached 57.3{\\%} IAA.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1521295905,"Goal":"Climate Action","Task":["Annotating Claims","Vaccination Debate","annotation","identification of argument components","vaccination debate","Identifying claims","public health","safety","annotating claims"],"Method":["annotation schemes","annotation schemes","annotation scheme"]},{"ID":"noble-maraev-2021-large","title":"Large-scale text pre-training helps with dialogue act recognition, but not without fine-tuning","abstract":"We use dialogue act recognition (DAR) to investigate how well BERT represents utterances in dialogue, and how fine-tuning and large-scale pre-training contribute to its performance. We find that while both the standard BERT pre-training and pretraining on dialogue-like data are useful, task-specific fine-tuning is essential for good performance.","year":2021,"title_abstract":"Large-scale text pre-training helps with dialogue act recognition, but not without fine-tuning We use dialogue act recognition (DAR) to investigate how well BERT represents utterances in dialogue, and how fine-tuning and large-scale pre-training contribute to its performance. We find that while both the standard BERT pre-training and pretraining on dialogue-like data are useful, task-specific fine-tuning is essential for good performance.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1520985216,"Goal":"Climate Action","Task":["Large - scale text pre - training","dialogue act recognition","large - scale pre - training","task - specific fine - tuning"],"Method":["fine - tuning","dialogue act recognition","BERT","fine - tuning","BERT","pretraining"]},{"ID":"zhang-etal-2021-human","title":"A Human-machine Collaborative Framework for Evaluating Malevolence in Dialogues","abstract":"Conversational dialogue systems (CDSs) are hard to evaluate due to the complexity of natural language. Automatic evaluation of dialogues often shows insufficient correlation with human judgements. Human evaluation is reliable but labor-intensive. We introduce a human-machine collaborative framework, HMCEval, that can guarantee reliability of the evaluation outcomes with reduced human effort. HMCEval casts dialogue evaluation as a sample assignment problem, where we need to decide to assign a sample to a human or a machine for evaluation. HMCEval includes a model confidence estimation module to estimate the confidence of the predicted sample assignment, and a human effort estimation module to estimate the human effort should the sample be assigned to human evaluation, as well as a sample assignment execution module that finds the optimum assignment solution based on the estimated confidence and effort. We assess the performance of HMCEval on the task of evaluating malevolence in dialogues. The experimental results show that HMCEval achieves around 99{\\%} evaluation accuracy with half of the human effort spared, showing that HMCEval provides reliable evaluation outcomes while reducing human effort by a large amount.","year":2021,"title_abstract":"A Human-machine Collaborative Framework for Evaluating Malevolence in Dialogues Conversational dialogue systems (CDSs) are hard to evaluate due to the complexity of natural language. Automatic evaluation of dialogues often shows insufficient correlation with human judgements. Human evaluation is reliable but labor-intensive. We introduce a human-machine collaborative framework, HMCEval, that can guarantee reliability of the evaluation outcomes with reduced human effort. HMCEval casts dialogue evaluation as a sample assignment problem, where we need to decide to assign a sample to a human or a machine for evaluation. HMCEval includes a model confidence estimation module to estimate the confidence of the predicted sample assignment, and a human effort estimation module to estimate the human effort should the sample be assigned to human evaluation, as well as a sample assignment execution module that finds the optimum assignment solution based on the estimated confidence and effort. We assess the performance of HMCEval on the task of evaluating malevolence in dialogues. The experimental results show that HMCEval achieves around 99{\\%} evaluation accuracy with half of the human effort spared, showing that HMCEval provides reliable evaluation outcomes while reducing human effort by a large amount.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1520427763,"Goal":"Gender Equality","Task":["Evaluating Malevolence in Dialogues","Conversational dialogue systems","Automatic evaluation of dialogues","Human evaluation","dialogue evaluation","sample assignment problem","evaluation","predicted sample assignment","human evaluation","evaluating malevolence in dialogues"],"Method":["Human - machine Collaborative Framework","human - machine collaborative framework","HMCEval","HMCEval","HMCEval","model confidence estimation module","human effort estimation module","sample assignment execution module","assignment solution","HMCEval","HMCEval","HMCEval"]},{"ID":"sadat-etal-2006-systeme","title":"Syst{\\`e}me de traduction automatique statistique combinant diff{\\'e}rentes ressources","abstract":"Cet article d{\\'e}crit une approche combinant diff{\\'e}rents mod{\\`e}les statistiques pour la traduction automatique bas{\\'e}e sur les segments. Pour ce faire, diff{\\'e}rentes ressources sont utilis{\\'e}es, dont deux corpus parall{\\`e}les aux caract{\\'e}ristiques diff{\\'e}rentes et un dictionnaire de terminologie bilingue et ce, afin d{'}am{\\'e}liorer la performance quantitative et qualitative du syst{\\`e}me de traduction. Nous {\\'e}valuons notre approche sur la paire de langues fran{\\c{c}}ais-anglais et montrons comment la combinaison des ressources propos{\\'e}es am{\\'e}liore de fa{\\c{c}}on significative les r{\\'e}sultats.","year":2006,"title_abstract":"Syst{\\`e}me de traduction automatique statistique combinant diff{\\'e}rentes ressources Cet article d{\\'e}crit une approche combinant diff{\\'e}rents mod{\\`e}les statistiques pour la traduction automatique bas{\\'e}e sur les segments. Pour ce faire, diff{\\'e}rentes ressources sont utilis{\\'e}es, dont deux corpus parall{\\`e}les aux caract{\\'e}ristiques diff{\\'e}rentes et un dictionnaire de terminologie bilingue et ce, afin d{'}am{\\'e}liorer la performance quantitative et qualitative du syst{\\`e}me de traduction. Nous {\\'e}valuons notre approche sur la paire de langues fran{\\c{c}}ais-anglais et montrons comment la combinaison des ressources propos{\\'e}es am{\\'e}liore de fa{\\c{c}}on significative les r{\\'e}sultats.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1520162076,"Goal":"Reduced Inequalities","Task":["traduction automatique"],"Method":["traduction automatique statistique","mod{\\`e}les statistiques"]},{"ID":"rezapour-etal-2020-beyond","title":"Beyond Citations: Corpus-based Methods for Detecting the Impact of Research Outcomes on Society","abstract":"This paper proposes, implements and evaluates a novel, corpus-based approach for identifying categories indicative of the impact of research via a deductive (top-down, from theory to data) and an inductive (bottom-up, from data to theory) approach. The resulting categorization schemes differ in substance. Research outcomes are typically assessed by using bibliometric methods, such as citation counts and patterns, or alternative metrics, such as references to research in the media. Shortcomings with these methods are their inability to identify impact of research beyond academia (bibliometrics) and considering text-based impact indicators beyond those that capture attention (altmetrics). We address these limitations by leveraging a mixed-methods approach for eliciting impact categories from experts, project personnel (deductive) and texts (inductive). Using these categories, we label a corpus of project reports per category schema, and apply supervised machine learning to infer these categories from project reports. The classification results show that we can predict deductively and inductively derived impact categories with 76.39{\\%} and 78.81{\\%} accuracy (F1-score), respectively. Our approach can complement solutions from bibliometrics and scientometrics for assessing the impact of research and studying the scope and types of advancements transferred from academia to society.","year":2020,"title_abstract":"Beyond Citations: Corpus-based Methods for Detecting the Impact of Research Outcomes on Society This paper proposes, implements and evaluates a novel, corpus-based approach for identifying categories indicative of the impact of research via a deductive (top-down, from theory to data) and an inductive (bottom-up, from data to theory) approach. The resulting categorization schemes differ in substance. Research outcomes are typically assessed by using bibliometric methods, such as citation counts and patterns, or alternative metrics, such as references to research in the media. Shortcomings with these methods are their inability to identify impact of research beyond academia (bibliometrics) and considering text-based impact indicators beyond those that capture attention (altmetrics). We address these limitations by leveraging a mixed-methods approach for eliciting impact categories from experts, project personnel (deductive) and texts (inductive). Using these categories, we label a corpus of project reports per category schema, and apply supervised machine learning to infer these categories from project reports. The classification results show that we can predict deductively and inductively derived impact categories with 76.39{\\%} and 78.81{\\%} accuracy (F1-score), respectively. Our approach can complement solutions from bibliometrics and scientometrics for assessing the impact of research and studying the scope and types of advancements transferred from academia to society.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1519929767,"Goal":"Sustainable Cities and Communities","Task":["Impact of Research Outcomes","classification","impact of research"],"Method":["Corpus - based Methods","corpus - based approach","inductive (bottom - up","categorization schemes","bibliometric methods","text - based impact indicators","mixed - methods approach","supervised machine learning","bibliometrics","scientometrics"]},{"ID":"shin-etal-2020-risk","title":"A Risk Communication Event Detection Model via Contrastive Learning","abstract":"This paper presents a time-topic cohesive model describing the communication patterns on the coronavirus pandemic from three Asian countries. The strength of our model is two-fold. First, it detects contextualized events based on topical and temporal information via contrastive learning. Second, it can be applied to multiple languages, enabling a comparison of risk communication across cultures. We present a case study and discuss future implications of the proposed model.","year":2020,"title_abstract":"A Risk Communication Event Detection Model via Contrastive Learning This paper presents a time-topic cohesive model describing the communication patterns on the coronavirus pandemic from three Asian countries. The strength of our model is two-fold. First, it detects contextualized events based on topical and temporal information via contrastive learning. Second, it can be applied to multiple languages, enabling a comparison of risk communication across cultures. We present a case study and discuss future implications of the proposed model.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1519817263,"Goal":"Climate Action","Task":["risk communication"],"Method":["Risk Communication Event Detection Model","Contrastive Learning","time - topic cohesive model","contrastive learning"]},{"ID":"ravenscroft-etal-2018-harrigt","title":"{H}arri{GT}: A Tool for Linking News to Science","abstract":"Being able to reliably link scientific works to the newspaper articles that discuss them could provide a breakthrough in the way we rationalise and measure the impact of science on our society. Linking these articles is challenging because the language used in the two domains is very different, and the gathering of online resources to align the two is a substantial information retrieval endeavour. We present HarriGT, a semi-automated tool for building corpora of news articles linked to the scientific papers that they discuss. Our aim is to facilitate future development of information-retrieval tools for newspaper\/scientific work citation linking. HarriGT retrieves newspaper articles from an archive containing 17 years of UK web content. It also integrates with 3 large external citation networks, leveraging named entity extraction, and document classification to surface relevant examples of scientific literature to the user. We also provide a tuned candidate ranking algorithm to highlight potential links between scientific papers and newspaper articles to the user, in order of likelihood. HarriGT is provided as an open source tool (\\url{http:\/\/harrigt.xyz}).","year":2018,"title_abstract":"{H}arri{GT}: A Tool for Linking News to Science Being able to reliably link scientific works to the newspaper articles that discuss them could provide a breakthrough in the way we rationalise and measure the impact of science on our society. Linking these articles is challenging because the language used in the two domains is very different, and the gathering of online resources to align the two is a substantial information retrieval endeavour. We present HarriGT, a semi-automated tool for building corpora of news articles linked to the scientific papers that they discuss. Our aim is to facilitate future development of information-retrieval tools for newspaper\/scientific work citation linking. HarriGT retrieves newspaper articles from an archive containing 17 years of UK web content. It also integrates with 3 large external citation networks, leveraging named entity extraction, and document classification to surface relevant examples of scientific literature to the user. We also provide a tuned candidate ranking algorithm to highlight potential links between scientific papers and newspaper articles to the user, in order of likelihood. HarriGT is provided as an open source tool (\\url{http:\/\/harrigt.xyz}).","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1519678235,"Goal":"Climate Action","Task":["Linking News to Science","information retrieval endeavour","newspaper\/scientific work citation linking"],"Method":["HarriGT","semi - automated tool","information - retrieval tools","HarriGT","external citation networks","named entity extraction","document classification","tuned candidate ranking algorithm","HarriGT"]},{"ID":"chauhan-2020-neu","title":"{NEU} at {WNUT}-2020 Task 2: Data Augmentation To Tell {BERT} That Death Is Not Necessarily Informative","abstract":"Millions of people around the world are sharing COVID-19 related information on social media platforms. Since not all the information shared on the social media is useful, a machine learning system to identify informative posts can help users in finding relevant information. In this paper, we present a BERT classifier system for W-NUT2020 Shared Task 2: Identification of Informative COVID-19 English Tweets. Further, we show that BERT exploits some easy signals to identify informative tweets, and adding simple patterns to uninformative tweets drastically degrades BERT performance. In particular, simply adding {``}10 deaths{''} to tweets in dev set, reduces BERT F1- score from 92.63 to 7.28. We also propose a simple data augmentation technique that helps in improving the robustness and generalization ability of the BERT classifier.","year":2020,"title_abstract":"{NEU} at {WNUT}-2020 Task 2: Data Augmentation To Tell {BERT} That Death Is Not Necessarily Informative Millions of people around the world are sharing COVID-19 related information on social media platforms. Since not all the information shared on the social media is useful, a machine learning system to identify informative posts can help users in finding relevant information. In this paper, we present a BERT classifier system for W-NUT2020 Shared Task 2: Identification of Informative COVID-19 English Tweets. Further, we show that BERT exploits some easy signals to identify informative tweets, and adding simple patterns to uninformative tweets drastically degrades BERT performance. In particular, simply adding {``}10 deaths{''} to tweets in dev set, reduces BERT F1- score from 92.63 to 7.28. We also propose a simple data augmentation technique that helps in improving the robustness and generalization ability of the BERT classifier.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1518892795,"Goal":"Climate Action","Task":["Data Augmentation","W - NUT2020","Identification of Informative COVID - 19"],"Method":["machine learning system","BERT","BERT","BERT","data augmentation technique","BERT"]},{"ID":"muti-etal-2022-leaningtower","title":"{L}eaning{T}ower@{LT}-{EDI}-{ACL}2022: When Hope and Hate Collide","abstract":"The 2022 edition of LT-EDI proposed two tasks in various languages. Task Hope Speech Detection required models for the automatic identification of hopeful comments for equality, diversity, and inclusion. Task Homophobia\/Transphobia Detection focused on the identification of homophobic and transphobic comments. We targeted both tasks in English by using reinforced BERT-based approaches. Our core strategy aimed at exploiting the data available for each given task to augment the amount of supervised instances in the other. On the basis of an active learning process, we trained a model on the dataset for Task $i$ and applied it to the dataset for Task $j$ to iteratively integrate new silver data for Task $i$. Our official submissions to the shared task obtained a macro-averaged F$_1$ score of 0.53 for Hope Speech and 0.46 for Homo\/Transphobia, placing our team in the third and fourth positions out of 11 and 12 participating teams respectively.","year":2022,"title_abstract":"{L}eaning{T}ower@{LT}-{EDI}-{ACL}2022: When Hope and Hate Collide The 2022 edition of LT-EDI proposed two tasks in various languages. Task Hope Speech Detection required models for the automatic identification of hopeful comments for equality, diversity, and inclusion. Task Homophobia\/Transphobia Detection focused on the identification of homophobic and transphobic comments. We targeted both tasks in English by using reinforced BERT-based approaches. Our core strategy aimed at exploiting the data available for each given task to augment the amount of supervised instances in the other. On the basis of an active learning process, we trained a model on the dataset for Task $i$ and applied it to the dataset for Task $j$ to iteratively integrate new silver data for Task $i$. Our official submissions to the shared task obtained a macro-averaged F$_1$ score of 0.53 for Hope Speech and 0.46 for Homo\/Transphobia, placing our team in the third and fourth positions out of 11 and 12 participating teams respectively.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1518624425,"Goal":"Gender Equality","Task":["Hope Speech Detection","automatic identification of hopeful comments","equality","inclusion","Task Homophobia\/Transphobia Detection","identification of homophobic and transphobic comments","Hope Speech"],"Method":["LT - EDI","reinforced BERT - based approaches","active learning process"]},{"ID":"mckillop-2019-predicting","title":"Predicting the Outcome of Deliberative Democracy: A Research Proposal","abstract":"As liberal states across the world face a decline in political participation by citizens, deliberative democracy is a promising solution for the public{'}s decreasing confidence and apathy towards the democratic process. Deliberative dialogue is method of public interaction that is fundamental to the concept of deliberative democracy. The ability to identify and predict consensus in the dialogues could bring greater accessibility and transparency to the face-to-face participatory process. The paper sets out a research plan for the first steps at automatically identifying and predicting consensus in a corpus of German language debates on hydraulic fracking. It proposes the use of a unique combination of lexical, sentiment, durational and further {`}derivative{'} features of adjacency pairs to train traditional classification models. In addition to this, the use of deep learning techniques to improve the accuracy of the classification and prediction tasks is also discussed. Preliminary results at the classification of utterances are also presented, with an F1 between 0.61 and 0.64 demonstrating that the task of recognising agreement is demanding but possible.","year":2019,"title_abstract":"Predicting the Outcome of Deliberative Democracy: A Research Proposal As liberal states across the world face a decline in political participation by citizens, deliberative democracy is a promising solution for the public{'}s decreasing confidence and apathy towards the democratic process. Deliberative dialogue is method of public interaction that is fundamental to the concept of deliberative democracy. The ability to identify and predict consensus in the dialogues could bring greater accessibility and transparency to the face-to-face participatory process. The paper sets out a research plan for the first steps at automatically identifying and predicting consensus in a corpus of German language debates on hydraulic fracking. It proposes the use of a unique combination of lexical, sentiment, durational and further {`}derivative{'} features of adjacency pairs to train traditional classification models. In addition to this, the use of deep learning techniques to improve the accuracy of the classification and prediction tasks is also discussed. Preliminary results at the classification of utterances are also presented, with an F1 between 0.61 and 0.64 demonstrating that the task of recognising agreement is demanding but possible.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1518603265,"Goal":"Climate Action","Task":["Predicting the Outcome of Deliberative Democracy","democratic process","public interaction","deliberative democracy","face - to - face participatory process","automatically identifying and predicting consensus","hydraulic fracking","classification and prediction tasks","classification of utterances","recognising agreement"],"Method":["Deliberative dialogue","classification models","deep learning techniques"]},{"ID":"he-etal-2021-speaker-turn","title":"Speaker Turn Modeling for Dialogue Act Classification","abstract":"Dialogue Act (DA) classification is the task of classifying utterances with respect to the function they serve in a dialogue. Existing approaches to DA classification model utterances without incorporating the turn changes among speakers throughout the dialogue, therefore treating it no different than non-interactive written text. In this paper, we propose to integrate the turn changes in conversations among speakers when modeling DAs. Specifically, we learn conversation-invariant speaker turn embeddings to represent the speaker turns in a conversation; the learned speaker turn embeddings are then merged with the utterance embeddings for the downstream task of DA classification. With this simple yet effective mechanism, our model is able to capture the semantics from the dialogue content while accounting for different speaker turns in a conversation. Validation on three benchmark public datasets demonstrates superior performance of our model.","year":2021,"title_abstract":"Speaker Turn Modeling for Dialogue Act Classification Dialogue Act (DA) classification is the task of classifying utterances with respect to the function they serve in a dialogue. Existing approaches to DA classification model utterances without incorporating the turn changes among speakers throughout the dialogue, therefore treating it no different than non-interactive written text. In this paper, we propose to integrate the turn changes in conversations among speakers when modeling DAs. Specifically, we learn conversation-invariant speaker turn embeddings to represent the speaker turns in a conversation; the learned speaker turn embeddings are then merged with the utterance embeddings for the downstream task of DA classification. With this simple yet effective mechanism, our model is able to capture the semantics from the dialogue content while accounting for different speaker turns in a conversation. Validation on three benchmark public datasets demonstrates superior performance of our model.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1518555284,"Goal":"Climate Action","Task":["Dialogue Act Classification","Dialogue Act","classification","classifying utterances","DA classification","DAs","DA classification"],"Method":["Speaker Turn Modeling"]},{"ID":"tomeh-etal-2010-refining","title":"Refining Word Alignment with Discriminative Training","abstract":"The quality of statistical machine translation systems depends on the quality of the word alignments that are computed during the translation model training phase. IBM alignment models, as implemented in the GIZA++ toolkit, constitute the de facto standard for performing these computations. The resulting alignments and translation models are however very noisy, and several authors have tried to improve them. In this work, we propose a simple and effective approach, which considers alignment as a series of independent binary classification problems in the alignment matrix. Through extensive feature engineering and the use of stacking techniques, we were able to obtain alignments much closer to manually defined references than those obtained by the IBM models. These alignments also yield better translation models, delivering improved performance in a large scale Arabic to English translation task.","year":2010,"title_abstract":"Refining Word Alignment with Discriminative Training The quality of statistical machine translation systems depends on the quality of the word alignments that are computed during the translation model training phase. IBM alignment models, as implemented in the GIZA++ toolkit, constitute the de facto standard for performing these computations. The resulting alignments and translation models are however very noisy, and several authors have tried to improve them. In this work, we propose a simple and effective approach, which considers alignment as a series of independent binary classification problems in the alignment matrix. Through extensive feature engineering and the use of stacking techniques, we were able to obtain alignments much closer to manually defined references than those obtained by the IBM models. These alignments also yield better translation models, delivering improved performance in a large scale Arabic to English translation task.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1518308222,"Goal":"Gender Equality","Task":["Word Alignment","alignment","independent binary classification problems","Arabic to English translation task"],"Method":["Discriminative Training","statistical machine translation systems","translation model training phase","IBM alignment models","GIZA++ toolkit","translation models","feature engineering","stacking techniques","IBM models","translation models"]},{"ID":"mojapelo-2016-semantics","title":"Semantics of body parts in {A}frican {W}ord{N}et: a case of {N}orthern {S}otho","abstract":"This paper presents a linguistic account of the lexical semantics of body parts in African WordNet, with special reference to Northern Sotho. It focuses on external human body parts synsets in Northern Sotho. The paper seeks to support the effectiveness of African WordNet as a resource for services such as in the healthcare and medical field in South Africa. It transpired from this exploration that there is either a one-to-one correspondence or some form of misalignment of lexicalisation with regard to the sample of examined synsets. The paper concludes by making suggestions on how African WordNet can deal with such semantic misalignments in order to improve its efficiency as a resource for the targeted purpose.","year":2016,"title_abstract":"Semantics of body parts in {A}frican {W}ord{N}et: a case of {N}orthern {S}otho This paper presents a linguistic account of the lexical semantics of body parts in African WordNet, with special reference to Northern Sotho. It focuses on external human body parts synsets in Northern Sotho. The paper seeks to support the effectiveness of African WordNet as a resource for services such as in the healthcare and medical field in South Africa. It transpired from this exploration that there is either a one-to-one correspondence or some form of misalignment of lexicalisation with regard to the sample of examined synsets. The paper concludes by making suggestions on how African WordNet can deal with such semantic misalignments in order to improve its efficiency as a resource for the targeted purpose.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.1518241763,"Goal":"Peace, Justice and Strong Institutions","Task":["Semantics of body parts","lexical semantics of body parts","healthcare and medical field"],"Method":["linguistic account","WordNet"]},{"ID":"hoang-vu-2020-nuts","title":"Not-{NUT}s at {WNUT}-2020 Task 2: A {BERT}-based System in Identifying Informative {COVID}-19 {E}nglish Tweets","abstract":"As of 2020 when the COVID-19 pandemic is full-blown on a global scale, people{'}s need to have access to legitimate information regarding COVID-19 is more urgent than ever, especially via online media where the abundance of irrelevant information overshadows the more informative ones. In response to such, we proposed a model that, given an English tweet, automatically identifies whether that tweet bears informative content regarding COVID-19 or not. By ensembling different BERTweet model configurations, we have achieved competitive results that are only shy of those by top performing teams by roughly 1{\\%} in terms of F1 score on the informative class. In the post-competition period, we have also experimented with various other approaches that potentially boost generalization to a new dataset.","year":2020,"title_abstract":"Not-{NUT}s at {WNUT}-2020 Task 2: A {BERT}-based System in Identifying Informative {COVID}-19 {E}nglish Tweets As of 2020 when the COVID-19 pandemic is full-blown on a global scale, people{'}s need to have access to legitimate information regarding COVID-19 is more urgent than ever, especially via online media where the abundance of irrelevant information overshadows the more informative ones. In response to such, we proposed a model that, given an English tweet, automatically identifies whether that tweet bears informative content regarding COVID-19 or not. By ensembling different BERTweet model configurations, we have achieved competitive results that are only shy of those by top performing teams by roughly 1{\\%} in terms of F1 score on the informative class. In the post-competition period, we have also experimented with various other approaches that potentially boost generalization to a new dataset.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1517920196,"Goal":"Climate Action","Task":["Identifying Informative {COVID} - 19","generalization"],"Method":["BERTweet model configurations"]},{"ID":"lewis-yang-2012-building","title":"Building {MT} for a Severely Under-Resourced Language: White {H}mong","abstract":"In this paper, we discuss the development of statistical machine translation for English to\/from White Hmong (Language code: mww). White Hmong is a Hmong-Mien language, originally spoken mostly in Southeast Asia, but now predominantly spoken by a large diaspora throughout the world, with populations in the United States, Australia, France, Thailand and elsewhere. Building statistical translation systems for Hmong proved to be incredibly challenging since there are no known parallel or monolingual corpora for the language; in fact, finding data for Hmong proved to be one of the biggest challenges to getting the project off the ground. It was only through a close collaboration with the Hmong community, and active and tireless participation of Hmong speakers, that it became possible to build up a critical mass of data to make the translation project a reality. We see this effort as potentially replicable for other severely resource poor languages of the world, which is likely the case for the majority of the languages still spoken on the planet. Further, the work here suggests that research and work on other severely under-resourced languages can have significant positive impacts for the affected communities, both for accessibility and language preservation.","year":2012,"title_abstract":"Building {MT} for a Severely Under-Resourced Language: White {H}mong In this paper, we discuss the development of statistical machine translation for English to\/from White Hmong (Language code: mww). White Hmong is a Hmong-Mien language, originally spoken mostly in Southeast Asia, but now predominantly spoken by a large diaspora throughout the world, with populations in the United States, Australia, France, Thailand and elsewhere. Building statistical translation systems for Hmong proved to be incredibly challenging since there are no known parallel or monolingual corpora for the language; in fact, finding data for Hmong proved to be one of the biggest challenges to getting the project off the ground. It was only through a close collaboration with the Hmong community, and active and tireless participation of Hmong speakers, that it became possible to build up a critical mass of data to make the translation project a reality. We see this effort as potentially replicable for other severely resource poor languages of the world, which is likely the case for the majority of the languages still spoken on the planet. Further, the work here suggests that research and work on other severely under-resourced languages can have significant positive impacts for the affected communities, both for accessibility and language preservation.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1517831981,"Goal":"Sustainable Cities and Communities","Task":["statistical machine translation","translation project","accessibility","language preservation"],"Method":["{MT}","Hmong - Mien language","statistical translation systems"]},{"ID":"del-gratta-etal-2016-lrec","title":"{LREC} as a Graph: People and Resources in a Network","abstract":"This proposal describes a new way to visualise resources in the LREMap, a community-built repository of language resource descriptions and uses. The LREMap is represented as a force-directed graph, where resources, papers and authors are nodes. The analysis of the visual representation of the underlying graph is used to study how the community gathers around LRs and how LRs are used in research.","year":2016,"title_abstract":"{LREC} as a Graph: People and Resources in a Network This proposal describes a new way to visualise resources in the LREMap, a community-built repository of language resource descriptions and uses. The LREMap is represented as a force-directed graph, where resources, papers and authors are nodes. The analysis of the visual representation of the underlying graph is used to study how the community gathers around LRs and how LRs are used in research.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1517674327,"Goal":"Sustainable Cities and Communities","Task":["visualise resources","community - built repository of language resource descriptions"],"Method":["LREMap","LREMap","force - directed graph","visual representation","LRs"]},{"ID":"flint-etal-2017-text","title":"A Text Normalisation System for Non-Standard {E}nglish Words","abstract":"This paper investigates the problem of text normalisation; specifically, the normalisation of non-standard words (NSWs) in English. Non-standard words can be defined as those word tokens which do not have a dictionary entry, and cannot be pronounced using the usual letter-to-phoneme conversion rules; e.g. lbs, 99.3{\\%}, {\\#}EMNLP2017. NSWs pose a challenge to the proper functioning of text-to-speech technology, and the solution is to spell them out in such a way that they can be pronounced appropriately. We describe our four-stage normalisation system made up of components for detection, classification, division and expansion of NSWs. Performance is favourabe compared to previous work in the field (Sproat et al. 2001, Normalization of non-standard words), as well as state-of-the-art text-to-speech software. Further, we update Sproat et al.{'}s NSW taxonomy, and create a more customisable system where users are able to input their own abbreviations and specify into which variety of English (currently available: British or American) they wish to normalise.","year":2017,"title_abstract":"A Text Normalisation System for Non-Standard {E}nglish Words This paper investigates the problem of text normalisation; specifically, the normalisation of non-standard words (NSWs) in English. Non-standard words can be defined as those word tokens which do not have a dictionary entry, and cannot be pronounced using the usual letter-to-phoneme conversion rules; e.g. lbs, 99.3{\\%}, {\\#}EMNLP2017. NSWs pose a challenge to the proper functioning of text-to-speech technology, and the solution is to spell them out in such a way that they can be pronounced appropriately. We describe our four-stage normalisation system made up of components for detection, classification, division and expansion of NSWs. Performance is favourabe compared to previous work in the field (Sproat et al. 2001, Normalization of non-standard words), as well as state-of-the-art text-to-speech software. Further, we update Sproat et al.{'}s NSW taxonomy, and create a more customisable system where users are able to input their own abbreviations and specify into which variety of English (currently available: British or American) they wish to normalise.","social_need":"No Poverty End poverty in all its forms everywhere","cosine_similarity":0.1517520845,"Goal":"No Poverty","Task":["Non - Standard {E}nglish Words","text normalisation;","normalisation of non - standard words","text - to - speech technology","detection","classification","division","expansion of NSWs","Normalization of non - standard words)"],"Method":["Text Normalisation System","letter - to - phoneme conversion rules;","NSWs","four - stage normalisation system","text - to - speech software"]},{"ID":"benjamin-2014-collaboration","title":"Collaboration in the Production of a Massively Multilingual Lexicon","abstract":"This paper discusses the multiple approaches to collaboration that the Kamusi Project is employing in the creation of a massively multilingual lexical resource. The project\u0092s data structure enables the inclusion of large amounts of rich data within each sense-specific entry, with transitive concept-based links across languages. Data collection involves mining existing data sets, language experts using an online editing system, crowdsourcing, and games with a purpose. The paper discusses the benefits and drawbacks of each of these elements, and the steps the project is taking to account for those. Special attention is paid to guiding crowd members with targeted questions that produce results in a specific format. Collaboration is seen as an essential method for generating large amounts of linguistic data, as well as for validating the data so it can be considered trustworthy.","year":2014,"title_abstract":"Collaboration in the Production of a Massively Multilingual Lexicon This paper discusses the multiple approaches to collaboration that the Kamusi Project is employing in the creation of a massively multilingual lexical resource. The project\u0092s data structure enables the inclusion of large amounts of rich data within each sense-specific entry, with transitive concept-based links across languages. Data collection involves mining existing data sets, language experts using an online editing system, crowdsourcing, and games with a purpose. The paper discusses the benefits and drawbacks of each of these elements, and the steps the project is taking to account for those. Special attention is paid to guiding crowd members with targeted questions that produce results in a specific format. Collaboration is seen as an essential method for generating large amounts of linguistic data, as well as for validating the data so it can be considered trustworthy.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1517429054,"Goal":"Sustainable Cities and Communities","Task":["Massively Multilingual Lexicon","massively multilingual lexical resource","Data collection"],"Method":["Kamusi Project","online editing system","Collaboration"]},{"ID":"begum-etal-2008-developing","title":"Developing Verb Frames for {H}indi","abstract":"This paper introduces an ongoing work on developing verb frames for Hindi. Verb frames capture syntactic commonalities of semantically related verbs. The main objective of this work is to create a linguistic resource which will prove to be indispensable for various NLP applications. We also hope this resource to help us better understand Hindi verbs. We motivate the basic verb argument structure using relations as introduced by Panini. We show the methodology used in preparing these frames and the criteria followed for classifying Hindi verbs.","year":2008,"title_abstract":"Developing Verb Frames for {H}indi This paper introduces an ongoing work on developing verb frames for Hindi. Verb frames capture syntactic commonalities of semantically related verbs. The main objective of this work is to create a linguistic resource which will prove to be indispensable for various NLP applications. We also hope this resource to help us better understand Hindi verbs. We motivate the basic verb argument structure using relations as introduced by Panini. We show the methodology used in preparing these frames and the criteria followed for classifying Hindi verbs.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1517230719,"Goal":"Climate Action","Task":["verb frames","linguistic resource","NLP applications","classifying Hindi verbs"],"Method":["Verb Frames","Verb frames"]},{"ID":"popescu-2007-architecture","title":"Architecture modulaire portable pour la g{\\'e}n{\\'e}ration du langage naturel en dialogue homme-machine","abstract":"La g{\\'e}n{\\'e}ration du langage naturel pour le dialogue oral homme-machine pose des contraintes sp{\\'e}cifiques, telles que la spontan{\\'e}it{\\'e} et le caract{\\`e}re fragment{\\'e} des {\\'e}nonc{\\'e}s, les types des locuteurs ou les contraintes de temps de r{\\'e}ponse de la part du syst{\\`e}me. Dans ce contexte, le probl{\\`e}me d{'}une architecture rigoureusement sp{\\'e}cifi{\\'e}e se pose, autant au niveau des {\\'e}tapes de traitement et des modules impliqu{\\'e}s, qu{'}au niveau des interfaces entre ces modules. Afin de permettre une libert{\\'e} quasi-totale {\\`a} l{'}{\\'e}gard des d{\\'e}marches th{\\'e}oriques, une telle architecture doit {\\^e}tre {\\`a} la fois modulaire (c{'}est-{\\`a}-dire, permettre l{'}ind{\\'e}pendance des niveaux de traitement les uns des autres) et portable (c{'}est-{\\`a}-dire, permettre l{'}interop{\\'e}rabilit{\\'e} avec des modules con{\\c{c}}us selon des architectures standard en g{\\'e}n{\\'e}ration du langage naturel, telles que le mod{\\`e}le RAGS - \u00ab Reference Architecture for Generation Systems \u00bb). Ainsi, dans cet article on pr{\\'e}sente de mani{\\`e}re concise l{'}architecture propos{\\'e}e, la comparant ensuite au mod{\\`e}le RAGS, pour argumenter les choix op{\\'e}r{\\'e}s en conception. Dans un second temps, la portabilit{\\'e} de l{'}architecture sera d{\\'e}crite {\\`a} travers un exemple {\\'e}tendu, dont la g{\\'e}n{\\'e}ralit{\\'e} r{\\'e}side dans l{'}obtention d{'}un ensemble de r{\\`e}gles permettant de plonger automatiquement les repr{\\'e}sentations des informations de notre architecture vers le format du mod{\\`e}le RAGS et inversement. Finalement, un ensemble de conclusions et perspectives cl{\\^o}turera l{'}article.","year":2007,"title_abstract":"Architecture modulaire portable pour la g{\\'e}n{\\'e}ration du langage naturel en dialogue homme-machine La g{\\'e}n{\\'e}ration du langage naturel pour le dialogue oral homme-machine pose des contraintes sp{\\'e}cifiques, telles que la spontan{\\'e}it{\\'e} et le caract{\\`e}re fragment{\\'e} des {\\'e}nonc{\\'e}s, les types des locuteurs ou les contraintes de temps de r{\\'e}ponse de la part du syst{\\`e}me. Dans ce contexte, le probl{\\`e}me d{'}une architecture rigoureusement sp{\\'e}cifi{\\'e}e se pose, autant au niveau des {\\'e}tapes de traitement et des modules impliqu{\\'e}s, qu{'}au niveau des interfaces entre ces modules. Afin de permettre une libert{\\'e} quasi-totale {\\`a} l{'}{\\'e}gard des d{\\'e}marches th{\\'e}oriques, une telle architecture doit {\\^e}tre {\\`a} la fois modulaire (c{'}est-{\\`a}-dire, permettre l{'}ind{\\'e}pendance des niveaux de traitement les uns des autres) et portable (c{'}est-{\\`a}-dire, permettre l{'}interop{\\'e}rabilit{\\'e} avec des modules con{\\c{c}}us selon des architectures standard en g{\\'e}n{\\'e}ration du langage naturel, telles que le mod{\\`e}le RAGS - \u00ab Reference Architecture for Generation Systems \u00bb). Ainsi, dans cet article on pr{\\'e}sente de mani{\\`e}re concise l{'}architecture propos{\\'e}e, la comparant ensuite au mod{\\`e}le RAGS, pour argumenter les choix op{\\'e}r{\\'e}s en conception. Dans un second temps, la portabilit{\\'e} de l{'}architecture sera d{\\'e}crite {\\`a} travers un exemple {\\'e}tendu, dont la g{\\'e}n{\\'e}ralit{\\'e} r{\\'e}side dans l{'}obtention d{'}un ensemble de r{\\`e}gles permettant de plonger automatiquement les repr{\\'e}sentations des informations de notre architecture vers le format du mod{\\`e}le RAGS et inversement. Finalement, un ensemble de conclusions et perspectives cl{\\^o}turera l{'}article.","social_need":"Responsible Consumption and Production Ensure sustainable consumption and production patterns","cosine_similarity":0.1517091841,"Goal":"Responsible Consumption and Production","Task":["Generation Systems"],"Method":["Reference Architecture"]},{"ID":"hovy-purschke-2018-capturing","title":"Capturing Regional Variation with Distributed Place Representations and Geographic Retrofitting","abstract":"Dialects are one of the main drivers of language variation, a major challenge for natural language processing tools. In most languages, dialects exist along a continuum, and are commonly discretized by combining the extent of several preselected linguistic variables. However, the selection of these variables is theory-driven and itself insensitive to change. We use Doc2Vec on a corpus of 16.8M anonymous online posts in the German-speaking area to learn continuous document representations of cities. These representations capture continuous regional linguistic distinctions, and can serve as input to downstream NLP tasks sensitive to regional variation. By incorporating geographic information via retrofitting and agglomerative clustering with structure, we recover dialect areas at various levels of granularity. Evaluating these clusters against an existing dialect map, we achieve a match of up to 0.77 V-score (harmonic mean of cluster completeness and homogeneity). Our results show that representation learning with retrofitting offers a robust general method to automatically expose dialectal differences and regional variation at a finer granularity than was previously possible.","year":2018,"title_abstract":"Capturing Regional Variation with Distributed Place Representations and Geographic Retrofitting Dialects are one of the main drivers of language variation, a major challenge for natural language processing tools. In most languages, dialects exist along a continuum, and are commonly discretized by combining the extent of several preselected linguistic variables. However, the selection of these variables is theory-driven and itself insensitive to change. We use Doc2Vec on a corpus of 16.8M anonymous online posts in the German-speaking area to learn continuous document representations of cities. These representations capture continuous regional linguistic distinctions, and can serve as input to downstream NLP tasks sensitive to regional variation. By incorporating geographic information via retrofitting and agglomerative clustering with structure, we recover dialect areas at various levels of granularity. Evaluating these clusters against an existing dialect map, we achieve a match of up to 0.77 V-score (harmonic mean of cluster completeness and homogeneity). Our results show that representation learning with retrofitting offers a robust general method to automatically expose dialectal differences and regional variation at a finer granularity than was previously possible.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1516540945,"Goal":"Sustainable Cities and Communities","Task":["Capturing Regional Variation","language variation","natural language processing tools","NLP tasks"],"Method":["Distributed Place Representations","Geographic Retrofitting Dialects","Doc2Vec","continuous document representations of cities","retrofitting","agglomerative clustering","representation learning","retrofitting"]},{"ID":"medina-maza-etal-2020-event","title":"Event-Related Bias Removal for Real-time Disaster Events","abstract":"Social media has become an important tool to share information about crisis events such as natural disasters and mass attacks. Detecting actionable posts that contain useful information requires rapid analysis of huge volumes of data in real-time. This poses a complex problem due to the large amount of posts that do not contain any actionable information. Furthermore, the classification of information in real-time systems requires training on out-of-domain data, as we do not have any data from a new emerging crisis. Prior work focuses on models pre-trained on similar event types. However, those models capture unnecessary event-specific biases, like the location of the event, which affect the generalizability and performance of the classifiers on new unseen data from an emerging new event. In our work, we train an adversarial neural model to remove latent event-specific biases and improve the performance on tweet importance classification.","year":2020,"title_abstract":"Event-Related Bias Removal for Real-time Disaster Events Social media has become an important tool to share information about crisis events such as natural disasters and mass attacks. Detecting actionable posts that contain useful information requires rapid analysis of huge volumes of data in real-time. This poses a complex problem due to the large amount of posts that do not contain any actionable information. Furthermore, the classification of information in real-time systems requires training on out-of-domain data, as we do not have any data from a new emerging crisis. Prior work focuses on models pre-trained on similar event types. However, those models capture unnecessary event-specific biases, like the location of the event, which affect the generalizability and performance of the classifiers on new unseen data from an emerging new event. In our work, we train an adversarial neural model to remove latent event-specific biases and improve the performance on tweet importance classification.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1516329944,"Goal":"Climate Action","Task":["Event - Related Bias Removal","Real - time Disaster Events","Detecting actionable posts","classification of information","real - time systems","tweet importance classification"],"Method":["classifiers","adversarial neural model"]},{"ID":"zetzsche-2010-crowdsourcing","title":"Crowdsourcing and the Professional Translator","abstract":"The recent emergence of crowdsourced translation a\u0300 la Facebook or Twitter has exposed a raw nerve in the translation industry. Perceptions of ill-placed entitlement -- we are the professionals who have the ``right'' to translate these products -- abound. And many have felt threatened by something that carries not only a relatively newly coined term -- crowdsourcing -- but seems in and of itself completely new. Or is it?","year":2010,"title_abstract":"Crowdsourcing and the Professional Translator The recent emergence of crowdsourced translation a\u0300 la Facebook or Twitter has exposed a raw nerve in the translation industry. Perceptions of ill-placed entitlement -- we are the professionals who have the ``right'' to translate these products -- abound. And many have felt threatened by something that carries not only a relatively newly coined term -- crowdsourcing -- but seems in and of itself completely new. Or is it?","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1515920311,"Goal":"Sustainable Cities and Communities","Task":["Professional Translator","crowdsourced translation","translation industry"],"Method":["Crowdsourcing"]},{"ID":"marcinczuk-etal-2017-inforex","title":"{I}nforex {---} a collaborative system for text corpora annotation and analysis","abstract":"We report a first major upgrade of Inforex {---} a web-based system for qualitative and collaborative text corpora annotation and analysis. Inforex is a part of Polish CLARIN infrastructure. It is integrated with a digital repository for storing and publishing language resources and allows to visualize, browse and annotate text corpora stored in the repository. As a result of a series of workshops for researches from humanities and social sciences fields we improved the graphical interface to make the system more friendly and readable for non-experienced users. We also implemented a new functionality for gold standard annotation which includes private annotations and annotation agreement by a super-annotator.","year":2017,"title_abstract":"{I}nforex {---} a collaborative system for text corpora annotation and analysis We report a first major upgrade of Inforex {---} a web-based system for qualitative and collaborative text corpora annotation and analysis. Inforex is a part of Polish CLARIN infrastructure. It is integrated with a digital repository for storing and publishing language resources and allows to visualize, browse and annotate text corpora stored in the repository. As a result of a series of workshops for researches from humanities and social sciences fields we improved the graphical interface to make the system more friendly and readable for non-experienced users. We also implemented a new functionality for gold standard annotation which includes private annotations and annotation agreement by a super-annotator.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1515709162,"Goal":"Partnership for the Goals","Task":["text corpora annotation and analysis","qualitative and collaborative text corpora annotation and analysis","storing and publishing language resources","gold standard annotation"],"Method":["collaborative system","Inforex","web - based system","Inforex","CLARIN infrastructure","digital repository"]},{"ID":"ozcelik-etal-2021-hisnet","title":"{H}is{N}et: A Polarity Lexicon based on {W}ord{N}et for Emotion Analysis","abstract":"Dictionary-based methods in sentiment analysis have received scholarly attention recently, the most comprehensive examples of which can be found in English. However, many other languages lack polarity dictionaries, or the existing ones are small in size as in the case of SentiTurkNet, the first and only polarity dictionary in Turkish. Thus, this study aims to extend the content of SentiTurkNet by comparing the two available WordNets in Turkish, namely KeNet and TR-wordnet of BalkaNet. To this end, a current Turkish polarity dictionary has been created relying on 76,825 synsets matching KeNet, where each synset has been annotated with three polarity labels, which are positive, negative and neutral. Meanwhile, the comparison of KeNet and TR-wordnet of BalkaNet has revealed their weaknesses such as the repetition of the same senses, lack of necessary merges of the items belonging to the same synset and the presence of redundant narrower versions of synsets, which are discussed in light of their potential to the improvement of the current lexical databases of Turkish.","year":2021,"title_abstract":"{H}is{N}et: A Polarity Lexicon based on {W}ord{N}et for Emotion Analysis Dictionary-based methods in sentiment analysis have received scholarly attention recently, the most comprehensive examples of which can be found in English. However, many other languages lack polarity dictionaries, or the existing ones are small in size as in the case of SentiTurkNet, the first and only polarity dictionary in Turkish. Thus, this study aims to extend the content of SentiTurkNet by comparing the two available WordNets in Turkish, namely KeNet and TR-wordnet of BalkaNet. To this end, a current Turkish polarity dictionary has been created relying on 76,825 synsets matching KeNet, where each synset has been annotated with three polarity labels, which are positive, negative and neutral. Meanwhile, the comparison of KeNet and TR-wordnet of BalkaNet has revealed their weaknesses such as the repetition of the same senses, lack of necessary merges of the items belonging to the same synset and the presence of redundant narrower versions of synsets, which are discussed in light of their potential to the improvement of the current lexical databases of Turkish.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1515160352,"Goal":"Reduced Inequalities","Task":["Emotion Analysis","sentiment analysis","SentiTurkNet"],"Method":["Polarity Lexicon","Dictionary - based methods","SentiTurkNet","KeNet","TR - wordnet of BalkaNet","KeNet","TR - wordnet"]},{"ID":"sen-etal-2019-iitp","title":"{IITP}-{MT} System for {G}ujarati-{E}nglish News Translation Task at {WMT} 2019","abstract":"We describe our submission to WMT 2019 News translation shared task for Gujarati-English language pair. We submit constrained systems, i.e, we rely on the data provided for this language pair and do not use any external data. We train Transformer based subword-level neural machine translation (NMT) system using original parallel corpus along with synthetic parallel corpus obtained through back-translation of monolingual data. Our primary systems achieve BLEU scores of 10.4 and 8.1 for Gujarati\u2192English and English\u2192Gujarati, respectively. We observe that incorporating monolingual data through back-translation improves the BLEU score significantly over baseline NMT and SMT systems for this language pair.","year":2019,"title_abstract":"{IITP}-{MT} System for {G}ujarati-{E}nglish News Translation Task at {WMT} 2019 We describe our submission to WMT 2019 News translation shared task for Gujarati-English language pair. We submit constrained systems, i.e, we rely on the data provided for this language pair and do not use any external data. We train Transformer based subword-level neural machine translation (NMT) system using original parallel corpus along with synthetic parallel corpus obtained through back-translation of monolingual data. Our primary systems achieve BLEU scores of 10.4 and 8.1 for Gujarati\u2192English and English\u2192Gujarati, respectively. We observe that incorporating monolingual data through back-translation improves the BLEU score significantly over baseline NMT and SMT systems for this language pair.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1515080482,"Goal":"Gender Equality","Task":["Translation","WMT","translation"],"Method":["constrained systems","Transformer based subword - level neural machine translation (NMT) system","back - translation","NMT","SMT systems"]},{"ID":"yates-etal-2014-framework","title":"A Framework for Public Health Surveillance","abstract":"With the rapid growth of social media, there is increasing potential to augment traditional public health surveillance methods with data from social media. We describe a framework for performing public health surveillance on Twitter data. Our framework, which is publicly available, consists of three components that work together to detect health-related trends in social media: a concept extraction component for identifying health-related concepts, a concept aggregation component for identifying how the extracted health-related concepts relate to each other, and a trend detection component for determining when the aggregated health-related concepts are trending. We describe the architecture of the framework and several components that have been implemented in the framework, identify other components that could be used with the framework, and evaluate our framework on approximately 1.5 years of tweets. While it is difficult to determine how accurately a Twitter trend reflects a trend in the real world, we discuss the differences in trends detected by several different methods and compare flu trends detected by our framework to data from Google Flu Trends.","year":2014,"title_abstract":"A Framework for Public Health Surveillance With the rapid growth of social media, there is increasing potential to augment traditional public health surveillance methods with data from social media. We describe a framework for performing public health surveillance on Twitter data. Our framework, which is publicly available, consists of three components that work together to detect health-related trends in social media: a concept extraction component for identifying health-related concepts, a concept aggregation component for identifying how the extracted health-related concepts relate to each other, and a trend detection component for determining when the aggregated health-related concepts are trending. We describe the architecture of the framework and several components that have been implemented in the framework, identify other components that could be used with the framework, and evaluate our framework on approximately 1.5 years of tweets. While it is difficult to determine how accurately a Twitter trend reflects a trend in the real world, we discuss the differences in trends detected by several different methods and compare flu trends detected by our framework to data from Google Flu Trends.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1515056193,"Goal":"Climate Action","Task":["Public Health Surveillance","public health surveillance","health - related trends","health - related concepts"],"Method":["public health surveillance methods","concept extraction component","concept aggregation component","trend detection component"]},{"ID":"chen-etal-2021-building","title":"Building Goal-oriented Document-grounded Dialogue Systems","abstract":"In this paper, we describe our systems for solving the two Doc2Dial shared task: knowledge identification and response generation. We proposed several pre-processing and post-processing methods, and we experimented with data augmentation by pre-training the models on other relevant datasets. Our best model for knowledge identification outperformed the baseline by 10.5+ f1-score on the test-dev split, and our best model for response generation outperformed the baseline by 11+ Sacrebleu score on the test-dev split.","year":2021,"title_abstract":"Building Goal-oriented Document-grounded Dialogue Systems In this paper, we describe our systems for solving the two Doc2Dial shared task: knowledge identification and response generation. We proposed several pre-processing and post-processing methods, and we experimented with data augmentation by pre-training the models on other relevant datasets. Our best model for knowledge identification outperformed the baseline by 10.5+ f1-score on the test-dev split, and our best model for response generation outperformed the baseline by 11+ Sacrebleu score on the test-dev split.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1514500976,"Goal":"Partnership for the Goals","Task":["Goal - oriented Document - grounded Dialogue Systems","Doc2Dial shared task","knowledge identification","response generation","data augmentation","knowledge identification","response generation"],"Method":["pre - processing and post - processing methods"]},{"ID":"molla-2018-macquarie","title":"{M}acquarie {U}niversity at {B}io{ASQ} 6b: Deep learning and deep reinforcement learning for query-based summarisation","abstract":"This paper describes Macquarie University{'}s contribution to the BioASQ Challenge (BioASQ 6b, Phase B). We focused on the extraction of the ideal answers, and the task was approached as an instance of query-based multi-document summarisation. In particular, this paper focuses on the experiments related to the deep learning and reinforcement learning approaches used in the submitted runs. The best run used a deep learning model under a regression-based framework. The deep learning architecture used features derived from the output of LSTM chains on word embeddings, plus features based on similarity with the query, and sentence position. The reinforcement learning approach was a proof-of-concept prototype that trained a global policy using REINFORCE. The global policy was implemented as a neural network that used tf.idf features encoding the candidate sentence, question, and context.","year":2018,"title_abstract":"{M}acquarie {U}niversity at {B}io{ASQ} 6b: Deep learning and deep reinforcement learning for query-based summarisation This paper describes Macquarie University{'}s contribution to the BioASQ Challenge (BioASQ 6b, Phase B). We focused on the extraction of the ideal answers, and the task was approached as an instance of query-based multi-document summarisation. In particular, this paper focuses on the experiments related to the deep learning and reinforcement learning approaches used in the submitted runs. The best run used a deep learning model under a regression-based framework. The deep learning architecture used features derived from the output of LSTM chains on word embeddings, plus features based on similarity with the query, and sentence position. The reinforcement learning approach was a proof-of-concept prototype that trained a global policy using REINFORCE. The global policy was implemented as a neural network that used tf.idf features encoding the candidate sentence, question, and context.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.1514179707,"Goal":"Peace, Justice and Strong Institutions","Task":["query - based summarisation","query - based multi - document summarisation"],"Method":["Deep learning","deep reinforcement learning","deep learning and reinforcement learning approaches","deep learning model","regression - based framework","deep learning architecture","LSTM chains","reinforcement learning approach","global policy","REINFORCE","global policy","neural network","tf . idf features"]},{"ID":"petrits-etal-2001-commission","title":"The Commission \u0301s {MT} system: today and tomorrow","abstract":"This paper presents a snapshot of how the Commission's MT system (EC SYSTRAN) is used today and a glimpse of how that picture will change tomorrow. It looks in turn at: the origins of the system; how it is accessed; who requests MT and why; how users can influence the quality of output; the Rapid Post-editing Service; and the latest usage statistics, which augur well for the future. The paper closes with a look at that future, touching on the move to a new computer platform and plans for new language pairs, concluding that after twenty-five years of development, MT has become an integral part of the Commission's working environment.","year":2001,"title_abstract":"The Commission \u0301s {MT} system: today and tomorrow This paper presents a snapshot of how the Commission's MT system (EC SYSTRAN) is used today and a glimpse of how that picture will change tomorrow. It looks in turn at: the origins of the system; how it is accessed; who requests MT and why; how users can influence the quality of output; the Rapid Post-editing Service; and the latest usage statistics, which augur well for the future. The paper closes with a look at that future, touching on the move to a new computer platform and plans for new language pairs, concluding that after twenty-five years of development, MT has become an integral part of the Commission's working environment.","social_need":"Clean Water and Sanitation Ensure availability and sustainable management of water and sanitation for all","cosine_similarity":0.1514155567,"Goal":"Clean Water and Sanitation","Task":["MT","Rapid Post - editing Service;"],"Method":["Commission \u0301s {MT} system","MT system","SYSTRAN)","MT"]},{"ID":"baumler-ray-2022-hybrid","title":"Hybrid Semantics for Goal-Directed Natural Language Generation","abstract":"We consider the problem of generating natural language given a communicative goal and a world description. We ask the question: is it possible to combine complementary meaning representations to scale a goal-directed NLG system without losing expressiveness? In particular, we consider using two meaning representations, one based on logical semantics and the other based on distributional semantics. We build upon an existing goal-directed generation system, S-STRUCT, which models sentence generation as planning in a Markov decision process. We develop a hybrid approach, which uses distributional semantics to quickly and imprecisely add the main elements of the sentence and then uses first-order logic based semantics to more slowly add the precise details. We find that our hybrid method allows S-STRUCT{'}s generation to scale significantly better in early phases of generation and that the hybrid can often generate sentences with the same quality as S-STRUCT in substantially less time. However, we also observe and give insight into cases where the imprecision in distributional semantics leads to generation that is not as good as using pure logical semantics.","year":2022,"title_abstract":"Hybrid Semantics for Goal-Directed Natural Language Generation We consider the problem of generating natural language given a communicative goal and a world description. We ask the question: is it possible to combine complementary meaning representations to scale a goal-directed NLG system without losing expressiveness? In particular, we consider using two meaning representations, one based on logical semantics and the other based on distributional semantics. We build upon an existing goal-directed generation system, S-STRUCT, which models sentence generation as planning in a Markov decision process. We develop a hybrid approach, which uses distributional semantics to quickly and imprecisely add the main elements of the sentence and then uses first-order logic based semantics to more slowly add the precise details. We find that our hybrid method allows S-STRUCT{'}s generation to scale significantly better in early phases of generation and that the hybrid can often generate sentences with the same quality as S-STRUCT in substantially less time. However, we also observe and give insight into cases where the imprecision in distributional semantics leads to generation that is not as good as using pure logical semantics.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1514116973,"Goal":"Partnership for the Goals","Task":["Goal - Directed Natural Language Generation","generating natural language","NLG","sentence generation","planning","generation","generation","generation"],"Method":["Hybrid Semantics","complementary meaning representations","meaning representations","logical semantics","distributional semantics","goal - directed generation system","S - STRUCT","Markov decision process","hybrid approach","first - order logic based semantics","hybrid method","hybrid","S - STRUCT","logical semantics"]},{"ID":"mahsut-etal-2004-experiment","title":"An experiment on {J}apanese-{U}ighur machine translation and its evaluation","abstract":"This paper describes an evaluation experiment about a Japanese-Uighur machine translation system which consists of verbal suffix processing, case suffix processing, phonetic change processing, and a Japanese-Uighur dictionary including about 20,000 words. Japanese and Uighur have many syntactical and language structural similarities, including word order, existence and same functions of case suffixes and verbal suffixes, morphological structure, etc. For these reasons, we can consider that we can translate Japanese into Uighur in such a manner as word-by-word aligning after morphological analysis of the input sentences without complicated syntactical analysis. From the point of view of practical usage, we have chosen three articles about environmental issue appeared in Nippon Keizai Shinbun, and conducted a translation experiment on the articles with our MT system, for clarifying our argument. Here, we have counted the correctness of phrases in the Output sentences to be evaluating criteria. As a results of the experiment, 84.8{\\%} of precision has been achieved.","year":2004,"title_abstract":"An experiment on {J}apanese-{U}ighur machine translation and its evaluation This paper describes an evaluation experiment about a Japanese-Uighur machine translation system which consists of verbal suffix processing, case suffix processing, phonetic change processing, and a Japanese-Uighur dictionary including about 20,000 words. Japanese and Uighur have many syntactical and language structural similarities, including word order, existence and same functions of case suffixes and verbal suffixes, morphological structure, etc. For these reasons, we can consider that we can translate Japanese into Uighur in such a manner as word-by-word aligning after morphological analysis of the input sentences without complicated syntactical analysis. From the point of view of practical usage, we have chosen three articles about environmental issue appeared in Nippon Keizai Shinbun, and conducted a translation experiment on the articles with our MT system, for clarifying our argument. Here, we have counted the correctness of phrases in the Output sentences to be evaluating criteria. As a results of the experiment, 84.8{\\%} of precision has been achieved.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1513614953,"Goal":"Gender Equality","Task":["machine translation","translation","word - by - word aligning","translation"],"Method":["verbal suffix processing","case suffix processing","phonetic change processing","morphological analysis","syntactical analysis","MT system"]},{"ID":"thorne-vlachos-2017-extensible","title":"An Extensible Framework for Verification of Numerical Claims","abstract":"In this paper we present our automated fact checking system demonstration which we developed in order to participate in the Fast and Furious Fact Check challenge. We focused on simple numerical claims such as {``}population of Germany in 2015 was 80 million{''} which comprised a quarter of the test instances in the challenge, achieving 68{\\%} accuracy. Our system extends previous work on semantic parsing and claim identification to handle temporal expressions and knowledge bases consisting of multiple tables, while relying solely on automatically generated training data. We demonstrate the extensible nature of our system by evaluating it on relations used in previous work. We make our system publicly available so that it can be used and extended by the community.","year":2017,"title_abstract":"An Extensible Framework for Verification of Numerical Claims In this paper we present our automated fact checking system demonstration which we developed in order to participate in the Fast and Furious Fact Check challenge. We focused on simple numerical claims such as {``}population of Germany in 2015 was 80 million{''} which comprised a quarter of the test instances in the challenge, achieving 68{\\%} accuracy. Our system extends previous work on semantic parsing and claim identification to handle temporal expressions and knowledge bases consisting of multiple tables, while relying solely on automatically generated training data. We demonstrate the extensible nature of our system by evaluating it on relations used in previous work. We make our system publicly available so that it can be used and extended by the community.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1513461173,"Goal":"Climate Action","Task":["Verification of Numerical Claims","automated fact checking system demonstration","Fact Check challenge","semantic parsing","claim identification"],"Method":["Extensible Framework"]},{"ID":"tagarev-etal-2021-tackling","title":"Tackling Multilinguality and Internationality in Fake News","abstract":"The last several years have seen a massive increase in the quantity and influence of disinformation being spread online. Various approaches have been developed to target the process at different stages from identifying sources to tracking distribution in social media to providing follow up debunks to people who have encountered the disinformation. One common conclusion in each of these approaches is that disinformation is too nuanced and subjective a topic for fully automated solutions to work but the quantity of data to process and cross-reference is too high for humans to handle unassisted. Ultimately, the problem calls for a hybrid approach of human experts with technological assistance. In this paper we will demonstrate the application of certain state-of-the-art NLP techniques in assisting expert debunkers and fact checkers as well as the role of these NLP algorithms within a more holistic approach to analyzing and countering the spread of disinformation. We will present a multilingual corpus of disinformation and debunks which contains text, concept tags, images and videos as well as various methods for searching and leveraging the content.","year":2021,"title_abstract":"Tackling Multilinguality and Internationality in Fake News The last several years have seen a massive increase in the quantity and influence of disinformation being spread online. Various approaches have been developed to target the process at different stages from identifying sources to tracking distribution in social media to providing follow up debunks to people who have encountered the disinformation. One common conclusion in each of these approaches is that disinformation is too nuanced and subjective a topic for fully automated solutions to work but the quantity of data to process and cross-reference is too high for humans to handle unassisted. Ultimately, the problem calls for a hybrid approach of human experts with technological assistance. In this paper we will demonstrate the application of certain state-of-the-art NLP techniques in assisting expert debunkers and fact checkers as well as the role of these NLP algorithms within a more holistic approach to analyzing and countering the spread of disinformation. We will present a multilingual corpus of disinformation and debunks which contains text, concept tags, images and videos as well as various methods for searching and leveraging the content.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1513236612,"Goal":"Climate Action","Task":["Tackling Multilinguality and Internationality in Fake News","tracking distribution","expert debunkers","fact checkers","spread of disinformation","searching"],"Method":["automated solutions","hybrid approach","NLP techniques","NLP algorithms"]},{"ID":"hosseinia-etal-2020-stance","title":"Stance Prediction for Contemporary Issues: Data and Experiments","abstract":"We investigate whether pre-trained bidirectional transformers with sentiment and emotion information improve stance detection in long discussions of contemporary issues. As a part of this work, we create a novel stance detection dataset covering 419 different controversial issues and their related pros and cons collected by procon.org in nonpartisan format. Experimental results show that a shallow recurrent neural network with sentiment or emotion information can reach competitive results compared to fine-tuned BERT with 20x fewer parameters. We also use a simple approach that explains which input phrases contribute to stance detection.","year":2020,"title_abstract":"Stance Prediction for Contemporary Issues: Data and Experiments We investigate whether pre-trained bidirectional transformers with sentiment and emotion information improve stance detection in long discussions of contemporary issues. As a part of this work, we create a novel stance detection dataset covering 419 different controversial issues and their related pros and cons collected by procon.org in nonpartisan format. Experimental results show that a shallow recurrent neural network with sentiment or emotion information can reach competitive results compared to fine-tuned BERT with 20x fewer parameters. We also use a simple approach that explains which input phrases contribute to stance detection.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1513041258,"Goal":"Climate Action","Task":["Stance Prediction","stance detection","stance detection","stance detection"],"Method":["bidirectional transformers","shallow recurrent neural network","fine - tuned BERT"]},{"ID":"conia-etal-2020-invero","title":"{I}n{V}e{R}o: Making Semantic Role Labeling Accessible with Intelligible Verbs and Roles","abstract":"Semantic Role Labeling (SRL) is deeply dependent on complex linguistic resources and sophisticated neural models, which makes the task difficult to approach for non-experts. To address this issue we present a new platform named Intelligible Verbs and Roles (InVeRo). This platform provides access to a new verb resource, VerbAtlas, and a state-of-the-art pretrained implementation of a neural, span-based architecture for SRL. Both the resource and the system provide human-readable verb sense and semantic role information, with an easy to use Web interface and RESTful APIs available at http:\/\/nlp.uniroma1.it\/invero.","year":2020,"title_abstract":"{I}n{V}e{R}o: Making Semantic Role Labeling Accessible with Intelligible Verbs and Roles Semantic Role Labeling (SRL) is deeply dependent on complex linguistic resources and sophisticated neural models, which makes the task difficult to approach for non-experts. To address this issue we present a new platform named Intelligible Verbs and Roles (InVeRo). This platform provides access to a new verb resource, VerbAtlas, and a state-of-the-art pretrained implementation of a neural, span-based architecture for SRL. Both the resource and the system provide human-readable verb sense and semantic role information, with an easy to use Web interface and RESTful APIs available at http:\/\/nlp.uniroma1.it\/invero.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1512880921,"Goal":"Life on Land","Task":["Semantic Role Labeling","Semantic Role Labeling","SRL"],"Method":["neural models","pretrained implementation","neural , span - based architecture"]},{"ID":"cao-etal-2022-intrinsic","title":"On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations","abstract":"Multiple metrics have been introduced to measure fairness in various natural language processing tasks. These metrics can be roughly categorized into two categories: 1) extrinsic metrics for evaluating fairness in downstream applications and 2) intrinsic metrics for estimating fairness in upstream contextualized language representation models. In this paper, we conduct an extensive correlation study between intrinsic and extrinsic metrics across bias notions using 19 contextualized language models. We find that intrinsic and extrinsic metrics do not necessarily correlate in their original setting, even when correcting for metric misalignments, noise in evaluation datasets, and confounding factors such as experiment configuration for extrinsic metrics.","year":2022,"title_abstract":"On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations Multiple metrics have been introduced to measure fairness in various natural language processing tasks. These metrics can be roughly categorized into two categories: 1) extrinsic metrics for evaluating fairness in downstream applications and 2) intrinsic metrics for estimating fairness in upstream contextualized language representation models. In this paper, we conduct an extensive correlation study between intrinsic and extrinsic metrics across bias notions using 19 contextualized language models. We find that intrinsic and extrinsic metrics do not necessarily correlate in their original setting, even when correcting for metric misalignments, noise in evaluation datasets, and confounding factors such as experiment configuration for extrinsic metrics.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1512800157,"Goal":"Reduced Inequalities","Task":["fairness","natural language processing tasks","evaluating fairness","downstream applications","estimating fairness"],"Method":["Contextualized Language Representations","upstream contextualized language representation models","contextualized language models"]},{"ID":"madukwe-etal-2020-data","title":"In Data We Trust: A Critical Analysis of Hate Speech Detection Datasets","abstract":"Recently, a few studies have discussed the limitations of datasets collected for the task of detecting hate speech from different viewpoints. We intend to contribute to the conversation by providing a consolidated overview of these issues pertaining to the data that debilitate research in this area. Specifically, we discuss how the varying pre-processing steps and the format for making data publicly available result in highly varying datasets that make an objective comparison between studies difficult and unfair. There is currently no study (to the best of our knowledge) focused on comparing the attributes of existing datasets for hate speech detection, outlining their limitations and recommending approaches for future research. This work intends to fill that gap and become the one-stop shop for information regarding hate speech datasets.","year":2020,"title_abstract":"In Data We Trust: A Critical Analysis of Hate Speech Detection Datasets Recently, a few studies have discussed the limitations of datasets collected for the task of detecting hate speech from different viewpoints. We intend to contribute to the conversation by providing a consolidated overview of these issues pertaining to the data that debilitate research in this area. Specifically, we discuss how the varying pre-processing steps and the format for making data publicly available result in highly varying datasets that make an objective comparison between studies difficult and unfair. There is currently no study (to the best of our knowledge) focused on comparing the attributes of existing datasets for hate speech detection, outlining their limitations and recommending approaches for future research. This work intends to fill that gap and become the one-stop shop for information regarding hate speech datasets.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1512493789,"Goal":"Reduced Inequalities","Task":["Hate Speech","Detection","detecting hate speech","hate speech detection"],"Method":["pre - processing steps"]},{"ID":"xie-etal-2020-contextual","title":"A Contextual Alignment Enhanced Cross Graph Attention Network for Cross-lingual Entity Alignment","abstract":"Cross-lingual entity alignment, which aims to match equivalent entities in KGs with different languages, has attracted considerable focus in recent years. Recently, many graph neural network (GNN) based methods are proposed for entity alignment and obtain promising results. However, existing GNN-based methods consider the two KGs independently and learn embeddings for different KGs separately, which ignore the useful pre-aligned links between two KGs. In this paper, we propose a novel Contextual Alignment Enhanced Cross Graph Attention Network (CAECGAT) for the task of cross-lingual entity alignment, which is able to jointly learn the embeddings in different KGs by propagating cross-KG information through pre-aligned seed alignments. We conduct extensive experiments on three benchmark cross-lingual entity alignment datasets. The experimental results demonstrate that our proposed method obtains remarkable performance gains compared to state-of-the-art methods.","year":2020,"title_abstract":"A Contextual Alignment Enhanced Cross Graph Attention Network for Cross-lingual Entity Alignment Cross-lingual entity alignment, which aims to match equivalent entities in KGs with different languages, has attracted considerable focus in recent years. Recently, many graph neural network (GNN) based methods are proposed for entity alignment and obtain promising results. However, existing GNN-based methods consider the two KGs independently and learn embeddings for different KGs separately, which ignore the useful pre-aligned links between two KGs. In this paper, we propose a novel Contextual Alignment Enhanced Cross Graph Attention Network (CAECGAT) for the task of cross-lingual entity alignment, which is able to jointly learn the embeddings in different KGs by propagating cross-KG information through pre-aligned seed alignments. We conduct extensive experiments on three benchmark cross-lingual entity alignment datasets. The experimental results demonstrate that our proposed method obtains remarkable performance gains compared to state-of-the-art methods.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1512120962,"Goal":"Gender Equality","Task":["Cross - lingual Entity Alignment","Cross - lingual entity alignment","entity alignment","cross - lingual entity alignment","cross - lingual entity alignment datasets"],"Method":["Contextual Alignment Enhanced Cross Graph Attention Network","graph neural network","GNN - based methods","KGs","Contextual Alignment Enhanced Cross Graph Attention Network"]},{"ID":"panda-levitan-2021-detecting","title":"Detecting Multilingual {COVID}-19 Misinformation on Social Media via Contextualized Embeddings","abstract":"We present machine learning classifiers to automatically identify COVID-19 misinformation on social media in three languages: English, Bulgarian, and Arabic. We compared 4 multitask learning models for this task and found that a model trained with English BERT achieves the best results for English, and multilingual BERT achieves the best results for Bulgarian and Arabic. We experimented with zero shot, few shot, and target-only conditions to evaluate the impact of target-language training data on classifier performance, and to understand the capabilities of different models to generalize across languages in detecting misinformation online. This work was performed as a submission to the shared task, NLP4IF 2021: Fighting the COVID-19 Infodemic. Our best models achieved the second best evaluation test results for Bulgarian and Arabic among all the participating teams and obtained competitive scores for English.","year":2021,"title_abstract":"Detecting Multilingual {COVID}-19 Misinformation on Social Media via Contextualized Embeddings We present machine learning classifiers to automatically identify COVID-19 misinformation on social media in three languages: English, Bulgarian, and Arabic. We compared 4 multitask learning models for this task and found that a model trained with English BERT achieves the best results for English, and multilingual BERT achieves the best results for Bulgarian and Arabic. We experimented with zero shot, few shot, and target-only conditions to evaluate the impact of target-language training data on classifier performance, and to understand the capabilities of different models to generalize across languages in detecting misinformation online. This work was performed as a submission to the shared task, NLP4IF 2021: Fighting the COVID-19 Infodemic. Our best models achieved the second best evaluation test results for Bulgarian and Arabic among all the participating teams and obtained competitive scores for English.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.151199311,"Goal":"Climate Action","Task":["Detecting Multilingual {COVID} - 19","COVID - 19","classifier","detecting misinformation online","shared task","NLP4IF"],"Method":["Contextualized Embeddings","machine learning classifiers","multitask learning models","multilingual BERT"]},{"ID":"hettiarachchi-ranasinghe-2020-infominer","title":"{I}nfo{M}iner at {WNUT}-2020 Task 2: Transformer-based Covid-19 Informative Tweet Extraction","abstract":"Identifying informative tweets is an important step when building information extraction systems based on social media. WNUT-2020 Task 2 was organised to recognise informative tweets from noise tweets. In this paper, we present our approach to tackle the task objective using transformers. Overall, our approach achieves 10th place in the final rankings scoring 0.9004 F1 score for the test set.","year":2020,"title_abstract":"{I}nfo{M}iner at {WNUT}-2020 Task 2: Transformer-based Covid-19 Informative Tweet Extraction Identifying informative tweets is an important step when building information extraction systems based on social media. WNUT-2020 Task 2 was organised to recognise informative tweets from noise tweets. In this paper, we present our approach to tackle the task objective using transformers. Overall, our approach achieves 10th place in the final rankings scoring 0.9004 F1 score for the test set.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1511881202,"Goal":"Climate Action","Task":["Transformer - based Covid - 19 Informative Tweet Extraction","Identifying informative tweets","WNUT - 2020"],"Method":["information extraction systems"]},{"ID":"scherrer-etal-2020-university","title":"The {U}niversity of {H}elsinki and Aalto University submissions to the {WMT} 2020 news and low-resource translation tasks","abstract":"This paper describes the joint participation of University of Helsinki and Aalto University to two shared tasks of WMT 2020: the news translation between Inuktitut and English and the low-resource translation between German and Upper Sorbian. For both tasks, our efforts concentrate on efficient use of monolingual and related bilingual corpora with scheduled multi-task learning as well as an optimized subword segmentation with sampling. Our submission obtained the highest score for Upper Sorbian -{\\textgreater} German and was ranked second for German -{\\textgreater} Upper Sorbian according to BLEU scores. For English{--}Inuktitut, we reached ranks 8 and 10 out of 11 according to BLEU scores.","year":2020,"title_abstract":"The {U}niversity of {H}elsinki and Aalto University submissions to the {WMT} 2020 news and low-resource translation tasks This paper describes the joint participation of University of Helsinki and Aalto University to two shared tasks of WMT 2020: the news translation between Inuktitut and English and the low-resource translation between German and Upper Sorbian. For both tasks, our efforts concentrate on efficient use of monolingual and related bilingual corpora with scheduled multi-task learning as well as an optimized subword segmentation with sampling. Our submission obtained the highest score for Upper Sorbian -{\\textgreater} German and was ranked second for German -{\\textgreater} Upper Sorbian according to BLEU scores. For English{--}Inuktitut, we reached ranks 8 and 10 out of 11 according to BLEU scores.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1511667967,"Goal":"Gender Equality","Task":["low - resource translation tasks","WMT 2020","translation","translation","multi - task learning"],"Method":["subword segmentation","sampling"]},{"ID":"hande-etal-2022-best","title":"The Best of both Worlds: Dual Channel Language modeling for Hope Speech Detection in low-resourced {K}annada","abstract":"In recent years, various methods have been developed to control the spread of negativity by removing profane, aggressive, and offensive comments from social media platforms. There is, however, a scarcity of research focusing on embracing positivity and reinforcing supportive and reassuring content in online forums. As a result, we concentrate our research on developing systems to detect hope speech in code-mixed Kannada. As a result, we present DC-LM, a dual-channel language model that sees hope speech by using the English translations of the code-mixed dataset for additional training. The approach is jointly modelled on both English and code-mixed Kannada to enable effective cross-lingual transfer between the languages. With a weighted F1-score of 0.756, the method outperforms other models. We aim to initiate research in Kannada while encouraging researchers to take a pragmatic approach to inspire positive and supportive online content.","year":2022,"title_abstract":"The Best of both Worlds: Dual Channel Language modeling for Hope Speech Detection in low-resourced {K}annada In recent years, various methods have been developed to control the spread of negativity by removing profane, aggressive, and offensive comments from social media platforms. There is, however, a scarcity of research focusing on embracing positivity and reinforcing supportive and reassuring content in online forums. As a result, we concentrate our research on developing systems to detect hope speech in code-mixed Kannada. As a result, we present DC-LM, a dual-channel language model that sees hope speech by using the English translations of the code-mixed dataset for additional training. The approach is jointly modelled on both English and code-mixed Kannada to enable effective cross-lingual transfer between the languages. With a weighted F1-score of 0.756, the method outperforms other models. We aim to initiate research in Kannada while encouraging researchers to take a pragmatic approach to inspire positive and supportive online content.","social_need":"No Poverty End poverty in all its forms everywhere","cosine_similarity":0.1511490196,"Goal":"No Poverty","Task":["Hope Speech Detection"],"Method":["Dual Channel Language modeling","DC - LM","dual - channel language model"]},{"ID":"zhu-etal-2020-hierarchical","title":"A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining","abstract":"With the abundance of automatic meeting transcripts, meeting summarization is of great interest to both participants and other parties. Traditional methods of summarizing meetings depend on complex multi-step pipelines that make joint optimization intractable. Meanwhile, there are a handful of deep neural models for text summarization and dialogue systems. However, the semantic structure and styles of meeting transcripts are quite different from articles and conversations. In this paper, we propose a novel abstractive summary network that adapts to the meeting scenario. We design a hierarchical structure to accommodate long meeting transcripts and a role vector to depict the difference among speakers. Furthermore, due to the inadequacy of meeting summary data, we pretrain the model on large-scale news summary data. Empirical results show that our model outperforms previous approaches in both automatic metrics and human evaluation. For example, on ICSI dataset, the ROUGE-1 score increases from 34.66{\\%} to 46.28{\\%}.","year":2020,"title_abstract":"A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining With the abundance of automatic meeting transcripts, meeting summarization is of great interest to both participants and other parties. Traditional methods of summarizing meetings depend on complex multi-step pipelines that make joint optimization intractable. Meanwhile, there are a handful of deep neural models for text summarization and dialogue systems. However, the semantic structure and styles of meeting transcripts are quite different from articles and conversations. In this paper, we propose a novel abstractive summary network that adapts to the meeting scenario. We design a hierarchical structure to accommodate long meeting transcripts and a role vector to depict the difference among speakers. Furthermore, due to the inadequacy of meeting summary data, we pretrain the model on large-scale news summary data. Empirical results show that our model outperforms previous approaches in both automatic metrics and human evaluation. For example, on ICSI dataset, the ROUGE-1 score increases from 34.66{\\%} to 46.28{\\%}.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.151127845,"Goal":"Partnership for the Goals","Task":["Abstractive Meeting Summarization","Cross - Domain Pretraining","summarization","summarizing meetings","joint optimization","text summarization","dialogue systems","meeting scenario"],"Method":["Hierarchical Network","multi - step pipelines","deep neural models","abstractive summary network"]},{"ID":"hoque-etal-2016-interactive","title":"An Interactive System for Exploring Community Question Answering Forums","abstract":"We present an interactive system to provide effective and efficient search capabilities in Community Question Answering (cQA) forums. The system integrates state-of-the-art technology for answer search with a Web-based user interface specifically tailored to support the cQA forum readers. The answer search module automatically finds relevant answers for a new question by exploring related questions and the comments within their threads. The graphical user interface presents the search results and supports the exploration of related information. The system is running live at \\url{http:\/\/www.qatarliving.com\/betasearch\/}.","year":2016,"title_abstract":"An Interactive System for Exploring Community Question Answering Forums We present an interactive system to provide effective and efficient search capabilities in Community Question Answering (cQA) forums. The system integrates state-of-the-art technology for answer search with a Web-based user interface specifically tailored to support the cQA forum readers. The answer search module automatically finds relevant answers for a new question by exploring related questions and the comments within their threads. The graphical user interface presents the search results and supports the exploration of related information. The system is running live at \\url{http:\/\/www.qatarliving.com\/betasearch\/}.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1510692388,"Goal":"Sustainable Cities and Communities","Task":["Exploring Community Question Answering Forums","search capabilities","Community Question Answering","answer search","cQA"],"Method":["Interactive System","interactive system","Web - based user interface","answer search module"]},{"ID":"iwai-etal-2020-acquiring","title":"Acquiring Social Knowledge about Personality and Driving-related Behavior","abstract":"In this paper, we introduce our psychological approach to collect human-specific social knowledge from a text corpus, using NLP techniques. It is often not explicitly described but shared among people, which we call social knowledge. We focus on the social knowledge, especially personality and driving. We used the language resources that were developed based on psychological research methods; a Japanese personality dictionary (317 words) and a driving experience corpus (8,080 sentences) annotated with behavior and subjectivity. Using them, we automatically extracted collocations between personality descriptors and driving-related behavior from a driving behavior and subjectivity corpus (1,803,328 sentences after filtering) and obtained unique 5,334 collocations. To evaluate the collocations as social knowledge, we designed four step-by-step crowdsourcing tasks. They resulted in 266 pieces of social knowledge. They include the knowledge that might be difficult to recall by themselves but easy to agree with. We discuss the acquired social knowledge and the contribution to implementations into systems.","year":2020,"title_abstract":"Acquiring Social Knowledge about Personality and Driving-related Behavior In this paper, we introduce our psychological approach to collect human-specific social knowledge from a text corpus, using NLP techniques. It is often not explicitly described but shared among people, which we call social knowledge. We focus on the social knowledge, especially personality and driving. We used the language resources that were developed based on psychological research methods; a Japanese personality dictionary (317 words) and a driving experience corpus (8,080 sentences) annotated with behavior and subjectivity. Using them, we automatically extracted collocations between personality descriptors and driving-related behavior from a driving behavior and subjectivity corpus (1,803,328 sentences after filtering) and obtained unique 5,334 collocations. To evaluate the collocations as social knowledge, we designed four step-by-step crowdsourcing tasks. They resulted in 266 pieces of social knowledge. They include the knowledge that might be difficult to recall by themselves but easy to agree with. We discuss the acquired social knowledge and the contribution to implementations into systems.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1510260105,"Goal":"Sustainable Cities and Communities","Task":["Acquiring Social Knowledge about Personality and Driving - related Behavior","crowdsourcing tasks"],"Method":["psychological approach","NLP techniques","psychological research methods;"]},{"ID":"pustejovsky-etal-2019-modeling","title":"Modeling Quantification and Scope in {A}bstract {M}eaning {R}epresentations","abstract":"In this paper, we propose an extension to Abstract Meaning Representations (AMRs) to encode scope information of quantifiers and negation, in a way that overcomes the semantic gaps of the schema while maintaining its cognitive simplicity. Specifically, we address three phenomena not previously part of the AMR specification: quantification, negation (generally), and modality. The resulting representation, which we call {``}Uniform Meaning Representation{''} (UMR), adopts the predicative core of AMR and embeds it under a {``}scope{''} graph when appropriate. UMR representations differ from other treatments of quantification and modal scope phenomena in two ways: (a) they are more transparent; and (b) they specify default scope when possible.{`}","year":2019,"title_abstract":"Modeling Quantification and Scope in {A}bstract {M}eaning {R}epresentations In this paper, we propose an extension to Abstract Meaning Representations (AMRs) to encode scope information of quantifiers and negation, in a way that overcomes the semantic gaps of the schema while maintaining its cognitive simplicity. Specifically, we address three phenomena not previously part of the AMR specification: quantification, negation (generally), and modality. The resulting representation, which we call {``}Uniform Meaning Representation{''} (UMR), adopts the predicative core of AMR and embeds it under a {``}scope{''} graph when appropriate. UMR representations differ from other treatments of quantification and modal scope phenomena in two ways: (a) they are more transparent; and (b) they specify default scope when possible.{`}","social_need":"Clean Water and Sanitation Ensure availability and sustainable management of water and sanitation for all","cosine_similarity":0.1510236114,"Goal":"Clean Water and Sanitation","Task":["Modeling Quantification and Scope","quantification and modal scope phenomena"],"Method":["Abstract Meaning Representations","AMR specification","Meaning Representation{''}","AMR","UMR representations"]},{"ID":"estevez-velarde-etal-2019-demo","title":"Demo Application for {LETO}: Learning Engine Through Ontologies","abstract":"The massive amount of multi-formatted information available on the Web necessitates the design of software systems that leverage this information to obtain knowledge that is valid and useful. The main challenge is to discover relevant information and continuously update, enrich and integrate knowledge from various sources of structured and unstructured data. This paper presents the Learning Engine Through Ontologies(LETO) framework, an architecture for the continuous and incremental discovery of knowledge from multiple sources of unstructured and structured data. We justify the main design decision behind LETO{'}s architecture and evaluate the framework{'}s feasibility using the Internet Movie Data Base(IMDB) and Twitter as a practical application.","year":2019,"title_abstract":"Demo Application for {LETO}: Learning Engine Through Ontologies The massive amount of multi-formatted information available on the Web necessitates the design of software systems that leverage this information to obtain knowledge that is valid and useful. The main challenge is to discover relevant information and continuously update, enrich and integrate knowledge from various sources of structured and unstructured data. This paper presents the Learning Engine Through Ontologies(LETO) framework, an architecture for the continuous and incremental discovery of knowledge from multiple sources of unstructured and structured data. We justify the main design decision behind LETO{'}s architecture and evaluate the framework{'}s feasibility using the Internet Movie Data Base(IMDB) and Twitter as a practical application.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1509371251,"Goal":"Quality Education","Task":["continuous and incremental discovery of knowledge"],"Method":["Demo Application","Learning Engine","software systems","Learning Engine","Ontologies(LETO) framework","LETO{'}s architecture"]},{"ID":"peter-etal-2018-sisyphus","title":"Sisyphus, a Workflow Manager Designed for Machine Translation and Automatic Speech Recognition","abstract":"Training and testing many possible parameters or model architectures of state-of-the-art machine translation or automatic speech recognition system is a cumbersome task. They usually require a long pipeline of commands reaching from pre-processing the training data to post-processing and evaluating the output.","year":2018,"title_abstract":"Sisyphus, a Workflow Manager Designed for Machine Translation and Automatic Speech Recognition Training and testing many possible parameters or model architectures of state-of-the-art machine translation or automatic speech recognition system is a cumbersome task. They usually require a long pipeline of commands reaching from pre-processing the training data to post-processing and evaluating the output.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1509319246,"Goal":"Gender Equality","Task":["Machine Translation","Automatic Speech Recognition","machine translation"],"Method":["Sisyphus","Workflow Manager","model architectures","automatic speech recognition system"]},{"ID":"gemes-recski-2021-tuw","title":"{TUW}-{I}nf at {G}erm{E}val2021: Rule-based and Hybrid Methods for Detecting Toxic, Engaging, and Fact-Claiming Comments","abstract":"This paper describes our methods submitted for the GermEval 2021 shared task on identifying toxic, engaging and fact-claiming comments in social media texts (Risch et al., 2021). We explore simple strategies for semi-automatic generation of rule-based systems with high precision and low recall, and use them to achieve slight overall improvements over a standard BERT-based classifier.","year":2021,"title_abstract":"{TUW}-{I}nf at {G}erm{E}val2021: Rule-based and Hybrid Methods for Detecting Toxic, Engaging, and Fact-Claiming Comments This paper describes our methods submitted for the GermEval 2021 shared task on identifying toxic, engaging and fact-claiming comments in social media texts (Risch et al., 2021). We explore simple strategies for semi-automatic generation of rule-based systems with high precision and low recall, and use them to achieve slight overall improvements over a standard BERT-based classifier.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1509211063,"Goal":"Climate Action","Task":["Detecting Toxic , Engaging , and Fact - Claiming Comments","GermEval 2021 shared task","identifying toxic , engaging and fact - claiming comments","semi - automatic generation"],"Method":["Rule - based and Hybrid Methods","rule - based systems","BERT - based classifier"]},{"ID":"demszky-etal-2021-measuring","title":"Measuring Conversational Uptake: A Case Study on Student-Teacher Interactions","abstract":"In conversation, uptake happens when a speaker builds on the contribution of their interlocutor by, for example, acknowledging, repeating or reformulating what they have said. In education, teachers{'} uptake of student contributions has been linked to higher student achievement. Yet measuring and improving teachers{'} uptake at scale is challenging, as existing methods require expensive annotation by experts. We propose a framework for computationally measuring uptake, by (1) releasing a dataset of student-teacher exchanges extracted from US math classroom transcripts annotated for uptake by experts; (2) formalizing uptake as pointwise Jensen-Shannon Divergence (pJSD), estimated via next utterance classification; (3) conducting a linguistically-motivated comparison of different unsupervised measures and (4) correlating these measures with educational outcomes. We find that although repetition captures a significant part of uptake, pJSD outperforms repetition-based baselines, as it is capable of identifying a wider range of uptake phenomena like question answering and reformulation. We apply our uptake measure to three different educational datasets with outcome indicators. Unlike baseline measures, pJSD correlates significantly with instruction quality in all three, providing evidence for its generalizability and for its potential to serve as an automated professional development tool for teachers.","year":2021,"title_abstract":"Measuring Conversational Uptake: A Case Study on Student-Teacher Interactions In conversation, uptake happens when a speaker builds on the contribution of their interlocutor by, for example, acknowledging, repeating or reformulating what they have said. In education, teachers{'} uptake of student contributions has been linked to higher student achievement. Yet measuring and improving teachers{'} uptake at scale is challenging, as existing methods require expensive annotation by experts. We propose a framework for computationally measuring uptake, by (1) releasing a dataset of student-teacher exchanges extracted from US math classroom transcripts annotated for uptake by experts; (2) formalizing uptake as pointwise Jensen-Shannon Divergence (pJSD), estimated via next utterance classification; (3) conducting a linguistically-motivated comparison of different unsupervised measures and (4) correlating these measures with educational outcomes. We find that although repetition captures a significant part of uptake, pJSD outperforms repetition-based baselines, as it is capable of identifying a wider range of uptake phenomena like question answering and reformulation. We apply our uptake measure to three different educational datasets with outcome indicators. Unlike baseline measures, pJSD correlates significantly with instruction quality in all three, providing evidence for its generalizability and for its potential to serve as an automated professional development tool for teachers.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1509157419,"Goal":"Quality Education","Task":["Measuring Conversational Uptake","Student - Teacher Interactions","conversation","uptake","education","teachers{'} uptake","computationally measuring uptake","uptake","linguistically","uptake","uptake phenomena","question answering","reformulation","teachers"],"Method":["pointwise Jensen - Shannon Divergence","next utterance classification;","repetition","pJSD","repetition - based baselines","pJSD","automated professional development tool"]},{"ID":"dhanani-rafi-2020-attention","title":"Attention Transformer Model for Translation of Similar Languages","abstract":"This paper illustrates our approach to the shared task on similar language translation in the fifth conference on machine translation (WMT-20). Our motivation comes from the latest state of the art neural machine translation in which Transformers and Recurrent Attention models are effectively used. A typical sequence-sequence architecture consists of an encoder and a decoder Recurrent Neural Network (RNN). The encoder recursively processes a source sequence and reduces it into a fixed-length vector (context), and the decoder generates a target sequence, token by token, conditioned on the same context. In contrast, the advantage of transformers is to reduce the training time by offering a higher degree of parallelism at the cost of freedom for sequential order. With the introduction of Recurrent Attention, it allows the decoder to focus effectively on order of the source sequence at different decoding steps. In our approach, we have combined the recurrence based layered encoder-decoder model with the Transformer model. Our Attention Transformer model enjoys the benefits of both Recurrent Attention and Transformer to quickly learn the most probable sequence for decoding in the target language. The architecture is especially suited for similar languages (languages coming from the same family). We have submitted our system for both Indo-Aryan Language forward (Hindi to Marathi) and reverse (Marathi to Hindi) pair. Our system trains on the parallel corpus of the training dataset provided by the organizers and achieved an average BLEU point of 3.68 with 97.64 TER score for the Hindi-Marathi, along with 9.02 BLEU point and 88.6 TER score for Marathi-Hindi testing set.","year":2020,"title_abstract":"Attention Transformer Model for Translation of Similar Languages This paper illustrates our approach to the shared task on similar language translation in the fifth conference on machine translation (WMT-20). Our motivation comes from the latest state of the art neural machine translation in which Transformers and Recurrent Attention models are effectively used. A typical sequence-sequence architecture consists of an encoder and a decoder Recurrent Neural Network (RNN). The encoder recursively processes a source sequence and reduces it into a fixed-length vector (context), and the decoder generates a target sequence, token by token, conditioned on the same context. In contrast, the advantage of transformers is to reduce the training time by offering a higher degree of parallelism at the cost of freedom for sequential order. With the introduction of Recurrent Attention, it allows the decoder to focus effectively on order of the source sequence at different decoding steps. In our approach, we have combined the recurrence based layered encoder-decoder model with the Transformer model. Our Attention Transformer model enjoys the benefits of both Recurrent Attention and Transformer to quickly learn the most probable sequence for decoding in the target language. The architecture is especially suited for similar languages (languages coming from the same family). We have submitted our system for both Indo-Aryan Language forward (Hindi to Marathi) and reverse (Marathi to Hindi) pair. Our system trains on the parallel corpus of the training dataset provided by the organizers and achieved an average BLEU point of 3.68 with 97.64 TER score for the Hindi-Marathi, along with 9.02 BLEU point and 88.6 TER score for Marathi-Hindi testing set.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1509126127,"Goal":"Gender Equality","Task":["Translation of Similar Languages","translation","machine translation","neural machine translation","decoding"],"Method":["Attention Transformer Model","Transformers","Recurrent Attention models","sequence - sequence architecture","encoder","decoder Recurrent Neural Network","encoder","decoder","Recurrent Attention","decoder","recurrence based layered encoder - decoder model","Transformer model","Attention Transformer model","Recurrent Attention","Transformer"]},{"ID":"boratko-etal-2018-interface","title":"An Interface for Annotating Science Questions","abstract":"Recent work introduces the AI2 Reasoning Challenge (ARC) and the associated ARC dataset that partitions open domain, complex science questions into an Easy Set and a Challenge Set. That work includes an analysis of 100 questions with respect to the types of knowledge and reasoning required to answer them. However, it does not include clear definitions of these types, nor does it offer information about the quality of the labels or the annotation process used. In this paper, we introduce a novel interface for human annotation of science question-answer pairs with their respective knowledge and reasoning types, in order that the classification of new questions may be improved. We build on the classification schema proposed by prior work on the ARC dataset, and evaluate the effectiveness of our interface with a preliminary study involving 10 participants.","year":2018,"title_abstract":"An Interface for Annotating Science Questions Recent work introduces the AI2 Reasoning Challenge (ARC) and the associated ARC dataset that partitions open domain, complex science questions into an Easy Set and a Challenge Set. That work includes an analysis of 100 questions with respect to the types of knowledge and reasoning required to answer them. However, it does not include clear definitions of these types, nor does it offer information about the quality of the labels or the annotation process used. In this paper, we introduce a novel interface for human annotation of science question-answer pairs with their respective knowledge and reasoning types, in order that the classification of new questions may be improved. We build on the classification schema proposed by prior work on the ARC dataset, and evaluate the effectiveness of our interface with a preliminary study involving 10 participants.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1508707255,"Goal":"Life on Land","Task":["Annotating Science Questions","AI2 Reasoning Challenge","human annotation of science question - answer pairs","classification"],"Method":["annotation process","classification schema"]},{"ID":"scholman-demberg-2017-crowdsourcing","title":"Crowdsourcing discourse interpretations: On the influence of context and the reliability of a connective insertion task","abstract":"Traditional discourse annotation tasks are considered costly and time-consuming, and the reliability and validity of these tasks is in question. In this paper, we investigate whether crowdsourcing can be used to obtain reliable discourse relation annotations. We also examine the influence of context on the reliability of the data. The results of a crowdsourced connective insertion task showed that the method can be used to obtain reliable annotations: The majority of the inserted connectives converged with the original label. Further, the method is sensitive to the fact that multiple senses can often be inferred for a single relation. Regarding the presence of context, the results show no significant difference in distributions of insertions between conditions overall. However, a by-item comparison revealed several characteristics of segments that determine whether the presence of context makes a difference in annotations. The findings discussed in this paper can be taken as evidence that crowdsourcing can be used as a valuable method to obtain insights into the sense(s) of relations.","year":2017,"title_abstract":"Crowdsourcing discourse interpretations: On the influence of context and the reliability of a connective insertion task Traditional discourse annotation tasks are considered costly and time-consuming, and the reliability and validity of these tasks is in question. In this paper, we investigate whether crowdsourcing can be used to obtain reliable discourse relation annotations. We also examine the influence of context on the reliability of the data. The results of a crowdsourced connective insertion task showed that the method can be used to obtain reliable annotations: The majority of the inserted connectives converged with the original label. Further, the method is sensitive to the fact that multiple senses can often be inferred for a single relation. Regarding the presence of context, the results show no significant difference in distributions of insertions between conditions overall. However, a by-item comparison revealed several characteristics of segments that determine whether the presence of context makes a difference in annotations. The findings discussed in this paper can be taken as evidence that crowdsourcing can be used as a valuable method to obtain insights into the sense(s) of relations.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1508650035,"Goal":"Sustainable Cities and Communities","Task":["Crowdsourcing discourse interpretations","connective insertion task","discourse annotation tasks","crowdsourcing","crowdsourced connective insertion task","sense(s) of relations"],"Method":["crowdsourcing"]},{"ID":"boitet-1999-research","title":"A research perspective on how to democratize machine translation and translation aids aiming at high quality final output","abstract":"Machine Translation (MT) systems and Translation Aids (TA) aiming at cost-effective high quality final translation are not yet usable by small firms, departments and individuals, and handle only a few languages and language pairs. This is due to a variety of reasons, some of them not frequently mentioned. But commercial, technical and cultural reasons make it mandatory to find ways to democratize MT and TA. This goal could be attained by: (1) giving users, free of charge, TA client tools and server resources in exchange for the permission to store and refine on the server linguistic resources produced while using TA; (2) establishing a synergy between MT and TA, in particular by using them jointly in translation projects where translators codevelop the lexical resources specific to MT; (3) renouncing the illusion of fully automatic general purpose high quality MT (FAHQMT) and go for semi-automaticity (SAHQMT), where user participation, made possible by recent technical network-oriented advances, is used to solve ambiguities otherwise computationnally unsolvable due to the impossibility, intractability or cost of accessing the necessary knowledge; (4) adopting a hybrid (symbolic {\\&} numerical) and ``pivot'' approach for MT, where pivot lexemes arc UNL or UNL inspired English-oriented denotations of (sets of) interlingual acceptions or word\/term senses, and the rest of the representation of utterances is either fully abstract and interlingual as in UNL, or, less ambitiously but more realistically, obtained by adding to an abstract English multilevel structure features underspecified in English but essential for other languages, including minority languages.","year":1999,"title_abstract":"A research perspective on how to democratize machine translation and translation aids aiming at high quality final output Machine Translation (MT) systems and Translation Aids (TA) aiming at cost-effective high quality final translation are not yet usable by small firms, departments and individuals, and handle only a few languages and language pairs. This is due to a variety of reasons, some of them not frequently mentioned. But commercial, technical and cultural reasons make it mandatory to find ways to democratize MT and TA. This goal could be attained by: (1) giving users, free of charge, TA client tools and server resources in exchange for the permission to store and refine on the server linguistic resources produced while using TA; (2) establishing a synergy between MT and TA, in particular by using them jointly in translation projects where translators codevelop the lexical resources specific to MT; (3) renouncing the illusion of fully automatic general purpose high quality MT (FAHQMT) and go for semi-automaticity (SAHQMT), where user participation, made possible by recent technical network-oriented advances, is used to solve ambiguities otherwise computationnally unsolvable due to the impossibility, intractability or cost of accessing the necessary knowledge; (4) adopting a hybrid (symbolic {\\&} numerical) and ``pivot'' approach for MT, where pivot lexemes arc UNL or UNL inspired English-oriented denotations of (sets of) interlingual acceptions or word\/term senses, and the rest of the representation of utterances is either fully abstract and interlingual as in UNL, or, less ambitiously but more realistically, obtained by adding to an abstract English multilevel structure features underspecified in English but essential for other languages, including minority languages.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1508276612,"Goal":"Gender Equality","Task":["machine translation","translation aids","Machine Translation","Translation Aids","translation","TA","translation projects","MT;","fully automatic general purpose high quality MT","go","semi - automaticity","MT"],"Method":["MT","TA;","MT","TA","numerical)","UNL"]},{"ID":"cieri-liberman-2008-15","title":"15 Years of Language Resource Creation and Sharing: a Progress Report on {LDC} Activities","abstract":"This paper, the fifth in a series of biennial progress reports, reviews the activities of the Linguistic Data Consortium with particular emphasis on general trends in the language resource landscape and on changes that distinguish the two years since LDC\u0092s last report at LREC from the preceding 8 years. After providing a perspective on the current landscape of language resources, the paper goes on to describe our vision of the role of LDC within the research communities it serves before sketching briefly specific publications and resources creations projects that have been the focus our attention since the last report.","year":2008,"title_abstract":"15 Years of Language Resource Creation and Sharing: a Progress Report on {LDC} Activities This paper, the fifth in a series of biennial progress reports, reviews the activities of the Linguistic Data Consortium with particular emphasis on general trends in the language resource landscape and on changes that distinguish the two years since LDC\u0092s last report at LREC from the preceding 8 years. After providing a perspective on the current landscape of language resources, the paper goes on to describe our vision of the role of LDC within the research communities it serves before sketching briefly specific publications and resources creations projects that have been the focus our attention since the last report.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1508267969,"Goal":"Partnership for the Goals","Task":["Language Resource Creation","Sharing","language resources","resources creations projects"],"Method":["LDC"]},{"ID":"hardmeier-etal-2018-forms","title":"Forms of Anaphoric Reference to Organisational Named Entities: Hoping to widen appeal, they diversified","abstract":"Proper names of organisations are a special case of collective nouns. Their meaning can be conceptualised as a collective unit or as a plurality of persons, allowing for different morphological marking of coreferent anaphoric pronouns. This paper explores the variability of references to organisation names with 1) a corpus analysis and 2) two crowd-sourced story continuation experiments. The first shows that the preference for singular vs. plural conceptualisation is dependent on the level of formality of a text. In the second, we observe a strong preference for the plural they otherwise typical of informal speech. Using edited corpus data instead of constructed sentences as stimuli reduces this preference.","year":2018,"title_abstract":"Forms of Anaphoric Reference to Organisational Named Entities: Hoping to widen appeal, they diversified Proper names of organisations are a special case of collective nouns. Their meaning can be conceptualised as a collective unit or as a plurality of persons, allowing for different morphological marking of coreferent anaphoric pronouns. This paper explores the variability of references to organisation names with 1) a corpus analysis and 2) two crowd-sourced story continuation experiments. The first shows that the preference for singular vs. plural conceptualisation is dependent on the level of formality of a text. In the second, we observe a strong preference for the plural they otherwise typical of informal speech. Using edited corpus data instead of constructed sentences as stimuli reduces this preference.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1507333964,"Goal":"Sustainable Cities and Communities","Task":["crowd - sourced story continuation experiments"],"Method":["corpus analysis","plural conceptualisation"]},{"ID":"specia-etal-2018-findings","title":"Findings of the {WMT} 2018 Shared Task on Quality Estimation","abstract":"We report the results of the WMT18 shared task on Quality Estimation, i.e. the task of predicting the quality of the output of machine translation systems at various granularity levels: word, phrase, sentence and document. This year we include four language pairs, three text domains, and translations produced by both statistical and neural machine translation systems. Participating teams from ten institutions submitted a variety of systems to different task variants and language pairs.","year":2018,"title_abstract":"Findings of the {WMT} 2018 Shared Task on Quality Estimation We report the results of the WMT18 shared task on Quality Estimation, i.e. the task of predicting the quality of the output of machine translation systems at various granularity levels: word, phrase, sentence and document. This year we include four language pairs, three text domains, and translations produced by both statistical and neural machine translation systems. Participating teams from ten institutions submitted a variety of systems to different task variants and language pairs.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1507317573,"Goal":"Quality Education","Task":["Quality Estimation","WMT18 shared task","Quality Estimation","machine translation"],"Method":["statistical and neural machine translation systems"]},{"ID":"wang-etal-2016-exploring","title":"Exploring Text Links for Coherent Multi-Document Summarization","abstract":"Summarization aims to represent source documents by a shortened passage. Existing methods focus on the extraction of key information, but often neglect coherence. Hence the generated summaries suffer from a lack of readability. To address this problem, we have developed a graph-based method by exploring the links between text to produce coherent summaries. Our approach involves finding a sequence of sentences that best represent the key information in a coherent way. In contrast to the previous methods that focus only on salience, the proposed method addresses both coherence and informativeness based on textual linkages. We conduct experiments on the DUC2004 summarization task data set. A performance comparison reveals that the summaries generated by the proposed system achieve comparable results in terms of the ROUGE metric, and show improvements in readability by human evaluation.","year":2016,"title_abstract":"Exploring Text Links for Coherent Multi-Document Summarization Summarization aims to represent source documents by a shortened passage. Existing methods focus on the extraction of key information, but often neglect coherence. Hence the generated summaries suffer from a lack of readability. To address this problem, we have developed a graph-based method by exploring the links between text to produce coherent summaries. Our approach involves finding a sequence of sentences that best represent the key information in a coherent way. In contrast to the previous methods that focus only on salience, the proposed method addresses both coherence and informativeness based on textual linkages. We conduct experiments on the DUC2004 summarization task data set. A performance comparison reveals that the summaries generated by the proposed system achieve comparable results in terms of the ROUGE metric, and show improvements in readability by human evaluation.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1507082134,"Goal":"Partnership for the Goals","Task":["Exploring Text Links","Coherent Multi - Document Summarization Summarization","extraction of key information","summarization"],"Method":["graph - based method"]},{"ID":"piad-morffis-etal-2020-knowledge","title":"Knowledge Discovery in {COVID}-19 Research Literature","abstract":"This paper presents the preliminary results of an ongoing project that analyzes the growing body of scientific research published around the COVID-19 pandemic. In this research, a general-purpose semantic model is used to double annotate a batch of {\\$}500{\\$} sentences that were manually selected by the researchers from the CORD-19 corpus. Afterwards, a baseline text-mining pipeline is designed and evaluated via a large batch of {\\$}100,959{\\$} sentences. We present a qualitative analysis of the most interesting facts automatically extracted and highlight possible future lines of development. The preliminary results show that general-purpose semantic models are a useful tool for discovering fine-grained knowledge in large corpora of scientific documents.","year":2020,"title_abstract":"Knowledge Discovery in {COVID}-19 Research Literature This paper presents the preliminary results of an ongoing project that analyzes the growing body of scientific research published around the COVID-19 pandemic. In this research, a general-purpose semantic model is used to double annotate a batch of {\\$}500{\\$} sentences that were manually selected by the researchers from the CORD-19 corpus. Afterwards, a baseline text-mining pipeline is designed and evaluated via a large batch of {\\$}100,959{\\$} sentences. We present a qualitative analysis of the most interesting facts automatically extracted and highlight possible future lines of development. The preliminary results show that general-purpose semantic models are a useful tool for discovering fine-grained knowledge in large corpora of scientific documents.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1507049799,"Goal":"Climate Action","Task":["Knowledge Discovery","discovering fine - grained knowledge"],"Method":["general - purpose semantic model","text - mining pipeline","general - purpose semantic models"]},{"ID":"park-etal-2022-faviq","title":"{F}a{VIQ}: {FA}ct Verification from Information-seeking Questions","abstract":"Despite significant interest in developing general purpose fact checking models, it is challenging to construct a large-scale fact verification dataset with realistic real-world claims. Existing claims are either authored by crowdworkers, thereby introducing subtle biases thatare difficult to control for, or manually verified by professional fact checkers, causing them to be expensive and limited in scale. In this paper, we construct a large-scale challenging fact verification dataset called FAVIQ, consisting of 188k claims derived from an existing corpus of ambiguous information-seeking questions. The ambiguities in the questions enable automatically constructing true and false claims that reflect user confusions (e.g., the year of the movie being filmed vs. being released). Claims in FAVIQ are verified to be natural, contain little lexical bias, and require a complete understanding of the evidence for verification. Our experiments show that the state-of-the-art models are far from solving our new task. Moreover, training on our data helps in professional fact-checking, outperforming models trained on the widely used dataset FEVER or in-domain data by up to 17{\\%} absolute. Altogether, our data will serve as a challenging benchmark for natural language understanding and support future progress in professional fact checking.","year":2022,"title_abstract":"{F}a{VIQ}: {FA}ct Verification from Information-seeking Questions Despite significant interest in developing general purpose fact checking models, it is challenging to construct a large-scale fact verification dataset with realistic real-world claims. Existing claims are either authored by crowdworkers, thereby introducing subtle biases thatare difficult to control for, or manually verified by professional fact checkers, causing them to be expensive and limited in scale. In this paper, we construct a large-scale challenging fact verification dataset called FAVIQ, consisting of 188k claims derived from an existing corpus of ambiguous information-seeking questions. The ambiguities in the questions enable automatically constructing true and false claims that reflect user confusions (e.g., the year of the movie being filmed vs. being released). Claims in FAVIQ are verified to be natural, contain little lexical bias, and require a complete understanding of the evidence for verification. Our experiments show that the state-of-the-art models are far from solving our new task. Moreover, training on our data helps in professional fact-checking, outperforming models trained on the widely used dataset FEVER or in-domain data by up to 17{\\%} absolute. Altogether, our data will serve as a challenging benchmark for natural language understanding and support future progress in professional fact checking.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1507009119,"Goal":"Climate Action","Task":["Verification","Information - seeking Questions","fact verification dataset","verification","verification","professional fact - checking","natural language understanding","fact checking"],"Method":["fact checking models","fact checkers"]},{"ID":"cavar-etal-2016-global","title":"Global Open Resources and Information for Language and Linguistic Analysis ({GORILLA})","abstract":"The infrastructure Global Open Resources and Information for Language and Linguistic Analysis (GORILLA) was created as a resource that provides a bridge between disciplines such as documentary, theoretical, and corpus linguistics, speech and language technologies, and digital language archiving services. GORILLA is designed as an interface between digital language archive services and language data producers. It addresses various problems of common digital language archive infrastructures. At the same time it serves the speech and language technology communities by providing a platform to create and share speech and language data from low-resourced and endangered languages. It hosts an initial collection of language models for speech and natural language processing (NLP), and technologies or software tools for corpus creation and annotation. GORILLA is designed to address the Transcription Bottleneck in language documentation, and, at the same time to provide solutions to the general Language Resource Bottleneck in speech and language technologies. It does so by facilitating the cooperation between documentary and theoretical linguistics, and speech and language technologies research and development, in particular for low-resourced and endangered languages.","year":2016,"title_abstract":"Global Open Resources and Information for Language and Linguistic Analysis ({GORILLA}) The infrastructure Global Open Resources and Information for Language and Linguistic Analysis (GORILLA) was created as a resource that provides a bridge between disciplines such as documentary, theoretical, and corpus linguistics, speech and language technologies, and digital language archiving services. GORILLA is designed as an interface between digital language archive services and language data producers. It addresses various problems of common digital language archive infrastructures. At the same time it serves the speech and language technology communities by providing a platform to create and share speech and language data from low-resourced and endangered languages. It hosts an initial collection of language models for speech and natural language processing (NLP), and technologies or software tools for corpus creation and annotation. GORILLA is designed to address the Transcription Bottleneck in language documentation, and, at the same time to provide solutions to the general Language Resource Bottleneck in speech and language technologies. It does so by facilitating the cooperation between documentary and theoretical linguistics, and speech and language technologies research and development, in particular for low-resourced and endangered languages.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1506880969,"Goal":"Life on Land","Task":["Language and Linguistic Analysis","Language and Linguistic Analysis","documentary , theoretical , and corpus linguistics","speech and language technologies","digital language archiving services","language data producers","digital language archive infrastructures","speech and language technology communities","speech","natural language processing","corpus creation","annotation","Transcription Bottleneck","language documentation","Language Resource Bottleneck","speech and language technologies","documentary and theoretical linguistics","speech and language technologies","development","low - resourced and endangered languages"],"Method":["GORILLA","digital language archive services","language models","software tools","GORILLA"]},{"ID":"subramanian-etal-2019-deep","title":"Deep Ordinal Regression for Pledge Specificity Prediction","abstract":"Many pledges are made in the course of an election campaign, forming important corpora for political analysis of campaign strategy and governmental accountability. At present, there are no publicly available annotated datasets of pledges, and most political analyses rely on manual annotations. In this paper we collate a novel dataset of manifestos from eleven Australian federal election cycles, with over 12,000 sentences annotated with specificity (e.g., rhetorical vs detailed pledge) on a fine-grained scale. We propose deep ordinal regression approaches for specificity prediction, under both supervised and semi-supervised settings, and provide empirical results demonstrating the effectiveness of the proposed techniques over several baseline approaches. We analyze the utility of pledge specificity modeling across a spectrum of policy issues in performing ideology prediction, and further provide qualitative analysis in terms of capturing party-specific issue salience across election cycles.","year":2019,"title_abstract":"Deep Ordinal Regression for Pledge Specificity Prediction Many pledges are made in the course of an election campaign, forming important corpora for political analysis of campaign strategy and governmental accountability. At present, there are no publicly available annotated datasets of pledges, and most political analyses rely on manual annotations. In this paper we collate a novel dataset of manifestos from eleven Australian federal election cycles, with over 12,000 sentences annotated with specificity (e.g., rhetorical vs detailed pledge) on a fine-grained scale. We propose deep ordinal regression approaches for specificity prediction, under both supervised and semi-supervised settings, and provide empirical results demonstrating the effectiveness of the proposed techniques over several baseline approaches. We analyze the utility of pledge specificity modeling across a spectrum of policy issues in performing ideology prediction, and further provide qualitative analysis in terms of capturing party-specific issue salience across election cycles.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.1506678164,"Goal":"Peace, Justice and Strong Institutions","Task":["Pledge Specificity Prediction","political analysis of campaign strategy","governmental accountability","political analyses","specificity prediction","supervised and semi - supervised settings","ideology prediction","party - specific issue salience"],"Method":["Deep Ordinal Regression","deep ordinal regression approaches","pledge specificity modeling"]},{"ID":"wang-etal-2018-ontology","title":"Ontology alignment in the biomedical domain using entity definitions and context","abstract":"Ontology alignment is the task of identifying semantically equivalent entities from two given ontologies. Different ontologies have different representations of the same entity, resulting in a need to de-duplicate entities when merging ontologies. We propose a method for enriching entities in an ontology with external definition and context information, and use this additional information for ontology alignment. We develop a neural architecture capable of encoding the additional information when available, and show that the addition of external data results in an F1-score of 0.69 on the Ontology Alignment Evaluation Initiative (OAEI) largebio SNOMED-NCI subtask, comparable with the entity-level matchers in a SOTA system.","year":2018,"title_abstract":"Ontology alignment in the biomedical domain using entity definitions and context Ontology alignment is the task of identifying semantically equivalent entities from two given ontologies. Different ontologies have different representations of the same entity, resulting in a need to de-duplicate entities when merging ontologies. We propose a method for enriching entities in an ontology with external definition and context information, and use this additional information for ontology alignment. We develop a neural architecture capable of encoding the additional information when available, and show that the addition of external data results in an F1-score of 0.69 on the Ontology Alignment Evaluation Initiative (OAEI) largebio SNOMED-NCI subtask, comparable with the entity-level matchers in a SOTA system.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1506648362,"Goal":"Gender Equality","Task":["Ontology alignment","biomedical domain","Ontology alignment","identifying semantically equivalent entities","ontology alignment","Ontology Alignment Evaluation Initiative","largebio SNOMED - NCI subtask"],"Method":["neural architecture","entity - level matchers","SOTA system"]},{"ID":"tian-etal-2021-identifying","title":"Identifying Distributional Perspectives from Colingual Groups","abstract":"Discrepancies exist among different cultures or languages. A lack of mutual understanding among different colingual groups about the perspectives on specific values or events may lead to uninformed decisions or biased opinions. Thus, automatically understanding the group perspectives can provide essential back-ground for many natural language processing tasks. In this paper, we study colingual groups and use language corpora as a proxy to identify their distributional perspectives. We present a novel computational approach to learn shared understandings, and benchmark our method by building culturally-aware models for the English, Chinese, and Japanese languages. Ona held out set of diverse topics, including marriage, corruption, democracy, etc., our model achieves high correlation with human judgements regarding intra-group values and inter-group differences","year":2021,"title_abstract":"Identifying Distributional Perspectives from Colingual Groups Discrepancies exist among different cultures or languages. A lack of mutual understanding among different colingual groups about the perspectives on specific values or events may lead to uninformed decisions or biased opinions. Thus, automatically understanding the group perspectives can provide essential back-ground for many natural language processing tasks. In this paper, we study colingual groups and use language corpora as a proxy to identify their distributional perspectives. We present a novel computational approach to learn shared understandings, and benchmark our method by building culturally-aware models for the English, Chinese, and Japanese languages. Ona held out set of diverse topics, including marriage, corruption, democracy, etc., our model achieves high correlation with human judgements regarding intra-group values and inter-group differences","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1506586671,"Goal":"Reduced Inequalities","Task":["Identifying Distributional Perspectives","natural language processing tasks","shared understandings"],"Method":["computational approach","culturally - aware models"]},{"ID":"yavuz-etal-2017-recovering","title":"Recovering Question Answering Errors via Query Revision","abstract":"The existing factoid QA systems often lack a post-inspection component that can help models recover from their own mistakes. In this work, we propose to crosscheck the corresponding KB relations behind the predicted answers and identify potential inconsistencies. Instead of developing a new model that accepts evidences collected from these relations, we choose to plug them back to the original questions directly and check if the revised question makes sense or not. A bidirectional LSTM is applied to encode revised questions. We develop a scoring mechanism over the revised question encodings to refine the predictions of a base QA system. This approach can improve the F1 score of STAGG (Yih et al., 2015), one of the leading QA systems, from 52.5{\\%} to 53.9{\\%} on WEBQUESTIONS data.","year":2017,"title_abstract":"Recovering Question Answering Errors via Query Revision The existing factoid QA systems often lack a post-inspection component that can help models recover from their own mistakes. In this work, we propose to crosscheck the corresponding KB relations behind the predicted answers and identify potential inconsistencies. Instead of developing a new model that accepts evidences collected from these relations, we choose to plug them back to the original questions directly and check if the revised question makes sense or not. A bidirectional LSTM is applied to encode revised questions. We develop a scoring mechanism over the revised question encodings to refine the predictions of a base QA system. This approach can improve the F1 score of STAGG (Yih et al., 2015), one of the leading QA systems, from 52.5{\\%} to 53.9{\\%} on WEBQUESTIONS data.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1506273597,"Goal":"Quality Education","Task":["Recovering Question Answering Errors","Query Revision","QA","QA"],"Method":["factoid QA systems","post - inspection component","bidirectional LSTM","scoring mechanism","revised question encodings"]},{"ID":"benites-etal-2020-zhaw","title":"{ZHAW}-{I}n{IT} - Social Media Geolocation at {V}ar{D}ial 2020","abstract":"We describe our approaches for the Social Media Geolocation (SMG) task at the VarDial Evaluation Campaign 2020. The goal was to predict geographical location (latitudes and longitudes) given an input text. There were three subtasks corresponding to German-speaking Switzerland (CH), Germany and Austria (DE-AT), and Croatia, Bosnia and Herzegovina, Montenegro and Serbia (BCMS). We submitted solutions to all subtasks but focused our development efforts on the CH subtask, where we achieved third place out of 16 submissions with a median distance of 15.93 km and had the best result of 14 unconstrained systems. In the DE-AT subtask, we ranked sixth out of ten submissions (fourth of 8 unconstrained systems) and for BCMS we achieved fourth place out of 13 submissions (second of 11 unconstrained systems).","year":2020,"title_abstract":"{ZHAW}-{I}n{IT} - Social Media Geolocation at {V}ar{D}ial 2020 We describe our approaches for the Social Media Geolocation (SMG) task at the VarDial Evaluation Campaign 2020. The goal was to predict geographical location (latitudes and longitudes) given an input text. There were three subtasks corresponding to German-speaking Switzerland (CH), Germany and Austria (DE-AT), and Croatia, Bosnia and Herzegovina, Montenegro and Serbia (BCMS). We submitted solutions to all subtasks but focused our development efforts on the CH subtask, where we achieved third place out of 16 submissions with a median distance of 15.93 km and had the best result of 14 unconstrained systems. In the DE-AT subtask, we ranked sixth out of ten submissions (fourth of 8 unconstrained systems) and for BCMS we achieved fourth place out of 13 submissions (second of 11 unconstrained systems).","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1506081969,"Goal":"Sustainable Cities and Communities","Task":["Social Media Geolocation","VarDial Evaluation Campaign","DE - AT subtask"],"Method":["unconstrained systems"]},{"ID":"sarthou-etal-2019-semantic","title":"Semantic Spatial Representation: a unique representation of an environment based on an ontology for robotic applications","abstract":"It is important, for human-robot interaction, to endow the robot with the knowledge necessary to understand human needs and to be able to respond to them. We present a formalized and unified representation for indoor environments using an ontology devised for a route description task in which a robot must provide explanations to a person. We show that this representation can be used to choose a route to explain to a human as well as to verbalize it using a route perspective. Based on ontology, this representation has a strong possibility of evolution to adapt to many other applications. With it, we get the semantics of the environment elements while keeping a description of the known connectivity of the environment. This representation and the illustration algorithms, to find and verbalize a route, have been tested in two environments of different scales.","year":2019,"title_abstract":"Semantic Spatial Representation: a unique representation of an environment based on an ontology for robotic applications It is important, for human-robot interaction, to endow the robot with the knowledge necessary to understand human needs and to be able to respond to them. We present a formalized and unified representation for indoor environments using an ontology devised for a route description task in which a robot must provide explanations to a person. We show that this representation can be used to choose a route to explain to a human as well as to verbalize it using a route perspective. Based on ontology, this representation has a strong possibility of evolution to adapt to many other applications. With it, we get the semantics of the environment elements while keeping a description of the known connectivity of the environment. This representation and the illustration algorithms, to find and verbalize a route, have been tested in two environments of different scales.","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1505640447,"Goal":"Life on Land","Task":["robotic applications","human - robot interaction","indoor environments","route description task"],"Method":["Semantic Spatial Representation","unique representation of an environment","unified representation","illustration algorithms"]},{"ID":"waszczuk-etal-2020-contemplata","title":"Contemplata, a Free Platform for Constituency Treebank Annotation","abstract":"This paper describes Contemplata, an annotation platform that offers a generic solution for treebank building as well as treebank enrichment with relations between syntactic nodes. Contemplata is dedicated to the annotation of constituency trees. The framework includes support for syntactic parsers, which provide automatic annotations to be manually revised. The balanced strategy of annotation between automatic parsing and manual revision allows to reduce the annotator workload, which favours data reliability. The paper presents the software architecture of Contemplata, describes its practical use and eventually gives two examples of annotation projects that were conducted on the platform.","year":2020,"title_abstract":"Contemplata, a Free Platform for Constituency Treebank Annotation This paper describes Contemplata, an annotation platform that offers a generic solution for treebank building as well as treebank enrichment with relations between syntactic nodes. Contemplata is dedicated to the annotation of constituency trees. The framework includes support for syntactic parsers, which provide automatic annotations to be manually revised. The balanced strategy of annotation between automatic parsing and manual revision allows to reduce the annotator workload, which favours data reliability. The paper presents the software architecture of Contemplata, describes its practical use and eventually gives two examples of annotation projects that were conducted on the platform.","social_need":"Sustainable Cities and Communities Make cities and human settlements inclusive, safe, resilient and sustainable","cosine_similarity":0.1505452842,"Goal":"Sustainable Cities and Communities","Task":["Constituency Treebank Annotation","treebank building","treebank enrichment","annotation of constituency trees","annotation","automatic parsing","manual revision","annotation projects"],"Method":["Contemplata","Free Platform","Contemplata","annotation platform","Contemplata","syntactic parsers","balanced strategy","software architecture","Contemplata"]},{"ID":"graca-etal-2018-rwth","title":"The {RWTH} {A}achen {U}niversity {E}nglish-{G}erman and {G}erman-{E}nglish Unsupervised Neural Machine Translation Systems for {WMT} 2018","abstract":"This paper describes the unsupervised neural machine translation (NMT) systems of the RWTH Aachen University developed for the English \u2194 German news translation task of the \\textit{EMNLP 2018 Third Conference on Machine Translation} (WMT 2018). Our work is based on iterative back-translation using a shared encoder-decoder NMT model. We extensively compare different vocabulary types, word embedding initialization schemes and optimization methods for our model. We also investigate gating and weight normalization for the word embedding layer.","year":2018,"title_abstract":"The {RWTH} {A}achen {U}niversity {E}nglish-{G}erman and {G}erman-{E}nglish Unsupervised Neural Machine Translation Systems for {WMT} 2018 This paper describes the unsupervised neural machine translation (NMT) systems of the RWTH Aachen University developed for the English \u2194 German news translation task of the \\textit{EMNLP 2018 Third Conference on Machine Translation} (WMT 2018). Our work is based on iterative back-translation using a shared encoder-decoder NMT model. We extensively compare different vocabulary types, word embedding initialization schemes and optimization methods for our model. We also investigate gating and weight normalization for the word embedding layer.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1505292803,"Goal":"Gender Equality","Task":["Unsupervised Neural Machine Translation Systems","unsupervised neural machine translation","English \u2194 German news translation task","Machine Translation}"],"Method":["RWTH Aachen University","iterative back - translation","encoder - decoder NMT model","word embedding initialization schemes","optimization methods","gating","weight normalization","word embedding layer"]},{"ID":"webber-2020-bridging","title":"Bridging Question Answering and Discourse The case of Multi-Sentence Questions","abstract":"In human question-answering (QA), questions are often expressed in the form of multiple sentences. One can see this in both spoken QA interactions, when one person asks a question of another, and written QA, such as are found on-line in FAQs and in what are called {''}Community Question-Answering Forums{''}. Computer-based QA has taken the challenge of these {''}multi-sentence questions{''} to be that of breaking them into an appropriately ordered sequence of separate questions, with both the previous questions and their answers serving as context for the next question. This can be seen, for example, in two recent workshops at AAAI called {''}Reasoning for Complex QA{''} [https:\/\/rcqa-ws.github.io\/program\/]. We claim that, while appropriate for some types of {''}multi-sentence questions{''} (MSQs), it is not appropriate for all, because they are essentially different types of discourse. To support this claim, we need to provide evidence that: {\\mbox{$\\bullet$}} different types of MSQs are answered differently in written or spoken QA between people; {\\mbox{$\\bullet$}} people can (and do) distinguish these different types of MSQs; {\\mbox{$\\bullet$}} systems can be made to both distinguish different types of MSQs and provide appropriate answers.","year":2020,"title_abstract":"Bridging Question Answering and Discourse The case of Multi-Sentence Questions In human question-answering (QA), questions are often expressed in the form of multiple sentences. One can see this in both spoken QA interactions, when one person asks a question of another, and written QA, such as are found on-line in FAQs and in what are called {''}Community Question-Answering Forums{''}. Computer-based QA has taken the challenge of these {''}multi-sentence questions{''} to be that of breaking them into an appropriately ordered sequence of separate questions, with both the previous questions and their answers serving as context for the next question. This can be seen, for example, in two recent workshops at AAAI called {''}Reasoning for Complex QA{''} [https:\/\/rcqa-ws.github.io\/program\/]. We claim that, while appropriate for some types of {''}multi-sentence questions{''} (MSQs), it is not appropriate for all, because they are essentially different types of discourse. To support this claim, we need to provide evidence that: {\\mbox{$\\bullet$}} different types of MSQs are answered differently in written or spoken QA between people; {\\mbox{$\\bullet$}} people can (and do) distinguish these different types of MSQs; {\\mbox{$\\bullet$}} systems can be made to both distinguish different types of MSQs and provide appropriate answers.","social_need":"Good Health and Well-Being Ensure healthy lives and promote well-being for all at all ages","cosine_similarity":0.1505275965,"Goal":"Good Health and Well-Being","Task":["Bridging Question Answering","Discourse","Multi - Sentence Questions","human question - answering","spoken QA interactions","written QA","Question - Answering Forums{''}","Computer - based QA","Complex QA{''}","{''}multi - sentence questions{''}","written","spoken QA"],"Method":["MSQs","MSQs;","MSQs"]},{"ID":"armbrust-etal-2020-computational","title":"A Computational Analysis of Financial and Environmental Narratives within Financial Reports and its Value for Investors","abstract":"Public companies are obliged to include financial and non-financial information within their cor- porate filings under Regulation S-K, in the United States (SEC, 2010). However, the requirements still allow for manager{'}s discretion. This raises the question to which extent the information is actually included and if this information is at all relevant for investors. We answer this question by training and evaluating an end-to-end deep learning approach (based on BERT and GloVe embeddings) to predict the financial and environmental performance of the company from the {``}Management{'}s Discussion and Analysis of Financial Conditions and Results of Operations{''} (MD{\\&}A) section of 10-K (yearly) and 10-Q (quarterly) filings. We further analyse the mediating effect of the environmental performance on the relationship between the company{'}s disclosures and financial performance. Hereby, we address the results of previous studies regarding environ- mental performance. We find that the textual information contained within the MD{\\&}A section does not allow for conclusions about the future (corporate) financial performance. However, there is evidence that the environmental performance can be extracted by natural language processing methods.","year":2020,"title_abstract":"A Computational Analysis of Financial and Environmental Narratives within Financial Reports and its Value for Investors Public companies are obliged to include financial and non-financial information within their cor- porate filings under Regulation S-K, in the United States (SEC, 2010). However, the requirements still allow for manager{'}s discretion. This raises the question to which extent the information is actually included and if this information is at all relevant for investors. We answer this question by training and evaluating an end-to-end deep learning approach (based on BERT and GloVe embeddings) to predict the financial and environmental performance of the company from the {``}Management{'}s Discussion and Analysis of Financial Conditions and Results of Operations{''} (MD{\\&}A) section of 10-K (yearly) and 10-Q (quarterly) filings. We further analyse the mediating effect of the environmental performance on the relationship between the company{'}s disclosures and financial performance. Hereby, we address the results of previous studies regarding environ- mental performance. We find that the textual information contained within the MD{\\&}A section does not allow for conclusions about the future (corporate) financial performance. However, there is evidence that the environmental performance can be extracted by natural language processing methods.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1505241394,"Goal":"Climate Action","Task":["Computational Analysis of Financial and Environmental Narratives","financial and environmental performance"],"Method":["end - to - end deep learning approach","BERT and GloVe embeddings)","natural language processing methods"]},{"ID":"doxolodeo-mahendra-2020-ui","title":"{UI} at {S}em{E}val-2020 Task 4: Commonsense Validation and Explanation by Exploiting Contradiction","abstract":"This paper describes our submissions into the ComVe challenge, the SemEval 2020 Task 4. This evaluation task consists of three sub-tasks that test commonsense comprehension by identifying sentences that do not make sense and explain why they do not. In subtask A, we use Roberta to find which sentence does not make sense. In subtask B, besides using BERT, we also experiment with replacing the dataset with MNLI when selecting the best explanation from the provided options why the given sentence does not make sense. In subtask C, we utilize the MNLI model from subtask B to evaluate the explanation generated by Roberta and GPT-2 by exploiting the contradiction of the sentence and their explanation. Our system submission records a performance of 88.2{\\%}, 80.5{\\%}, and BLEU 5.5 for those three subtasks, respectively.","year":2020,"title_abstract":"{UI} at {S}em{E}val-2020 Task 4: Commonsense Validation and Explanation by Exploiting Contradiction This paper describes our submissions into the ComVe challenge, the SemEval 2020 Task 4. This evaluation task consists of three sub-tasks that test commonsense comprehension by identifying sentences that do not make sense and explain why they do not. In subtask A, we use Roberta to find which sentence does not make sense. In subtask B, besides using BERT, we also experiment with replacing the dataset with MNLI when selecting the best explanation from the provided options why the given sentence does not make sense. In subtask C, we utilize the MNLI model from subtask B to evaluate the explanation generated by Roberta and GPT-2 by exploiting the contradiction of the sentence and their explanation. Our system submission records a performance of 88.2{\\%}, 80.5{\\%}, and BLEU 5.5 for those three subtasks, respectively.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1504921764,"Goal":"Climate Action","Task":["Commonsense Validation","Explanation","ComVe challenge","commonsense comprehension","subtask A","explanation"],"Method":["Roberta","BERT","MNLI model","Roberta","GPT - 2"]},{"ID":"manerba-tonelli-2021-fine","title":"Fine-Grained Fairness Analysis of Abusive Language Detection Systems with {C}heck{L}ist","abstract":"Current abusive language detection systems have demonstrated unintended bias towards sensitive features such as nationality or gender. This is a crucial issue, which may harm minorities and underrepresented groups if such systems were integrated in real-world applications. In this paper, we create ad hoc tests through the CheckList tool (Ribeiro et al., 2020) to detect biases within abusive language classifiers for English. We compare the behaviour of two BERT-based models, one trained on a generic hate speech dataset and the other on a dataset for misogyny detection. Our evaluation shows that, although BERT-based classifiers achieve high accuracy levels on a variety of natural language processing tasks, they perform very poorly as regards fairness and bias, in particular on samples involving implicit stereotypes, expressions of hate towards minorities and protected attributes such as race or sexual orientation. We release both the notebooks implemented to extend the Fairness tests and the synthetic datasets usable to evaluate systems bias independently of CheckList.","year":2021,"title_abstract":"Fine-Grained Fairness Analysis of Abusive Language Detection Systems with {C}heck{L}ist Current abusive language detection systems have demonstrated unintended bias towards sensitive features such as nationality or gender. This is a crucial issue, which may harm minorities and underrepresented groups if such systems were integrated in real-world applications. In this paper, we create ad hoc tests through the CheckList tool (Ribeiro et al., 2020) to detect biases within abusive language classifiers for English. We compare the behaviour of two BERT-based models, one trained on a generic hate speech dataset and the other on a dataset for misogyny detection. Our evaluation shows that, although BERT-based classifiers achieve high accuracy levels on a variety of natural language processing tasks, they perform very poorly as regards fairness and bias, in particular on samples involving implicit stereotypes, expressions of hate towards minorities and protected attributes such as race or sexual orientation. We release both the notebooks implemented to extend the Fairness tests and the synthetic datasets usable to evaluate systems bias independently of CheckList.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1504784673,"Goal":"Gender Equality","Task":["Fine - Grained Fairness Analysis","abusive language detection","real - world applications","misogyny detection","natural language processing tasks","bias","Fairness tests"],"Method":["Abusive Language Detection Systems","{C}heck{L}ist","CheckList tool","abusive language classifiers","BERT - based models","BERT - based classifiers"]},{"ID":"stengel-eskin-etal-2019-discriminative","title":"A Discriminative Neural Model for Cross-Lingual Word Alignment","abstract":"We introduce a novel discriminative word alignment model, which we integrate into a Transformer-based machine translation model. In experiments based on a small number of labeled examples (\u223c1.7K{--}5K sentences) we evaluate its performance intrinsically on both English-Chinese and English-Arabic alignment, where we achieve major improvements over unsupervised baselines (11{--}27 F1). We evaluate the model extrinsically on data projection for Chinese NER, showing that our alignments lead to higher performance when used to project NER tags from English to Chinese. Finally, we perform an ablation analysis and an annotation experiment that jointly support the utility and feasibility of future manual alignment elicitation.","year":2019,"title_abstract":"A Discriminative Neural Model for Cross-Lingual Word Alignment We introduce a novel discriminative word alignment model, which we integrate into a Transformer-based machine translation model. In experiments based on a small number of labeled examples (\u223c1.7K{--}5K sentences) we evaluate its performance intrinsically on both English-Chinese and English-Arabic alignment, where we achieve major improvements over unsupervised baselines (11{--}27 F1). We evaluate the model extrinsically on data projection for Chinese NER, showing that our alignments lead to higher performance when used to project NER tags from English to Chinese. Finally, we perform an ablation analysis and an annotation experiment that jointly support the utility and feasibility of future manual alignment elicitation.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1504480094,"Goal":"Gender Equality","Task":["Cross - Lingual Word Alignment","English - Arabic alignment","data projection","Chinese NER","NER","ablation analysis","annotation","manual alignment elicitation"],"Method":["Discriminative Neural Model","discriminative word alignment model","Transformer - based machine translation model","unsupervised baselines"]},{"ID":"titeux-etal-2020-seshat","title":"{S}eshat: a Tool for Managing and Verifying Annotation Campaigns of Audio Data","abstract":"We introduce Seshat, a new, simple and open-source software to efficiently manage annotations of speech corpora. The Seshat software allows users to easily customise and manage annotations of large audio corpora while ensuring compliance with the formatting and naming conventions of the annotated output files. In addition, it includes procedures for checking the content of annotations following specific rules that can be implemented in personalised parsers. Finally, we propose a double-annotation mode, for which Seshat computes automatically an associated inter-annotator agreement with the gamma measure taking into account the categorisation and segmentation discrepancies.","year":2020,"title_abstract":"{S}eshat: a Tool for Managing and Verifying Annotation Campaigns of Audio Data We introduce Seshat, a new, simple and open-source software to efficiently manage annotations of speech corpora. The Seshat software allows users to easily customise and manage annotations of large audio corpora while ensuring compliance with the formatting and naming conventions of the annotated output files. In addition, it includes procedures for checking the content of annotations following specific rules that can be implemented in personalised parsers. Finally, we propose a double-annotation mode, for which Seshat computes automatically an associated inter-annotator agreement with the gamma measure taking into account the categorisation and segmentation discrepancies.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1504080296,"Goal":"Partnership for the Goals","Task":["Verifying Annotation Campaigns of Audio Data","segmentation"],"Method":["Seshat","Seshat software","personalised parsers","double - annotation mode","Seshat"]},{"ID":"kondo-etal-2021-machine","title":"Machine Translation with Pre-specified Target-side Words Using a Semi-autoregressive Model","abstract":"We introduce our TMU Japanese-to-English system, which employs a semi-autoregressive model, to tackle the WAT 2021 restricted translation task. In this task, we translate an input sentence with the constraint that some words, called restricted target vocabularies (RTVs), must be contained in the output sentence. To satisfy this constraint, we use a semi-autoregressive model, namely, RecoverSAT, due to its ability (known as {``}forced translation{''}) to insert specified words into the output sentence. When using {``}forced translation,{''} the order of inserting RTVs is a critical problem. In this work, we aligned the source sentence and the corresponding RTVs using GIZA++. In our system, we obtain word alignment between a source sentence and the corresponding RTVs and then sort the RTVs in the order of their corresponding words or phrases in the source sentence. Using the model with sorted order RTVs, we succeeded in inserting all the RTVs into output sentences in more than 96{\\%} of the test sentences. Moreover, we confirmed that sorting RTVs improved the BLEU score compared with random order RTVs.","year":2021,"title_abstract":"Machine Translation with Pre-specified Target-side Words Using a Semi-autoregressive Model We introduce our TMU Japanese-to-English system, which employs a semi-autoregressive model, to tackle the WAT 2021 restricted translation task. In this task, we translate an input sentence with the constraint that some words, called restricted target vocabularies (RTVs), must be contained in the output sentence. To satisfy this constraint, we use a semi-autoregressive model, namely, RecoverSAT, due to its ability (known as {``}forced translation{''}) to insert specified words into the output sentence. When using {``}forced translation,{''} the order of inserting RTVs is a critical problem. In this work, we aligned the source sentence and the corresponding RTVs using GIZA++. In our system, we obtain word alignment between a source sentence and the corresponding RTVs and then sort the RTVs in the order of their corresponding words or phrases in the source sentence. Using the model with sorted order RTVs, we succeeded in inserting all the RTVs into output sentences in more than 96{\\%} of the test sentences. Moreover, we confirmed that sorting RTVs improved the BLEU score compared with random order RTVs.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1503678709,"Goal":"Gender Equality","Task":["Machine Translation","WAT 2021 restricted translation task","translation{''})","translation","word alignment"],"Method":["Semi - autoregressive Model","TMU Japanese - to - English system","semi - autoregressive model","semi - autoregressive model","RecoverSAT","RTVs","GIZA++","sorted order RTVs","RTVs","random order RTVs"]},{"ID":"abzianidze-bos-2019-thirty","title":"Thirty Musts for Meaning Banking","abstract":"Meaning banking{---}creating a semantically annotated corpus for the purpose of semantic parsing or generation{---}is a challenging task. It is quite simple to come up with a complex meaning representation, but it is hard to design a simple meaning representation that captures many nuances of meaning. This paper lists some lessons learned in nearly ten years of meaning annotation during the development of the Groningen Meaning Bank (Bos et al., 2017) and the Parallel Meaning Bank (Abzianidze et al., 2017). The paper{'}s format is rather unconventional: there is no explicit related work, no methodology section, no results, and no discussion (and the current snippet is not an abstract but actually an introductory preface). Instead, its structure is inspired by work of Traum (2000) and Bender (2013). The list starts with a brief overview of the existing meaning banks (Section 1) and the rest of the items are roughly divided into three groups: corpus collection (Section 2 and 3, annotation methods (Section 4{--}11), and design of meaning representations (Section 12{--}30). We hope this overview will give inspiration and guidance in creating improved meaning banks in the future","year":2019,"title_abstract":"Thirty Musts for Meaning Banking Meaning banking{---}creating a semantically annotated corpus for the purpose of semantic parsing or generation{---}is a challenging task. It is quite simple to come up with a complex meaning representation, but it is hard to design a simple meaning representation that captures many nuances of meaning. This paper lists some lessons learned in nearly ten years of meaning annotation during the development of the Groningen Meaning Bank (Bos et al., 2017) and the Parallel Meaning Bank (Abzianidze et al., 2017). The paper{'}s format is rather unconventional: there is no explicit related work, no methodology section, no results, and no discussion (and the current snippet is not an abstract but actually an introductory preface). Instead, its structure is inspired by work of Traum (2000) and Bender (2013). The list starts with a brief overview of the existing meaning banks (Section 1) and the rest of the items are roughly divided into three groups: corpus collection (Section 2 and 3, annotation methods (Section 4{--}11), and design of meaning representations (Section 12{--}30). We hope this overview will give inspiration and guidance in creating improved meaning banks in the future","social_need":"Life on Land Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss","cosine_similarity":0.1503640115,"Goal":"Life on Land","Task":["Meaning Banking","Meaning banking{","semantic parsing","generation{","meaning annotation","meaning banks"],"Method":["meaning representation","meaning representation","annotation methods","meaning representations"]},{"ID":"castro-ferreira-etal-2018-surface","title":"Surface Realization Shared Task 2018 ({SR}18): The {T}ilburg {U}niversity Approach","abstract":"This study describes the approach developed by the Tilburg University team to the shallow task of the Multilingual Surface Realization Shared Task 2018 (SR18). Based on (Castro Ferreira et al., 2017), the approach works by first preprocessing an input dependency tree into an ordered linearized string, which is then realized using a statistical machine translation model. Our approach shows promising results, with BLEU scores above 50 for 5 different languages (English, French, Italian, Portuguese and Spanish) and above 35 for the Dutch language.","year":2018,"title_abstract":"Surface Realization Shared Task 2018 ({SR}18): The {T}ilburg {U}niversity Approach This study describes the approach developed by the Tilburg University team to the shallow task of the Multilingual Surface Realization Shared Task 2018 (SR18). Based on (Castro Ferreira et al., 2017), the approach works by first preprocessing an input dependency tree into an ordered linearized string, which is then realized using a statistical machine translation model. Our approach shows promising results, with BLEU scores above 50 for 5 different languages (English, French, Italian, Portuguese and Spanish) and above 35 for the Dutch language.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1503238827,"Goal":"Partnership for the Goals","Task":["Surface Realization","shallow task","Multilingual Surface Realization Shared Task"],"Method":["statistical machine translation model"]},{"ID":"sweeney-najafian-2019-transparent","title":"A Transparent Framework for Evaluating Unintended Demographic Bias in Word Embeddings","abstract":"Word embedding models have gained a lot of traction in the Natural Language Processing community, however, they suffer from unintended demographic biases. Most approaches to evaluate these biases rely on vector space based metrics like the Word Embedding Association Test (WEAT). While these approaches offer great geometric insights into unintended biases in the embedding vector space, they fail to offer an interpretable meaning for how the embeddings could cause discrimination in downstream NLP applications. In this work, we present a transparent framework and metric for evaluating discrimination across protected groups with respect to their word embedding bias. Our metric (Relative Negative Sentiment Bias, RNSB) measures fairness in word embeddings via the relative negative sentiment associated with demographic identity terms from various protected groups. We show that our framework and metric enable useful analysis into the bias in word embeddings.","year":2019,"title_abstract":"A Transparent Framework for Evaluating Unintended Demographic Bias in Word Embeddings Word embedding models have gained a lot of traction in the Natural Language Processing community, however, they suffer from unintended demographic biases. Most approaches to evaluate these biases rely on vector space based metrics like the Word Embedding Association Test (WEAT). While these approaches offer great geometric insights into unintended biases in the embedding vector space, they fail to offer an interpretable meaning for how the embeddings could cause discrimination in downstream NLP applications. In this work, we present a transparent framework and metric for evaluating discrimination across protected groups with respect to their word embedding bias. Our metric (Relative Negative Sentiment Bias, RNSB) measures fairness in word embeddings via the relative negative sentiment associated with demographic identity terms from various protected groups. We show that our framework and metric enable useful analysis into the bias in word embeddings.","social_need":"Reduced Inequalities Reduce inequality within and among countries","cosine_similarity":0.1502983868,"Goal":"Reduced Inequalities","Task":["Evaluating Unintended Demographic Bias","Word Embeddings","Natural Language Processing community","discrimination","downstream NLP applications","discrimination","word embeddings"],"Method":["Transparent Framework","Word embedding models","vector space based metrics","Word Embedding Association Test","transparent framework","RNSB)"]},{"ID":"villegas-etal-2010-case","title":"A Case Study on Interoperability for Language Resources and Applications","abstract":"This paper reports our experience when integrating differ resources and services into a grid environment. The use case we address implies the deployment of several NLP applications as web services. The ultimate objective of this task was to create a scenario where researchers have access to a variety of services they can operate. These services should be easy to invoke and able to interoperate between one another. We essentially describe the interoperability problems we faced, which involve metadata interoperability, data interoperability and service interoperability. We devote special attention to service interoperability and explore the possibility to define common interfaces and semantic description of services. While the web services paradigm suits the integration of different services very well, this requires mutual understanding and the accommodation to common interfaces that not only provide technical solution but also ease the user{\\^a}\u0080\u009fs work. Defining common interfaces benefits interoperability but requires the agreement about operations and the set of inputs\/outputs. Semantic annotation allows defining some sort of taxonomy that organizes and collects the set of admissible operations and types input\/output parameters.","year":2010,"title_abstract":"A Case Study on Interoperability for Language Resources and Applications This paper reports our experience when integrating differ resources and services into a grid environment. The use case we address implies the deployment of several NLP applications as web services. The ultimate objective of this task was to create a scenario where researchers have access to a variety of services they can operate. These services should be easy to invoke and able to interoperate between one another. We essentially describe the interoperability problems we faced, which involve metadata interoperability, data interoperability and service interoperability. We devote special attention to service interoperability and explore the possibility to define common interfaces and semantic description of services. While the web services paradigm suits the integration of different services very well, this requires mutual understanding and the accommodation to common interfaces that not only provide technical solution but also ease the user{\\^a}\u0080\u009fs work. Defining common interfaces benefits interoperability but requires the agreement about operations and the set of inputs\/outputs. Semantic annotation allows defining some sort of taxonomy that organizes and collects the set of admissible operations and types input\/output parameters.","social_need":"Industry, Innovation and Infrastrucure Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation","cosine_similarity":0.1502358913,"Goal":"Industry, Innovation and Infrastrucure","Task":["Interoperability","Language Resources","NLP applications","interoperability problems","data interoperability","service interoperability","service interoperability","semantic description of services","Semantic annotation"],"Method":["web services","web services paradigm"]},{"ID":"emelin-etal-2021-moral","title":"Moral Stories: Situated Reasoning about Norms, Intents, Actions, and their Consequences","abstract":"In social settings, much of human behavior is governed by unspoken rules of conduct rooted in societal norms. For artificial systems to be fully integrated into social environments, adherence to such norms is a central prerequisite. To investigate whether language generation models can serve as behavioral priors for systems deployed in social settings, we evaluate their ability to generate action descriptions that achieve predefined goals under normative constraints. Moreover, we examine if models can anticipate likely consequences of actions that either observe or violate known norms, or explain why certain actions are preferable by generating relevant norm hypotheses. For this purpose, we introduce Moral Stories, a crowd-sourced dataset of structured, branching narratives for the study of grounded, goal-oriented social reasoning. Finally, we propose decoding strategies that combine multiple expert models to significantly improve the quality of generated actions, consequences, and norms compared to strong baselines.","year":2021,"title_abstract":"Moral Stories: Situated Reasoning about Norms, Intents, Actions, and their Consequences In social settings, much of human behavior is governed by unspoken rules of conduct rooted in societal norms. For artificial systems to be fully integrated into social environments, adherence to such norms is a central prerequisite. To investigate whether language generation models can serve as behavioral priors for systems deployed in social settings, we evaluate their ability to generate action descriptions that achieve predefined goals under normative constraints. Moreover, we examine if models can anticipate likely consequences of actions that either observe or violate known norms, or explain why certain actions are preferable by generating relevant norm hypotheses. For this purpose, we introduce Moral Stories, a crowd-sourced dataset of structured, branching narratives for the study of grounded, goal-oriented social reasoning. Finally, we propose decoding strategies that combine multiple expert models to significantly improve the quality of generated actions, consequences, and norms compared to strong baselines.","social_need":"Peace, Justice and Strong Institutions Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels","cosine_similarity":0.1502247304,"Goal":"Peace, Justice and Strong Institutions","Task":["Situated Reasoning","artificial systems","social settings","grounded , goal - oriented social reasoning"],"Method":["language generation models","decoding strategies","expert models"]},{"ID":"ide-romary-2006-representing","title":"Representing Linguistic Corpora and Their Annotations","abstract":"A Linguistic Annotation Framework (LAF) is being developed within the International Standards Organization Technical Committee 37 Sub-committee on Language Resource Management (ISO TC37 SC4). LAF is intended to provide a standardized means to represent linguistic data and its annotations that is defined broadly enough to accommodate all types of linguistic annotations, and at the same time provide means to represent precise and potentially complex linguistic information. The general principles informing the design of LAF have been previously reported (Ide and Romary, 2003; Ide and Romary, 2004a). This paper describes some of the more technical aspects of the LAF design that have been addressed in the process of finalizing the specifications for the standard.","year":2006,"title_abstract":"Representing Linguistic Corpora and Their Annotations A Linguistic Annotation Framework (LAF) is being developed within the International Standards Organization Technical Committee 37 Sub-committee on Language Resource Management (ISO TC37 SC4). LAF is intended to provide a standardized means to represent linguistic data and its annotations that is defined broadly enough to accommodate all types of linguistic annotations, and at the same time provide means to represent precise and potentially complex linguistic information. The general principles informing the design of LAF have been previously reported (Ide and Romary, 2003; Ide and Romary, 2004a). This paper describes some of the more technical aspects of the LAF design that have been addressed in the process of finalizing the specifications for the standard.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1501929611,"Goal":"Partnership for the Goals","Task":["Language Resource Management"],"Method":["Linguistic Annotation Framework","LAF","LAF","LAF"]},{"ID":"fancellu-etal-2017-universal","title":"{U}niversal {D}ependencies to Logical Form with Negation Scope","abstract":"Many language technology applications would benefit from the ability to represent negation and its scope on top of widely-used linguistic resources. In this paper, we investigate the possibility of obtaining a first-order logic representation with negation scope marked using \\textit{Universal Dependencies}. To do so, we enhance \\textit{UDepLambda}, a framework that converts dependency graphs to logical forms. The resulting \\textit{UDepLambda$\\lnot$ }is able to handle phenomena related to scope by means of an higher-order type theory, relevant not only to negation but also to universal quantification and other complex semantic phenomena. The initial conversion we did for English is promising, in that one can represent the scope of negation also in the presence of more complex phenomena such as universal quantifiers.","year":2017,"title_abstract":"{U}niversal {D}ependencies to Logical Form with Negation Scope Many language technology applications would benefit from the ability to represent negation and its scope on top of widely-used linguistic resources. In this paper, we investigate the possibility of obtaining a first-order logic representation with negation scope marked using \\textit{Universal Dependencies}. To do so, we enhance \\textit{UDepLambda}, a framework that converts dependency graphs to logical forms. The resulting \\textit{UDepLambda$\\lnot$ }is able to handle phenomena related to scope by means of an higher-order type theory, relevant not only to negation but also to universal quantification and other complex semantic phenomena. The initial conversion we did for English is promising, in that one can represent the scope of negation also in the presence of more complex phenomena such as universal quantifiers.","social_need":"No Poverty End poverty in all its forms everywhere","cosine_similarity":0.1501623094,"Goal":"No Poverty","Task":["language technology applications"],"Method":["first - order logic representation","\\textit{UDepLambda}","higher - order type theory"]},{"ID":"qian-etal-2019-benchmark","title":"A Benchmark Dataset for Learning to Intervene in Online Hate Speech","abstract":"Countering online hate speech is a critical yet challenging task, but one which can be aided by the use of Natural Language Processing (NLP) techniques. Previous research has primarily focused on the development of NLP methods to automatically and effectively detect online hate speech while disregarding further action needed to calm and discourage individuals from using hate speech in the future. In addition, most existing hate speech datasets treat each post as an isolated instance, ignoring the conversational context. In this paper, we propose a novel task of generative hate speech intervention, where the goal is to automatically generate responses to intervene during online conversations that contain hate speech. As a part of this work, we introduce two fully-labeled large-scale hate speech intervention datasets collected from Gab and Reddit. These datasets provide conversation segments, hate speech labels, as well as intervention responses written by Mechanical Turk Workers. In this paper, we also analyze the datasets to understand the common intervention strategies and explore the performance of common automatic response generation methods on these new datasets to provide a benchmark for future research.","year":2019,"title_abstract":"A Benchmark Dataset for Learning to Intervene in Online Hate Speech Countering online hate speech is a critical yet challenging task, but one which can be aided by the use of Natural Language Processing (NLP) techniques. Previous research has primarily focused on the development of NLP methods to automatically and effectively detect online hate speech while disregarding further action needed to calm and discourage individuals from using hate speech in the future. In addition, most existing hate speech datasets treat each post as an isolated instance, ignoring the conversational context. In this paper, we propose a novel task of generative hate speech intervention, where the goal is to automatically generate responses to intervene during online conversations that contain hate speech. As a part of this work, we introduce two fully-labeled large-scale hate speech intervention datasets collected from Gab and Reddit. These datasets provide conversation segments, hate speech labels, as well as intervention responses written by Mechanical Turk Workers. In this paper, we also analyze the datasets to understand the common intervention strategies and explore the performance of common automatic response generation methods on these new datasets to provide a benchmark for future research.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1501077563,"Goal":"Climate Action","Task":["Learning","Online Hate Speech","generative hate speech intervention"],"Method":["Natural Language Processing","NLP methods","intervention strategies","automatic response generation methods"]},{"ID":"lu-paladini-adell-2012-beyond","title":"Beyond {MT}: Source Content Quality and Process Automation","abstract":"This document introduces the strategy implemented at CA Technologies to exploit Machine Translation (MT) at the corporate-wide level. We will introduce the different approaches followed to further improve the quality of the output of the machine translation engine once the engines have reached a maximum level of customization. Senior team support, clear communication between the parties involved and improvement measurement are the key components for the success of the initiative.","year":2012,"title_abstract":"Beyond {MT}: Source Content Quality and Process Automation This document introduces the strategy implemented at CA Technologies to exploit Machine Translation (MT) at the corporate-wide level. We will introduce the different approaches followed to further improve the quality of the output of the machine translation engine once the engines have reached a maximum level of customization. Senior team support, clear communication between the parties involved and improvement measurement are the key components for the success of the initiative.","social_need":"Decent Work and Economic Growth Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all","cosine_similarity":0.1501055062,"Goal":"Decent Work and Economic Growth","Task":["Process Automation","Machine Translation"],"Method":["CA Technologies","machine translation engine"]},{"ID":"sanchez-martinez-etal-2020-english","title":"An {E}nglish-{S}wahili parallel corpus and its use for neural machine translation in the news domain","abstract":"This paper describes our approach to create a neural machine translation system to translate between English and Swahili (both directions) in the news domain, as well as the process we followed to crawl the necessary parallel corpora from the Internet. We report the results of a pilot human evaluation performed by the news media organisations participating in the H2020 EU-funded project GoURMET.","year":2020,"title_abstract":"An {E}nglish-{S}wahili parallel corpus and its use for neural machine translation in the news domain This paper describes our approach to create a neural machine translation system to translate between English and Swahili (both directions) in the news domain, as well as the process we followed to crawl the necessary parallel corpora from the Internet. We report the results of a pilot human evaluation performed by the news media organisations participating in the H2020 EU-funded project GoURMET.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.150084585,"Goal":"Gender Equality","Task":["neural machine translation"],"Method":["neural machine translation system"]},{"ID":"murray-2017-modelling","title":"Modelling Participation in Small Group Social Sequences with {M}arkov Rewards Analysis","abstract":"We explore a novel computational approach for analyzing member participation in small group social sequences. Using a complex state representation combining information about dialogue act types, sentiment expression, and participant roles, we explore which sequence states are associated with high levels of member participation. Using a Markov Rewards framework, we associate particular states with immediate positive and negative rewards, and employ a Value Iteration algorithm to calculate the expected value of all states. In our findings, we focus on discourse states belonging to team leaders and project managers which are either very likely or very unlikely to lead to participation from the rest of the group members.","year":2017,"title_abstract":"Modelling Participation in Small Group Social Sequences with {M}arkov Rewards Analysis We explore a novel computational approach for analyzing member participation in small group social sequences. Using a complex state representation combining information about dialogue act types, sentiment expression, and participant roles, we explore which sequence states are associated with high levels of member participation. Using a Markov Rewards framework, we associate particular states with immediate positive and negative rewards, and employ a Value Iteration algorithm to calculate the expected value of all states. In our findings, we focus on discourse states belonging to team leaders and project managers which are either very likely or very unlikely to lead to participation from the rest of the group members.","social_need":"Partnership for the Goals Strengthen the means of implementation and revitalize the global partnership for sustainable development","cosine_similarity":0.1500808001,"Goal":"Partnership for the Goals","Task":["Modelling Participation in Small Group Social Sequences","member participation in small group social sequences"],"Method":["{M}arkov Rewards Analysis","computational approach","complex state representation","Markov Rewards framework","Value Iteration algorithm"]},{"ID":"prato-etal-2020-fully","title":"Fully Quantized Transformer for Machine Translation","abstract":"State-of-the-art neural machine translation methods employ massive amounts of parameters. Drastically reducing computational costs of such methods without affecting performance has been up to this point unsuccessful. To this end, we propose FullyQT: an all-inclusive quantization strategy for the Transformer. To the best of our knowledge, we are the first to show that it is possible to avoid any loss in translation quality with a fully quantized Transformer. Indeed, compared to full-precision, our 8-bit models score greater or equal BLEU on most tasks. Comparing ourselves to all previously proposed methods, we achieve state-of-the-art quantization results.","year":2020,"title_abstract":"Fully Quantized Transformer for Machine Translation State-of-the-art neural machine translation methods employ massive amounts of parameters. Drastically reducing computational costs of such methods without affecting performance has been up to this point unsuccessful. To this end, we propose FullyQT: an all-inclusive quantization strategy for the Transformer. To the best of our knowledge, we are the first to show that it is possible to avoid any loss in translation quality with a fully quantized Transformer. Indeed, compared to full-precision, our 8-bit models score greater or equal BLEU on most tasks. Comparing ourselves to all previously proposed methods, we achieve state-of-the-art quantization results.","social_need":"Gender Equality Achieve gender equality and empower all women and girls","cosine_similarity":0.1500686705,"Goal":"Gender Equality","Task":["Machine Translation","Transformer"],"Method":["Fully Quantized Transformer","neural machine translation methods","FullyQT","all - inclusive quantization strategy","fully quantized Transformer","8 - bit models"]},{"ID":"pielka-etal-2020-fraunhofer","title":"Fraunhofer {IAIS} at {F}in{C}ausal 2020, Tasks 1 {\\&} 2: Using Ensemble Methods and Sequence Tagging to Detect Causality in Financial Documents","abstract":"The FinCausal 2020 shared task aims to detect causality on financial news and identify those parts of the causal sentences related to the underlying cause and effect. We apply ensemble-based and sequence tagging methods for identifying causality, and extracting causal subsequences. Our models yield promising results on both sub-tasks, with the prospect of further improvement given more time and computing resources. With respect to task 1, we achieved an F1 score of 0.9429 on the evaluation data, and a corresponding ranking of 12\/14. For task 2, we were ranked 6\/10, with an F1 score of 0.76 and an ExactMatch score of 0.1912.","year":2020,"title_abstract":"Fraunhofer {IAIS} at {F}in{C}ausal 2020, Tasks 1 {\\&} 2: Using Ensemble Methods and Sequence Tagging to Detect Causality in Financial Documents The FinCausal 2020 shared task aims to detect causality on financial news and identify those parts of the causal sentences related to the underlying cause and effect. We apply ensemble-based and sequence tagging methods for identifying causality, and extracting causal subsequences. Our models yield promising results on both sub-tasks, with the prospect of further improvement given more time and computing resources. With respect to task 1, we achieved an F1 score of 0.9429 on the evaluation data, and a corresponding ranking of 12\/14. For task 2, we were ranked 6\/10, with an F1 score of 0.76 and an ExactMatch score of 0.1912.","social_need":"Climate Action Take urgent action to combat climate change and its impacts*","cosine_similarity":0.1500509828,"Goal":"Climate Action","Task":["Detect Causality in Financial Documents","FinCausal 2020 shared task","identifying causality","extracting causal subsequences"],"Method":["Ensemble Methods","Sequence Tagging","ensemble - based and sequence tagging methods"]},{"ID":"tamari-etal-2020-language","title":"{L}anguage (Re)modelling: {T}owards Embodied Language Understanding","abstract":"While natural language understanding (NLU) is advancing rapidly, today{'}s technology differs from human-like language understanding in fundamental ways, notably in its inferior efficiency, interpretability, and generalization. This work proposes an approach to representation and learning based on the tenets of embodied cognitive linguistics (ECL). According to ECL, natural language is inherently executable (like programming languages), driven by mental simulation and metaphoric mappings over hierarchical compositions of structures and schemata learned through embodied interaction. This position paper argues that the use of grounding by metaphoric reasoning and simulation will greatly benefit NLU systems, and proposes a system architecture along with a roadmap towards realizing this vision.","year":2020,"title_abstract":"{L}anguage (Re)modelling: {T}owards Embodied Language Understanding While natural language understanding (NLU) is advancing rapidly, today{'}s technology differs from human-like language understanding in fundamental ways, notably in its inferior efficiency, interpretability, and generalization. This work proposes an approach to representation and learning based on the tenets of embodied cognitive linguistics (ECL). According to ECL, natural language is inherently executable (like programming languages), driven by mental simulation and metaphoric mappings over hierarchical compositions of structures and schemata learned through embodied interaction. This position paper argues that the use of grounding by metaphoric reasoning and simulation will greatly benefit NLU systems, and proposes a system architecture along with a roadmap towards realizing this vision.","social_need":"Life Below Water Conserve and sustainably use the oceans, seas and marine resources for sustainable development","cosine_similarity":0.1500490606,"Goal":"Life Below Water","Task":["Embodied Language Understanding","natural language understanding","human - like language understanding","generalization","representation and learning","simulation","NLU"],"Method":["embodied cognitive linguistics","mental simulation","metaphoric mappings","metaphoric reasoning","system architecture"]},{"ID":"wang-etal-2018-alibaba","title":"{A}libaba Submission for {WMT}18 Quality Estimation Task","abstract":"The goal of WMT 2018 Shared Task on Translation Quality Estimation is to investigate automatic methods for estimating the quality of machine translation results without reference translations. This paper presents the QE Brain system, which proposes the neural Bilingual Expert model as a feature extractor based on conditional target language model with a bidirectional transformer and then processes the semantic representations of source and the translation output with a Bi-LSTM predictive model for automatic quality estimation. The system has been applied to the sentence-level scoring and ranking tasks as well as the word-level tasks for finding errors for each word in translations. An extensive set of experimental results have shown that our system outperformed the best results in WMT 2017 Quality Estimation tasks and obtained top results in WMT 2018.","year":2018,"title_abstract":"{A}libaba Submission for {WMT}18 Quality Estimation Task The goal of WMT 2018 Shared Task on Translation Quality Estimation is to investigate automatic methods for estimating the quality of machine translation results without reference translations. This paper presents the QE Brain system, which proposes the neural Bilingual Expert model as a feature extractor based on conditional target language model with a bidirectional transformer and then processes the semantic representations of source and the translation output with a Bi-LSTM predictive model for automatic quality estimation. The system has been applied to the sentence-level scoring and ranking tasks as well as the word-level tasks for finding errors for each word in translations. An extensive set of experimental results have shown that our system outperformed the best results in WMT 2017 Quality Estimation tasks and obtained top results in WMT 2018.","social_need":"Quality Education Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all","cosine_similarity":0.1500489116,"Goal":"Quality Education","Task":["Quality Estimation Task","WMT 2018 Shared Task","Translation Quality Estimation","machine translation results","automatic quality estimation","sentence - level scoring and ranking tasks","word - level tasks","finding errors","WMT 2017 Quality Estimation tasks"],"Method":["automatic methods","QE Brain system","neural Bilingual Expert model","feature extractor","conditional target language model","bidirectional transformer","semantic representations","Bi - LSTM predictive model"]}]