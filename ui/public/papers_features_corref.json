[{"ID":"hieu-etal-2020-reintel","text":"{R}e{INTEL} Challenge 2020: {V}ietnamese Fake News Detection using{E}nsemble Model with {P}ho{BERT} embeddings Along with the increasing traffic of social networks in Vietnam in recent years, the number of unreliable news has also grown rapidly. As we make decisions based on the information we come across daily, fake news, depending on the severity of the matter, can lead to disastrous consequences. This paper presents our approach for the Fake News Detection on Social Network Sites (SNSs), using an ensemble method with linguistic features extracted using PhoBERT (Nguyen and Nguyen, 2020). Our method achieves AUC score of 0.9521 and got 1 st place on the private test at the 7 th International Workshop on Vietnamese Language and Speech Processing (VLSP). For reproducing the result, the code can be found at https:\/\/gitlab.com\/thuan.","task_annotation":["fake news detection"],"method_annotation":["linguistic feature extraction","phobert","bert","phobert embeddings","ensemble method"],"tasks":[["vietnamese fake news detection","fake news detection on social network sites"],["vietnamese language and speech processing"]],"methods":[["phobert embeddings","phobert"],["ensemble method"]],"task_unmatch_golden":[],"task_match_golden":["fake news detection"],"task_match_predicted":["vietnamese fake news detection"],"task_unmatch_pred":["vietnamese language and speech processing"],"task_match_group":["fake news detection on social network sites"],"task_match_total":["vietnamese fake news detection","fake news detection"],"method_unmatch_golden":["bert","linguistic feature extraction","phobert embeddings"],"method_match_golden":["phobert","ensemble method"],"method_match_predicted":["ensemble method","phobert embeddings"],"method_unmatch_pred":[],"method_match_group":["phobert"],"method_match_total":["phobert","ensemble method","phobert embeddings"],"title_clean":"ReINTEL Challenge 2020: Vietnamese Fake News Detection usingEnsemble Model with PhoBERT embeddings","abstract_clean":"Along with the increasing traffic of social networks in Vietnam in recent years, the number of unreliable news has also grown rapidly. As we make decisions based on the information we come across daily, fake news, depending on the severity of the matter, can lead to disastrous consequences. This paper presents our approach for the Fake News Detection on Social Network Sites (SNSs), using an ensemble method with linguistic features extracted using PhoBERT (Nguyen and Nguyen, 2020). Our method achieves AUC score of 0.9521 and got 1 st place on the private test at the 7 th International Workshop on Vietnamese Language and Speech Processing (VLSP). For reproducing the result, the code can be found at https:\/\/gitlab.com\/thuan."},{"ID":"aggarwal-etal-2019-ltl","text":"{LTL}-{UDE} at {S}em{E}val-2019 Task 6: {BERT} and Two-Vote Classification for Categorizing Offensiveness This paper describes LTL-UDE's systems for the SemEval 2019 Shared Task 6. We present results for Subtask A and C. In Subtask A, we experiment with an embedding representation of postings and use a Multi-Layer Perceptron and BERT to categorize postings. Our best result reaches the 10th place (out of 103) using BERT. In Subtask C, we applied a two-vote classification approach with minority fallback, which is placed on the 19th rank (out of 65).","task_annotation":["offensive language detection"],"method_annotation":["embedding representation","multi layer perceptron","bert"],"tasks":[["categorizing offensiveness"]],"methods":[["two vote classification","two vote classification approach"],["bert","bert","bert"],["ltl ude","ltl udes systems"],["minority fallback"],["multi layer perceptron"],["embedding representation of postings"]],"task_unmatch_golden":["offensive language detection"],"task_match_golden":[],"task_match_predicted":[],"task_unmatch_pred":["categorizing offensiveness"],"task_match_group":[],"task_match_total":[],"method_unmatch_golden":[],"method_match_golden":["embedding representation","multi-layer perceptron","bert"],"method_match_predicted":["multi layer perceptron","embedding representation of postings","bert"],"method_unmatch_pred":["ltl ude","minority fallback","two vote classification","ltl udes systems","two vote classification approach"],"method_match_group":[],"method_match_total":["multi layer perceptron","embedding representation","embedding representation of postings","multi-layer perceptron","bert"],"title_clean":"LTL-UDE at SemEval-2019 Task 6: BERT and Two-Vote Classification for Categorizing Offensiveness","abstract_clean":"This paper describes LTL-UDE's systems for the SemEval 2019 Shared Task 6. We present results for Subtask A and C. In Subtask A, we experiment with an embedding representation of postings and use a Multi-Layer Perceptron and BERT to categorize postings. Our best result reaches the 10th place (out of 103) using BERT. In Subtask C, we applied a two-vote classification approach with minority fallback, which is placed on the 19th rank (out of 65)."},{"ID":"somers-etal-1997-multilingual","text":"Multilingual Generation and Summarization of Job Adverts: the {TREE} Project A multilingual Internet-based employment advertisement system is described. Job ads are submitted as e-mail texts, analysed by an example-based pattern matcher and stored in language-independent schemas in an object-oriented database. Users can search the database in their own language and get customized summaries of the job ads. The query engine uses symbolic case-based reasoning techniques, while the generation module integrates canned text, templates, and grammar rules to produce texts and hypertexts in a simple way.","task_annotation":["multilingual generation","summarization"],"method_annotation":["pattern matching","language independent schemas","query system","object oriented database","symbolic case based reasoning"],"tasks":[["multilingual generation and summarization of job adverts"]],"methods":[["example based pattern matcher","query engine","symbolic case based reasoning techniques","generation module"],["multilingual internet based employment advertisement system","grammar rules"]],"task_unmatch_golden":["summarization"],"task_match_golden":["multilingual generation"],"task_match_predicted":["multilingual generation and summarization of job adverts"],"task_unmatch_pred":[],"task_match_group":[],"task_match_total":["multilingual generation","multilingual generation and summarization of job adverts"],"method_unmatch_golden":["language-independent schemas","object-oriented database","query system","symbolic case-based reasoning"],"method_match_golden":["pattern matching"],"method_match_predicted":["example based pattern matcher"],"method_unmatch_pred":["multilingual internet based employment advertisement system","grammar rules"],"method_match_group":["query engine","generation module","symbolic case based reasoning techniques"],"method_match_total":["example based pattern matcher","pattern matching"],"title_clean":"Multilingual Generation and Summarization of Job Adverts: the TREE Project","abstract_clean":"A multilingual Internet-based employment advertisement system is described. Job ads are submitted as e-mail texts, analysed by an example-based pattern matcher and stored in language-independent schemas in an object-oriented database. Users can search the database in their own language and get customized summaries of the job ads. The query engine uses symbolic case-based reasoning techniques, while the generation module integrates canned text, templates, and grammar rules to produce texts and hypertexts in a simple way."},{"ID":"pergola-etal-2021-boosting","text":"Boosting Low-Resource Biomedical {QA} via Entity-Aware Masking Strategies Biomedical question-answering (QA) has gained increased attention for its capability to provide users with high-quality information from a vast scientific literature. Although an increasing number of biomedical QA datasets has been recently made available, those resources are still rather limited and expensive to produce. Transfer learning via pre-trained language models (LMs) has been shown as a promising approach to leverage existing general-purpose knowledge. However, finetuning these large models can be costly and time consuming, often yielding limited benefits when adapting to specific themes of specialised domains, such as the COVID-19 literature. To bootstrap further their domain adaptation, we propose a simple yet unexplored approach, which we call biomedical entity-aware masking (BEM). We encourage masked language models to learn entity-centric knowledge based on the pivotal entities characterizing the domain at hand, and employ those entities to drive the LM fine-tuning. The resulting strategy is a downstream process applicable to a wide variety of masked LMs, not requiring additional memory or components in the neural architectures. Experimental results show performance on par with state-of-the-art models on several biomedical QA datasets.","task_annotation":["question answering","domain adaptaion","transfer learning"],"method_annotation":["masked language models","language models","biomedical entity aware masking"],"tasks":[["lm fine tuning"],["domain adaptation"],["boosting low resource biomedical qa","biomedical question answering"]],"methods":[["entity aware masking strategies","biomedical entity aware masking"],["neural architectures"],["masked language models","masked lms"],["pre trained language models"],["transfer learning"]],"task_unmatch_golden":["transfer learning"],"task_match_golden":["domain adaptaion","question answering"],"task_match_predicted":["domain adaptation","biomedical question answering"],"task_unmatch_pred":["lm fine tuning"],"task_match_group":["boosting low resource biomedical qa"],"task_match_total":["domain adaptaion","biomedical question answering","domain adaptation","question answering"],"method_unmatch_golden":[],"method_match_golden":["masked language models","language models","biomedical entity-aware masking"],"method_match_predicted":["masked language models","pre trained language models","biomedical entity aware masking"],"method_unmatch_pred":["transfer learning","neural architectures"],"method_match_group":["masked lms","entity aware masking strategies"],"method_match_total":["language models","masked language models","pre trained language models","biomedical entity-aware masking","biomedical entity aware masking"],"title_clean":"Boosting Low-Resource Biomedical QA via Entity-Aware Masking Strategies","abstract_clean":"Biomedical question-answering (QA) has gained increased attention for its capability to provide users with high-quality information from a vast scientific literature. Although an increasing number of biomedical QA datasets has been recently made available, those resources are still rather limited and expensive to produce. Transfer learning via pre-trained language models (LMs) has been shown as a promising approach to leverage existing general-purpose knowledge. However, finetuning these large models can be costly and time consuming, often yielding limited benefits when adapting to specific themes of specialised domains, such as the COVID-19 literature. To bootstrap further their domain adaptation, we propose a simple yet unexplored approach, which we call biomedical entity-aware masking (BEM). We encourage masked language models to learn entity-centric knowledge based on the pivotal entities characterizing the domain at hand, and employ those entities to drive the LM fine-tuning. The resulting strategy is a downstream process applicable to a wide variety of masked LMs, not requiring additional memory or components in the neural architectures. Experimental results show performance on par with state-of-the-art models on several biomedical QA datasets."},{"ID":"meng-etal-2018-automatic","text":"Automatic Labeling of Problem-Solving Dialogues for Computational Microgenetic Learning Analytics This paper presents a recurrent neural network model to automate the analysis of students' computational thinking in problem-solving dialogue. We have collected and annotated dialogue transcripts from middle school students solving a robotics challenge, and each dialogue turn is assigned a code. We use sentence embeddings and speaker identities as features, and experiment with linear chain CRFs and RNNs with a CRF layer (LSTM-CRF). Both the linear chain CRF model and the LSTM-CRF model outperform the na\u00efve baselines by a large margin, and LSTM-CRF has an edge between the two. To our knowledge, this is the first study on dialogue segment annotation using neural network models. This study is also a stepping-stone to automating the microgenetic analysis of cognitive interactions between students.","task_annotation":["dialogue act annotation"],"method_annotation":["sentence embeddings","linear chain crf model","recurrent neural network model","lstm crf","microgenetic learning analytics"],"tasks":[["automatic labeling of problem solving dialogues","problem solving dialogue"],["dialogue segment annotation"],["microgenetic analysis of cognitive interactions between students"],["robotics challenge"],["computational microgenetic learning analytics"],["analysis of students computational thinking"]],"methods":[["linear chain crfs","linear chain crf model"],["recurrent neural network model","rnns"],["crf layer","crf"],["neural network models"],["lstm crf model","lstm crf"]],"task_unmatch_golden":["dialogue act annotation"],"task_match_golden":[],"task_match_predicted":[],"task_unmatch_pred":["microgenetic analysis of cognitive interactions between students","analysis of students computational thinking","problem solving dialogue","dialogue segment annotation","robotics challenge","computational microgenetic learning analytics","automatic labeling of problem solving dialogues"],"task_match_group":[],"task_match_total":[],"method_unmatch_golden":["microgenetic learning analytics","sentence embeddings"],"method_match_golden":["linear chain crf model","recurrent neural network model","lstm-crf"],"method_match_predicted":["lstm crf model","neural network models","crf"],"method_unmatch_pred":["rnns","linear chain crfs","linear chain crf model","recurrent neural network model"],"method_match_group":["crf layer","lstm crf"],"method_match_total":["linear chain crf model","lstm crf model","recurrent neural network model","neural network models","lstm-crf","crf"],"title_clean":"Automatic Labeling of Problem-Solving Dialogues for Computational Microgenetic Learning Analytics","abstract_clean":"This paper presents a recurrent neural network model to automate the analysis of students' computational thinking in problem-solving dialogue. We have collected and annotated dialogue transcripts from middle school students solving a robotics challenge, and each dialogue turn is assigned a code. We use sentence embeddings and speaker identities as features, and experiment with linear chain CRFs and RNNs with a CRF layer (LSTM-CRF). Both the linear chain CRF model and the LSTM-CRF model outperform the na\u00efve baselines by a large margin, and LSTM-CRF has an edge between the two. To our knowledge, this is the first study on dialogue segment annotation using neural network models. This study is also a stepping-stone to automating the microgenetic analysis of cognitive interactions between students."},{"ID":"li-etal-2020-adviser","text":"{ADVISER}: A Toolkit for Developing Multi-modal, Multi-domain and Socially-engaged Conversational Agents We present ADVISER 1-an open-source, multi-domain dialog system toolkit that enables the development of multi-modal (incorporating speech, text and vision), sociallyengaged (e.g. emotion recognition, engagement level prediction and backchanneling) conversational agents. The final Python-based implementation of our toolkit is flexible, easy to use, and easy to extend not only for technically experienced users, such as machine learning researchers, but also for less technically experienced users, such as linguists or cognitive scientists, thereby providing a flexible platform for collaborative research.","task_annotation":["multi domain dialog system"],"method_annotation":["emotion recognition","engagement level prediction","backchanneling"],"tasks":[["multi modal multi domain and socially engaged conversational agents","backchanneling conversational agents"],["collaborative research"],["linguists"],["machine learning researchers"],["engagement level prediction"],["emotion recognition"],["vision"],["multi modal incorporating speech"]],"methods":[["adviser","adviser"],["python based implementation"],["cognitive scientists"],["multi domain dialog system toolkit"]],"task_unmatch_golden":["multi-domain dialog system"],"task_match_golden":[],"task_match_predicted":[],"task_unmatch_pred":["multi modal incorporating speech","engagement level prediction","machine learning researchers","linguists","emotion recognition","collaborative research","multi modal multi domain and socially engaged conversational agents","vision","backchanneling conversational agents"],"task_match_group":[],"task_match_total":[],"method_unmatch_golden":["engagement level prediction","backchanneling","emotion recognition"],"method_match_golden":[],"method_match_predicted":[],"method_unmatch_pred":["cognitive scientists","python based implementation","adviser","multi domain dialog system toolkit"],"method_match_group":[],"method_match_total":[],"title_clean":"ADVISER: A Toolkit for Developing Multi-modal, Multi-domain and Socially-engaged Conversational Agents","abstract_clean":"We present ADVISER 1-an open-source, multi-domain dialog system toolkit that enables the development of multi-modal (incorporating speech, text and vision), sociallyengaged (e.g. emotion recognition, engagement level prediction and backchanneling) conversational agents. The final Python-based implementation of our toolkit is flexible, easy to use, and easy to extend not only for technically experienced users, such as machine learning researchers, but also for less technically experienced users, such as linguists or cognitive scientists, thereby providing a flexible platform for collaborative research."},{"ID":"huang-bai-2021-hub","text":"{HUB}@{D}ravidian{L}ang{T}ech-{EACL}2021: Identify and Classify Offensive Text in Multilingual Code Mixing in Social Media This paper introduces the system description of the HUB team participating in Dravidian-LangTech-EACL2021: Offensive Language Identification in Dravidian Languages. The theme of this shared task is the detection of offensive content in social media. Among the known tasks related to offensive speech detection, this is the first task to detect offensive comments posted in social media comments in the Dravidian language. The task organizer team provided us with the code-mixing task data set mainly composed of three different languages: Malayalam, Kannada, and Tamil. The tasks on the code mixed data in these three different languages can be seen as three different comment\/post-level classification tasks. The task on the Malayalam data set is a five-category classification task, and the Kannada and Tamil language data sets are two six-category classification tasks. Based on our analysis of the task description and task data set, we chose to use the multilingual BERT model to complete this task. In this paper, we will discuss our fine-tuning methods, models, experiments, and results.","task_annotation":["offensive text classification"],"method_annotation":["multilingual bert"],"tasks":[["detection of offensive content in social media","offensive speech detection"],["classify offensive text","commentpost level classification tasks","five category classification task","six category classification tasks"],["multilingual code mixing"],["offensive language identification"]],"methods":[["multilingual bert model"],["fine tuning methods"],["hubdravidianlangtech eacl2021"]],"task_unmatch_golden":[],"task_match_golden":["offensive text classification"],"task_match_predicted":["five category classification task"],"task_unmatch_pred":["offensive language identification","multilingual code mixing","offensive speech detection","detection of offensive content in social media"],"task_match_group":["classify offensive text","six category classification tasks","commentpost level classification tasks"],"task_match_total":["five category classification task","offensive text classification"],"method_unmatch_golden":[],"method_match_golden":["multilingual bert"],"method_match_predicted":["multilingual bert model"],"method_unmatch_pred":["fine tuning methods","hubdravidianlangtech eacl2021"],"method_match_group":[],"method_match_total":["multilingual bert model","multilingual bert"],"title_clean":"HUB@DravidianLangTech-EACL2021: Identify and Classify Offensive Text in Multilingual Code Mixing in Social Media","abstract_clean":"This paper introduces the system description of the HUB team participating in Dravidian-LangTech-EACL2021: Offensive Language Identification in Dravidian Languages. The theme of this shared task is the detection of offensive content in social media. Among the known tasks related to offensive speech detection, this is the first task to detect offensive comments posted in social media comments in the Dravidian language. The task organizer team provided us with the code-mixing task data set mainly composed of three different languages: Malayalam, Kannada, and Tamil. The tasks on the code mixed data in these three different languages can be seen as three different comment\/post-level classification tasks. The task on the Malayalam data set is a five-category classification task, and the Kannada and Tamil language data sets are two six-category classification tasks. Based on our analysis of the task description and task data set, we chose to use the multilingual BERT model to complete this task. In this paper, we will discuss our fine-tuning methods, models, experiments, and results."},{"ID":"chakraborti-tendulkar-2013-parallels","text":"Parallels between Linguistics and Biology In this paper we take a fresh look at parallels between linguistics and biology. We expect that this new line of thinking will propel cross fertilization of two disciplines and open up new research avenues.","task_annotation":["parallels between linguistics and biology"],"method_annotation":["parallel construction","analogies"],"tasks":[["cross fertilization"],["new research avenues"]],"methods":[["method1"],["method2"],["...]n\/a"]],"task_unmatch_golden":["parallels between linguistics and biology"],"task_match_golden":[],"task_match_predicted":[],"task_unmatch_pred":["cross fertilization","new research avenues"],"task_match_group":[],"task_match_total":[],"method_unmatch_golden":["analogies","parallel construction"],"method_match_golden":[],"method_match_predicted":[],"method_unmatch_pred":["method1","method2","...]n\/a"],"method_match_group":[],"method_match_total":[],"title_clean":"Parallels between Linguistics and Biology","abstract_clean":"In this paper we take a fresh look at parallels between linguistics and biology. We expect that this new line of thinking will propel cross fertilization of two disciplines and open up new research avenues."},{"ID":"salawu-etal-2021-large","text":"A Large-Scale {E}nglish Multi-Label {T}witter Dataset for Cyberbullying and Online Abuse Detection In this paper, we introduce a new English Twitter-based dataset for online abuse and cyberbullying detection. Comprising 62,587 tweets, this dataset was sourced from Twitter using specific query terms designed to retrieve tweets with high probabilities of various forms of bullying and offensive content, including insult, profanity, sarcasm, threat, porn and exclusion. Analysis performed on the dataset confirmed common cyberbullying themes reported by other studies and revealed interesting relationships between the classes. The dataset was used to train a number of transformer-based deep learning models returning impressive results.","task_annotation":["online abuse detection"],"method_annotation":["transformer based deep learning models","dataset"],"tasks":[["online abuse and cyberbullying detection"],["cyberbullying and online abuse detection"]],"methods":[["transformer - based deep learning models"]],"task_unmatch_golden":[],"task_match_golden":["online abuse detection"],"task_match_predicted":["cyberbullying and online abuse detection"],"task_unmatch_pred":["online abuse and cyberbullying detection"],"task_match_group":[],"task_match_total":["cyberbullying and online abuse detection","online abuse detection"],"method_unmatch_golden":["dataset"],"method_match_golden":["transformer-based deep learning models"],"method_match_predicted":["transformer - based deep learning models"],"method_unmatch_pred":[],"method_match_group":[],"method_match_total":["transformer - based deep learning models","transformer-based deep learning models"],"title_clean":"A Large-Scale English Multi-Label Twitter Dataset for Cyberbullying and Online Abuse Detection","abstract_clean":"In this paper, we introduce a new English Twitter-based dataset for online abuse and cyberbullying detection. Comprising 62,587 tweets, this dataset was sourced from Twitter using specific query terms designed to retrieve tweets with high probabilities of various forms of bullying and offensive content, including insult, profanity, sarcasm, threat, porn and exclusion. Analysis performed on the dataset confirmed common cyberbullying themes reported by other studies and revealed interesting relationships between the classes. The dataset was used to train a number of transformer-based deep learning models returning impressive results."},{"ID":"flekova-etal-2016-exploring","text":"Exploring Stylistic Variation with Age and Income on {T}witter Writing style allows NLP tools to adjust to the traits of an author. In this paper, we explore the relation between stylistic and syntactic features and authors' age and income. We confirm our hypothesis that for numerous feature types writing style is predictive of income even beyond age. We analyze the predictive power of writing style features in a regression task on two data sets of around 5,000 Twitter users each. Additionally, we use our validated features to study daily variations in writing style of users from distinct income groups. Temporal stylistic patterns not only provide novel psychological insight into user behavior, but are useful for future research and applications in social media.","task_annotation":["exploring stylistic variation"],"method_annotation":["linear regression","non linear regression","rsvm","reduced support vector machines","writing style features"],"tasks":[["social media"],["regression task"]],"methods":[["nlp tools"]],"task_unmatch_golden":["exploring stylistic variation"],"task_match_golden":[],"task_match_predicted":[],"task_unmatch_pred":["regression task","social media"],"task_match_group":[],"task_match_total":[],"method_unmatch_golden":["non-linear regression","reduced support vector machines","rsvm","linear regression","writing style features"],"method_match_golden":[],"method_match_predicted":[],"method_unmatch_pred":["nlp tools"],"method_match_group":[],"method_match_total":[],"title_clean":"Exploring Stylistic Variation with Age and Income on Twitter","abstract_clean":"Writing style allows NLP tools to adjust to the traits of an author. In this paper, we explore the relation between stylistic and syntactic features and authors' age and income. We confirm our hypothesis that for numerous feature types writing style is predictive of income even beyond age. We analyze the predictive power of writing style features in a regression task on two data sets of around 5,000 Twitter users each. Additionally, we use our validated features to study daily variations in writing style of users from distinct income groups. Temporal stylistic patterns not only provide novel psychological insight into user behavior, but are useful for future research and applications in social media."},{"ID":"rio-2002-compiling","text":"Compiling an Interactive Literary Translation Web Site for Education Purposes The project under discussion represents an attempt to exploit the potential of web resources for higher education and, more particularly, on a domain (that of literary translation) which is traditionally considered not very much in relation to technology and computer science. Translation and Interpreting students at the Universidad de M\u00e1laga are offered the possibility to take an English-Spanish Literary Translation module, which epitomises the need for debate in the field of Humanities. Sadly enough, implementation of course methodology is rendered very difficult or impossible owing to time restrictions and overcrowded classrooms. It is our contention that the setting up of a web site may solve some of these issues. We intend to provide both students and the literary translation-aware Internet audience with an integrated, scalable, multifunctional debate forum. Project contents will include a detailed course description, relevant reference materials and interaction services (mailing list, debate forum and chat rooms). This is obviously without limitation, as the Forum is open to any other contents that users may consider necessary or convenient, with a view to a more interdisciplinary approach, further research on the field of Literary Translation and future developments within the project framework.","task_annotation":["compiling an interactive literary translation site"],"method_annotation":["computer mediated communication","literary translation","web site"],"tasks":[["interactive literary translation","literary translation","literary translation","literary translation"],["education purposes","higher education"],["web site"],["translation","translation"],["interpreting students"],["computer science"]],"methods":[["web resources"],["course methodology"]],"task_unmatch_golden":[],"task_match_golden":["compiling an interactive literary translation site"],"task_match_predicted":["translation"],"task_unmatch_pred":["web site","interpreting students","interactive literary translation","higher education","computer science","education purposes","literary translation"],"task_match_group":[],"task_match_total":["translation","compiling an interactive literary translation site"],"method_unmatch_golden":["web site","literary translation","computer-mediated communication"],"method_match_golden":[],"method_match_predicted":[],"method_unmatch_pred":["web resources","course methodology"],"method_match_group":[],"method_match_total":[],"title_clean":"Compiling an Interactive Literary Translation Web Site for Education Purposes","abstract_clean":"The project under discussion represents an attempt to exploit the potential of web resources for higher education and, more particularly, on a domain (that of literary translation) which is traditionally considered not very much in relation to technology and computer science. Translation and Interpreting students at the Universidad de M\u00e1laga are offered the possibility to take an English-Spanish Literary Translation module, which epitomises the need for debate in the field of Humanities. Sadly enough, implementation of course methodology is rendered very difficult or impossible owing to time restrictions and overcrowded classrooms. It is our contention that the setting up of a web site may solve some of these issues. We intend to provide both students and the literary translation-aware Internet audience with an integrated, scalable, multifunctional debate forum. Project contents will include a detailed course description, relevant reference materials and interaction services (mailing list, debate forum and chat rooms). This is obviously without limitation, as the Forum is open to any other contents that users may consider necessary or convenient, with a view to a more interdisciplinary approach, further research on the field of Literary Translation and future developments within the project framework."},{"ID":"hu-etal-2021-collaborative","text":"Collaborative Data Relabeling for Robust and Diverse Voice Apps Recommendation in Intelligent Personal Assistants Intelligent personal assistants (IPAs) such as Amazon Alexa, Google Assistant and Apple Siri extend their built-in capabilities by supporting voice apps developed by third-party developers. Sometimes the smart assistant is not able to successfully respond to user voice commands (aka utterances). There are many reasons including automatic speech recognition (ASR) error, natural language understanding (NLU) error, routing utterances to an irrelevant voice app or simply that the user is asking for a capability that is not supported yet. The failure to handle a voice command leads to customer frustration. In this paper, we introduce a fallback skill recommendation system to suggest a voice app to a customer for an unhandled voice command. One of the prominent challenges of developing a skill recommender system for IPAs is partial observation. To solve the partial observation problem, we propose collaborative data relabeling (CDR) method. In addition, CDR also improves the diversity of the recommended skills. We evaluate the proposed method both offline and online. The offline evaluation results show that the proposed system outperforms the baselines. The online A\/B testing results show significant gain of customer experience metrics.","task_annotation":["recommendation system"],"method_annotation":["collaborative data relabeling"],"tasks":[["automatic speech recognition asr error","natural language understanding"],["partial observation","partial observation problem"],["intelligent personal assistants","intelligent personal assistants"],["robust and diverse voice apps recommendation"]],"methods":[["fallback skill recommendation system","skill recommender system"],["cdr"],["ipas"],["collaborative data relabeling","collaborative data relabeling"],["smart assistant"],["voice apps"],["apple siri"]],"task_unmatch_golden":["recommendation system"],"task_match_golden":[],"task_match_predicted":[],"task_unmatch_pred":["intelligent personal assistants","natural language understanding","partial observation problem","partial observation","robust and diverse voice apps recommendation","automatic speech recognition asr error"],"task_match_group":[],"task_match_total":[],"method_unmatch_golden":[],"method_match_golden":["collaborative data relabeling"],"method_match_predicted":["collaborative data relabeling"],"method_unmatch_pred":["smart assistant","skill recommender system","voice apps","ipas","fallback skill recommendation system","apple siri","cdr"],"method_match_group":[],"method_match_total":["collaborative data relabeling"],"title_clean":"Collaborative Data Relabeling for Robust and Diverse Voice Apps Recommendation in Intelligent Personal Assistants","abstract_clean":"Intelligent personal assistants (IPAs) such as Amazon Alexa, Google Assistant and Apple Siri extend their built-in capabilities by supporting voice apps developed by third-party developers. Sometimes the smart assistant is not able to successfully respond to user voice commands (aka utterances). There are many reasons including automatic speech recognition (ASR) error, natural language understanding (NLU) error, routing utterances to an irrelevant voice app or simply that the user is asking for a capability that is not supported yet. The failure to handle a voice command leads to customer frustration. In this paper, we introduce a fallback skill recommendation system to suggest a voice app to a customer for an unhandled voice command. One of the prominent challenges of developing a skill recommender system for IPAs is partial observation. To solve the partial observation problem, we propose collaborative data relabeling (CDR) method. In addition, CDR also improves the diversity of the recommended skills. We evaluate the proposed method both offline and online. The offline evaluation results show that the proposed system outperforms the baselines. The online A\/B testing results show significant gain of customer experience metrics."},{"ID":"guo-etal-2021-pre","text":"Pre-trained Transformer-based Classification and Span Detection Models for Social Media Health Applications This paper describes our approach for six classification tasks (Tasks 1a, 3a, 3b, 4 and 5) and one span detection task (Task 1b) from the Social Media Mining for Health (SMM4H) 2021 shared tasks. We developed two separate systems for classification and span detection, both based on pre-trained Transformer-based models. In addition, we applied oversampling and classifier ensembling in the classification tasks. The results of our submissions are over the median scores in all tasks except for Task 1a. Furthermore, our model achieved first place in Task 4 and obtained a 7% higher F 1-score than the median in Task 1b.","task_annotation":["span detection","classification"],"method_annotation":["pre trained transformer","span detection models","classifier ensembling"],"tasks":[["classification tasks","classification tasks"],["span detection task","classification and span detection"],["social media health applications","social media mining for health"]],"methods":[["transformer based classification and span detection models","classifier ensembling"],["oversampling"],["transformer based models"]],"task_unmatch_golden":[],"task_match_golden":["classification","span detection"],"task_match_predicted":["classification tasks","span detection task"],"task_unmatch_pred":["social media health applications","social media mining for health"],"task_match_group":["classification and span detection"],"task_match_total":["classification tasks","span detection task","classification","span detection"],"method_unmatch_golden":["pre-trained transformer","classifier ensembling"],"method_match_golden":["span detection models"],"method_match_predicted":["transformer based classification and span detection models"],"method_unmatch_pred":["oversampling","transformer based models"],"method_match_group":["classifier ensembling"],"method_match_total":["transformer based classification and span detection models","span detection models"],"title_clean":"Pre-trained Transformer-based Classification and Span Detection Models for Social Media Health Applications","abstract_clean":"This paper describes our approach for six classification tasks (Tasks 1a, 3a, 3b, 4 and 5) and one span detection task (Task 1b) from the Social Media Mining for Health (SMM4H) 2021 shared tasks. We developed two separate systems for classification and span detection, both based on pre-trained Transformer-based models. In addition, we applied oversampling and classifier ensembling in the classification tasks. The results of our submissions are over the median scores in all tasks except for Task 1a. Furthermore, our model achieved first place in Task 4 and obtained a 7% higher F 1-score than the median in Task 1b."},{"ID":"sikdar-etal-2018-flytxt","text":"{F}lytxt{\\_}{NTNU} at {S}em{E}val-2018 Task 8: Identifying and Classifying Malware Text Using Conditional Random Fields and Na{\\\"\\i}ve {B}ayes Classifiers Cybersecurity risks such as malware threaten the personal safety of users, but to identify malware text is a major challenge. The paper proposes a supervised learning approach to identifying malware sentences given a document (subTask1 of SemEval 2018, Task 8), as well as to classifying malware tokens in the sentences (subTask2). The approach achieved good results, ranking second of twelve participants for both subtasks, with F-scores of 57% for subTask1 and 28% for subTask2.","task_annotation":["identifying and classifying malware text"],"method_annotation":["supervised learning","conditional random fields"],"tasks":[["identifying and classifying malware text","semeval 2018"]],"methods":[["conditional random fields","naive bayes classifiers","supervised learning approach"]],"task_unmatch_golden":[],"task_match_golden":["identifying and classifying malware text"],"task_match_predicted":["identifying and classifying malware text"],"task_unmatch_pred":[],"task_match_group":["semeval 2018"],"task_match_total":["identifying and classifying malware text"],"method_unmatch_golden":["conditional random fields"],"method_match_golden":["supervised learning"],"method_match_predicted":["supervised learning approach"],"method_unmatch_pred":[],"method_match_group":["conditional random fields","naive bayes classifiers"],"method_match_total":["supervised learning approach","supervised learning"],"title_clean":"Flytxt\\_NTNU at SemEval-2018 Task 8: Identifying and Classifying Malware Text Using Conditional Random Fields and Na\\\"\\ive Bayes Classifiers","abstract_clean":"Cybersecurity risks such as malware threaten the personal safety of users, but to identify malware text is a major challenge. The paper proposes a supervised learning approach to identifying malware sentences given a document (subTask1 of SemEval 2018, Task 8), as well as to classifying malware tokens in the sentences (subTask2). The approach achieved good results, ranking second of twelve participants for both subtasks, with F-scores of 57% for subTask1 and 28% for subTask2."},{"ID":"stajner-etal-2017-effects","text":"Effects of Lexical Properties on Viewing Time per Word in Autistic and Neurotypical Readers Eye tracking studies from the past few decades have shaped the way we think of word complexity and cognitive load: words that are long, rare and ambiguous are more difficult to read. However, online processing techniques have been scarcely applied to investigating the reading difficulties of people with autism and what vocabulary is challenging for them. We present parallel gaze data obtained from adult readers with autism and a control group of neurotypical readers and show that the former required higher cognitive effort to comprehend the texts as evidenced by three gaze-based measures. We divide all words into four classes based on their viewing times for both groups and investigate the relationship between longer viewing times and word length, word frequency, and four cognitively-based measures (word concreteness, familiarity, age of acquisition and imagability).","task_annotation":["eye tracking data prediction"],"method_annotation":["lexical properties"," parallel gaze data"],"tasks":[["eye tracking studies","reading difficulties"]],"methods":[["online processing techniques"]],"task_unmatch_golden":["eye-tracking data prediction"],"task_match_golden":[],"task_match_predicted":[],"task_unmatch_pred":["eye tracking studies","reading difficulties"],"task_match_group":[],"task_match_total":[],"method_unmatch_golden":[" parallel gaze data","lexical properties"],"method_match_golden":[],"method_match_predicted":[],"method_unmatch_pred":["online processing techniques"],"method_match_group":[],"method_match_total":[],"title_clean":"Effects of Lexical Properties on Viewing Time per Word in Autistic and Neurotypical Readers","abstract_clean":"Eye tracking studies from the past few decades have shaped the way we think of word complexity and cognitive load: words that are long, rare and ambiguous are more difficult to read. However, online processing techniques have been scarcely applied to investigating the reading difficulties of people with autism and what vocabulary is challenging for them. We present parallel gaze data obtained from adult readers with autism and a control group of neurotypical readers and show that the former required higher cognitive effort to comprehend the texts as evidenced by three gaze-based measures. We divide all words into four classes based on their viewing times for both groups and investigate the relationship between longer viewing times and word length, word frequency, and four cognitively-based measures (word concreteness, familiarity, age of acquisition and imagability)."},{"ID":"dernoncourt-etal-2017-neural","text":"Neural Networks for Joint Sentence Classification in Medical Paper Abstracts Existing models based on artificial neural networks (ANNs) for sentence classification often do not incorporate the context in which sentences appear, and classify sentences individually. However, traditional sentence classification approaches have been shown to greatly benefit from jointly classifying subsequent sentences, such as with conditional random fields. In this work, we present an ANN architecture that combines the effectiveness of typical ANN models to classify sentences in isolation, with the strength of structured prediction. Our model outperforms the state-ofthe-art results on two different datasets for sequential sentence classification in medical abstracts.","task_annotation":["sentence classification"],"method_annotation":["artificial neural network"],"tasks":[["joint sentence classification","sentence classification","sequential sentence classification"],["structured prediction"]],"methods":[["ann architecture","ann models"],["neural networks","artificial neural networks"],["conditional random fields"],["sentence classification approaches"]],"task_unmatch_golden":[],"task_match_golden":["sentence classification"],"task_match_predicted":["joint sentence classification"],"task_unmatch_pred":["structured prediction"],"task_match_group":["sequential sentence classification","sentence classification"],"task_match_total":["joint sentence classification","sentence classification"],"method_unmatch_golden":[],"method_match_golden":["artificial neural network"],"method_match_predicted":["artificial neural networks"],"method_unmatch_pred":["sentence classification approaches","conditional random fields","ann models","ann architecture"],"method_match_group":["neural networks"],"method_match_total":["artificial neural networks","artificial neural network"],"title_clean":"Neural Networks for Joint Sentence Classification in Medical Paper Abstracts","abstract_clean":"Existing models based on artificial neural networks (ANNs) for sentence classification often do not incorporate the context in which sentences appear, and classify sentences individually. However, traditional sentence classification approaches have been shown to greatly benefit from jointly classifying subsequent sentences, such as with conditional random fields. In this work, we present an ANN architecture that combines the effectiveness of typical ANN models to classify sentences in isolation, with the strength of structured prediction. Our model outperforms the state-ofthe-art results on two different datasets for sequential sentence classification in medical abstracts."},{"ID":"mcinnes-2008-unsupervised","text":"An Unsupervised Vector Approach to Biomedical Term Disambiguation: Integrating {UMLS} and {M}edline This paper introduces an unsupervised vector approach to disambiguate words in biomedical text that can be applied to all-word disambiguation. We explore using contextual information from the Unified Medical Language System (UMLS) to describe the possible senses of a word. We experiment with automatically creating individualized stoplists to help reduce the noise in our dataset. We compare our results to SenseClusters and Humphrey et al. (2006) using the NLM-WSD dataset and with SenseClusters using conflated data from the 2005 Medline Baseline.","task_annotation":["biomedical term disambiguation"],"method_annotation":["unsupervised vector approach","feature vectors","nlm wsd"],"tasks":[["biomedical term disambiguation","all word disambiguation"]],"methods":[["umls","unified medical language system"],["unsupervised vector approach","unsupervised vector approach"],["senseclusters"]],"task_unmatch_golden":[],"task_match_golden":["biomedical term disambiguation"],"task_match_predicted":["biomedical term disambiguation"],"task_unmatch_pred":[],"task_match_group":["all word disambiguation"],"task_match_total":["biomedical term disambiguation"],"method_unmatch_golden":["feature vectors","nlm-wsd"],"method_match_golden":["unsupervised vector approach"],"method_match_predicted":["unsupervised vector approach"],"method_unmatch_pred":["unified medical language system","umls","senseclusters"],"method_match_group":[],"method_match_total":["unsupervised vector approach"],"title_clean":"An Unsupervised Vector Approach to Biomedical Term Disambiguation: Integrating UMLS and Medline","abstract_clean":"This paper introduces an unsupervised vector approach to disambiguate words in biomedical text that can be applied to all-word disambiguation. We explore using contextual information from the Unified Medical Language System (UMLS) to describe the possible senses of a word. We experiment with automatically creating individualized stoplists to help reduce the noise in our dataset. We compare our results to SenseClusters and Humphrey et al. (2006) using the NLM-WSD dataset and with SenseClusters using conflated data from the 2005 Medline Baseline."},{"ID":"rinaldi-etal-2008-dependency","text":"Dependency-Based Relation Mining for Biomedical Literature We describe techniques for the automatic detection of relationships among domain entities (e.g. genes, proteins, diseases) mentioned in the biomedical literature. Our approach is based on the adaptive selection of candidate interactions sentences, which are then parsed using our own dependency parser. Specific syntax-based filters are used to limit the number of possible candidate interacting pairs. The approach has been implemented as a demonstrator over a corpus of 2000 richly annotated MedLine abstracts, and later tested by participation to a text mining competition. In both cases, the results obtained have proved the adequacy of the proposed approach to the task of interaction detection.","task_annotation":["dependency based relation mining","text mining"],"method_annotation":["dependency parser","syntax based filters","text mining"],"tasks":[["adaptive selection of candidate interactions sentences","interaction detection"],["dependency based relation mining","automatic detection of relationships among domain entities"],["text mining competition"]],"methods":[["syntax based filters"],["dependency parser"]],"task_unmatch_golden":[],"task_match_golden":["dependency-based relation mining","text mining"],"task_match_predicted":["dependency based relation mining","text mining competition"],"task_unmatch_pred":["interaction detection","adaptive selection of candidate interactions sentences"],"task_match_group":["automatic detection of relationships among domain entities"],"task_match_total":["text mining competition","dependency-based relation mining","dependency based relation mining","text mining"],"method_unmatch_golden":["text mining"],"method_match_golden":["syntax-based filters","dependency parser"],"method_match_predicted":["syntax based filters","dependency parser"],"method_unmatch_pred":[],"method_match_group":[],"method_match_total":["syntax based filters","syntax-based filters","dependency parser"],"title_clean":"Dependency-Based Relation Mining for Biomedical Literature","abstract_clean":"We describe techniques for the automatic detection of relationships among domain entities (e.g. genes, proteins, diseases) mentioned in the biomedical literature. Our approach is based on the adaptive selection of candidate interactions sentences, which are then parsed using our own dependency parser. Specific syntax-based filters are used to limit the number of possible candidate interacting pairs. The approach has been implemented as a demonstrator over a corpus of 2000 richly annotated MedLine abstracts, and later tested by participation to a text mining competition. In both cases, the results obtained have proved the adequacy of the proposed approach to the task of interaction detection."},{"ID":"akasaki-kaji-2019-conversation","text":"Conversation Initiation by Diverse News Contents Introduction In our everyday chitchat , there is a conversation initiator, who proactively casts an initial utterance to start chatting. However, most existing conversation systems cannot play this role. Previous studies on conversation systems assume that the user always initiates conversation, and have placed emphasis on how to respond to the given user's utterance. As a result, existing conversation systems become passive. Namely they continue waiting until being spoken to by the users. In this paper, we consider the system as a conversation initiator and propose a novel task of generating the initial utterance in open-domain non-task-oriented conversation. Here, in order not to make users bored, it is necessary to generate diverse utterances to initiate conversation without relying on boilerplate utterances like greetings. To this end, we propose to generate initial utterance by summarizing and chatting about news articles, which provide fresh and various contents everyday. To address the lack of the training data for this task, we constructed a novel largescale dataset through crowd-sourcing. We also analyzed the dataset in detail to examine how humans initiate conversations (the dataset will be released to facilitate future research activities). We present several approaches to conversation initiation including information retrieval based and generation based models. Experimental results showed that the proposed models trained on our dataset performed reasonably well and outperformed baselines that utilize automatically collected training data in both automatic and manual evaluation. * This work was done during research internship at Yahoo Japan Corporation. 1 \"Conversation\" in this paper refers to open-domain nontask-oriented conversations and chitchat .","task_annotation":["conversation initiation"],"method_annotation":["encoder decoder model","mmi antilm","information retrieval"],"tasks":[["conversation initiation","conversation initiator","conversation initiation"],["open domain non task oriented conversation","open domain nontask oriented conversations"]],"methods":[["conversation initiator","conversation systems","conversation systems","conversation systems"],["generation based models"],["information retrieval"]],"task_unmatch_golden":[],"task_match_golden":["conversation initiation"],"task_match_predicted":["conversation initiation"],"task_unmatch_pred":["open domain non task oriented conversation","open domain nontask oriented conversations"],"task_match_group":["conversation initiator"],"task_match_total":["conversation initiation"],"method_unmatch_golden":["mmi-antilm","encoder-decoder model"],"method_match_golden":["information retrieval"],"method_match_predicted":["information retrieval"],"method_unmatch_pred":["conversation systems","generation based models","conversation initiator"],"method_match_group":[],"method_match_total":["information retrieval"],"title_clean":"Conversation Initiation by Diverse News Contents Introduction","abstract_clean":"In our everyday chitchat , there is a conversation initiator, who proactively casts an initial utterance to start chatting. However, most existing conversation systems cannot play this role. Previous studies on conversation systems assume that the user always initiates conversation, and have placed emphasis on how to respond to the given user's utterance. As a result, existing conversation systems become passive. Namely they continue waiting until being spoken to by the users. In this paper, we consider the system as a conversation initiator and propose a novel task of generating the initial utterance in open-domain non-task-oriented conversation. Here, in order not to make users bored, it is necessary to generate diverse utterances to initiate conversation without relying on boilerplate utterances like greetings. To this end, we propose to generate initial utterance by summarizing and chatting about news articles, which provide fresh and various contents everyday. To address the lack of the training data for this task, we constructed a novel largescale dataset through crowd-sourcing. We also analyzed the dataset in detail to examine how humans initiate conversations (the dataset will be released to facilitate future research activities). We present several approaches to conversation initiation including information retrieval based and generation based models. Experimental results showed that the proposed models trained on our dataset performed reasonably well and outperformed baselines that utilize automatically collected training data in both automatic and manual evaluation. * This work was done during research internship at Yahoo Japan Corporation. 1 \"Conversation\" in this paper refers to open-domain nontask-oriented conversations and chitchat ."},{"ID":"nguyen-2019-question","text":"Question Answering in the Biomedical Domain Question answering techniques have mainly been investigated in open domains. However, there are particular challenges in extending these open-domain techniques to extend into the biomedical domain. Question answering focusing on patients is less studied. We find that there are some challenges in patient question answering such as limited annotated data, lexical gap and quality of answer spans. We aim to address some of these gaps by extending and developing upon the literature to design a question answering system that can decide on the most appropriate answers for patients attempting to self-diagnose while including the ability to abstain from answering when confidence is low.","task_annotation":["question answering"],"method_annotation":["multi task learning","neural architectural search","reinforcement learning"],"tasks":[["question answering","answering","question answering","question answering"]],"methods":[["question answering system"],["open domain techniques"]],"task_unmatch_golden":[],"task_match_golden":["question answering"],"task_match_predicted":["question answering"],"task_unmatch_pred":[],"task_match_group":["answering"],"task_match_total":["question answering"],"method_unmatch_golden":["multi-task learning","neural architectural search","reinforcement learning"],"method_match_golden":[],"method_match_predicted":[],"method_unmatch_pred":["open domain techniques","question answering system"],"method_match_group":[],"method_match_total":[],"title_clean":"Question Answering in the Biomedical Domain","abstract_clean":"Question answering techniques have mainly been investigated in open domains. However, there are particular challenges in extending these open-domain techniques to extend into the biomedical domain. Question answering focusing on patients is less studied. We find that there are some challenges in patient question answering such as limited annotated data, lexical gap and quality of answer spans. We aim to address some of these gaps by extending and developing upon the literature to design a question answering system that can decide on the most appropriate answers for patients attempting to self-diagnose while including the ability to abstain from answering when confidence is low."},{"ID":"tang-shen-2020-categorizing","text":"Categorizing Offensive Language in Social Networks: A {C}hinese Corpus, Systems and an Explainable Tool Recently, more and more data have been generated in the online world, filled with offensive language such as threats, swear words or straightforward insults. It is disgraceful for a progressive society, and then the question arises on how language resources and technologies can cope with this challenge. However, previous work only analyzes the problem as a whole but fails to detect particular types of offensive content in a more fine-grained way, mainly because of the lack of annotated data. In this work, we present a densely annotated data-set COLA (Categorizing Offensive LAnguage), consists of fine-grained insulting language, antisocial language and illegal language. We study different strategies for automatically identifying offensive language on COLA data. Further, we design a capsule system with hierarchical attention to aggregate and fully utilize information, which obtains a state-of-the-art result. Results from experiments prove that our hierarchical attention capsule network (HACN) performs significantly better than existing methods in offensive classification with the precision of 94.37% and recall of 95.28%. We also explain what our model has learned with an explanation tool called Integrated Gradients. Meanwhile, our system's processing speed can handle each sentence in 10msec, suggesting the potential for efficient deployment in real situations.","task_annotation":["categorizing offensive language"],"method_annotation":["hierarchical attention capsule network","categorizing offensive language datset","integrated gradients"],"tasks":[["automatically identifying offensive language","offensive classification"],["categorizing offensive language in social networks"]],"methods":[["hierarchical attention","hierarchical attention capsule network"],["explainable tool","explanation tool"],["integrated gradients"],["capsule system"]],"task_unmatch_golden":[],"task_match_golden":["categorizing offensive language"],"task_match_predicted":["categorizing offensive language in social networks"],"task_unmatch_pred":["offensive classification","automatically identifying offensive language"],"task_match_group":[],"task_match_total":["categorizing offensive language","categorizing offensive language in social networks"],"method_unmatch_golden":["categorizing offensive language datset"],"method_match_golden":["integrated gradients","hierarchical attention capsule network"],"method_match_predicted":["integrated gradients","hierarchical attention"],"method_unmatch_pred":["explanation tool","capsule system","explainable tool"],"method_match_group":["hierarchical attention capsule network"],"method_match_total":["integrated gradients","hierarchical attention","hierarchical attention capsule network"],"title_clean":"Categorizing Offensive Language in Social Networks: A Chinese Corpus, Systems and an Explainable Tool","abstract_clean":"Recently, more and more data have been generated in the online world, filled with offensive language such as threats, swear words or straightforward insults. It is disgraceful for a progressive society, and then the question arises on how language resources and technologies can cope with this challenge. However, previous work only analyzes the problem as a whole but fails to detect particular types of offensive content in a more fine-grained way, mainly because of the lack of annotated data. In this work, we present a densely annotated data-set COLA (Categorizing Offensive LAnguage), consists of fine-grained insulting language, antisocial language and illegal language. We study different strategies for automatically identifying offensive language on COLA data. Further, we design a capsule system with hierarchical attention to aggregate and fully utilize information, which obtains a state-of-the-art result. Results from experiments prove that our hierarchical attention capsule network (HACN) performs significantly better than existing methods in offensive classification with the precision of 94.37% and recall of 95.28%. We also explain what our model has learned with an explanation tool called Integrated Gradients. Meanwhile, our system's processing speed can handle each sentence in 10msec, suggesting the potential for efficient deployment in real situations."},{"ID":"berzak-etal-2015-contrastive","text":"Contrastive Analysis with Predictive Power: Typology Driven Estimation of Grammatical Error Distributions in {ESL} This work examines the impact of crosslinguistic transfer on grammatical errors in English as Second Language (ESL) texts. Using a computational framework that formalizes the theory of Contrastive Analysis (CA), we demonstrate that language specific error distributions in ESL writing can be predicted from the typological properties of the native language and their relation to the typology of English. Our typology driven model enables to obtain accurate estimates of such distributions without access to any ESL data for the target languages. Furthermore, we present a strategy for adjusting our method to low-resource languages that lack typological documentation using a bootstrapping approach which approximates native language typology from ESL texts. Finally, we show that our framework is instrumental for linguistic inquiry seeking to identify first language factors that contribute to a wide range of difficulties in second language acquisition.","task_annotation":["contrastive analysis","typology driven estimation"],"method_annotation":["regression model","typology model","bootstrapping"],"tasks":[["esl","esl","esl"],["linguistic inquiry"],["second language acquisition"],["typology driven estimation of grammatical error distributions"],["crosslinguistic transfer"],["grammatical errors"]],"methods":[["contrastive analysis","theory of contrastive analysis"],["bootstrapping approach"],["computational framework"],["typology driven model"],["predictive power"]],"task_unmatch_golden":["contrastive analysis"],"task_match_golden":["typology driven estimation"],"task_match_predicted":["typology driven estimation of grammatical error distributions"],"task_unmatch_pred":["grammatical errors","crosslinguistic transfer","esl","linguistic inquiry","second language acquisition"],"task_match_group":[],"task_match_total":["typology driven estimation","typology driven estimation of grammatical error distributions"],"method_unmatch_golden":["regression model"],"method_match_golden":["bootstrapping","typology model"],"method_match_predicted":["typology driven model","bootstrapping approach"],"method_unmatch_pred":["predictive power","theory of contrastive analysis","contrastive analysis","computational framework"],"method_match_group":[],"method_match_total":["bootstrapping approach","bootstrapping","typology driven model","typology model"],"title_clean":"Contrastive Analysis with Predictive Power: Typology Driven Estimation of Grammatical Error Distributions in ESL","abstract_clean":"This work examines the impact of crosslinguistic transfer on grammatical errors in English as Second Language (ESL) texts. Using a computational framework that formalizes the theory of Contrastive Analysis (CA), we demonstrate that language specific error distributions in ESL writing can be predicted from the typological properties of the native language and their relation to the typology of English. Our typology driven model enables to obtain accurate estimates of such distributions without access to any ESL data for the target languages. Furthermore, we present a strategy for adjusting our method to low-resource languages that lack typological documentation using a bootstrapping approach which approximates native language typology from ESL texts. Finally, we show that our framework is instrumental for linguistic inquiry seeking to identify first language factors that contribute to a wide range of difficulties in second language acquisition."},{"ID":"sotnikova-etal-2021-analyzing","text":"Analyzing Stereotypes in Generative Text Inference Tasks Stereotypes are inferences drawn about people based on their demographic attributes, which may result in harms to users when a system is deployed. In generative language-inference tasks, given a premise, a model produces plausible hypotheses that follow either logically (natural language inference) or commonsensically (commonsense inference). Such tasks are therefore a fruitful setting in which to explore the degree to which NLP systems encode stereotypes. In our work, we study how stereotypes manifest when the potential targets of stereotypes are situated in real-life, neutral contexts. We collect human judgments on the presence of stereotypes in generated inferences, and compare how perceptions of stereotypes vary due to annotator positionality. Domain Target Categories Gender man, woman, non-binary person, trans man, trans woman, cis man, cis woman","task_annotation":["analyzing stereotypes"],"method_annotation":["annotation","human judgement"],"tasks":[["generative language inference tasks","language inference"],["generated inferences"],["nlp"],["analyzing stereotypes"],["generative text inference tasks"]],"methods":[["inference)"]],"task_unmatch_golden":[],"task_match_golden":["analyzing stereotypes"],"task_match_predicted":["analyzing stereotypes"],"task_unmatch_pred":["language inference","generative text inference tasks","generative language inference tasks","nlp","generated inferences"],"task_match_group":[],"task_match_total":["analyzing stereotypes"],"method_unmatch_golden":["human judgement","annotation"],"method_match_golden":[],"method_match_predicted":[],"method_unmatch_pred":["inference)"],"method_match_group":[],"method_match_total":[],"title_clean":"Analyzing Stereotypes in Generative Text Inference Tasks","abstract_clean":"Stereotypes are inferences drawn about people based on their demographic attributes, which may result in harms to users when a system is deployed. In generative language-inference tasks, given a premise, a model produces plausible hypotheses that follow either logically (natural language inference) or commonsensically (commonsense inference). Such tasks are therefore a fruitful setting in which to explore the degree to which NLP systems encode stereotypes. In our work, we study how stereotypes manifest when the potential targets of stereotypes are situated in real-life, neutral contexts. We collect human judgments on the presence of stereotypes in generated inferences, and compare how perceptions of stereotypes vary due to annotator positionality. Domain Target Categories Gender man, woman, non-binary person, trans man, trans woman, cis man, cis woman"},{"ID":"moore-etal-1997-commandtalk","text":"{C}ommand{T}alk: A Spoken-Language Interface for Battlefield Simulations CommandTalk is a spoken-language interface to battlefield simulations that allows the use of ordinary spoken English to create forces and control measures, assign missions to forces, modify missions during execution, and control simulation system functions. CommandTalk combines a number of separate components integrated through the use of the Open Agent Architecture, including the Nuance speech recognition system, the Gemini naturallanguage parsing and interpretation system, a contextual-interpretation modhle, a \"push-to-talk\" agent, the ModSAF battlefield simulator, and \"Start-It\" (a graphical processing-spawning agent). Com-mandTalk is installed at a number of Government and contractor sites, including NRaD and the Marine Corps Air Ground Combat Center. It is currently being extended to provide exercise-time control of all simulated U.S. forces in DARPA's STOW 97 demonstration. Put Checkpoint 1 at 937 965. Create a point called Checkpoint 2 at 930 960. Objective Alpha is 92 96. Charlie 4 5, at my command, advance in a column to Checkpoint 1. Next, proceed to Checkpoint 2. Then assault Objective Alpha. Charlie 4 5, move out. With the simulation under way, the user can exercise direct control over the simulated forces by giving commands such as the following for immediate execution: Charlie 4 5, speed up. Change formation to echelon right. Get in a line. Withdraw to Checkpoint 2. Examples of voice commands for controlling Mod-SAF system functions include the following: Show contour lines. Center on M1 platoon.","task_annotation":["spoken language interface"],"method_annotation":["nuance speech recognition system","gemini natural language parsing","contextual interpretation module"],"tasks":[["battlefield simulations","battlefield simulations"],["immediate execution"],["mod saf system functions"],["exercise time control"],["control simulation system functions"]],"methods":[["commandtalk","commandtalk","commandtalk"],["spoken language interface","spoken language interface"],["graphical processing spawning agent"],["com mandtalk"],["start it"],["nuance speech recognition system"],["open agent architecture"],["push to talk agent"],["contextual interpretation modhle"],["modsaf battlefield simulator"],["gemini naturallanguage parsing and interpretation system"]],"task_unmatch_golden":["spoken-language interface"],"task_match_golden":[],"task_match_predicted":[],"task_unmatch_pred":["exercise time control","control simulation system functions","mod saf system functions","battlefield simulations","immediate execution"],"task_match_group":[],"task_match_total":[],"method_unmatch_golden":[],"method_match_golden":["gemini natural language parsing","contextual-interpretation module","nuance speech recognition system"],"method_match_predicted":["contextual interpretation modhle","gemini naturallanguage parsing and interpretation system","nuance speech recognition system"],"method_unmatch_pred":["open agent architecture","com mandtalk","commandtalk","spoken language interface","push to talk agent","graphical processing spawning agent","start it","modsaf battlefield simulator"],"method_match_group":[],"method_match_total":["nuance speech recognition system","contextual interpretation modhle","contextual-interpretation module","gemini naturallanguage parsing and interpretation system","gemini natural language parsing"],"title_clean":"CommandTalk: A Spoken-Language Interface for Battlefield Simulations","abstract_clean":"CommandTalk is a spoken-language interface to battlefield simulations that allows the use of ordinary spoken English to create forces and control measures, assign missions to forces, modify missions during execution, and control simulation system functions. CommandTalk combines a number of separate components integrated through the use of the Open Agent Architecture, including the Nuance speech recognition system, the Gemini naturallanguage parsing and interpretation system, a contextual-interpretation modhle, a \"push-to-talk\" agent, the ModSAF battlefield simulator, and \"Start-It\" (a graphical processing-spawning agent). Com-mandTalk is installed at a number of Government and contractor sites, including NRaD and the Marine Corps Air Ground Combat Center. It is currently being extended to provide exercise-time control of all simulated U.S. forces in DARPA's STOW 97 demonstration. Put Checkpoint 1 at 937 965. Create a point called Checkpoint 2 at 930 960. Objective Alpha is 92 96. Charlie 4 5, at my command, advance in a column to Checkpoint 1. Next, proceed to Checkpoint 2. Then assault Objective Alpha. Charlie 4 5, move out. With the simulation under way, the user can exercise direct control over the simulated forces by giving commands such as the following for immediate execution: Charlie 4 5, speed up. Change formation to echelon right. Get in a line. Withdraw to Checkpoint 2. Examples of voice commands for controlling Mod-SAF system functions include the following: Show contour lines. Center on M1 platoon."},{"ID":"wandji-tchami-grabar-2014-towards","text":"Towards Automatic Distinction between Specialized and Non-Specialized Occurrences of Verbs in Medical Corpora The medical field gathers people of different social statuses, such as students, pharmacists, managers, biologists, nurses and mainly medical doctors and patients, who represent the main actors. Despite their different levels of expertise, these actors need to interact and understand each other but the communication is not always easy and effective. This paper describes a method for a contrastive automatic analysis of verbs in medical corpora, based on the semantic annotation of the verbs nominal co-occurents. The corpora used are specialized in cardiology and distinguished according to their levels of expertise (high and low). The semantic annotation of these corpora is performed by using an existing medical terminology. The results indicate that the same verbs occurring in the two corpora show different specialization levels, which are indicated by the words (nouns and adjectives derived from medical terms) they occur with.","task_annotation":["contrastive automatic analysis of verbs"],"method_annotation":["semantic annotation"],"tasks":[["automatic distinction","contrastive automatic analysis of verbs"],["semantic annotation","semantic annotation"]],"methods":[["semantic annotation"],["automatic analysis"],["medical terminology"]],"task_unmatch_golden":[],"task_match_golden":["contrastive automatic analysis of verbs"],"task_match_predicted":["contrastive automatic analysis of verbs"],"task_unmatch_pred":["semantic annotation"],"task_match_group":["automatic distinction"],"task_match_total":["contrastive automatic analysis of verbs"],"method_unmatch_golden":[],"method_match_golden":["semantic annotation"],"method_match_predicted":["semantic annotation"],"method_unmatch_pred":["automatic analysis","medical terminology"],"method_match_group":[],"method_match_total":["semantic annotation"],"title_clean":"Towards Automatic Distinction between Specialized and Non-Specialized Occurrences of Verbs in Medical Corpora","abstract_clean":"The medical field gathers people of different social statuses, such as students, pharmacists, managers, biologists, nurses and mainly medical doctors and patients, who represent the main actors. Despite their different levels of expertise, these actors need to interact and understand each other but the communication is not always easy and effective. This paper describes a method for a contrastive automatic analysis of verbs in medical corpora, based on the semantic annotation of the verbs nominal co-occurents. The corpora used are specialized in cardiology and distinguished according to their levels of expertise (high and low). The semantic annotation of these corpora is performed by using an existing medical terminology. The results indicate that the same verbs occurring in the two corpora show different specialization levels, which are indicated by the words (nouns and adjectives derived from medical terms) they occur with."},{"ID":"kiritchenko-cherry-2011-lexically","text":"Lexically-Triggered Hidden {M}arkov Models for Clinical Document Coding The automatic coding of clinical documents is an important task for today's healthcare providers. Though it can be viewed as multi-label document classification, the coding problem has the interesting property that most code assignments can be supported by a single phrase found in the input document. We propose a Lexically-Triggered Hidden Markov Model (LT-HMM) that leverages these phrases to improve coding accuracy. The LT-HMM works in two stages: first, a lexical match is performed against a term dictionary to collect a set of candidate codes for a document. Next, a discriminative HMM selects the best subset of codes to assign to the document by tagging candidates as present or absent. By confirming codes proposed by a dictionary, the LT-HMM can share features across codes, enabling strong performance even on rare codes. In fact, we are able to recover codes that do not occur in the training set at all. Our approach achieves the best ever performance on the 2007 Medical NLP Challenge test set, with an F-measure of 89.84.","task_annotation":["clinical document coding","document classification"],"method_annotation":[" lexically triggered hidden markov model"],"tasks":[["clinical document coding","automatic coding of clinical documents"],["multi label document classification"],["coding problem"]],"methods":[["lexically triggered hidden markov models","lexically triggered hidden markov model"],["lt hmm","lt hmm"],["discriminative hmm"],["lexical match"]],"task_unmatch_golden":[],"task_match_golden":["document classification","clinical document coding"],"task_match_predicted":["multi label document classification","clinical document coding"],"task_unmatch_pred":["coding problem"],"task_match_group":["automatic coding of clinical documents"],"task_match_total":["multi label document classification","document classification","clinical document coding"],"method_unmatch_golden":[],"method_match_golden":[" lexically-triggered hidden markov model"],"method_match_predicted":["lexically triggered hidden markov model"],"method_unmatch_pred":["lt hmm","lexical match","discriminative hmm"],"method_match_group":["lexically triggered hidden markov models"],"method_match_total":[" lexically-triggered hidden markov model","lexically triggered hidden markov model"],"title_clean":"Lexically-Triggered Hidden Markov Models for Clinical Document Coding","abstract_clean":"The automatic coding of clinical documents is an important task for today's healthcare providers. Though it can be viewed as multi-label document classification, the coding problem has the interesting property that most code assignments can be supported by a single phrase found in the input document. We propose a Lexically-Triggered Hidden Markov Model (LT-HMM) that leverages these phrases to improve coding accuracy. The LT-HMM works in two stages: first, a lexical match is performed against a term dictionary to collect a set of candidate codes for a document. Next, a discriminative HMM selects the best subset of codes to assign to the document by tagging candidates as present or absent. By confirming codes proposed by a dictionary, the LT-HMM can share features across codes, enabling strong performance even on rare codes. In fact, we are able to recover codes that do not occur in the training set at all. Our approach achieves the best ever performance on the 2007 Medical NLP Challenge test set, with an F-measure of 89.84."},{"ID":"harbusch-etal-2003-domain","text":"Domain-Specific Disambiguation for Typing with Ambiguous Keyboards ","task_annotation":["word disambiguation"],"method_annotation":["interpolated language model"],"tasks":[["typing"],["domain specific disambiguation"]],"methods":[["part-of-speech tagging"],["lemmatization"],["word sense disambiguation"]],"task_unmatch_golden":[],"task_match_golden":["word disambiguation"],"task_match_predicted":["domain specific disambiguation"],"task_unmatch_pred":["typing"],"task_match_group":[],"task_match_total":["word disambiguation","domain specific disambiguation"],"method_unmatch_golden":["interpolated language model"],"method_match_golden":[],"method_match_predicted":[],"method_unmatch_pred":["part-of-speech tagging","lemmatization","word sense disambiguation"],"method_match_group":[],"method_match_total":[],"title_clean":"Domain-Specific Disambiguation for Typing with Ambiguous Keyboards","abstract_clean":""},{"ID":"wang-etal-2019-bigodm","text":"{BIGODM} System in the Social Media Mining for Health Applications Shared Task 2019 In this study, we describe our methods to automatically classify Twitter posts conveying events of adverse drug reaction (ADR). Based on our previous experience in tackling the ADR classification task, we empirically applied the vote-based undersampling ensemble approach along with linear support vector machine (SVM) to develop our classifiers as part of our participation in ACL 2019 Social Media Mining for Health Applications (SMM4H) shared task 1. The best-performed model on the test sets were trained on a merged corpus consisting of the datasets released by SMM4H 2017 and 2019. By using VUE, the corpus was randomly under-sampled with 2:1 ratio between the negative and positive classes to create an ensemble using the linear kernel trained with features including bag-of-word, domain knowledge, negation and word embedding. The best performing model achieved an F-measure of 0.551 which is about 5% higher than the average F-scores of 16 teams.","task_annotation":["social media mining for health applications"],"method_annotation":["support vector machines","word embedding","linear kernel","bag of word","domain knowledge","negation"],"tasks":[["health applications","health applications"],["social media mining","social media mining"]],"methods":[["bigodm system","adr classification task","classifiers"],["linear support vector machine","linear kernel"],["vote based undersampling ensemble approach","ensemble"],["vue"],["word embedding"]],"task_unmatch_golden":[],"task_match_golden":["social media mining for health applications"],"task_match_predicted":["social media mining"],"task_unmatch_pred":["health applications"],"task_match_group":[],"task_match_total":["social media mining","social media mining for health applications"],"method_unmatch_golden":["bag-of-word","domain-knowledge","linear kernel","negation"],"method_match_golden":["support vector machines","word embedding"],"method_match_predicted":["word embedding","linear support vector machine"],"method_unmatch_pred":["classifiers","vote based undersampling ensemble approach","adr classification task","ensemble","vue","bigodm system"],"method_match_group":["linear kernel"],"method_match_total":["support vector machines","word embedding","linear support vector machine"],"title_clean":"BIGODM System in the Social Media Mining for Health Applications Shared Task 2019","abstract_clean":"In this study, we describe our methods to automatically classify Twitter posts conveying events of adverse drug reaction (ADR). Based on our previous experience in tackling the ADR classification task, we empirically applied the vote-based undersampling ensemble approach along with linear support vector machine (SVM) to develop our classifiers as part of our participation in ACL 2019 Social Media Mining for Health Applications (SMM4H) shared task 1. The best-performed model on the test sets were trained on a merged corpus consisting of the datasets released by SMM4H 2017 and 2019. By using VUE, the corpus was randomly under-sampled with 2:1 ratio between the negative and positive classes to create an ensemble using the linear kernel trained with features including bag-of-word, domain knowledge, negation and word embedding. The best performing model achieved an F-measure of 0.551 which is about 5% higher than the average F-scores of 16 teams."},{"ID":"del-tredici-fernandez-2020-words","text":"Words are the Window to the Soul: Language-based User Representations for Fake News Detection Cognitive and social traits of individuals are reflected in language use. Moreover, individuals who are prone to spread fake news online often share common traits. Building on these ideas, we introduce a model that creates representations of individuals on social media based only on the language they produce, and use them to detect fake news. We show that language-based user representations are beneficial for this task. We also present an extended analysis of the language of fake news spreaders, showing that its main features are mostly domain independent and consistent across two English datasets. Finally, we exploit the relation between language use and connections in the social graph to assess the presence of the Echo Chamber effect in our data.","task_annotation":["fake news detection"],"method_annotation":["user representation","convolutional neural networks"],"tasks":[["fake news detection","language of fake news spreaders"]],"methods":[["language based user representations","language based user representations"]],"task_unmatch_golden":[],"task_match_golden":["fake news detection"],"task_match_predicted":["fake news detection"],"task_unmatch_pred":[],"task_match_group":["language of fake news spreaders"],"task_match_total":["fake news detection"],"method_unmatch_golden":["convolutional neural networks"],"method_match_golden":["user representation"],"method_match_predicted":["language based user representations"],"method_unmatch_pred":[],"method_match_group":[],"method_match_total":["language based user representations","user representation"],"title_clean":"Words are the Window to the Soul: Language-based User Representations for Fake News Detection","abstract_clean":"Cognitive and social traits of individuals are reflected in language use. Moreover, individuals who are prone to spread fake news online often share common traits. Building on these ideas, we introduce a model that creates representations of individuals on social media based only on the language they produce, and use them to detect fake news. We show that language-based user representations are beneficial for this task. We also present an extended analysis of the language of fake news spreaders, showing that its main features are mostly domain independent and consistent across two English datasets. Finally, we exploit the relation between language use and connections in the social graph to assess the presence of the Echo Chamber effect in our data."},{"ID":"barreiro-cabral-2009-reescreve","text":"{R}e{E}screve: a Translator-friendly Multi-purpose Paraphrasing Software Tool ","task_annotation":["multi purpose paraphraser"],"method_annotation":["port4nooj","pre processing","lexicon grammar","multi purpose paraphrasing software tool"],"tasks":[["text generation"],["text summarization"],["text simplification"],["text normalization"]],"methods":[["translator friendly multi purpose paraphrasing software"],["reescreve"]],"task_unmatch_golden":["multi-purpose paraphraser"],"task_match_golden":[],"task_match_predicted":[],"task_unmatch_pred":["text simplification","text normalization","text generation","text summarization"],"task_match_group":[],"task_match_total":[],"method_unmatch_golden":["lexicon-grammar","pre-processing","port4nooj"],"method_match_golden":["multi-purpose paraphrasing software tool"],"method_match_predicted":["translator friendly multi purpose paraphrasing software"],"method_unmatch_pred":["reescreve"],"method_match_group":[],"method_match_total":["multi-purpose paraphrasing software tool","translator friendly multi purpose paraphrasing software"],"title_clean":"ReEscreve: a Translator-friendly Multi-purpose Paraphrasing Software Tool","abstract_clean":""},{"ID":"maegaard-etal-2008-medar","text":"{MEDAR}: Collaboration between {E}uropean and Mediterranean {A}rabic Partners to Support the Development of Language Technology for {A}rabic After the successful completion of the NEMLAR project 2003-2005, a new opportunity for a project was opened by the European Commission, and a group of largely the same partners is now executing the MEDAR project. MEDAR will be updating the surveys and BLARK for Arabic already made, and will then focus on machine translation (and other tools for translation) and information retrieval with a focus on language resources, tools and evaluation for these applications. A very important part of the MEDAR project is to reinforce and extend the NEMLAR network and to create a cooperation roadmap for Human Language Technologies for Arabic. It is expected that the cooperation roadmap will attract wide attention from other parties and that it can help create a larger platform for collaborative projects. Finally, the project will focus on dissemination of knowledge about existing resources and tools, as well as actors and activities; this will happen through newsletter, website and an international conference which will follow up on the Cairo conference of 2004. Dissemination to user communities will also be important, e.g. through participation in translators' conferences. The goal of these activities is to create a stronger and lasting collaboration between EU countries and Arabic speaking countries.","task_annotation":["machine translation","information retrieval"],"method_annotation":["surveys","questionnaires"],"tasks":[["language technology","human language technologies"],["machine translation","translation"],["arabic","arabic","arabic"],["cooperation roadmap"],["medar project"],["information retrieval"],["dissemination"]],"methods":[["medar","medar"],["nemlar network"]],"task_unmatch_golden":[],"task_match_golden":["information retrieval","machine translation"],"task_match_predicted":["information retrieval","machine translation"],"task_unmatch_pred":["human language technologies","dissemination","arabic","medar project","language technology","cooperation roadmap"],"task_match_group":["translation"],"task_match_total":["information retrieval","machine translation"],"method_unmatch_golden":["surveys","questionnaires"],"method_match_golden":[],"method_match_predicted":[],"method_unmatch_pred":["nemlar network","medar"],"method_match_group":[],"method_match_total":[],"title_clean":"MEDAR: Collaboration between European and Mediterranean Arabic Partners to Support the Development of Language Technology for Arabic","abstract_clean":"After the successful completion of the NEMLAR project 2003-2005, a new opportunity for a project was opened by the European Commission, and a group of largely the same partners is now executing the MEDAR project. MEDAR will be updating the surveys and BLARK for Arabic already made, and will then focus on machine translation (and other tools for translation) and information retrieval with a focus on language resources, tools and evaluation for these applications. A very important part of the MEDAR project is to reinforce and extend the NEMLAR network and to create a cooperation roadmap for Human Language Technologies for Arabic. It is expected that the cooperation roadmap will attract wide attention from other parties and that it can help create a larger platform for collaborative projects. Finally, the project will focus on dissemination of knowledge about existing resources and tools, as well as actors and activities; this will happen through newsletter, website and an international conference which will follow up on the Cairo conference of 2004. Dissemination to user communities will also be important, e.g. through participation in translators' conferences. The goal of these activities is to create a stronger and lasting collaboration between EU countries and Arabic speaking countries."},{"ID":"muti-barron-cedeno-2022-checkpoint","text":"A Checkpoint on Multilingual Misogyny Identification We address the problem of identifying misogyny in tweets in mono and multilingual settings in three languages: English, Italian and Spanish. We explore model variations considering single and multiple languages both in the pre-training of the transformer and in the training of the downstream task to explore the feasibility of detecting misogyny through a transfer learning approach across multiple languages. That is, we train monolingual transformers with monolingual data and multilingual transformers with both monolingual and multilingual data. Our models reach state-of-the-art performance on all three languages. The single-language BERT models perform the best, closely followed by different configurations of multilingual BERT models. The performance drops in zero-shot classification across languages. Our error analysis shows that multilingual and monolingual models tend to make the same mistakes.","task_annotation":["misogyny identification"],"method_annotation":["transformers","bert"],"tasks":[["identifying misogyny in tweets","detecting misogyny"],["zero shot classification"],["downstream task"],["pre training"],["transformer"],["multilingual misogyny identification"]],"methods":[["multilingual bert models","multilingual and monolingual models"],["monolingual transformers","multilingual transformers"],["single language bert models"],["error analysis"],["transfer learning approach"]],"task_unmatch_golden":[],"task_match_golden":["misogyny identification"],"task_match_predicted":["multilingual misogyny identification"],"task_unmatch_pred":["pre training","zero shot classification","identifying misogyny in tweets","transformer","detecting misogyny","downstream task"],"task_match_group":[],"task_match_total":["multilingual misogyny identification","misogyny identification"],"method_unmatch_golden":[],"method_match_golden":["transformers","bert"],"method_match_predicted":["monolingual transformers","single language bert models"],"method_unmatch_pred":["error analysis","multilingual and monolingual models","multilingual bert models","transfer learning approach"],"method_match_group":["multilingual transformers"],"method_match_total":["monolingual transformers","transformers","single language bert models","bert"],"title_clean":"A Checkpoint on Multilingual Misogyny Identification","abstract_clean":"We address the problem of identifying misogyny in tweets in mono and multilingual settings in three languages: English, Italian and Spanish. We explore model variations considering single and multiple languages both in the pre-training of the transformer and in the training of the downstream task to explore the feasibility of detecting misogyny through a transfer learning approach across multiple languages. That is, we train monolingual transformers with monolingual data and multilingual transformers with both monolingual and multilingual data. Our models reach state-of-the-art performance on all three languages. The single-language BERT models perform the best, closely followed by different configurations of multilingual BERT models. The performance drops in zero-shot classification across languages. Our error analysis shows that multilingual and monolingual models tend to make the same mistakes."},{"ID":"soh-etal-2019-legal","text":"Legal Area Classification: A Comparative Study of Text Classifiers on {S}ingapore {S}upreme {C}ourt Judgments This paper conducts a comparative study on the performance of various machine learning (\"ML\") approaches for classifying judgments into legal areas. Using a novel dataset of 6,227 Singapore Supreme Court judgments, we investigate how state-of-the-art NLP methods compare against traditional statistical models when applied to a legal corpus that comprised few but lengthy documents. All approaches tested, including topic model, word embedding, and language model-based classifiers, performed well with as little as a few hundred judgments. However, more work needs to be done to optimize state-of-the-art methods for the legal domain.","task_annotation":["legal area classification"],"method_annotation":["topic model","word embedding","language model based classifiers"],"tasks":[["legal area classification","classifying judgments into legal areas"]],"methods":[["text classifiers","language model based classifiers"],["word embedding"],["topic model"],["statistical models"],["nlp methods"],["machine learning ml approaches"]],"task_unmatch_golden":[],"task_match_golden":["legal area classification"],"task_match_predicted":["legal area classification"],"task_unmatch_pred":[],"task_match_group":["classifying judgments into legal areas"],"task_match_total":["legal area classification"],"method_unmatch_golden":[],"method_match_golden":["language model-based classifiers","topic model","word embedding"],"method_match_predicted":["language model based classifiers","statistical models","word embedding"],"method_unmatch_pred":["machine learning ml approaches","nlp methods","topic model"],"method_match_group":["text classifiers"],"method_match_total":["language model-based classifiers","word embedding","language model based classifiers","statistical models","topic model"],"title_clean":"Legal Area Classification: A Comparative Study of Text Classifiers on Singapore Supreme Court Judgments","abstract_clean":"This paper conducts a comparative study on the performance of various machine learning (\"ML\") approaches for classifying judgments into legal areas. Using a novel dataset of 6,227 Singapore Supreme Court judgments, we investigate how state-of-the-art NLP methods compare against traditional statistical models when applied to a legal corpus that comprised few but lengthy documents. All approaches tested, including topic model, word embedding, and language model-based classifiers, performed well with as little as a few hundred judgments. However, more work needs to be done to optimize state-of-the-art methods for the legal domain."},{"ID":"shreevastava-foltz-2021-detecting","text":"Detecting Cognitive Distortions from Patient-Therapist Interactions An important part of Cognitive Behavioral Therapy (CBT) is to recognize and restructure certain negative thinking patterns that are also known as cognitive distortions. This project aims to detect these distortions using natural language processing. We compare and contrast different types of linguistic features as well as different classification algorithms and explore the limitations of applying these techniques on a small dataset. We find that pretrained Sentence-BERT embeddings to train an SVM classifier yields the best results with an F1-score of 0.79. Lastly, we discuss how this work provides insights into the types of linguistic features that are inherent in cognitive distortions.","task_annotation":["cognitive distortion detection"],"method_annotation":["pretrained sentence bert embeddings","svm classifier"],"tasks":[["detecting cognitive distortions","cognitive behavioral therapy"]],"methods":[["svm classifier"],["natural language processing","classification algorithms"]],"task_unmatch_golden":[],"task_match_golden":["cognitive distortion detection"],"task_match_predicted":["detecting cognitive distortions"],"task_unmatch_pred":[],"task_match_group":["cognitive behavioral therapy"],"task_match_total":["detecting cognitive distortions","cognitive distortion detection"],"method_unmatch_golden":["pretrained sentence-bert embeddings"],"method_match_golden":["svm classifier"],"method_match_predicted":["svm classifier"],"method_unmatch_pred":["natural language processing","classification algorithms"],"method_match_group":[],"method_match_total":["svm classifier"],"title_clean":"Detecting Cognitive Distortions from Patient-Therapist Interactions","abstract_clean":"An important part of Cognitive Behavioral Therapy (CBT) is to recognize and restructure certain negative thinking patterns that are also known as cognitive distortions. This project aims to detect these distortions using natural language processing. We compare and contrast different types of linguistic features as well as different classification algorithms and explore the limitations of applying these techniques on a small dataset. We find that pretrained Sentence-BERT embeddings to train an SVM classifier yields the best results with an F1-score of 0.79. Lastly, we discuss how this work provides insights into the types of linguistic features that are inherent in cognitive distortions."},{"ID":"huang-bai-2021-team","text":"{TEAM} {HUB}@{LT}-{EDI}-{EACL}2021: Hope Speech Detection Based On Pre-trained Language Model This article introduces the system description of TEAM HUB team participating in LT-EDI 2021: Hope Speech Detection. This shared task is the first task related to the desired voice detection. The data set in the shared task consists of three different languages (English, Tamil, and Malayalam). The task type is text classification. Based on the analysis and understanding of the task description and data set, we designed a system based on a pre-trained language model to complete this shared task. In this system, we use methods and models that combine the XLM-RoBERTa pre-trained language model and the Tf-Idf algorithm. In the final result ranking announced by the task organizer, our system obtained F1 scores of 0.93, 0.84, 0.59 on the English dataset, Malayalam dataset, and Tamil dataset. Our submission results are ranked 1, 2, and 3 respectively.","task_annotation":["hope speech detection","text classification"],"method_annotation":["language model","xlm roberta","tf idf"],"tasks":[["hope speech detection","hope speech detection","voice detection"],["text classification"],["lt edi 2021"]],"methods":[["pre trained language model","xlm roberta pre trained language model"],["language model"],["tf idf algorithm"]],"task_unmatch_golden":[],"task_match_golden":["text classification","hope speech detection"],"task_match_predicted":["text classification","hope speech detection"],"task_unmatch_pred":["lt edi 2021"],"task_match_group":["voice detection"],"task_match_total":["text classification","hope speech detection"],"method_unmatch_golden":[],"method_match_golden":["xlm-roberta","tf-idf","language model"],"method_match_predicted":["xlm roberta pre trained language model","language model","tf idf algorithm"],"method_unmatch_pred":[],"method_match_group":["pre trained language model"],"method_match_total":["xlm-roberta","xlm roberta pre trained language model","tf-idf","language model","tf idf algorithm"],"title_clean":"TEAM HUB@LT-EDI-EACL2021: Hope Speech Detection Based On Pre-trained Language Model","abstract_clean":"This article introduces the system description of TEAM HUB team participating in LT-EDI 2021: Hope Speech Detection. This shared task is the first task related to the desired voice detection. The data set in the shared task consists of three different languages (English, Tamil, and Malayalam). The task type is text classification. Based on the analysis and understanding of the task description and data set, we designed a system based on a pre-trained language model to complete this shared task. In this system, we use methods and models that combine the XLM-RoBERTa pre-trained language model and the Tf-Idf algorithm. In the final result ranking announced by the task organizer, our system obtained F1 scores of 0.93, 0.84, 0.59 on the English dataset, Malayalam dataset, and Tamil dataset. Our submission results are ranked 1, 2, and 3 respectively."},{"ID":"lee-etal-2021-unifying","text":"On Unifying Misinformation Detection In this paper, we introduce UNIFIEDM2, a general-purpose misinformation model that jointly models multiple domains of misinformation with a single, unified setup. The model is trained to handle four tasks: detecting news bias, clickbait, fake news and verifying rumors. By grouping these tasks together, UNIFIEDM2 learns a richer representation of misinformation, which leads to stateof-the-art or comparable performance across all tasks. Furthermore, we demonstrate that UNIFIEDM2's learned representation is helpful for few-shot learning of unseen misinformation tasks\/datasets and model's generalizability to unseen events. * Work partially done while interning at Facebook AI. \u2020 Work partially done while working at Facebook AI.","task_annotation":["misinformation detection","detecting news bias","verifying rumors"],"method_annotation":["few shot learning","unifiedm2"],"tasks":[["few shot learning of unseen misinformation tasksdatasets"],["detecting news bias"],["unifying misinformation detection"]],"methods":[["unifiedm2","unifiedm2","unifiedm2s learned representation"],["general purpose misinformation model"]],"task_unmatch_golden":["verifying rumors"],"task_match_golden":["misinformation detection","detecting news bias"],"task_match_predicted":["unifying misinformation detection","detecting news bias"],"task_unmatch_pred":["few shot learning of unseen misinformation tasksdatasets"],"task_match_group":[],"task_match_total":["unifying misinformation detection","misinformation detection","detecting news bias"],"method_unmatch_golden":["few-shot learning"],"method_match_golden":["unifiedm2"],"method_match_predicted":["unifiedm2"],"method_unmatch_pred":["general purpose misinformation model"],"method_match_group":["unifiedm2s learned representation"],"method_match_total":["unifiedm2"],"title_clean":"On Unifying Misinformation Detection","abstract_clean":"In this paper, we introduce UNIFIEDM2, a general-purpose misinformation model that jointly models multiple domains of misinformation with a single, unified setup. The model is trained to handle four tasks: detecting news bias, clickbait, fake news and verifying rumors. By grouping these tasks together, UNIFIEDM2 learns a richer representation of misinformation, which leads to stateof-the-art or comparable performance across all tasks. Furthermore, we demonstrate that UNIFIEDM2's learned representation is helpful for few-shot learning of unseen misinformation tasks\/datasets and model's generalizability to unseen events. * Work partially done while interning at Facebook AI. \u2020 Work partially done while working at Facebook AI."},{"ID":"fischer-1979-powerful","text":"Powerful ideas in computational linquistics - Implications for problem solving, and education ","task_annotation":["ideas in computational linguistics"],"method_annotation":["cognitive science","education"],"tasks":[["problem solving"],["computational linquistics"]],"methods":[["tokenization"],["part-of-speech tagging"],["parsing"]],"task_unmatch_golden":[],"task_match_golden":["ideas in computational linguistics"],"task_match_predicted":["computational linquistics"],"task_unmatch_pred":["problem solving"],"task_match_group":[],"task_match_total":["computational linquistics","ideas in computational linguistics"],"method_unmatch_golden":["cognitive science","education"],"method_match_golden":[],"method_match_predicted":[],"method_unmatch_pred":["part-of-speech tagging","parsing","tokenization"],"method_match_group":[],"method_match_total":[],"title_clean":"Powerful ideas in computational linquistics - Implications for problem solving, and education","abstract_clean":""},{"ID":"oard-2007-invited","text":"Invited Talk: Lessons from the {MALACH} Project: Applying New Technologies to Improve Intellectual Access to Large Oral History Collections In this talk I will describe the goals of the MALACH project (Multilingual Access to Large Spoken Archives) and our research results. I'll begin by describing the unique characteristics of the oral history collection that we used, in which Holocaust survivors, witnesses and rescuers were interviewed in several languages. Each interview has been digitized and extensively catalogued by subject matter experts, thus producing a remarkably rich collection for the application of machine learning techniques. Automatic speech recognition techniques originally developed for the domain of conversational telephone speech were adapted to process these materials with word error rates that are adequate to provide useful features to support interactive search and automated clustering, boundary detection, and topic classification tasks. As I describe our results, I will focus particularly on the evaluation methods that that we have used to assess the potential utility of this technology. I'll conclude with some remarks about possible future directions for research on applying new technologies to improve intellectual access to oral history and other spoken word collections.","task_annotation":["speech archival","intellectual access to large oral history collections","topic classification","boundary detection"],"method_annotation":["automated clustering","automatic speech recognition techniques"],"tasks":[["topic classification tasks"],["boundary detection"],["automated clustering"],["interactive search"]],"methods":[["malach project","malach project"],["automatic speech recognition techniques"],["machine learning techniques"]],"task_unmatch_golden":["speech archival","intellectual access to large oral history collections"],"task_match_golden":["boundary detection","topic classification"],"task_match_predicted":["boundary detection","topic classification tasks"],"task_unmatch_pred":["interactive search","automated clustering"],"task_match_group":[],"task_match_total":["boundary detection","topic classification","topic classification tasks"],"method_unmatch_golden":["automated clustering"],"method_match_golden":["automatic speech recognition techniques"],"method_match_predicted":["automatic speech recognition techniques"],"method_unmatch_pred":["malach project","machine learning techniques"],"method_match_group":[],"method_match_total":["automatic speech recognition techniques"],"title_clean":"Invited Talk: Lessons from the MALACH Project: Applying New Technologies to Improve Intellectual Access to Large Oral History Collections","abstract_clean":"In this talk I will describe the goals of the MALACH project (Multilingual Access to Large Spoken Archives) and our research results. I'll begin by describing the unique characteristics of the oral history collection that we used, in which Holocaust survivors, witnesses and rescuers were interviewed in several languages. Each interview has been digitized and extensively catalogued by subject matter experts, thus producing a remarkably rich collection for the application of machine learning techniques. Automatic speech recognition techniques originally developed for the domain of conversational telephone speech were adapted to process these materials with word error rates that are adequate to provide useful features to support interactive search and automated clustering, boundary detection, and topic classification tasks. As I describe our results, I will focus particularly on the evaluation methods that that we have used to assess the potential utility of this technology. I'll conclude with some remarks about possible future directions for research on applying new technologies to improve intellectual access to oral history and other spoken word collections."},{"ID":"rehm-etal-2019-developing","text":"Developing and Orchestrating a Portfolio of Natural Legal Language Processing and Document Curation Services We present a portfolio of natural legal language processing and document curation services currently under development in a collaborative European project. First, we give an overview of the project and the different use cases, while, in the main part of the article, we focus upon the 13 different processing services that are being deployed in different prototype applications using a flexible and scalable microservices architecture. Their orchestration is operationalised using a content and document curation workflow manager.","task_annotation":["natural legal language processing and document curation"],"method_annotation":["microservices architecture","content and document curation workflow manager"],"tasks":[["document curation services","document curation services"],["natural legal language processing","natural legal language processing"]],"methods":[["microservices architecture"],["processing services"],["content and document curation workflow manager"]],"task_unmatch_golden":[],"task_match_golden":["natural legal language processing and document curation"],"task_match_predicted":["natural legal language processing"],"task_unmatch_pred":["document curation services"],"task_match_group":[],"task_match_total":["natural legal language processing and document curation","natural legal language processing"],"method_unmatch_golden":[],"method_match_golden":["microservices architecture","content and document curation workflow manager"],"method_match_predicted":["microservices architecture","content and document curation workflow manager"],"method_unmatch_pred":["processing services"],"method_match_group":[],"method_match_total":["microservices architecture","content and document curation workflow manager"],"title_clean":"Developing and Orchestrating a Portfolio of Natural Legal Language Processing and Document Curation Services","abstract_clean":"We present a portfolio of natural legal language processing and document curation services currently under development in a collaborative European project. First, we give an overview of the project and the different use cases, while, in the main part of the article, we focus upon the 13 different processing services that are being deployed in different prototype applications using a flexible and scalable microservices architecture. Their orchestration is operationalised using a content and document curation workflow manager."},{"ID":"polat-saraclar-2020-unsupervised","text":"Unsupervised Term Discovery for Continuous Sign Language Most of the sign language recognition (SLR) systems rely on supervision for training and available annotated sign language resources are scarce due to the difficulties of manual labeling. Unsupervised discovery of lexical units would facilitate the annotation process and thus lead to better SLR systems. Inspired by the unsupervised spoken term discovery in speech processing field, we investigate whether a similar approach can be applied in sign language to discover repeating lexical units. We adapt an algorithm that is designed for spoken term discovery by using hand shape and pose features instead of speech features. The experiments are run on a large scale continuous sign corpus and the performance is evaluated using gloss level annotations. This work introduces a new task for sign language processing that has not been addressed before.","task_annotation":["sign language recognition"],"method_annotation":["unsupervised term discovery","hand shape and pose features"],"tasks":[["unsupervised term discovery","unsupervised spoken term discovery","spoken term discovery"],["continuous sign language","sign language recognition","slr"],["sign language processing"],["unsupervised discovery of lexical units"],["annotation process"],["manual labeling"],["speech processing field"]],"methods":[["unsupervised learning"],["spoken term discovery"],["hand shape and pose features"]],"task_unmatch_golden":[],"task_match_golden":["sign language recognition"],"task_match_predicted":["sign language recognition"],"task_unmatch_pred":["manual labeling","speech processing field","unsupervised spoken term discovery","unsupervised discovery of lexical units","spoken term discovery","sign language processing","unsupervised term discovery","annotation process"],"task_match_group":["slr","continuous sign language"],"task_match_total":["sign language recognition"],"method_unmatch_golden":[],"method_match_golden":["unsupervised term discovery","hand shape and pose features"],"method_match_predicted":["spoken term discovery","hand shape and pose features"],"method_unmatch_pred":["unsupervised learning"],"method_match_group":[],"method_match_total":["spoken term discovery","unsupervised term discovery","hand shape and pose features"],"title_clean":"Unsupervised Term Discovery for Continuous Sign Language","abstract_clean":"Most of the sign language recognition (SLR) systems rely on supervision for training and available annotated sign language resources are scarce due to the difficulties of manual labeling. Unsupervised discovery of lexical units would facilitate the annotation process and thus lead to better SLR systems. Inspired by the unsupervised spoken term discovery in speech processing field, we investigate whether a similar approach can be applied in sign language to discover repeating lexical units. We adapt an algorithm that is designed for spoken term discovery by using hand shape and pose features instead of speech features. The experiments are run on a large scale continuous sign corpus and the performance is evaluated using gloss level annotations. This work introduces a new task for sign language processing that has not been addressed before."},{"ID":"kirk-etal-2021-memes","text":"Memes in the Wild: Assessing the Generalizability of the Hateful Memes Challenge Dataset Hateful memes pose a unique challenge for current machine learning systems because their message is derived from both text-and visual-modalities. To this effect, Facebook released the Hateful Memes Challenge, a dataset of memes with pre-extracted text captions, but it is unclear whether these synthetic examples generalize to 'memes in the wild'. In this paper, we collect hateful and non-hateful memes from Pinterest to evaluate out-of-sample performance on models pre-trained on the Facebook dataset. We find that memes in the wild differ in two key aspects: 1) Captions must be extracted via OCR, injecting noise and diminishing performance of multimodal models, and 2) Memes are more diverse than 'traditional memes', including screenshots of conversations or text on a plain background. This paper thus serves as a reality check for the current benchmark of hateful meme detection and its applicability for detecting real world hate.","task_annotation":["assessing dataset generalizability","hateful memes challenge","hateful meme detection"],"method_annotation":["tesseract","easyocr","naive bayes","clip model"],"tasks":[["hateful meme detection","detecting real world hate"]],"methods":[["multimodal models"],["ocr"],["machine learning systems"]],"task_unmatch_golden":["hateful memes challenge","assessing dataset generalizability"],"task_match_golden":["hateful meme detection"],"task_match_predicted":["hateful meme detection"],"task_unmatch_pred":[],"task_match_group":["detecting real world hate"],"task_match_total":["hateful meme detection"],"method_unmatch_golden":["clip model","naive-bayes","tesseract"],"method_match_golden":["easyocr"],"method_match_predicted":["ocr"],"method_unmatch_pred":["machine learning systems","multimodal models"],"method_match_group":[],"method_match_total":["easyocr","ocr"],"title_clean":"Memes in the Wild: Assessing the Generalizability of the Hateful Memes Challenge Dataset","abstract_clean":"Hateful memes pose a unique challenge for current machine learning systems because their message is derived from both text-and visual-modalities. To this effect, Facebook released the Hateful Memes Challenge, a dataset of memes with pre-extracted text captions, but it is unclear whether these synthetic examples generalize to 'memes in the wild'. In this paper, we collect hateful and non-hateful memes from Pinterest to evaluate out-of-sample performance on models pre-trained on the Facebook dataset. We find that memes in the wild differ in two key aspects: 1) Captions must be extracted via OCR, injecting noise and diminishing performance of multimodal models, and 2) Memes are more diverse than 'traditional memes', including screenshots of conversations or text on a plain background. This paper thus serves as a reality check for the current benchmark of hateful meme detection and its applicability for detecting real world hate."},{"ID":"kim-park-2015-statistical","text":"A Statistical Modeling of the Correlation between Island Effects and Working-memory Capacity for {L}2 Learners The cause of island effects has evoked considerable debate within syntax and other fields of linguistics. The two competing approaches stand out: the grammatical analysis","task_annotation":["statistical modeling"],"method_annotation":["grammatical analysis","working memory based processing"],"tasks":[["linguistics"]],"methods":[["working memory capacity","wm"],["l2 learners"],["working memory wm based processing analysis","wm based processing analysis"],["statistical modeling"],["grammatical analysis"]],"task_unmatch_golden":["statistical modeling"],"task_match_golden":[],"task_match_predicted":[],"task_unmatch_pred":["linguistics"],"task_match_group":[],"task_match_total":[],"method_unmatch_golden":[],"method_match_golden":["working-memory-based processing","grammatical analysis"],"method_match_predicted":["working memory wm based processing analysis","grammatical analysis"],"method_unmatch_pred":["wm","statistical modeling","l2 learners","working memory capacity"],"method_match_group":["wm based processing analysis"],"method_match_total":["working-memory-based processing","working memory wm based processing analysis","grammatical analysis"],"title_clean":"A Statistical Modeling of the Correlation between Island Effects and Working-memory Capacity for L2 Learners","abstract_clean":"The cause of island effects has evoked considerable debate within syntax and other fields of linguistics. The two competing approaches stand out: the grammatical analysis; and the working-memory (WM)-based processing analysis. In this paper we report three experiments designed to test one of the premises of the WM-based processing analysis: that the strength of island effects should vary as a function of individual differences in WM capacity. The results show that island effects present even for L2 learners are more likely attributed to grammatical constraints than to limited processing resources."},{"ID":"tanaka-etal-2014-linguistic","text":"Linguistic and Acoustic Features for Automatic Identification of Autism Spectrum Disorders in Children{'}s Narrative Autism spectrum disorders are developmental disorders characterised as deficits in social and communication skills, and they affect both verbal and non-verbal communication. Previous works measured differences in children with and without autism spectrum disorders in terms of linguistic and acoustic features, although they do not mention automatic identification using integration of these features. In this paper, we perform an exploratory study of several language and speech features of both single utterances and full narratives. We find that there are characteristic differences between children with autism spectrum disorders and typical development with respect to word categories, prosody, and voice quality, and that these differences can be used in automatic classifiers. We also examine the differences between American and Japanese children and find significant differences with regards to pauses before new turns and linguistic cues.","task_annotation":["automatic identification of autism spectrum disorders"],"method_annotation":["linguistic features","acoustic features"],"tasks":[["automatic identification of autism spectrum disorders","automatic identification"],["automatic classifiers"],["verbal and non verbal communication"]],"methods":[["prosody"],["automatic classification"],["linguistic cues"]],"task_unmatch_golden":[],"task_match_golden":["automatic identification of autism spectrum disorders"],"task_match_predicted":["automatic identification of autism spectrum disorders"],"task_unmatch_pred":["automatic classifiers","verbal and non verbal communication"],"task_match_group":["automatic identification"],"task_match_total":["automatic identification of autism spectrum disorders"],"method_unmatch_golden":["acoustic features"],"method_match_golden":["linguistic features"],"method_match_predicted":["linguistic cues"],"method_unmatch_pred":["prosody","automatic classification"],"method_match_group":[],"method_match_total":["linguistic features","linguistic cues"],"title_clean":"Linguistic and Acoustic Features for Automatic Identification of Autism Spectrum Disorders in Children's Narrative","abstract_clean":"Autism spectrum disorders are developmental disorders characterised as deficits in social and communication skills, and they affect both verbal and non-verbal communication. Previous works measured differences in children with and without autism spectrum disorders in terms of linguistic and acoustic features, although they do not mention automatic identification using integration of these features. In this paper, we perform an exploratory study of several language and speech features of both single utterances and full narratives. We find that there are characteristic differences between children with autism spectrum disorders and typical development with respect to word categories, prosody, and voice quality, and that these differences can be used in automatic classifiers. We also examine the differences between American and Japanese children and find significant differences with regards to pauses before new turns and linguistic cues."},{"ID":"finlayson-etal-2014-n2","text":"The N2 corpus: A semantically annotated collection of Islamist extremist stories We describe the N2 (Narrative Networks) Corpus, a new language resource. The corpus is unique in three important ways. First, every text in the corpus is a story, which is in contrast to other language resources that may contain stories or story-like texts, but are not specifically curated to contain only stories. Second, the unifying theme of the corpus is material relevant to Islamist Extremists, having been produced by or often referenced by them. Third, every text in the corpus has been annotated for 14 layers of syntax and semantics, including: referring expressions and co-reference; events, time expressions, and temporal relationships; semantic roles; and word senses. In cases where analyzers were not available to do high-quality automatic annotations, layers were manually doubleannotated and adjudicated by trained annotators. The corpus comprises 100 texts and 42,480 words. Most of the texts were originally in Arabic but all are provided in English translation. We explain the motivation for constructing the corpus, the process for selecting the texts, the detailed contents of the corpus itself, the rationale behind the choice of annotation layers, and the annotation procedure.","task_annotation":["corpus"],"method_annotation":["multi layed annotation","annotation procedure"],"tasks":[["annotation procedure"]],"methods":[["syntax"],["semantics"],["co-reference"],["events"],["time expressions"],["temporal relationships"],["semantic roles"],["word senses"]],"task_unmatch_golden":["corpus"],"task_match_golden":[],"task_match_predicted":[],"task_unmatch_pred":["annotation procedure"],"task_match_group":[],"task_match_total":[],"method_unmatch_golden":["multi-layed annotation","annotation procedure"],"method_match_golden":[],"method_match_predicted":[],"method_unmatch_pred":["temporal relationships","co-reference","time expressions","word senses","semantic roles","syntax","events","semantics"],"method_match_group":[],"method_match_total":[],"title_clean":"The N2 corpus: A semantically annotated collection of Islamist extremist stories","abstract_clean":"We describe the N2 (Narrative Networks) Corpus, a new language resource. The corpus is unique in three important ways. First, every text in the corpus is a story, which is in contrast to other language resources that may contain stories or story-like texts, but are not specifically curated to contain only stories. Second, the unifying theme of the corpus is material relevant to Islamist Extremists, having been produced by or often referenced by them. Third, every text in the corpus has been annotated for 14 layers of syntax and semantics, including: referring expressions and co-reference; events, time expressions, and temporal relationships; semantic roles; and word senses. In cases where analyzers were not available to do high-quality automatic annotations, layers were manually doubleannotated and adjudicated by trained annotators. The corpus comprises 100 texts and 42,480 words. Most of the texts were originally in Arabic but all are provided in English translation. We explain the motivation for constructing the corpus, the process for selecting the texts, the detailed contents of the corpus itself, the rationale behind the choice of annotation layers, and the annotation procedure."},{"ID":"du-etal-2019-extracting","text":"Extracting Symptoms and their Status from Clinical Conversations This paper describes novel models tailored for a new application, that of extracting the symptoms mentioned in clinical conversations along with their status. Lack of any publicly available corpus in this privacy-sensitive domain led us to develop our own corpus, consisting of about 3K conversations annotated by professional medical scribes. We propose two novel deep learning approaches to infer the symptom names and their status: (1) a new hierarchical span-attribute tagging (SA-T) model, trained using curriculum learning, and (2) a variant of sequence-to-sequence model which decodes the symptoms and their status from a few speaker turns within a sliding window over the conversation. This task stems from a realistic application of assisting medical providers in capturing symptoms mentioned by patients from their clinical conversations. To reflect this application, we define multiple metrics. From inter-rater agreement, we find that the task is inherently difficult. We conduct comprehensive evaluations on several contrasting conditions and observe that the performance of the models range from an F-score of 0.5 to 0.8 depending on the condition. Our analysis not only reveals the inherent challenges of the task, but also provides useful directions to improve the models.","task_annotation":["symptom extraction from clinical conversations","extracting symptoms"],"method_annotation":["span attribute tagging","curriculum learning","sequence to sequence","hierarchical span attribute tagging (sa t) model"],"tasks":[["medical providers"]],"methods":[["extracting symptoms","hierarchical span attribute tagging sa t model"],["deep learning approaches","curriculum learning"],["sequence to sequence model"]],"task_unmatch_golden":["extracting symptoms","symptom extraction from clinical conversations"],"task_match_golden":[],"task_match_predicted":[],"task_unmatch_pred":["medical providers"],"task_match_group":[],"task_match_total":[],"method_unmatch_golden":["hierarchical span-attribute tagging (sa-t) model"],"method_match_golden":["span-attribute tagging","curriculum learning","sequence-to-sequence"],"method_match_predicted":["curriculum learning","sequence to sequence model","hierarchical span attribute tagging sa t model"],"method_unmatch_pred":[],"method_match_group":["deep learning approaches","extracting symptoms"],"method_match_total":["span-attribute tagging","sequence to sequence model","sequence-to-sequence","hierarchical span attribute tagging sa t model","curriculum learning"],"title_clean":"Extracting Symptoms and their Status from Clinical Conversations","abstract_clean":"This paper describes novel models tailored for a new application, that of extracting the symptoms mentioned in clinical conversations along with their status. Lack of any publicly available corpus in this privacy-sensitive domain led us to develop our own corpus, consisting of about 3K conversations annotated by professional medical scribes. We propose two novel deep learning approaches to infer the symptom names and their status: (1) a new hierarchical span-attribute tagging (SA-T) model, trained using curriculum learning, and (2) a variant of sequence-to-sequence model which decodes the symptoms and their status from a few speaker turns within a sliding window over the conversation. This task stems from a realistic application of assisting medical providers in capturing symptoms mentioned by patients from their clinical conversations. To reflect this application, we define multiple metrics. From inter-rater agreement, we find that the task is inherently difficult. We conduct comprehensive evaluations on several contrasting conditions and observe that the performance of the models range from an F-score of 0.5 to 0.8 depending on the condition. Our analysis not only reveals the inherent challenges of the task, but also provides useful directions to improve the models."},{"ID":"yu-etal-2021-interpretable","text":"Interpretable Propaganda Detection in News Articles Online users today are exposed to misleading and propagandistic news articles and media posts on a daily basis. To counter thus, a number of approaches have been designed aiming to achieve a healthier and safer online news and media consumption. Automatic systems are able to support humans in detecting such content; yet, a major impediment to their broad adoption is that besides being accurate, the decisions of such systems need also to be interpretable in order to be trusted and widely adopted by users. Since misleading and propagandistic content influences readers through the use of a number of deception techniques, we propose to detect and to show the use of such techniques as a way to offer interpretability. In particular, we define qualitatively descriptive features and we analyze their suitability for detecting deception techniques. We further show that our interpretable features can be easily combined with pre-trained language models, yielding state-of-the-art results.","task_annotation":["propaganda detection"],"method_annotation":["qualitatively descriptive features","interpretable features","pre trained language models"],"tasks":[["interpretable propaganda detection"],["online news and media consumption"]],"methods":[["interpretable propaganda detection","online news and media consumption","automatic systems"],["language models"],["deception techniques","deception techniques"]],"task_unmatch_golden":[],"task_match_golden":["propaganda detection"],"task_match_predicted":["interpretable propaganda detection"],"task_unmatch_pred":["online news and media consumption"],"task_match_group":[],"task_match_total":["interpretable propaganda detection","propaganda detection"],"method_unmatch_golden":["interpretable features","qualitatively descriptive features"],"method_match_golden":["pre-trained language models"],"method_match_predicted":["language models"],"method_unmatch_pred":["online news and media consumption","interpretable propaganda detection","deception techniques","automatic systems"],"method_match_group":[],"method_match_total":["language models","pre-trained language models"],"title_clean":"Interpretable Propaganda Detection in News Articles","abstract_clean":"Online users today are exposed to misleading and propagandistic news articles and media posts on a daily basis. To counter thus, a number of approaches have been designed aiming to achieve a healthier and safer online news and media consumption. Automatic systems are able to support humans in detecting such content; yet, a major impediment to their broad adoption is that besides being accurate, the decisions of such systems need also to be interpretable in order to be trusted and widely adopted by users. Since misleading and propagandistic content influences readers through the use of a number of deception techniques, we propose to detect and to show the use of such techniques as a way to offer interpretability. In particular, we define qualitatively descriptive features and we analyze their suitability for detecting deception techniques. We further show that our interpretable features can be easily combined with pre-trained language models, yielding state-of-the-art results."},{"ID":"da-san-martino-etal-2020-prta","text":"{P}rta: A System to Support the Analysis of Propaganda Techniques in the News Recent events, such as the 2016 US Presidential Campaign, Brexit and the COVID-19 \"infodemic\", have brought into the spotlight the dangers of online disinformation. There has been a lot of research focusing on factchecking and disinformation detection. However, little attention has been paid to the specific rhetorical and psychological techniques used to convey propaganda messages. Revealing the use of such techniques can help promote media literacy and critical thinking, and eventually contribute to limiting the impact of \"fake news\" and disinformation campaigns. Prta (Propaganda Persuasion Techniques Analyzer) allows users to explore the articles crawled on a regular basis by highlighting the spans in which propaganda techniques occur and to compare them on the basis of their use of propaganda techniques. The system further reports statistics about the use of such techniques, overall and over time, or according to filtering criteria specified by the user based on time interval, keywords, and\/or political orientation of the media. Moreover, it allows users to analyze any text or URL through a dedicated interface or via an API. The system is available online: https:\/\/www.tanbih.org\/prta.","task_annotation":["propaganda technique analysis"],"method_annotation":["multi task learning","bert","propaganda persuasion techniques analyzer"],"tasks":[["factchecking and disinformation detection","disinformation campaigns"]],"methods":[["analysis of propaganda techniques","rhetorical and psychological techniques","media literacy","critical thinking"],["prta","prta propaganda persuasion techniques analyzer"],["propaganda techniques"]],"task_unmatch_golden":["propaganda technique analysis"],"task_match_golden":[],"task_match_predicted":[],"task_unmatch_pred":["disinformation campaigns","factchecking and disinformation detection"],"task_match_group":[],"task_match_total":[],"method_unmatch_golden":["multi-task learning","bert"],"method_match_golden":["propaganda persuasion techniques analyzer"],"method_match_predicted":["prta propaganda persuasion techniques analyzer"],"method_unmatch_pred":["media literacy","rhetorical and psychological techniques","critical thinking","propaganda techniques","analysis of propaganda techniques"],"method_match_group":["prta"],"method_match_total":["propaganda persuasion techniques analyzer","prta propaganda persuasion techniques analyzer"],"title_clean":"Prta: A System to Support the Analysis of Propaganda Techniques in the News","abstract_clean":"Recent events, such as the 2016 US Presidential Campaign, Brexit and the COVID-19 \"infodemic\", have brought into the spotlight the dangers of online disinformation. There has been a lot of research focusing on factchecking and disinformation detection. However, little attention has been paid to the specific rhetorical and psychological techniques used to convey propaganda messages. Revealing the use of such techniques can help promote media literacy and critical thinking, and eventually contribute to limiting the impact of \"fake news\" and disinformation campaigns. Prta (Propaganda Persuasion Techniques Analyzer) allows users to explore the articles crawled on a regular basis by highlighting the spans in which propaganda techniques occur and to compare them on the basis of their use of propaganda techniques. The system further reports statistics about the use of such techniques, overall and over time, or according to filtering criteria specified by the user based on time interval, keywords, and\/or political orientation of the media. Moreover, it allows users to analyze any text or URL through a dedicated interface or via an API. The system is available online: https:\/\/www.tanbih.org\/prta."},{"ID":"ma-etal-2017-detect","text":"Detect Rumors in Microblog Posts Using Propagation Structure via Kernel Learning How fake news goes viral via social media? How does its propagation pattern differ from real stories? In this paper, we attempt to address the problem of identifying rumors, i.e., fake information, out of microblog posts based on their propagation structure. We firstly model microblog posts diffusion with propagation trees, which provide valuable clues on how an original message is transmitted and developed over time. We then propose a kernel-based method called Propagation Tree Kernel, which captures high-order patterns differentiating different types of rumors by evaluating the similarities between their propagation tree structures. Experimental results on two real-world datasets demonstrate that the proposed kernel-based approach can detect rumors more quickly and accurately than state-ofthe-art rumor detection models.","task_annotation":["identifying rumors"],"method_annotation":["propogation trees","kernel based method"],"tasks":[["detect rumors","identifying rumors"],["microblog posts diffusion"]],"methods":[["rumor detection models"],["kernel based method","kernel based approach"],["propagation tree kernel"],["kernel learning"]],"task_unmatch_golden":[],"task_match_golden":["identifying rumors"],"task_match_predicted":["identifying rumors"],"task_unmatch_pred":["microblog posts diffusion"],"task_match_group":["detect rumors"],"task_match_total":["identifying rumors"],"method_unmatch_golden":[],"method_match_golden":["kernel-based method","propogation trees"],"method_match_predicted":["propagation tree kernel","kernel based method"],"method_unmatch_pred":["kernel learning","rumor detection models"],"method_match_group":["kernel based approach"],"method_match_total":["kernel-based method","propagation tree kernel","kernel based method","propogation trees"],"title_clean":"Detect Rumors in Microblog Posts Using Propagation Structure via Kernel Learning","abstract_clean":"How fake news goes viral via social media? How does its propagation pattern differ from real stories? In this paper, we attempt to address the problem of identifying rumors, i.e., fake information, out of microblog posts based on their propagation structure. We firstly model microblog posts diffusion with propagation trees, which provide valuable clues on how an original message is transmitted and developed over time. We then propose a kernel-based method called Propagation Tree Kernel, which captures high-order patterns differentiating different types of rumors by evaluating the similarities between their propagation tree structures. Experimental results on two real-world datasets demonstrate that the proposed kernel-based approach can detect rumors more quickly and accurately than state-ofthe-art rumor detection models."},{"ID":"u-etal-2008-statistical","text":"Statistical Machine Translation Models for Personalized Search Web search personalization has been well studied in the recent few years. Relevance feedback has been used in various ways to improve relevance of search results. In this paper, we propose a novel usage of relevance feedback to effectively model the process of query formulation and better characterize how a user relates his query to the document that he intends to retrieve using a noisy channel model. We model a user profile as the probabilities of translation of query to document in this noisy channel using the relevance feedback obtained from the user. The user profile thus learnt is applied in a re-ranking phase to rescore the search results retrieved using an underlying search engine. We evaluate our approach by conducting experiments using relevance feedback data collected from users using a popular search engine. The results have shown improvement over baseline, proving that our approach can be applied to personalization of web search. The experiments have also resulted in some valuable observations that learning these user profiles using snippets surrounding the results for a query gives better performance than learning from entire document collection.","task_annotation":["personalized search web"],"method_annotation":["statistical machine translation models","relevance feedback"],"tasks":[["re ranking phase"],["web search personalization","personalization of web search"],["query formulation"],["personalized search"]],"methods":[["search engine","search engine"],["relevance feedback"],["noisy channel model"],["statistical machine translation models"]],"task_unmatch_golden":[],"task_match_golden":["personalized search web"],"task_match_predicted":["personalized search"],"task_unmatch_pred":["web search personalization","query formulation","personalization of web search","re ranking phase"],"task_match_group":[],"task_match_total":["personalized search","personalized search web"],"method_unmatch_golden":[],"method_match_golden":["statistical machine translation models","relevance feedback"],"method_match_predicted":["statistical machine translation models","relevance feedback"],"method_unmatch_pred":["noisy channel model","search engine"],"method_match_group":[],"method_match_total":["statistical machine translation models","relevance feedback"],"title_clean":"Statistical Machine Translation Models for Personalized Search","abstract_clean":"Web search personalization has been well studied in the recent few years. Relevance feedback has been used in various ways to improve relevance of search results. In this paper, we propose a novel usage of relevance feedback to effectively model the process of query formulation and better characterize how a user relates his query to the document that he intends to retrieve using a noisy channel model. We model a user profile as the probabilities of translation of query to document in this noisy channel using the relevance feedback obtained from the user. The user profile thus learnt is applied in a re-ranking phase to rescore the search results retrieved using an underlying search engine. We evaluate our approach by conducting experiments using relevance feedback data collected from users using a popular search engine. The results have shown improvement over baseline, proving that our approach can be applied to personalization of web search. The experiments have also resulted in some valuable observations that learning these user profiles using snippets surrounding the results for a query gives better performance than learning from entire document collection."}]