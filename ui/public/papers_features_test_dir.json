[{"ID":"kiritchenko-cherry-2011-lexically","Goal":"Good Health and Well-Being","proba_max":0.999,"methods":["lexically triggered hidden markov models","lt hmm"],"center_method":["lexically triggered hidden markov models","lt hmm"],"tasks":["multi label document classification"],"center_task":["multi label document classification"],"title_clean":"Lexically Triggered Hidden Markov Models for Clinical Document Coding","abstract_clean":"The automatic coding of clinical documents is an important task for today's healthcare providers. Though it can be viewed as multi label document classification, the coding problem has the interesting property that most code assignments can be supported by a single phrase found in the input document. We propose a Lexically Triggered Hidden Markov Model (LT HMM) that leverages these phrases to improve coding accuracy. The LT HMM works in two stages: first, a lexical match is performed against a term dictionary to collect a set of candidate codes for a document. Next, a discriminative HMM selects the best subset of codes to assign to the document by tagging candidates as present or absent. By confirming codes proposed by a dictionary, the LT HMM can share features across codes, enabling strong performance even on rare codes. In fact, we are able to recover codes that do not occur in the training set at all. Our approach achieves the best ever performance on the 2007 Medical NLP Challenge test set, with an F measure of 89.84.","challenge":null,"direction":null,"task_annotation":["clinical document coding","document classification"],"method_annotation":[" lexically triggered hidden markov model"],"org_annotation":["ACL-HLT"]},{"ID":"da-san-martino-etal-2020-prta","Goal":"Quality Education","proba_max":0.999,"methods":["prta"],"center_method":["prta"],"tasks":["factchecking and disinformation detection"],"center_task":["factchecking and disinformation detection"],"title_clean":"Prta: A System to Support the Analysis of Propaganda Techniques in the News","abstract_clean":"Recent events, such as the 2016 US Presidential Campaign, Brexit and the COVID 19 \"infodemic\", have brought into the spotlight the dangers of online disinformation. There has been a lot of research focusing on factchecking and disinformation detection. However, little attention has been paid to the specific rhetorical and psychological techniques used to convey propaganda messages. Revealing the use of such techniques can help promote media literacy and critical thinking, and eventually contribute to limiting the impact of \"fake news\" and disinformation campaigns. Prta (Propaganda Persuasion Techniques Analyzer) allows users to explore the articles crawled on a regular basis by highlighting the spans in which propaganda techniques occur and to compare them on the basis of their use of propaganda techniques. The system further reports statistics about the use of such techniques, overall and over time, or according to filtering criteria specified by the user based on time interval, keywords, and\/or political orientation of the media. Moreover, it allows users to analyze any text or URL through a dedicated interface or via an API. The system is available online: https:\/\/www.tanbih.org\/prta.","challenge":null,"direction":null,"task_annotation":["propaganda technique analysis"],"method_annotation":["multi task learning","bert","propaganda persuasion techniques analyzer"],"org_annotation":["Qatar Computing Research Institute","MIT-CSAIL"]},{"ID":"ovrelid-etal-2018-lia","Goal":"Reduced Inequalities","proba_max":0.4719548523,"methods":["lia"],"center_method":["lia"],"tasks":["transliteration"],"center_task":["transliteration"],"title_clean":"The LIA Treebank of Spoken Norwegian Dialects","abstract_clean":"This article presents the LIA treebank of transcribed spoken Norwegian dialects. It consists of dialect recordings made in the period between 1950 1990, which have been digitised, transcribed, and subsequently annotated with morphological and dependency style syntactic analysis as part of the LIA (Language Infrastructure made Accessible) project at the University of Oslo. In this article, we describe the LIA material of dialect recordings and its transcription, transliteration and further morphosyntactic annotation. We focus in particular on the extension of the native NDT annotation scheme to spoken language phenomena, such as pauses and various types of disfluencies, and present the subsequent conversion of the treebank to the Universal Dependencies scheme. The treebank currently consists of 13,608 tokens, distributed over 1396 segments taken from three different dialects of spoken Norwegian. The LIA treebank annotation is an ongoing effort and future releases will extend on the current data set.","challenge":null,"direction":null,"task_annotation":["transcription","transliteration"],"method_annotation":["dependency style syntactic analysis","morphosyntactic annotation"],"org_annotation":["no organization"]},{"ID":"rio-2002-compiling","Goal":"Quality Education","proba_max":0.999,"methods":["web resources"],"center_method":["web resources"],"tasks":["interactive literary translation","literary translation)","literary translation"],"center_task":["interactive literary translation","literary translation)","literary translation"],"title_clean":"Compiling an Interactive Literary Translation Web Site for Education Purposes","abstract_clean":"The project under discussion represents an attempt to exploit the potential of web resources for higher education and, more particularly, on a domain (that of literary translation) which is traditionally considered not very much in relation to technology and computer science. Translation and Interpreting students at the Universidad de M\u00e1laga are offered the possibility to take an English Spanish Literary Translation module, which epitomises the need for debate in the field of Humanities. Sadly enough, implementation of course methodology is rendered very difficult or impossible owing to time restrictions and overcrowded classrooms. It is our contention that the setting up of a web site may solve some of these issues. We intend to provide both students and the literary translation aware Internet audience with an integrated, scalable, multifunctional debate forum. Project contents will include a detailed course description, relevant reference materials and interaction services (mailing list, debate forum and chat rooms). This is obviously without limitation, as the Forum is open to any other contents that users may consider necessary or convenient, with a view to a more interdisciplinary approach, further research on the field of Literary Translation and future developments within the project framework.","challenge":null,"direction":null,"task_annotation":["compiling an interactive literary translation site"],"method_annotation":["computer mediated communication","literary translation","web site"],"org_annotation":["no organization"]},{"ID":"li-etal-2020-adviser","Goal":"Reduced Inequalities","proba_max":0.3225770295,"methods":["adviser"],"center_method":["adviser"],"tasks":["multi modal (incorporating speech"],"center_task":["multi modal (incorporating speech"],"title_clean":"ADVISER: A Toolkit for Developing Multi modal, Multi domain and Socially engaged Conversational Agents","abstract_clean":"We present ADVISER 1 an open source, multi domain dialog system toolkit that enables the development of multi modal (incorporating speech, text and vision), sociallyengaged (e.g. emotion recognition, engagement level prediction and backchanneling) conversational agents. The final Python based implementation of our toolkit is flexible, easy to use, and easy to extend not only for technically experienced users, such as machine learning researchers, but also for less technically experienced users, such as linguists or cognitive scientists, thereby providing a flexible platform for collaborative research.","challenge":null,"direction":null,"task_annotation":["multi domain dialog system"],"method_annotation":["emotion recognition","engagement level prediction","backchanneling"],"org_annotation":["no organization"]},{"ID":"kirk-etal-2021-memes","Goal":"Life on Land","proba_max":0.1987407953,"methods":["ocr"],"center_method":["ocr"],"tasks":["hateful meme detection","detecting real world hate"],"center_task":["hateful meme detection","detecting real world hate"],"title_clean":"Memes in the Wild: Assessing the Generalizability of the Hateful Memes Challenge Dataset","abstract_clean":"Hateful memes pose a unique challenge for current machine learning systems because their message is derived from both text and visual modalities. To this effect, Facebook released the Hateful Memes Challenge, a dataset of memes with pre extracted text captions, but it is unclear whether these synthetic examples generalize to 'memes in the wild'. In this paper, we collect hateful and non hateful memes from Pinterest to evaluate out of sample performance on models pre trained on the Facebook dataset. We find that memes in the wild differ in two key aspects: 1) Captions must be extracted via OCR, injecting noise and diminishing performance of multimodal models, and 2) Memes are more diverse than 'traditional memes', including screenshots of conversations or text on a plain background. This paper thus serves as a reality check for the current benchmark of hateful meme detection and its applicability for detecting real world hate.","challenge":null,"direction":null,"task_annotation":["assessing dataset generalizability","hateful memes challenge","hateful meme detection"],"method_annotation":["tesseract","easyocr","naive bayes","clip model"],"org_annotation":["no organization"]},{"ID":"vasquez-etal-2018-toward","Goal":"Peace, Justice and Strong Institutions","proba_max":0.0997201949,"methods":["universal dependencies"],"center_method":["universal dependencies"],"tasks":["dependency syntax parsing"],"center_task":["dependency syntax parsing"],"title_clean":"Toward Universal Dependencies for Shipibo Konibo","abstract_clean":"We present an initial version of the Universal Dependencies (UD) treebank for Shipibo Konibo, the first South American, Amazonian, Panoan and Peruvian language with a resource built under UD. We describe the linguistic aspects of how the tagset was defined and the treebank was annotated; in addition we present our specific treatment of linguistic units called clitics. Although the treebank is still under development, it allowed us to perform a typological comparison against Spanish, the predominant language in Peru, and dependency syntax parsing experiments in both monolingual and cross lingual approaches.","challenge":null,"direction":null,"task_annotation":["universal dependency","dependency syntax parsing"],"method_annotation":["treebank","annotation"],"org_annotation":["Consejo Nacional de Ciencia","Tecnolog\u00eda e Innovaci\u00f3n Tecnol\u00f3gica"]},{"ID":"stajner-etal-2017-effects","Goal":"Responsible Consumption and Production","proba_max":0.5818450451,"methods":["online processing techniques"],"center_method":["online processing techniques"],"tasks":["eye tracking studies"],"center_task":["eye tracking studies"],"title_clean":"Effects of Lexical Properties on Viewing Time per Word in Autistic and Neurotypical Readers","abstract_clean":"Eye tracking studies from the past few decades have shaped the way we think of word complexity and cognitive load: words that are long, rare and ambiguous are more difficult to read. However, online processing techniques have been scarcely applied to investigating the reading difficulties of people with autism and what vocabulary is challenging for them. We present parallel gaze data obtained from adult readers with autism and a control group of neurotypical readers and show that the former required higher cognitive effort to comprehend the texts as evidenced by three gaze based measures. We divide all words into four classes based on their viewing times for both groups and investigate the relationship between longer viewing times and word length, word frequency, and four cognitively based measures (word concreteness, familiarity, age of acquisition and imagability).","challenge":null,"direction":null,"task_annotation":["eye tracking data prediction"],"method_annotation":["lexical properties"," parallel gaze data"],"org_annotation":["University of Mannheim Political Economy of Reforms","German Research Foundation","University of Wolverhampton University Innovation Funds"]},{"ID":"post-etal-2012-constructing","Goal":"Reduced Inequalities","proba_max":0.2986445725,"methods":["amazon's mechanical turk"],"center_method":["amazon's mechanical turk"],"tasks":["machine translation research","machine translation"],"center_task":["machine translation","machine translation"],"title_clean":"Constructing Parallel Corpora for Six Indian Languages via Crowdsourcing","abstract_clean":"Recent work has established the efficacy of Amazon's Mechanical Turk for constructing parallel corpora for machine translation research. We apply this to building a collection of parallel corpora between English and six languages from the Indian subcontinent: Bengali, Hindi, Malayalam, Tamil, Telugu, and Urdu. These languages are low resource, under studied, and exhibit linguistic phenomena that are difficult for machine translation. We conduct a variety of baseline experiments and analysis, and release the data to the community.","challenge":null,"direction":null,"task_annotation":["constructing parallel corpora","machine translation"],"method_annotation":["bootstrapping","annotation","crowdsourcing"],"org_annotation":["Google","Microsoft","European Comission Euro-MatrixPlus project","DARPA"]},{"ID":"wang-etal-2019-bigodm","Goal":"Good Health and Well-Being","proba_max":0.999,"methods":["vue"],"center_method":["vue"],"tasks":["adr classification task"],"center_task":["adr classification task"],"title_clean":"BIGODM System in the Social Media Mining for Health Applications Shared Task 2019","abstract_clean":"In this study, we describe our methods to automatically classify Twitter posts conveying events of adverse drug reaction (ADR). Based on our previous experience in tackling the ADR classification task, we empirically applied the vote based undersampling ensemble approach along with linear support vector machine (SVM) to develop our classifiers as part of our participation in ACL 2019 Social Media Mining for Health Applications (SMM4H) shared task 1. The best performed model on the test sets were trained on a merged corpus consisting of the datasets released by SMM4H 2017 and 2019. By using VUE, the corpus was randomly under sampled with 2:1 ratio between the negative and positive classes to create an ensemble using the linear kernel trained with features including bag of word, domain knowledge, negation and word embedding. The best performing model achieved an F measure of 0.551 which is about 5% higher than the average F scores of 16 teams.","challenge":null,"direction":null,"task_annotation":["social media mining for health applications"],"method_annotation":["support vector machines","word embedding","linear kernel","bag of word","domain knowledge","negation"],"org_annotation":["no organization"]},{"ID":"aggarwal-etal-2019-ltl","Goal":"Reduced Inequalities","proba_max":0.2678506672,"methods":["ltl ude","ltl ude's systems"],"center_method":["ltl ude","ltl ude's systems"],"tasks":["categorizing offensiveness"],"center_task":["categorizing offensiveness"],"title_clean":"LTL UDE at SemEval 2019 Task 6: BERT and Two Vote Classification for Categorizing Offensiveness","abstract_clean":"This paper describes LTL UDE's systems for the SemEval 2019 Shared Task 6. We present results for Subtask A and C. In Subtask A, we experiment with an embedding representation of postings and use a Multi Layer Perceptron and BERT to categorize postings. Our best result reaches the 10th place (out of 103) using BERT. In Subtask C, we applied a two vote classification approach with minority fallback, which is placed on the 19th rank (out of 65).","challenge":null,"direction":null,"task_annotation":["offensive language detection"],"method_annotation":["embedding representation","multi layer perceptron","bert"],"org_annotation":["no organization"]},{"ID":"nguyen-2019-question","Goal":"Good Health and Well-Being","proba_max":0.999,"methods":["question answering system"],"center_method":["question answering"],"tasks":["question answering","answering"],"center_task":["question answering","answering"],"title_clean":"Question Answering in the Biomedical Domain","abstract_clean":"Question answering techniques have mainly been investigated in open domains. However, there are particular challenges in extending these open domain techniques to extend into the biomedical domain. Question answering focusing on patients is less studied. We find that there are some challenges in patient question answering such as limited annotated data, lexical gap and quality of answer spans. We aim to address some of these gaps by extending and developing upon the literature to design a question answering system that can decide on the most appropriate answers for patients attempting to self diagnose while including the ability to abstain from answering when confidence is low.","challenge":null,"direction":null,"task_annotation":["question answering"],"method_annotation":["multi task learning","neural architectural search","reinforcement learning"],"org_annotation":["Australian Research Training Program","CSIRO"]},{"ID":"ma-etal-2017-detect","Goal":"Reduced Inequalities","proba_max":0.4475037456,"methods":["kernel based approach"],"center_method":["kernel based approach"],"tasks":["detect rumors"],"center_task":["detect rumors"],"title_clean":"Detect Rumors in Microblog Posts Using Propagation Structure via Kernel Learning","abstract_clean":"How fake news goes viral via social media? How does its propagation pattern differ from real stories? In this paper, we attempt to address the problem of identifying rumors, i.e., fake information, out of microblog posts based on their propagation structure. We firstly model microblog posts diffusion with propagation trees, which provide valuable clues on how an original message is transmitted and developed over time. We then propose a kernel based method called Propagation Tree Kernel, which captures high order patterns differentiating different types of rumors by evaluating the similarities between their propagation tree structures. Experimental results on two real world datasets demonstrate that the proposed kernel based approach can detect rumors more quickly and accurately than state ofthe art rumor detection models.","challenge":null,"direction":null,"task_annotation":["identifying rumors"],"method_annotation":["propogation trees","kernel based method"],"org_annotation":["General Research Fund of Hong Kong"]},{"ID":"kim-park-2015-statistical","Goal":"Reduced Inequalities","proba_max":0.4963862002,"methods":["wm based processing analysis"],"center_method":["wm based processing analysis"],"tasks":["linguistics"],"center_task":["computational linguistics"],"title_clean":"A Statistical Modeling of the Correlation between Island Effects and Working memory Capacity for L2 Learners","abstract_clean":"The cause of island effects has evoked considerable debate within syntax and other fields of linguistics. The two competing approaches stand out: the grammatical analysis; and the working memory (WM) based processing analysis. In this paper we report three experiments designed to test one of the premises of the WM based processing analysis: that the strength of island effects should vary as a function of individual differences in WM capacity. The results show that island effects present even for L2 learners are more likely attributed to grammatical constraints than to limited processing resources.","challenge":null,"direction":null,"task_annotation":["statistical modeling"],"method_annotation":["grammatical analysis","working memory based processing"],"org_annotation":["National Research Foundation of Korea"]},{"ID":"maxwelll-smith-etal-2020-applications","Goal":"Quality Education","proba_max":0.999,"methods":["fine grained error analysis"],"center_method":["error analysis"],"tasks":["bilingual language teaching","automated speech recognition"],"center_task":["bilingual language teaching","automated speech recognition"],"title_clean":"Applications of Natural Language Processing in Bilingual Language Teaching: An Indonesian English Case Study","abstract_clean":"Multilingual corpora are difficult to compile and a classroom setting adds pedagogy to the mix of factors which make this data so rich and problematic to classify. In this paper, we set out methodological considerations of using automated speech recognition to build a corpus of teacher speech in an Indonesian language classroom. Our preliminary results (64% word error rate) suggest these tools have the potential to speed data collection in this context. We provide practical examples of our data structure, details of our piloted computer assisted processes, and fine grained error analysis. Our study is informed and directed by genuine research questions and discussion in both the education and computational linguistics fields. We highlight some of the benefits and risks of using these emerging technologies to analyze the complex work of language teachers and in education more generally.","challenge":null,"direction":null,"task_annotation":["automated speech recognition"],"method_annotation":["speech recognition","corpus"],"org_annotation":["no organization"]},{"ID":"al-mannai-etal-2014-unsupervised","Goal":"Reduced Inequalities","proba_max":0.7477893233,"methods":["unsupervised morphological segmentation"],"center_method":["unsupervised morphological segmentation"],"tasks":["unsupervised word segmentation","dialectal arabic to english machine translation","unsupervised segmentation","qatari arabic to english machine translation"],"center_task":["unsupervised word segmentation","dialectal arabic to english machine translation","unsupervised segmentation","qatari arabic to english machine translation"],"title_clean":"Unsupervised Word Segmentation Improves Dialectal Arabic to English Machine Translation","abstract_clean":"We demonstrate the feasibility of using unsupervised morphological segmentation for dialects of Arabic, which are poor in linguistics resources. Our experiments using a Qatari Arabic to English machine translation system show that unsupervised segmentation helps to improve the translation quality as compared to using no segmentation or to using ATB segmentation, which was especially designed for Modern Standard Arabic (MSA). We use MSA and other dialects to improve Qatari Arabic to English machine translation, and we show that a uniform segmentation scheme across them yields an improvement of 1.5 BLEU points over using no segmentation.","challenge":null,"direction":null,"task_annotation":["machine translation"],"method_annotation":["unsupervised morphological segmentation","atb segmentation"],"org_annotation":["no organization"]},{"ID":"du-etal-2019-extracting","Goal":"Good Health and Well-Being","proba_max":0.999,"methods":["hierarchical span attribute tagging (sa t) model"],"center_method":["hierarchical span attribute tagging (sa t) model"],"tasks":["extracting symptoms"],"center_task":["extracting symptoms"],"title_clean":"Extracting Symptoms and their Status from Clinical Conversations","abstract_clean":"This paper describes novel models tailored for a new application, that of extracting the symptoms mentioned in clinical conversations along with their status. Lack of any publicly available corpus in this privacy sensitive domain led us to develop our own corpus, consisting of about 3K conversations annotated by professional medical scribes. We propose two novel deep learning approaches to infer the symptom names and their status: (1) a new hierarchical span attribute tagging (SA T) model, trained using curriculum learning, and (2) a variant of sequence to sequence model which decodes the symptoms and their status from a few speaker turns within a sliding window over the conversation. This task stems from a realistic application of assisting medical providers in capturing symptoms mentioned by patients from their clinical conversations. To reflect this application, we define multiple metrics. From inter rater agreement, we find that the task is inherently difficult. We conduct comprehensive evaluations on several contrasting conditions and observe that the performance of the models range from an F score of 0.5 to 0.8 depending on the condition. Our analysis not only reveals the inherent challenges of the task, but also provides useful directions to improve the models.","challenge":null,"direction":null,"task_annotation":["symptom extraction from clinical conversations,extracting symptoms"],"method_annotation":["span attribute tagging","curriculum learning","sequence to sequence","hierarchical span attribute tagging sa t model"],"org_annotation":["no organization"]},{"ID":"dernoncourt-etal-2017-neural","Goal":"Good Health and Well-Being","proba_max":0.999,"methods":["ann architecture"],"center_method":["ann architecture"],"tasks":["joint sentence classification","sentence classification","sequential sentence classification"],"center_task":["joint sentence classification","classification","sequential sentence classification"],"title_clean":"Neural Networks for Joint Sentence Classification in Medical Paper Abstracts","abstract_clean":"Existing models based on artificial neural networks (ANNs) for sentence classification often do not incorporate the context in which sentences appear, and classify sentences individually. However, traditional sentence classification approaches have been shown to greatly benefit from jointly classifying subsequent sentences, such as with conditional random fields. In this work, we present an ANN architecture that combines the effectiveness of typical ANN models to classify sentences in isolation, with the strength of structured prediction. Our model outperforms the state ofthe art results on two different datasets for sequential sentence classification in medical abstracts.","challenge":null,"direction":null,"task_annotation":["sentence classification"],"method_annotation":["artificial neural network"],"org_annotation":["Philips Research"]},{"ID":"hoang-anh-etal-2008-basic","Goal":"Life on Land","proba_max":0.175477773,"methods":["bktexts"],"center_method":["bktexts"],"tasks":["vietnamese text catergorization","vietnamese text categorization","text categorization"],"center_task":["vietnamese text catergorization","vietnamese text categorization","text categorization"],"title_clean":"A Basic Framework to Build a Test Collection for the Vietnamese Text Catergorization","abstract_clean":"The aim of this paper is to present a basic framework to build a test collection for a Vietnamese text categorization. The presented content includes our evaluations of some popular text categorization test collections, our researches on the requirements, the proposed model and the techniques to build the BKTexts test collection for a Vietnamese text categorization. The XML specification of both text and metadata of Vietnamese documents in the BKTexts also is presented. Our BKTexts test collection is built with the XML specification and currently has more than 17100 Vietnamese text documents collected from e newspapers.","challenge":null,"direction":null,"task_annotation":["text collection","text categorization"],"method_annotation":["xml specification"],"org_annotation":["no organization"]},{"ID":"oard-2007-invited","Goal":"Reduced Inequalities","proba_max":0.7190416455,"methods":["malach project"],"center_method":["malach project"],"tasks":["malach project"],"center_task":["malach project"],"title_clean":"Invited Talk: Lessons from the MALACH Project: Applying New Technologies to Improve Intellectual Access to Large Oral History Collections","abstract_clean":"In this talk I will describe the goals of the MALACH project (Multilingual Access to Large Spoken Archives) and our research results. I'll begin by describing the unique characteristics of the oral history collection that we used, in which Holocaust survivors, witnesses and rescuers were interviewed in several languages. Each interview has been digitized and extensively catalogued by subject matter experts, thus producing a remarkably rich collection for the application of machine learning techniques. Automatic speech recognition techniques originally developed for the domain of conversational telephone speech were adapted to process these materials with word error rates that are adequate to provide useful features to support interactive search and automated clustering, boundary detection, and topic classification tasks. As I describe our results, I will focus particularly on the evaluation methods that that we have used to assess the potential utility of this technology. I'll conclude with some remarks about possible future directions for research on applying new technologies to improve intellectual access to oral history and other spoken word collections.","challenge":null,"direction":null,"task_annotation":["speech archival","intellectual access to large oral history collections","topic classification","boundary detection"],"method_annotation":["automated clustering","automatic speech recognition techniques"],"org_annotation":["no organization"]},{"ID":"shreevastava-foltz-2021-detecting","Goal":"Reduced Inequalities","proba_max":0.4967263937,"methods":["svm classifier"],"center_method":["support vector machine"],"tasks":["cognitive behavioral therapy"],"center_task":["cognitive behavioral therapy"],"title_clean":"Detecting Cognitive Distortions from Patient Therapist Interactions","abstract_clean":"An important part of Cognitive Behavioral Therapy (CBT) is to recognize and restructure certain negative thinking patterns that are also known as cognitive distortions. This project aims to detect these distortions using natural language processing. We compare and contrast different types of linguistic features as well as different classification algorithms and explore the limitations of applying these techniques on a small dataset. We find that pretrained Sentence BERT embeddings to train an SVM classifier yields the best results with an F1 score of 0.79. Lastly, we discuss how this work provides insights into the types of linguistic features that are inherent in cognitive distortions.","challenge":null,"direction":null,"task_annotation":["cognitive distortion detection"],"method_annotation":["pretrained sentence bert embeddings","svm classifier"],"org_annotation":["University of Colorado","Boulder Computational Linguistics","Analytics","Search and Informatics"]},{"ID":"salawu-etal-2021-large","Goal":"Reduced Inequalities","proba_max":0.2489496917,"methods":["transformer based deep learning models"],"center_method":["transformer based deep learning models"],"tasks":["online abuse and cyberbullying detection"],"center_task":["online abuse and cyberbullying detection"],"title_clean":"A Large Scale English Multi Label Twitter Dataset for Cyberbullying and Online Abuse Detection","abstract_clean":"In this paper, we introduce a new English Twitter based dataset for online abuse and cyberbullying detection. Comprising 62,587 tweets, this dataset was sourced from Twitter using specific query terms designed to retrieve tweets with high probabilities of various forms of bullying and offensive content, including insult, profanity, sarcasm, threat, porn and exclusion. Analysis performed on the dataset confirmed common cyberbullying themes reported by other studies and revealed interesting relationships between the classes. The dataset was used to train a number of transformer based deep learning models returning impressive results.","challenge":null,"direction":null,"task_annotation":["online abuse detection"],"method_annotation":["transformer based deep learning models","dataset"],"org_annotation":["no organization"]},{"ID":"banea-etal-2010-multilingual","Goal":"Reduced Inequalities","proba_max":0.3460161686,"methods":["meta classifiers"],"center_method":["meta classifiers"],"tasks":["subjectivity analysis"],"center_task":["subjectivity analysis"],"title_clean":"Multilingual Subjectivity: Are More Languages Better?","abstract_clean":"While subjectivity related research in other languages has increased, most of the work focuses on single languages. This paper explores the integration of features originating from multiple languages into a machine learning approach to subjectivity analysis, and aims to show that this enriched feature set provides for more effective modeling for the source as well as the target languages. We show not only that we are able to achieve over 75% macro accuracy in all of the six languages we experiment with, but also that by using features drawn from multiple languages we can construct high precision meta classifiers with a precision of over 83%.","challenge":null,"direction":null,"task_annotation":["subjectivity analysis"],"method_annotation":["meta classifiers"],"org_annotation":["National Science Foundation"]},{"ID":"huang-bai-2021-team","Goal":"Industry, Innovation and Infrastrucure","proba_max":0.1691936105,"methods":["xlm roberta pre trained language model"],"center_method":["xlm roberta pre trained language model"],"tasks":["hope speech detection","lt edi 2021","voice detection"],"center_task":["hope speech detection","lt edi 2021","voice detection"],"title_clean":"TEAM HUB@LT EDI EACL2021: Hope Speech Detection Based On Pre trained Language Model","abstract_clean":"This article introduces the system description of TEAM HUB team participating in LT EDI 2021: Hope Speech Detection. This shared task is the first task related to the desired voice detection. The data set in the shared task consists of three different languages (English, Tamil, and Malayalam). The task type is text classification. Based on the analysis and understanding of the task description and data set, we designed a system based on a pre trained language model to complete this shared task. In this system, we use methods and models that combine the XLM RoBERTa pre trained language model and the Tf Idf algorithm. In the final result ranking announced by the task organizer, our system obtained F1 scores of 0.93, 0.84, 0.59 on the English dataset, Malayalam dataset, and Tamil dataset. Our submission results are ranked 1, 2, and 3 respectively.","challenge":null,"direction":null,"task_annotation":["hope speech detection","text classification"],"method_annotation":["language model","xlm roberta","tf idf"],"org_annotation":["no organization"]},{"ID":"garrette-alpert-abrams-2016-unsupervised","Goal":"Reduced Inequalities","proba_max":0.7689564824,"methods":["joint transcription model"],"center_method":["joint transcription model"],"tasks":["historical document transcription"],"center_task":["historical document transcription"],"title_clean":"An Unsupervised Model of Orthographic Variation for Historical Document Transcription","abstract_clean":"Historical documents frequently exhibit extensive orthographic variation, including archaic spellings and obsolete shorthand. OCR tools typically seek to produce so called diplomatic transcriptions that preserve these variants, but many end tasks require transcriptions with normalized orthography. In this paper, we present a novel joint transcription model that learns, unsupervised, a probabilistic mapping between modern orthography and that used in the document. Our system thus produces dual diplomatic and normalized transcriptions simultaneously, and achieves a 35% relative error reduction over a state of the art OCR model on diplomatic transcription, and a 46% reduction on normalized transcription.","challenge":null,"direction":null,"task_annotation":["historic document transcription"],"method_annotation":["joint transcription model","diplomatic transcription"],"org_annotation":["National Endowment for the Humanities"]},{"ID":"pyysalo-etal-2007-unification","Goal":"Good Health and Well-Being","proba_max":0.999,"methods":["bioinfer"],"center_method":["bioinfer"],"tasks":["biomedical information extraction"],"center_task":["biomedical information extraction"],"title_clean":"On the unification of syntactic annotations under the Stanford dependency scheme: A case study on BioInfer and GENIA","abstract_clean":"Several incompatible syntactic annotation schemes are currently used by parsers and corpora in biomedical information extraction. The recently introduced Stanford dependency scheme has been suggested to be a suitable unifying syntax formalism. In this paper, we present a step towards such unification by creating a conversion from the Link Grammar to the Stanford scheme. Further, we create a version of the BioInfer corpus with syntactic annotation in this scheme. We present an application oriented evaluation of the transformation and assess the suitability of the scheme and our conversion to the unification of the syntactic annotations of BioInfer and the GENIA Treebank. We find that a highly reliable conversion is both feasible to create and practical, increasing the applicability of both the parser and the corpus to information extraction.","challenge":null,"direction":null,"task_annotation":["biomedical information extraction"],"method_annotation":["dependency schemes","bioinfer","genia treebank"],"org_annotation":["Academy of Finland"]},{"ID":"maegaard-etal-2008-medar","Goal":"Reduced Inequalities","proba_max":0.5587144494,"methods":["medar"],"center_method":["medar"],"tasks":["machine translation","translation)"],"center_task":["machine translation","translation)"],"title_clean":"MEDAR: Collaboration between European and Mediterranean Arabic Partners to Support the Development of Language Technology for Arabic","abstract_clean":"After the successful completion of the NEMLAR project 2003 2005, a new opportunity for a project was opened by the European Commission, and a group of largely the same partners is now executing the MEDAR project. MEDAR will be updating the surveys and BLARK for Arabic already made, and will then focus on machine translation (and other tools for translation) and information retrieval with a focus on language resources, tools and evaluation for these applications. A very important part of the MEDAR project is to reinforce and extend the NEMLAR network and to create a cooperation roadmap for Human Language Technologies for Arabic. It is expected that the cooperation roadmap will attract wide attention from other parties and that it can help create a larger platform for collaborative projects. Finally, the project will focus on dissemination of knowledge about existing resources and tools, as well as actors and activities; this will happen through newsletter, website and an international conference which will follow up on the Cairo conference of 2004. Dissemination to user communities will also be important, e.g. through participation in translators' conferences. The goal of these activities is to create a stronger and lasting collaboration between EU countries and Arabic speaking countries.","challenge":null,"direction":null,"task_annotation":["machine translation","information retrieval"],"method_annotation":["surveys","questionnaires"],"org_annotation":["European Comission"]},{"ID":"li-etal-2020-hitrans","Goal":"Good Health and Well-Being","proba_max":0.5726739764,"methods":["hitrans","transformer based context and speaker sensitive model"],"center_method":["hitrans","transformer based context and speaker sensitive model"],"tasks":["emotion detection in conversations","edc","pairwise utterance speaker verification"],"center_task":["emotion detection in conversations","edc","pairwise utterance speaker verification"],"title_clean":"HiTrans: A Transformer Based Context and Speaker Sensitive Model for Emotion Detection in Conversations","abstract_clean":"Emotion detection in conversations (EDC) is to detect the emotion for each utterance in conversations that have multiple speakers. Different from the traditional non conversational emotion detection, the model for EDC should be context sensitive (e.g., understanding the whole conversation rather than one utterance) and speaker sensitive (e.g., understanding which utterance belongs to which speaker). In this paper, we propose a transformer based context and speakersensitive model for EDC, namely HiTrans, which consists of two hierarchical transformers. We utilize BERT as the low level transformer to generate local utterance representations, and feed them into another high level transformer so that utterance representations could be sensitive to the global context of the conversation. Moreover, we exploit an auxiliary task to make our model speaker sensitive, called pairwise utterance speaker verification (PUSV), which aims to classify whether two utterances belong to the same speaker. We evaluate our model on three benchmark datasets, namely EmoryNLP, MELD and IEMOCAP. Results show that our model outperforms previous state of the art models.","challenge":null,"direction":null,"task_annotation":["emotion detection in conversations"],"method_annotation":["transformers","context sensitive model","hierarchical transformers","bert"],"org_annotation":["National Natural Science Foundation of China","National Key Research and Development Program of China","Research Foundation of Ministry of Education of China","Major Projects of the National Social Science Foundation of China"]},{"ID":"hu-etal-2021-collaborative","Goal":"Reduced Inequalities","proba_max":0.7509547472,"methods":["cdr"],"center_method":["cdr"],"tasks":["intelligent personal assistants"],"center_task":["intelligent personal assistants"],"title_clean":"Collaborative Data Relabeling for Robust and Diverse Voice Apps Recommendation in Intelligent Personal Assistants","abstract_clean":"Intelligent personal assistants (IPAs) such as Amazon Alexa, Google Assistant and Apple Siri extend their built in capabilities by supporting voice apps developed by third party developers. Sometimes the smart assistant is not able to successfully respond to user voice commands (aka utterances). There are many reasons including automatic speech recognition (ASR) error, natural language understanding (NLU) error, routing utterances to an irrelevant voice app or simply that the user is asking for a capability that is not supported yet. The failure to handle a voice command leads to customer frustration. In this paper, we introduce a fallback skill recommendation system to suggest a voice app to a customer for an unhandled voice command. One of the prominent challenges of developing a skill recommender system for IPAs is partial observation. To solve the partial observation problem, we propose collaborative data relabeling (CDR) method. In addition, CDR also improves the diversity of the recommended skills. We evaluate the proposed method both offline and online. The offline evaluation results show that the proposed system outperforms the baselines. The online A\/B testing results show significant gain of customer experience metrics.","challenge":null,"direction":null,"task_annotation":["recommendation system"],"method_annotation":["collaborative data relabeling"],"org_annotation":["no organization"]},{"ID":"bhatta-etal-2020-nepali","Goal":"Reduced Inequalities","proba_max":0.5874660015,"methods":["ctc"],"center_method":["ctc"],"tasks":["nepali speech recognition","automatic speech recognition"],"center_task":["nepali speech recognition","automatic speech recognition"],"title_clean":"Nepali Speech Recognition Using CNN, GRU and CTC","abstract_clean":"Communication is an important part of life. To use communication technology efficiently we need to know how to use them or how to instruct these devices to perform tasks. Automatic speech recognition plays an important role in interaction with the technology. Nepali speech recognition involves in conversion of Nepali speech to its correct Nepali transcriptions. The purposed model consists of CNN, GRU and CTC network. The feature in the raw audio is extracted by using MFCC algorithm. CNN is for learning high level features. GRU is responsible for constructing the acoustic model. CTC is responsible for decoding. The dataset consists of 18 female speakers. It is provided by Open Speech and Language Resources. The build model can predict the with the WER of 11%.","challenge":null,"direction":null,"task_annotation":["speech recognition"],"method_annotation":["cnn","gru"],"org_annotation":["no organization"]},{"ID":"hieu-etal-2020-reintel","Goal":"Reduced Inequalities","proba_max":0.488899678,"methods":["phobert"],"center_method":["phobert"],"tasks":["vietnamese fake news detection"],"center_task":["vietnamese fake news detection"],"title_clean":"ReINTEL Challenge 2020: Vietnamese Fake News Detection usingEnsemble Model with PhoBERT embeddings","abstract_clean":"Along with the increasing traffic of social networks in Vietnam in recent years, the number of unreliable news has also grown rapidly. As we make decisions based on the information we come across daily, fake news, depending on the severity of the matter, can lead to disastrous consequences. This paper presents our approach for the Fake News Detection on Social Network Sites (SNSs), using an ensemble method with linguistic features extracted using PhoBERT (Nguyen and Nguyen, 2020). Our method achieves AUC score of 0.9521 and got 1 st place on the private test at the 7 th International Workshop on Vietnamese Language and Speech Processing (VLSP). For reproducing the result, the code can be found at https:\/\/gitlab.com\/thuan.","challenge":null,"direction":null,"task_annotation":["fake news detection"],"method_annotation":["linguistic feature extraction","tf idf","feature engineering","phobert","bert","phobert embeddings","ensemble method"],"org_annotation":["no organization"]},{"ID":"jansen-2020-visually","Goal":"Clean Water and Sanitation","proba_max":0.1117598861,"methods":["visually grounded planning without vision","virtual robotic agent","gpt 2 model"],"center_method":["visually grounded planning without vision","virtual robotic agent","gpt 2 model"],"tasks":["grounded virtual agents"],"center_task":["grounded virtual agents"],"title_clean":"Visually Grounded Planning without Vision: Language Models Infer Detailed Plans from High level Instructions","abstract_clean":"The recently proposed ALFRED challenge task aims for a virtual robotic agent to complete complex multi step everyday tasks in a virtual home environment from high level natural language directives, such as \"put a hot piece of bread on a plate\". Currently, the best performing models are able to complete less than 5% of these tasks successfully. In this work we focus on modeling the translation problem of converting natural language directives into detailed multi step sequences of actions that accomplish those goals in the virtual environment. We empirically demonstrate that it is possible to generate gold multi step plans from language directives alone without any visual input in 26% of unseen cases. When a small amount of visual information is incorporated, namely the starting location in the virtual environment, our best performing GPT 2 model successfully generates gold command sequences in 58% of cases. Our results suggest that contextualized language models may provide strong visual semantic planning modules for grounded virtual agents.","challenge":null,"direction":null,"task_annotation":["infer plans from language directives","visually grounded planning"],"method_annotation":["contextualized language models","gpt 2"],"org_annotation":["no organization"]},{"ID":"meng-etal-2018-automatic","Goal":"Reduced Inequalities","proba_max":0.4136859775,"methods":["lstm crf"],"center_method":["lstm"],"tasks":["robotics challenge"],"center_task":["robotics challenge"],"title_clean":"Automatic Labeling of Problem Solving Dialogues for Computational Microgenetic Learning Analytics","abstract_clean":"This paper presents a recurrent neural network model to automate the analysis of students' computational thinking in problem solving dialogue. We have collected and annotated dialogue transcripts from middle school students solving a robotics challenge, and each dialogue turn is assigned a code. We use sentence embeddings and speaker identities as features, and experiment with linear chain CRFs and RNNs with a CRF layer (LSTM CRF). Both the linear chain CRF model and the LSTM CRF model outperform the na\u00efve baselines by a large margin, and LSTM CRF has an edge between the two. To our knowledge, this is the first study on dialogue segment annotation using neural network models. This study is also a stepping stone to automating the microgenetic analysis of cognitive interactions between students.","challenge":null,"direction":null,"task_annotation":["dialogue act annotation"],"method_annotation":["sentence embeddings","linear chain crf model","recurrent neural network model","lstm crf","microgenetic learning analytics"],"org_annotation":["no organization"]},{"ID":"yu-etal-2021-interpretable","Goal":"Good Health and Well-Being","proba_max":0.999,"methods":["deception techniques"],"center_method":["deception techniques"],"tasks":["interpretable propaganda detection"],"center_task":["interpretable propaganda detection"],"title_clean":"Interpretable Propaganda Detection in News Articles","abstract_clean":"Online users today are exposed to misleading and propagandistic news articles and media posts on a daily basis. To counter thus, a number of approaches have been designed aiming to achieve a healthier and safer online news and media consumption. Automatic systems are able to support humans in detecting such content; yet, a major impediment to their broad adoption is that besides being accurate, the decisions of such systems need also to be interpretable in order to be trusted and widely adopted by users. Since misleading and propagandistic content influences readers through the use of a number of deception techniques, we propose to detect and to show the use of such techniques as a way to offer interpretability. In particular, we define qualitatively descriptive features and we analyze their suitability for detecting deception techniques. We further show that our interpretable features can be easily combined with pre trained language models, yielding state of the art results.","challenge":null,"direction":null,"task_annotation":["propaganda detection"],"method_annotation":["qualitatively descriptive features","interpretable features","pre trained language models"],"org_annotation":["Qatar Computing Research Institute","HBKU","Computer Science and Artificial Intelligence Laboratory"]},{"ID":"bhargava-kondrak-2011-pronounce","Goal":"Reduced Inequalities","proba_max":0.4931245148,"methods":["g2p"],"center_method":["g2p"],"tasks":["transliterations grapheme to phoneme conversion"],"center_task":["transliterations grapheme to phoneme conversion"],"title_clean":"How do you pronounce your name? Improving G2P with transliterations","abstract_clean":"Grapheme to phoneme conversion (G2P) of names is an important and challenging problem. The correct pronunciation of a name is often reflected in its transliterations, which are expressed within a different phonological inventory. We investigate the problem of using transliterations to correct errors produced by state of the art G2P systems. We present a novel re ranking approach that incorporates a variety of score and n gram features, in order to leverage transliterations from multiple languages. Our experiments demonstrate significant accuracy improvements when re ranking is applied to n best lists generated by three different G2P programs.","challenge":null,"direction":null,"task_annotation":["grapheme to phoneme conversion"],"method_annotation":["n grams","re ranking"],"org_annotation":["Natural Sciences and Engineering Research Council of Canada"]},{"ID":"muti-barron-cedeno-2022-checkpoint","Goal":"Reduced Inequalities","proba_max":0.2335500568,"methods":["multilingual transformers"],"center_method":["transformers"],"tasks":["identifying misogyny in tweets","detecting misogyny"],"center_task":["identifying misogyny in tweets","detecting misogyny"],"title_clean":"A Checkpoint on Multilingual Misogyny Identification","abstract_clean":"We address the problem of identifying misogyny in tweets in mono and multilingual settings in three languages: English, Italian and Spanish. We explore model variations considering single and multiple languages both in the pre training of the transformer and in the training of the downstream task to explore the feasibility of detecting misogyny through a transfer learning approach across multiple languages. That is, we train monolingual transformers with monolingual data and multilingual transformers with both monolingual and multilingual data. Our models reach state of the art performance on all three languages. The single language BERT models perform the best, closely followed by different configurations of multilingual BERT models. The performance drops in zero shot classification across languages. Our error analysis shows that multilingual and monolingual models tend to make the same mistakes.","challenge":null,"direction":null,"task_annotation":["misogyny identification"],"method_annotation":["transformers","bert"],"org_annotation":["no organization"]},{"ID":"sikdar-etal-2018-flytxt","Goal":"Reduced Inequalities","proba_max":0.4140563309,"methods":["conditional random fields"],"center_method":["conditional random field"],"tasks":["semeval 2018"],"center_task":["semeval"],"title_clean":"Flytxt\\_NTNU at SemEval 2018 Task 8: Identifying and Classifying Malware Text Using Conditional Random Fields and Na\\\"\\ive Bayes Classifiers","abstract_clean":"Cybersecurity risks such as malware threaten the personal safety of users, but to identify malware text is a major challenge. The paper proposes a supervised learning approach to identifying malware sentences given a document (subTask1 of SemEval 2018, Task 8), as well as to classifying malware tokens in the sentences (subTask2). The approach achieved good results, ranking second of twelve participants for both subtasks, with F scores of 57% for subTask1 and 28% for subTask2.","challenge":null,"direction":null,"task_annotation":["identifying and classifying malware text"],"method_annotation":["supervised learning","conditional random fields"],"org_annotation":["no organization"]},{"ID":"swanson-etal-2014-identifying","Goal":"Good Health and Well-Being","proba_max":0.4460400343,"methods":["classifier"],"center_method":["classification"],"tasks":["automatically identifying categories of narrative clauses"],"center_task":["automatically identifying categories of narrative clauses"],"title_clean":"Identifying Narrative Clause Types in Personal Stories","abstract_clean":"This paper describes work on automatically identifying categories of narrative clauses in personal stories written by ordinary people about their daily lives and experiences. We base our approach on Labov & Waletzky's theory of oral narrative which categorizes narrative clauses into subtypes, such as ORIENTATION, ACTION and EVALUATION. We describe an experiment where we annotate 50 personal narratives from weblogs and experiment with methods for achieving higher annotation reliability. We use the resulting annotated corpus to train a classifier to automatically identify narrative categories, achieving a best average F score of .658, which rises to an F score of .767 on the cases with the highest annotator agreement. We believe the identified narrative structure will enable new types of computational analysis of narrative discourse.","challenge":null,"direction":null,"task_annotation":["narrative identification"],"method_annotation":["annotation","corpus construction","classifier"],"org_annotation":["NSF"]},{"ID":"del-tredici-fernandez-2020-words","Goal":"Reduced Inequalities","proba_max":0.5318230391,"methods":["language based user representations"],"center_method":["language based user representations"],"tasks":["fake news detection"],"center_task":["fake news detection"],"title_clean":"Words are the Window to the Soul: Language based User Representations for Fake News Detection","abstract_clean":"Cognitive and social traits of individuals are reflected in language use. Moreover, individuals who are prone to spread fake news online often share common traits. Building on these ideas, we introduce a model that creates representations of individuals on social media based only on the language they produce, and use them to detect fake news. We show that language based user representations are beneficial for this task. We also present an extended analysis of the language of fake news spreaders, showing that its main features are mostly domain independent and consistent across two English datasets. Finally, we exploit the relation between language use and connections in the social graph to assess the presence of the Echo Chamber effect in our data.","challenge":null,"direction":null,"task_annotation":["fake news detection"],"method_annotation":["user representation","convolutional neural networks"],"org_annotation":["Netherlands Organisation for Scientific Research"]},{"ID":"rijhwani-etal-2020-soft","Goal":"Reduced Inequalities","proba_max":0.3562402427,"methods":["named entity recognition models"],"center_method":["named entity recognition models"],"tasks":["low resource named entity recognition","named entity recognition"],"center_task":["named entity recognition","named entity recognition"],"title_clean":"Soft Gazetteers for Low Resource Named Entity Recognition","abstract_clean":"Traditional named entity recognition models use gazetteers (lists of entities) as features to improve performance. Although modern neural network models do not require such handcrafted features for strong performance, recent work (Wu et al., 2018) has demonstrated their utility for named entity recognition on English data. However, designing such features for low resource languages is challenging, because exhaustive entity gazetteers do not exist in these languages. To address this problem, we propose a method of \"soft gazetteers\" that incorporates ubiquitously available information from English knowledge bases, such as Wikipedia, into neural named entity recognition models through cross lingual entity linking. Our experiments on four low resource languages show an average improvement of 4 points in F1 score. 1","challenge":null,"direction":null,"task_annotation":["named entity recognition"],"method_annotation":["named entity recognition model","gazetteers","soft gazetteers"],"org_annotation":["Bloomberg Data\nScience","DARPA Information Innovation Office Low Resource Languages for Emergent Incidents program"]},{"ID":"mcinnes-2008-unsupervised","Goal":"Good Health and Well-Being","proba_max":0.999,"methods":["senseclusters"],"center_method":["senseclusters"],"tasks":["biomedical term disambiguation","all word disambiguation"],"center_task":["biomedical term disambiguation","all word disambiguation"],"title_clean":"An Unsupervised Vector Approach to Biomedical Term Disambiguation: Integrating UMLS and Medline","abstract_clean":"This paper introduces an unsupervised vector approach to disambiguate words in biomedical text that can be applied to all word disambiguation. We explore using contextual information from the Unified Medical Language System (UMLS) to describe the possible senses of a word. We experiment with automatically creating individualized stoplists to help reduce the noise in our dataset. We compare our results to SenseClusters and Humphrey et al. (2006) using the NLM WSD dataset and with SenseClusters using conflated data from the 2005 Medline Baseline.","challenge":null,"direction":null,"task_annotation":["biomedical term disambiguation"],"method_annotation":["unsupervised vector approach","feature vectors","nlm wsd"],"org_annotation":["no organization"]},{"ID":"u-etal-2008-statistical","Goal":"Reduced Inequalities","proba_max":0.6984615922,"methods":["relevance feedback"],"center_method":["relevance feedback"],"tasks":["personalized search"],"center_task":["personalized search"],"title_clean":"Statistical Machine Translation Models for Personalized Search","abstract_clean":"Web search personalization has been well studied in the recent few years. Relevance feedback has been used in various ways to improve relevance of search results. In this paper, we propose a novel usage of relevance feedback to effectively model the process of query formulation and better characterize how a user relates his query to the document that he intends to retrieve using a noisy channel model. We model a user profile as the probabilities of translation of query to document in this noisy channel using the relevance feedback obtained from the user. The user profile thus learnt is applied in a re ranking phase to rescore the search results retrieved using an underlying search engine. We evaluate our approach by conducting experiments using relevance feedback data collected from users using a popular search engine. The results have shown improvement over baseline, proving that our approach can be applied to personalization of web search. The experiments have also resulted in some valuable observations that learning these user profiles using snippets surrounding the results for a query gives better performance than learning from entire document collection.","challenge":null,"direction":null,"task_annotation":["personalized search web"],"method_annotation":["statistical machine translation models","relevance feedback"],"org_annotation":["no organization"]},{"ID":"pergola-etal-2021-boosting","Goal":"Good Health and Well-Being","proba_max":0.999,"methods":["entity aware masking strategies","biomedical entity aware masking"],"center_method":["entity aware masking strategies","biomedical entity aware masking"],"tasks":["boosting low resource biomedical qa","biomedical question answering"],"center_task":["boosting low resource biomedical qa","question answering"],"title_clean":"Boosting Low Resource Biomedical QA via Entity Aware Masking Strategies","abstract_clean":"Biomedical question answering (QA) has gained increased attention for its capability to provide users with high quality information from a vast scientific literature. Although an increasing number of biomedical QA datasets has been recently made available, those resources are still rather limited and expensive to produce. Transfer learning via pre trained language models (LMs) has been shown as a promising approach to leverage existing general purpose knowledge. However, finetuning these large models can be costly and time consuming, often yielding limited benefits when adapting to specific themes of specialised domains, such as the COVID 19 literature. To bootstrap further their domain adaptation, we propose a simple yet unexplored approach, which we call biomedical entity aware masking (BEM). We encourage masked language models to learn entity centric knowledge based on the pivotal entities characterizing the domain at hand, and employ those entities to drive the LM fine tuning. The resulting strategy is a downstream process applicable to a wide variety of masked LMs, not requiring additional memory or components in the neural architectures. Experimental results show performance on par with state of the art models on several biomedical QA datasets.","challenge":null,"direction":null,"task_annotation":["question answering","domain adaptaion","transfer learning"],"method_annotation":["masked language models","language models","biomedical entity aware masking"],"org_annotation":["UK Research and Innovation","EPSRC"]},{"ID":"moore-etal-1997-commandtalk","Goal":"Reduced Inequalities","proba_max":0.3214614689,"methods":["commandtalk"],"center_method":["commandtalk"],"tasks":["battlefield simulations"],"center_task":["battlefield simulations"],"title_clean":"CommandTalk: A Spoken Language Interface for Battlefield Simulations","abstract_clean":"CommandTalk is a spoken language interface to battlefield simulations that allows the use of ordinary spoken English to create forces and control measures, assign missions to forces, modify missions during execution, and control simulation system functions. CommandTalk combines a number of separate components integrated through the use of the Open Agent Architecture, including the Nuance speech recognition system, the Gemini naturallanguage parsing and interpretation system, a contextual interpretation modhle, a \"push to talk\" agent, the ModSAF battlefield simulator, and \"Start It\" (a graphical processing spawning agent). Com mandTalk is installed at a number of Government and contractor sites, including NRaD and the Marine Corps Air Ground Combat Center. It is currently being extended to provide exercise time control of all simulated U.S. forces in DARPA's STOW 97 demonstration. Put Checkpoint 1 at 937 965. Create a point called Checkpoint 2 at 930 960. Objective Alpha is 92 96. Charlie 4 5, at my command, advance in a column to Checkpoint 1. Next, proceed to Checkpoint 2. Then assault Objective Alpha. Charlie 4 5, move out. With the simulation under way, the user can exercise direct control over the simulated forces by giving commands such as the following for immediate execution: Charlie 4 5, speed up. Change formation to echelon right. Get in a line. Withdraw to Checkpoint 2. Examples of voice commands for controlling Mod SAF system functions include the following: Show contour lines. Center on M1 platoon.","challenge":null,"direction":null,"task_annotation":["spoken language interface"],"method_annotation":["nuance speech recognition system","gemini natural language parsing","contextual interpretation module"],"org_annotation":["Defense Advanced Research Projects Agency","Naval Command","Control","and\nOcean Surveillance Center"]},{"ID":"oravecz-etal-2021-etranslations","Goal":"Reduced Inequalities","proba_max":0.4350775778,"methods":["etranslation team"],"center_method":["etranslation team"],"tasks":["wmt 2021 news translation task","nmt","translation"],"center_task":["wmt 2021 news translation task","machine translation","machine translation"],"title_clean":"eTranslation's Submissions to the WMT 2021 News Translation Task","abstract_clean":"The paper describes the 3 NMT models submitted by the eTranslation team to the WMT 2021 news translation shared task. We developed systems in language pairs that are actively used in the European Commission's eTranslation service. In the WMT news task, recent years have seen a steady increase in the need for computational resources to train deep and complex architectures to produce competitive systems. We took a different approach and explored alternative strategies focusing on data selection and filtering to improve the performance of baseline systems. In the domain constrained task for the French German language pair our approach resulted in the best system by a significant margin in BLEU. For the other two systems (English German and English Czech 1) we tried to build competitive models using standard best practices.","challenge":null,"direction":null,"task_annotation":["news translation"],"method_annotation":["nmt model","data selection"],"org_annotation":["European Commission"]},{"ID":"akasaki-kaji-2019-conversation","Goal":"Decent Work and Economic Growth","proba_max":0.4651255012,"methods":["information retrieval"],"center_method":["information retrieval"],"tasks":["conversation initiation"],"center_task":["conversation initiation"],"title_clean":"Conversation Initiation by Diverse News Contents Introduction","abstract_clean":"In our everyday chitchat , there is a conversation initiator, who proactively casts an initial utterance to start chatting. However, most existing conversation systems cannot play this role. Previous studies on conversation systems assume that the user always initiates conversation, and have placed emphasis on how to respond to the given user's utterance. As a result, existing conversation systems become passive. Namely they continue waiting until being spoken to by the users. In this paper, we consider the system as a conversation initiator and propose a novel task of generating the initial utterance in open domain non task oriented conversation. Here, in order not to make users bored, it is necessary to generate diverse utterances to initiate conversation without relying on boilerplate utterances like greetings. To this end, we propose to generate initial utterance by summarizing and chatting about news articles, which provide fresh and various contents everyday. To address the lack of the training data for this task, we constructed a novel largescale dataset through crowd sourcing. We also analyzed the dataset in detail to examine how humans initiate conversations (the dataset will be released to facilitate future research activities). We present several approaches to conversation initiation including information retrieval based and generation based models. Experimental results showed that the proposed models trained on our dataset performed reasonably well and outperformed baselines that utilize automatically collected training data in both automatic and manual evaluation. * This work was done during research internship at Yahoo Japan Corporation. 1 \"Conversation\" in this paper refers to open domain nontask oriented conversations and chitchat .","challenge":null,"direction":null,"task_annotation":["conversation initiation"],"method_annotation":["encoder decoder model","mmi antilm","information retrieval"],"org_annotation":["no organization"]},{"ID":"lee-etal-2021-unifying","Goal":"Decent Work and Economic Growth","proba_max":0.6194445491,"methods":["unifiedm2","unifiedm2's learned representation"],"center_method":["unifiedm2","unifiedm2's learned representation"],"tasks":["unifying misinformation detection"],"center_task":["unifying misinformation detection"],"title_clean":"On Unifying Misinformation Detection","abstract_clean":"In this paper, we introduce UNIFIEDM2, a general purpose misinformation model that jointly models multiple domains of misinformation with a single, unified setup. The model is trained to handle four tasks: detecting news bias, clickbait, fake news and verifying rumors. By grouping these tasks together, UNIFIEDM2 learns a richer representation of misinformation, which leads to stateof the art or comparable performance across all tasks. Furthermore, we demonstrate that UNIFIEDM2's learned representation is helpful for few shot learning of unseen misinformation tasks\/datasets and model's generalizability to unseen events. * Work partially done while interning at Facebook AI. \u2020 Work partially done while working at Facebook AI.","challenge":null,"direction":null,"task_annotation":["misinformation detection","detecting news bias","verifying rumors"],"method_annotation":["few shot learning","unifiedm2"],"org_annotation":["no organization"]},{"ID":"rehm-etal-2019-developing","Goal":"Partnership for the Goals","proba_max":0.2718031406,"methods":["content and document curation workflow manager"],"center_method":["content and document curation workflow manager"],"tasks":["document curation services"],"center_task":["document curation services"],"title_clean":"Developing and Orchestrating a Portfolio of Natural Legal Language Processing and Document Curation Services","abstract_clean":"We present a portfolio of natural legal language processing and document curation services currently under development in a collaborative European project. First, we give an overview of the project and the different use cases, while, in the main part of the article, we focus upon the 13 different processing services that are being deployed in different prototype applications using a flexible and scalable microservices architecture. Their orchestration is operationalised using a content and document curation workflow manager.","challenge":null,"direction":null,"task_annotation":["natural legal language processing and document curation"],"method_annotation":["microservices architecture","content and document curation workflow manager"],"org_annotation":["European Union\u2019s Horizon 2020 research and innovation programme Project LYNX"]},{"ID":"sotnikova-etal-2021-analyzing","Goal":"Gender Equality","proba_max":0.999,"methods":["inference)"],"center_method":["inference)"],"tasks":["language inference)"],"center_task":["language inference)"],"title_clean":"Analyzing Stereotypes in Generative Text Inference Tasks","abstract_clean":"Stereotypes are inferences drawn about people based on their demographic attributes, which may result in harms to users when a system is deployed. In generative language inference tasks, given a premise, a model produces plausible hypotheses that follow either logically (natural language inference) or commonsensically (commonsense inference). Such tasks are therefore a fruitful setting in which to explore the degree to which NLP systems encode stereotypes. In our work, we study how stereotypes manifest when the potential targets of stereotypes are situated in real life, neutral contexts. We collect human judgments on the presence of stereotypes in generated inferences, and compare how perceptions of stereotypes vary due to annotator positionality. Domain Target Categories Gender man, woman, non binary person, trans man, trans woman, cis man, cis woman","challenge":null,"direction":null,"task_annotation":["analyzing stereotypes"],"method_annotation":["annotation","human judgement"],"org_annotation":["University of Maryland CLIP lab"]},{"ID":"oh-2021-team","Goal":"Reduced Inequalities","proba_max":0.6351520419,"methods":["pretrained neural language model"],"center_method":["pretrained neural language model"],"tasks":["eye tracking data prediction"],"center_task":["eye tracking data prediction"],"title_clean":"Team Ohio State at CMCL 2021 Shared Task: Fine Tuned RoBERTa for Eye Tracking Data Prediction","abstract_clean":"This paper describes Team Ohio State's approach to the CMCL 2021 Shared Task, the goal of which is to predict five eye tracking features from naturalistic self paced reading corpora. For this task, we fine tune a pretrained neural language model (RoBERTa; Liu et al., 2019) to predict each feature based on the contextualized representations. Moreover, motivated by previous eye tracking studies, we include word length in characters and proportion of sentence processed as two additional input features. Our best model strongly outperforms the baseline and is also competitive with other systems submitted to the shared task. An ablation study shows that the word length feature contributes to making more accurate predictions, indicating the usefulness of features that are specific to the eye tracking paradigm.","challenge":null,"direction":null,"task_annotation":["eye tracking data prediction"],"method_annotation":["language model","roberta","contextualized word representations"],"org_annotation":["no organization"]},{"ID":"seo-etal-2022-debiasing","Goal":"Reduced Inequalities","proba_max":0.7428785563,"methods":["aggregation"],"center_method":["aggregation"],"tasks":["object based event understanding"],"center_task":["object based event understanding"],"title_clean":"Debiasing Event Understanding for Visual Commonsense Tasks","abstract_clean":"We study event understanding as a critical step towards visual commonsense tasks. Meanwhile, we argue that current object based event understanding is purely likelihood based, leading to incorrect event prediction, due to biased correlation between events and objects. We propose to mitigate such biases with do calculus, proposed in causality research, but overcoming its limited robustness, by an optimized aggregation with association based prediction. We show the effectiveness of our approach, intrinsically by comparing our generated events with ground truth event annotation, and extrinsically by downstream commonsense tasks. * Equal contribution \u2020 Corresponding author (GOLD) \"3 kneels next to the bed 1 is lying in\" \/ (confounder) chair (B GEN) \"sit up from the couch\" (D GEN) \"lying in the bed\"","challenge":null,"direction":null,"task_annotation":["debiasing event understanding"],"method_annotation":["do calculus","optimized aggregation","association based prediction"],"org_annotation":["Microsoft Research Asia","SNU-NAVER Hyperscale AI Center","Korea government MSIT"]},{"ID":"soh-etal-2019-legal","Goal":"Reduced Inequalities","proba_max":0.0614264049,"methods":["text classifiers"],"center_method":["classification"],"tasks":["legal area classification","classifying judgments into legal areas"],"center_task":["legal area classification","classifying judgments into legal areas"],"title_clean":"Legal Area Classification: A Comparative Study of Text Classifiers on Singapore Supreme Court Judgments","abstract_clean":"This paper conducts a comparative study on the performance of various machine learning (\"ML\") approaches for classifying judgments into legal areas. Using a novel dataset of 6,227 Singapore Supreme Court judgments, we investigate how state of the art NLP methods compare against traditional statistical models when applied to a legal corpus that comprised few but lengthy documents. All approaches tested, including topic model, word embedding, and language model based classifiers, performed well with as little as a few hundred judgments. However, more work needs to be done to optimize state of the art methods for the legal domain.","challenge":null,"direction":null,"task_annotation":["legal area classification"],"method_annotation":["topic model","word embedding","language model based classifiers"],"org_annotation":["Singapore Academy of Law","Singapore Supreme Court"]},{"ID":"flekova-etal-2016-exploring","Goal":"Reduced Inequalities","proba_max":0.7072613835,"methods":["nlp tools"],"center_method":["nlp applications"],"tasks":["regression task"],"center_task":["regression task"],"title_clean":"Exploring Stylistic Variation with Age and Income on Twitter","abstract_clean":"Writing style allows NLP tools to adjust to the traits of an author. In this paper, we explore the relation between stylistic and syntactic features and authors' age and income. We confirm our hypothesis that for numerous feature types writing style is predictive of income even beyond age. We analyze the predictive power of writing style features in a regression task on two data sets of around 5,000 Twitter users each. Additionally, we use our validated features to study daily variations in writing style of users from distinct income groups. Temporal stylistic patterns not only provide novel psychological insight into user behavior, but are useful for future research and applications in social media.","challenge":null,"direction":null,"task_annotation":["exploring stylistic variation"],"method_annotation":["linear regression","non linear regression","rsvm","reduced support vector machines","writing style features"],"org_annotation":["Templeton Religion Trust"]},{"ID":"guo-etal-2021-pre","Goal":"Good Health and Well-Being","proba_max":0.999,"methods":["transformer based models"],"center_method":["transformers"],"tasks":["classification tasks","span detection task","classification and span detection"],"center_task":["classification","span detection task","classification and span detection"],"title_clean":"Pre trained Transformer based Classification and Span Detection Models for Social Media Health Applications","abstract_clean":"This paper describes our approach for six classification tasks (Tasks 1a, 3a, 3b, 4 and 5) and one span detection task (Task 1b) from the Social Media Mining for Health (SMM4H) 2021 shared tasks. We developed two separate systems for classification and span detection, both based on pre trained Transformer based models. In addition, we applied oversampling and classifier ensembling in the classification tasks. The results of our submissions are over the median scores in all tasks except for Task 1a. Furthermore, our model achieved first place in Task 4 and obtained a 7% higher F 1 score than the median in Task 1b.","challenge":null,"direction":null,"task_annotation":["span detection","classification"],"method_annotation":["pre trained transformer","span detection models","classifier ensembling"],"org_annotation":["no organization"]},{"ID":"tang-shen-2020-categorizing","Goal":"Reduced Inequalities","proba_max":0.3671503663,"methods":["capsule system","hierarchical attention capsule network"],"center_method":["capsule system","hierarchical attention capsule network"],"tasks":["categorizing offensive language in social networks","automatically identifying offensive language","offensive classification"],"center_task":["categorizing offensive language in social networks","automatically identifying offensive language","offensive classification"],"title_clean":"Categorizing Offensive Language in Social Networks: A Chinese Corpus, Systems and an Explainable Tool","abstract_clean":"Recently, more and more data have been generated in the online world, filled with offensive language such as threats, swear words or straightforward insults. It is disgraceful for a progressive society, and then the question arises on how language resources and technologies can cope with this challenge. However, previous work only analyzes the problem as a whole but fails to detect particular types of offensive content in a more fine grained way, mainly because of the lack of annotated data. In this work, we present a densely annotated data set COLA (Categorizing Offensive LAnguage), consists of fine grained insulting language, antisocial language and illegal language. We study different strategies for automatically identifying offensive language on COLA data. Further, we design a capsule system with hierarchical attention to aggregate and fully utilize information, which obtains a state of the art result. Results from experiments prove that our hierarchical attention capsule network (HACN) performs significantly better than existing methods in offensive classification with the precision of 94.37% and recall of 95.28%. We also explain what our model has learned with an explanation tool called Integrated Gradients. Meanwhile, our system's processing speed can handle each sentence in 10msec, suggesting the potential for efficient deployment in real situations.","challenge":null,"direction":null,"task_annotation":["categorizing offensive language"],"method_annotation":["hierarchical attention capsule network","categorizing offensive language datset","integrated gradients"],"org_annotation":["National Language Commission Key Research Project","National Natural Science Foundation of China","National Science Foundation of China"]},{"ID":"rinaldi-etal-2008-dependency","Goal":"Good Health and Well-Being","proba_max":0.999,"methods":["dependency parser"],"center_method":["dependency parsing"],"tasks":["dependency based relation mining"],"center_task":["dependency based relation mining"],"title_clean":"Dependency Based Relation Mining for Biomedical Literature","abstract_clean":"We describe techniques for the automatic detection of relationships among domain entities (e.g. genes, proteins, diseases) mentioned in the biomedical literature. Our approach is based on the adaptive selection of candidate interactions sentences, which are then parsed using our own dependency parser. Specific syntax based filters are used to limit the number of possible candidate interacting pairs. The approach has been implemented as a demonstrator over a corpus of 2000 richly annotated MedLine abstracts, and later tested by participation to a text mining competition. In both cases, the results obtained have proved the adequacy of the proposed approach to the task of interaction detection.","challenge":null,"direction":null,"task_annotation":["dependency based relation mining","text mining"],"method_annotation":["dependency parser","syntax based filters","text mining"],"org_annotation":["Swiss National Science Foundation"]},{"ID":"somers-etal-1997-multilingual","Goal":"Decent Work and Economic Growth","proba_max":0.5533869863,"methods":["multilingual internet based employment advertisement system"],"center_method":["multilingual internet based employment advertisement system"],"tasks":["multilingual generation and summarization of job adverts"],"center_task":["multilingual generation and summarization of job adverts"],"title_clean":"Multilingual Generation and Summarization of Job Adverts: the TREE Project","abstract_clean":"A multilingual Internet based employment advertisement system is described. Job ads are submitted as e mail texts, analysed by an example based pattern matcher and stored in language independent schemas in an object oriented database. Users can search the database in their own language and get customized summaries of the job ads. The query engine uses symbolic case based reasoning techniques, while the generation module integrates canned text, templates, and grammar rules to produce texts and hypertexts in a simple way.","challenge":null,"direction":null,"task_annotation":["multilingual generation","summarization"],"method_annotation":["pattern matching","language independent schemas","query system","object oriented database","symbolic case based reasoning"],"org_annotation":["no organization"]},{"ID":"huang-bai-2021-hub","Goal":"Reduced Inequalities","proba_max":0.2367368639,"methods":["hub@dravidianlangtech eacl2021"],"center_method":["hub@dravidianlangtech eacl2021"],"tasks":["classify offensive text","offensive language identification","offensive speech detection","comment\/post level classification tasks","five category classification task"],"center_task":["classify offensive text","hate speech","hate speech","comment\/post level classification tasks","five category classification task"],"title_clean":"HUB@DravidianLangTech EACL2021: Identify and Classify Offensive Text in Multilingual Code Mixing in Social Media","abstract_clean":"This paper introduces the system description of the HUB team participating in Dravidian LangTech EACL2021: Offensive Language Identification in Dravidian Languages. The theme of this shared task is the detection of offensive content in social media. Among the known tasks related to offensive speech detection, this is the first task to detect offensive comments posted in social media comments in the Dravidian language. The task organizer team provided us with the code mixing task data set mainly composed of three different languages: Malayalam, Kannada, and Tamil. The tasks on the code mixed data in these three different languages can be seen as three different comment\/post level classification tasks. The task on the Malayalam data set is a five category classification task, and the Kannada and Tamil language data sets are two six category classification tasks. Based on our analysis of the task description and task data set, we chose to use the multilingual BERT model to complete this task. In this paper, we will discuss our fine tuning methods, models, experiments, and results.","challenge":null,"direction":null,"task_annotation":["offensive text classification"],"method_annotation":["multilingual bert"],"org_annotation":["no organization"]},{"ID":"berzak-etal-2015-contrastive","Goal":"Reduced Inequalities","proba_max":0.4435217977,"methods":["contrastive analysis"],"center_method":["contrastive analysis"],"tasks":["esl"],"center_task":["esl"],"title_clean":"Contrastive Analysis with Predictive Power: Typology Driven Estimation of Grammatical Error Distributions in ESL","abstract_clean":"This work examines the impact of crosslinguistic transfer on grammatical errors in English as Second Language (ESL) texts. Using a computational framework that formalizes the theory of Contrastive Analysis (CA), we demonstrate that language specific error distributions in ESL writing can be predicted from the typological properties of the native language and their relation to the typology of English. Our typology driven model enables to obtain accurate estimates of such distributions without access to any ESL data for the target languages. Furthermore, we present a strategy for adjusting our method to low resource languages that lack typological documentation using a bootstrapping approach which approximates native language typology from ESL texts. Finally, we show that our framework is instrumental for linguistic inquiry seeking to identify first language factors that contribute to a wide range of difficulties in second language acquisition.","challenge":null,"direction":null,"task_annotation":["contrastive analysis","typology driven estimation"],"method_annotation":["regression model","typology model","bootstrapping"],"org_annotation":["Center for Brains","Minds","and Machines"]}]